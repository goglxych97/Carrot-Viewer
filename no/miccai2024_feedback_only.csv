Paper URL,Author Feedback
https://papers.miccai.org/miccai-2024/001-Paper1861.html,"We thank the reviewers for their valuable comments, time and effort. We thank them for recognising the convincing results at a large scale and the novel use of the data.
R1-W1: âClinical significance - Does detailed 3D information really add value to the management/treatment of scoliosis patients?â  The analysis of spine in 3D is relatively new but the introduction of EOS scanners has shown that 3D is valuable for clinical management of patients. Crucially only in 3D, the effect of brace surgical treatment can be better evaluated [1] which impacts a patientâs overall response to treatment.
R1-W2a: âMore experiments in order to prove that the proposed framework was indeed the optimal designâ. We explored several architectures and found that a lightweight transformer with ResNet backbone gave the best pixel-wise regression performance (Table 1).  We will include a discussion of the models investigated in the final version. In line with our work, [2] showed that a lightweight transformer model using significantly less parameters also surpasses CNN-based and Vision Transformer (ViT) models on the Synapse dataset, SegPC, and ISIC 2017 dataset.
R1-W2b: âWhy 3 curves used instead of just one?â. We employ this design choice to predict spine masks. This is important as scoliosis is known to involve deformities due to vertebral axial rotation [3]. This will enable future work into understanding the effect of compression in scoliosis and potentially lead to tailored physiotherapeutic treatment.

R1-W3: âAre coronal and sagittal regressions learned together or separately?â  There is a single model, with two separate heads: a coronal and a sagittal head that produce 3 curves each. We agree that showing regression curves results in Table 1 separately for the coronal and sagittal plane could give a misleading impression. We will edit the paper to rectify this.

R3-W1: âIncremental nature of the workâ. As far as we know, we are the first to regress 3D patient-specific spine shapes from 2D AP DXA. The main technical advance lies in the 2 separate orthogonal planes (sagittal and coronal) curve regression of the spine from a 2D image. We then show through these curves, we can recover 3D geometry of the spine which is beneficial for scoliosis.
R3-W2: âRobustness of the method when used with external data is not clarifiedâ. Unfortunately, we do not have access to an external paired dataset of whole-body MRI and DXA scans. From experience, little to no adaptation would be needed in terms of data pre-processing. We will make our code publicly available with documentation to test on external data.
R3-W3: âThe paper looks somewhat incomplete, since the accuracy of the predicted curves is not analyzed in 3D (except for the plot in Supplementary).â This would be a good addition. We have labels for 3D spine masks on the MRI, and will add the 3D evaluation to the final version of the paper.
R4:âLiterature comparisonâ. We will cite the two papers. Note, however, that we use direct regression while the references fit 3D shape models.
R1&R4 :âDual Alignmentâ. This two-stage alignment procedure is crucial, as a significant proportion of the MRI and DXA do not have spines aligned after the first global alignment stage. Note, alignments often proceed iteratively, even under the  same transformation. The rough stage is the global alignment of MRI to DXA, and the second finer stage is the MRI spine to DXA spine alignment using spine-labels, this is explained in the last 2 paragraphs of section 2.2.

We will address all the minor points listed in the reviews in the final version.

[1]. Courvoisier et al. EOS 3D Imaging: assessing the impact of brace treatment in adolescent idiopathic scoliosis. Expert Review of Medical Devices, 11(1), 2014. 
[2] Heidari et al. Hiformer: Hierarchical multi-scale representations using transformers for medical image segmentation. WACV, 2023.
[3] IllÃ©s et al. The third dimension of scoliosis: The forgotten axial plane.OTSR, 2019."
https://papers.miccai.org/miccai-2024/002-Paper1908.html,"Dear Reviewers and Area Chairs,

We appreciate the reviewersâ valuable comments and are encouraged by their acknowledgment of the novelty of our two-stage diffusion model (R1, R3, R4), which effectively handles cycles addressing the limitation of previous tree-based methods (R1). We are humbled by their remark on a well-motivated work with well-thought-out design choices (R4), well-designed experiments (R3, R4) with clear results (R3, R4), and good performance on two real-world datasets (R4). Below, we address the raised concerns.

Motivation & Downstream Tasks (R1, R4): Synthetic vessel graph generation is an important topic [20, 22] and can augment real datasets, where annotating large-scale samples is expensive (CoW) and tedious (VesSAP) for downstream tasks, such as blood flow simulation [21], vessel labeling, and synthetic image-label pairs for segmentation [10, 14]. Previous studies have demonstrated improved downstream performance [10] when generated graphs capture real data distribution. Our analysis (Tab 1, Fig. 4) shows that our model captures the underlying distribution well in terms of various graph and topology properties, ensuring the usefulness of the generated samples. We have now included this point in our manuscript.

SOTA Comparison (R1, R3): Kindly note that it is challenging for us to identify a SOTA baseline since no concrete references to prior work were pointed out beyond our SOTA baselines [24, 25]. Below, we answer, to the best of our ability, why we believe our comparison is fair. We aim to develop a generative model for complex vessel graphs, including cycles. Notably, previous SOTA vessel generation models [20, 22] can only generate tree structures and NOT anatomically meaningful cycles. Hence, these models are fundamentally NOT APPLICABLE as a baseline to the datasets considered in our paper. Nevertheless, CoW resembles a benzene molecule-like ring, and we could interpret VesSAP samples as a collection of molecules. Hence, we compare against SOTA molecule generation methods [24, 25], which can handle cycles. @R3, please note that both baselines [24, 25] and our model consist of permutation equivariant layers, making it a fair comparison.

MICCAI Suitability (R3): Although some large arteries and veins resemble tree-like structures, the human circulatory system also contains cycles at the capillary level and in anastomotic structures (CoW). Hence, we respectfully disagree with R3 on explicitly enforcing tree-like characteristics as it will hamper cycle generationâa limitation of previous works [20, 22]. The complex structures (trees and cycles) make it difficult to determine the complete topological characteristics of a vascular graph accurately, and incorporating inadequate inductive bias would limit the modelâs capabilities. Instead, we opt for a data-driven approach without hand-crafted heuristics, and we make the following design choices to facilitate data-driven learning.

Although our method can not generate vessels if the training data is not vascular (a limitation of any data-driven approach), it can model complex topological structures, including arteries, veins, and capillaries, as shown in its performance on two real vessel datasets (Tab. 1), which is of interest to MICCAI community.

Metrics (R4): We acknowledge that the current literature lacks a ready-to-use metric to evaluate the anatomical plausibility of the vessel graphs. Hence, we opt for well-established topology and graph-structure metrics from vessels [4, 29] and molecules [25] for our evaluation.

Misc: @R3, we have included significant digits in Tab. 1. @R4, our models were trained from scratch on specific datasets. We have added baseline details and clarified other textual concerns."
https://papers.miccai.org/miccai-2024/003-Paper1001.html,"We thank the reviewers for their highly constructive and positive feedback. We focused on exploring the proposed dual-face depth and the CASI loss (which are this paperâs main contributions/novelty) together with the medical application. We understand the paper lacked details on 3D reconstruction, shape evaluation, and the SSM model.

Details about the 3D reconstruction (R1, R3):
When converting an estimated depth map to a point cloud, we created the rays based on the detector size and the source-to-detector distance (SDD) recorded in the DICOM header, assuming a pinhole camera with a regular viewing frustum without the skew. The SDD is a major factor in determining geometry. For hip X-ray imaging, the SDD of 120 cm is defined as the international standard in [10]. There can be flexibility in this imaging geometry (i.e., the position of the X-ray source relative to the detector)as the operators manually set the X-ray source. Thus, accurate geometry is not measurable unless time-consuming geometric calibration is used, which is not actually performed in clinical routines. Our 600-patient (2565 X-ray image) dataset was acquired in this fashion over more than five years in clinical routine by many and varied operators. Despite this, we demonstrate high 3D reconstruction accuracy in our cross-validation experiment, demonstrating high robustness against the small geometry variation due to the human operator in training and testing datasets. Since the term âfixed-geometryâ is confusing, we will change it to âstandard-geometryâ throughout the paper.

Our Figures had two major problems that confused the readers (R1, R3). First, the color map of the back-face depth map was inverted. We will correct this. Second, the G_c in Figure 1 was depicted as a neural network to show the implementation-agnostic nature, but it was unclear since our actual implementation used the SSM. Although some advanced neural networks can be applied in theory, we chose the fundamental/simple SSM-based 3D completion so that the reader can focus on our main points: the dual-face depth and CASI loss. We will correct the image for G_c in the revised paper.

Q1 (R1): the quality of the 3D surface reconstruction (boundary) could be affected by the segmentation result.
A1: We agree on this. Although our segmentation model showed high accuracy (0.988 dice indicated in Section 3), the segmentation performance degraded for the diseased patients. The revised paper will discuss the effect of segmentation performance.

Q2 (R1): It would be interesting to talk/report some examples of cases with larger errors.
A2: We will report those cases in the supplemental materials in the revised paper.

Q3 (R3): The details for the creation of the SSM are needed.
A3: We applied a four-fold cross-validation (mentioned in Section 3), which means four SSM models. We used the same cases (common training dataset) for training the depth map estimations and SSMs; thus, there was no contamination.

Q4 (R3): How is the 3D reconstruction registered to the CT ground truth to obtain error metrics?
A4: This is an important point. We aligned the depth center of the prediction (with or without SSM-based shape completion) to the ground truth shape, i.e., adding a shift in the Z direction before calculating the error in order to evaluate the reconstructed shape while ignoring the absolute distance from the X-ray source which is not possible to measure due to the manual variability mentioned above.

Q5 (R3): Reporting the mean and median error rather than the mean and standard deviation is highly irregular.
A5: The revised paper will report the mean and standard deviation.

Q6 (R4): I would be very curious how the modelâs reconstructions compare to the closest pelvis in the training set.
A6: In the revised paper, we will report the results using the closest shape in the training dataset.

Other comments:
The revised paper will improve the organization of the sections (R1) and the formulas (R3, R4)."
https://papers.miccai.org/miccai-2024/004-Paper0132.html,"We sincerely thank AC and all reviewers for their time and efforts to give useful comments. We below clarify the main concerns (C) first, and then reply to other questions (Q). We will revise the paper carefully to address all comments.

C: (@R1\&R4) Lacking comparison with other methods.

A: In fact, we are eager to compare with existing works, but they do not release the source codes. Moreover, we observe that they only compared between their proposed different versions [3, 11, 13, 15]. Besides, due to the properties of tumors, AE values vary greatly in different cases. Since they do not provide the specific testing CASE IDs, we could only compare our plans A-C.

Particularly, compared coarsely with [13] on LiTS, where their average AE is 60.02, our AE result (62.53) of Plan C is better. Note again that the comparison is really meaningless as the case IDs do not match since their case IDs are unknown. 
Further, in terms of the methodology, we have provided an advantageous analysis of our method in the introduction, compared with recent methods.

Q1: (@R1) The planning efficiency problem.

A: The proposed Plans A-C are independent and do not require the sum of computation times.
The computation time depends on the case complexity. Particularly Plan C takes 21-43 minutes, which is comparable to recent optimization-based methods [11, 13].
Besides, the efficiency of our method has considerable room for improvement.

Q2: (@R1) Insufficient test data.

A: Our current version focuses on radical tumor ablation, which clinically requires the tumor to be smaller than 50mm and limits our choice of LiTS. Besides, we have referred to several papers [11, 13, 15], and they also conduct experiments on a limited number of 10-15 patients. Moreover, since our method is optimization-based and independent of tumor shape, the result is reliable. We will show the results of more cases along with our code.

Q3: (@R3) The potential impact and broader applicability.

A: Except for the liver tumor cryoablation, our method can be directly generalized to kidney and other tumor ablation. Further, our method is modality-agnostic and can be applied to other modalities, e.g. MRI.

Q4: (@R4) The vesselsâ location should be considered.

A: Our method has considered the conflict between ablation zones and vessels via segmentation. Specifically, we integrate vessel masks in L_overlay and L_probe, avoiding vessels existing in the ablation zones and probe trajectories.

Besides, our current version is based on TotalSegmentator [21], which can segment some liver vessels and can be improved via clinical doctors to include more accurate vessels that potentially affect the operation.

Q5: (@R5) Clarifying whether the single-case treatment was conducted by Plan C.

A: The treatment was conducted empirically by clinical doctors, not based on Plan C. Comparisons on the IH dataset are performed to validate our plans. Especially the metric IoU quantitatively indicates the overlaps between empirical zones and our optimized zones.

Q6: (@R5) The concept of âprobe interactivityâ needs clarification.

A: Thanks for clarifying the definition of probe interactivity, which is exactly what we want to express.

In our method, we intend to model the probe/ablation zones with parameterized Gaussians/ellipsoids. In this manner, the interaction between ellipsoids represents the corresponding probe interactivity, which can be optimized automatically with our method.

Q7: (@R5) The reproducibility of the method.

A: We have tried our best to give the implementation details for reproducing. To understand Eq. (2) and other equations clearly, we have clarified the meaning of each variable following each equation and provided the values of thresholds we used in Section 3.And, we will release our code for usage later.

Besides, we will accordingly modify our paper, e.g. giving the full name Khachiyanâs Algorithm (KA) on Page 2. (@R5)"
https://papers.miccai.org/miccai-2024/005-Paper1015.html,"We appreciate the reviewersâ comments and insightful suggestions, especially some thought-provoking proposals (R3,R4). We thank for the acknowledgement of the interest(R1), novelty(R3,R4), organization(R3,R4), and impressive experimental results(R4) of our paper. Our responses are as follows:

3DGS-FBP CLARITY
R1-Q2: we clarify that for 3DGR-FBP, the Gaussian Centers (GC) derived from FBP introduce increasing noisy initialization as the number of views decreases, and much of this noisy GC canât be eliminated during the optimization process, leading to a decline in 3DGR-FBP performance. In contrast, for 3DGR-GCP, the GCP moduleâs prediction of a rough coronary sparse point cloud significantly reduces the introduction of non-coronary geometric GC, enhancing reconstruction quality even with fewer views.
R4-Q1&Q3: We calculated the proportion of GC close to 3D artery centerlines in the ImageCAS test set. We found that the ratio of 3DGR-GCP based on a single view is 21.78%. For 3DGR-FBP with 2,4,8,16 views, the percentages are 17.73%, 21.52%, 28.18%, and 34.72%. This confirms the earlier explanation. As the number of views increases, the reconstruction quality of 3DGR-GCP ranges from significantly better than 3DGR-FBP to nearly identical.
R4-Q1& R3-Q3: We will add 3DGR-FBP to Fig. 3 and 4 and release the code, data and checkpoints.

EXPERIMENT
R1-Q5: Lacking of arteries data with precise 3D labels, we focus on methods that require no external annotated data for training. Traditional methods such as FBP cope poorly with sparse view problems. Because Implicit Neural Representations use a continuous 3D representation to model scenes and are trained through view-consistency, they achieve good 3D reconstruction even with limited views and serves as good baselines. Therefore, we conducted extensive experiments on the FBP and NeRP methods. Additionally, we tried NAF[1], which improved results over NeRP when views exceeded two, but still conform to our conclusions. For voxel DSC metrics, NAF: 0.29-0.66-0.79-0.89; Ours: 0.70-0.89-0.92-0.92. This reinforces our confidence that 3DGS delivers SOTA results in reconstruction from ultra-sparse views without requiring 3D labels.
[1] NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction, MICCAI 2022.
R3-Q4: We extended FBP with orthogonality constraints for views consistency, and the results still showed artifacts similar to FBP. As suggested by the idea in the work of Unberath et al., using epipolar consistency helps improve segmentation quality, which we believe opens the path for us to add regularizations between projections to the 3DGS and GCP module, enhancing reconstruction performance.

CLINIC
R3-Q1: Our study is essentially an exploration of the limits of reconstructing 3D arteries by a minimal number of X-ray views. In future clinical practice, autonomous scanning might reduce physician intervention, and our method holds great promise.

DETAILS
R1-Q1&Q3: We will revise some indirect expression such as Sec-2.1âs subheading to â3DGR Reconstructionâ, Sec-2.2âs to âGaussian Center Predictor Trainingâ and complex sentences.
R1-Q4: Considering that only GCP requires training, it is reasonable to use most of the ImageCAS dataset for training GCP and to select 20 random samples for validating the comparative methods. Since none of the comparison methods require external training data, and the results on the 40-sample ASOCA dataset are also similar, this corroborates that we did not selectively choose 20 samples for testing.
R3-Q2: For 2-views, the interval is Ï/2. 4-viewsâÏ/4, 8-viewsâÏ/8, and so on.
R4-Q2: The time cost is labeled as âTrainâ in Fig. 1. We will include âTime costâ details in caption.
R4-Q4: Quite promising. Artifacts and noise present in real contrasted X-ray (from other tissue), might impact the reconstruction results. Notably, 3DGS focuses on the instance itself, avoiding the issue of distribution shifts between real and synthetic data."
https://papers.miccai.org/miccai-2024/006-Paper2442.html,"Reviewer #1
Q6&Q10-2: Missing comparison with baseline hybrid models
A: We did compare our model with UNETR, a hybrid transformer-CNN architecture in the paper. Unfortunately, we accidentally omitted the UNETR reference, which has now been corrected. 
Q10-1: Extend method to other cross-dimension transfer work like X2CT-GAN [1]?
A: Yes, the proposed method can be extended to X2CT-GAN by substituting the decoder blocks with hybrid MLP-CNN blocks and adding in the intermediate guidance. In general, variants of the encoder-decoder structures would be compatible with our techniques. In future work, we will conduct experiments to demonstrate this capability. 
Reviewer #3
Q6-1: Clinical PX images usually have resolution over 900 Ã 1500. Synthesized PX images are of size 128 Ã 256. 
A: The straightforward solution to the resolution discrepancy is to down-sample the clinical or synthesized PX when the method is applied. It is a common strategy to improve calculation effectiveness across the X-ray analysis domain [1] (X2CT-GAN). In future work, we propose to fill the information gap between the synthesized PX and clinical PX by exploring domain adaption methods with realistic CBCT resampling methods to simulate the clinical PX resolution. 
Q6-2: understandability / explainability of features. Incremental output feature map to follow the downscaled Y.
A: The manipulation of intermediate features is the core concept of the proposed progressive guidance. In the general concept of most deep learning tasks, e.g., classification and regression, the networks are trained to focus on the high-level semantic features that are abstract because it should âunderstandâ the scene to make predictions. However, the 2D-to-3D task is with a straightforward target, consistent throughout the reconstruction process in the shallow or deep layers.
Q9: Progressive Guided Reconstruction (PGR) requires more detailed information about the scaled label Y. 
A: As suggested, we have clarified the details of the PGR as follows: the intermediate guidance denoted by red arrows in Fig 2 is scaled label Y. In every layer of the encoder and decoder, the label Y was reshaped into the size of the intermediate feature for a proper comparison.
Q10-1: Typographical errors.
A: We have corrected the errors and carefully proofed the paper.
Q10-2: SSIM loss can be used to learn structural information [2] (Zhao, Hang, et al). 
A: As suggested, we will consider SSIM as part of the loss function in our future experiment.
Q10-3: Embedding the diffusion process at the high-level feature (orange arrow in Fig 2), which can improve the volume stability and quality after reconstruction [3] (Croitoru, Florinel-Alin, et al). 
A: As suggested, we will investigate the diffusion process further in our 2D-to-3D reconstruction tasks in future work. Thanks!
Reviewer #4
Q6 & Q10-1 & Q10-3: The paper is difficult to follow regarding the missing references in Tables 1 and 2. 
A: The missing reference for UNETR and unclear information for MLP-Unet has now been clarified. MLP-Unet is a custom-built Unet with MLP blocks for comparison and, therefore, there is no reference for it. The methods without reference in Table 1 are also custom-built for comparison. The reference has been updated accordingly and the description for custom-built comparison methods has been clarified.
Q10-2: Disconnect between DSC for UNet-based models consistently increase alongside PSNR and SSIM. 
A: The discrepancy arises because PSNR and SSIM measure different aspects of model performance compared to DSC. While Residual-based models may produce images that are perceptually and pixel-wise closer to the ground truth (higher PSNR and SSIM), they might not be as effective at precise segmentation (lower DSC) compared to UNet-based models, which are specifically designed for segmentation tasks. We will add a statement to this effect in the revised paper."
https://papers.miccai.org/miccai-2024/007-Paper2090.html,"Thanks to reviewers for their time and insightful comments. They found our work is novel (R4), well-organized (R1,R4,R5) and effective (R1,R4,R5), but also pointed out some issues. We will clarify the main points:
1.Details of methods.
We will release the code after acceptance to provide more details.
Q1:Training and inference(R1). During training, we conducted iterative training. Specifically, when training t slice, we first selects the previous n (random from 1-3) slices for iterative inference to obtain the token_his of t slice. Since our model is a slice-based method and token_his is shared and fixed in length, our model can flexibly deal with 3D volumes with different number of slices. 
Q2:Default query in t=0(R1,R5). Since there is no previous slicing result to initialize the query at t=0, we additionally define a learnable default query to make the initial prediction. Default query is actually the same as object query in DETR, and both are the learnable embedding vectors and optimized during training. In addition, we also tried to use a centered box directly to initialize the query at t=0, but the result shows that the learnable default query works better.
Q3:bbox prompt in begging slice(R1,R5). Our entire segmentation framework is fully automated without any human interaction. As mentioned in Q2, we use default query to predict the bbox prompt in the beginning central slice(t=0).
Q4:Inter-slice attention(R1,R5). For inter-slice attention, the self-attention is performed on the features of slices at t-1,t,t+1. Leveraging the continuity of targets in 3D medical images, we compute the attention only between features at the same spatial position across slices, effectively reducing computational complexity, as illustrated by the yellow patches in Figure 1.
Q5:Semantic embedding and information filter(R1). The semantic embedding is intended to allow each query to perceive the category for which it is responsible. Specifically, we add a corresponding learnable semantic token for each query separately. These semantic tokens are randomly initialized and optimized during training. For the information filter, it consists of MLP and Normalization, aiming at remaining the global inherent information and filter out redundancy.
Q6:Global 3D-aware(R1). We found that longer token sequences did not significantly improve performance in experiment. Because of the continuity of 3D medical images, the global feature information of model learning is relatively stable. Therefore, too large a historical information token will lead to redundancy.
Q7:LoRA(R5). Thanks for your suggestion. Indeed, we should briefly introduce the LoRA technique to make readers understand.
2.Experiment.
Q1:3D model comparison(R1). In fact, with our proposed 3D-aware strategy, our method is still competitive compared with the advanced 3D model. We will compare these methods in further work.
Q2:Comparison methods details(R4). Specifically, we follow the basic configuration parameters and training pipeline provided by the comparison methods.
Q3:Ablation study on global 3D aware+prompt generation(R1). Due to space constraints, we did not show the complete results of the ablation study, and we will refine them in further work. 
Q4:Ablation study on slice attention(R5). For the ablation study on the inter and intra slice attention component, we use Local 3D-aware to represent this component in Table 4. Due to space constraints, we do not show the ablation results for each component, and we will refine them in further work.
Q5: Comparison in the test set(R4). Because the test sets of these datasets do not provide labels, it is difficult to compare them directly in the test sets.
Q6:runtime and memory(R4). It is unfair to directly compare the runtime and memory in this paper. Our methodâs advantage is fully automatic, while other SAM variants need manual prompts.
3.Writing.
Q1:Writing(R5).We will check and revise the full text for writing problems."
https://papers.miccai.org/miccai-2024/008-Paper3648.html,"We thank the reviewers for their insightful feedback. In our edit, we will add a link for the code repository and reduce the use of passive tense in the writing.

Reviewers mentioned that the dataset size is limited. Due to the cost of collecting 3T and 7T images on the same subject, small data size is a common limitation in any other study that tries to enhance clinical 3T T1-weighted MRIs to their 7T counterparts [2, 3, 21, 26]. To combat this, we harmonize a publicly available 3T-7T MRI dataset with healthy subjects to augment our pathology rich TBI dataset.

Reviewers mentioned that additional benchmarks are warranted and an evaluation from clinicians would be valuable. In this study, we only compare against the WATNet Benchmark because [21] documents several comparisons to past works doing the same task. This established WATNet as the state-of-the-art benchmark for our task. While we can provide another benchmark to augment our argument, the primary comparison would remain between our proposed method and the WATNet benchmark. Considering the space limitation, we believe an additional benchmark is auxiliary, rather than essential. Regarding clinical evaluations, we are pursuing this for a future study, but would not have the space to include it in this submission.

Reviewer 4 suggests we cite two additional references [Karayumak-2018, and Lin-2023] and states that our contributions are very similar to [Lin-2023]. We will cite [Karayumak-2018] in our background section. We would like to point out that [Karayumak-2018] worked with diffusion images whereas we work with T1-weighted images. Similarly, [Lin-2023]âs task is fundamentally different from ours since they enhanced low field (0.36T) to 1.5/3T MRI, and they did this via a low-field image simulator that down-sampled a high-field image. While [Lin-2023]âs training strategy tried to predict real high-field MRIs from synthetic low-field MRIs, our work predicts real ultrahigh-field MRI from real high-field MRIs. Our 3T to 7T synthesization task is clinically valuable because pathological features at 7T have structural implications (i.e. central vein sign in multiple sclerosis lesions) that 3T MRIs often miss. Therefore, it is valuable to probe the translation from pathologies in real 3T scans to corresponding real 7T scans. This also differs from [Lin-2023]âs approach, where the low-field input is synthetically generated and displayed pathological features very coarsely. Space permitting, we will discuss [Lin-2023] in our background section. However, we do not believe their contributions directly overlap with our work.

Reviewer 5 points out we do not justify modifying the transposed convolutions to nearest neighbor upsampling. We agree and will add an explanation in our edit. The reason we make this architectural change is because transposed convolutions often lead to âcheckerboardâ artifacts in the model output, as documented in (Odena, et al., 2016). This artifact can be reduced by using nearest neighbor upsampling instead of transposed convolutions. During preliminary testing, we observed that this was indeed the case, and we decided to use nearest neighbor upsampling in our model implementation.

Additionally, reviewer 5 mentions a concern for over-fitting. We are aware of this and utilize cross-validation to more comprehensively evaluate the modelâs ability to generalize. We discuss our cross-validation approach under the dataset section in Experimental Design. We have realized how this can be confusing and will move it to the evaluation section. We also indicate the cross-validation results in Fig. 2 and Fig.4, where the individual data points represent model performance in one cross-validation fold. We realize this is not explicitly stated and will make edits accordingly.

Reference:
Odena, et al., âDeconvolution and Checkerboard Artifactsâ, Distill, 2016. http://doi.org/10.23915/distill.00003"
https://papers.miccai.org/miccai-2024/009-Paper0219.html,"We thank all reviewers for their valuable feedback and constructive comments. We present our responses to reviewersâ concerns and comments below.
R1Q1: Technical novelty and the differences compared to the previous method.
A: Similar to the previous Bayesian method for semi-supervised segmentation, our work models p(x,y) using a general Bayesian formulation. However, we tailor our approach to address weakly-supervised learning with sparse annotations, distinguishing our focus and application. Besides, a significant technical distinction lies in how we model p(yâ£x,z). Specifically, we integrate a conditional random field (CRF) that characteristics a Gibbs distribution into our Bayesian framework, differing in the optimization target and loss function from the previous method. This novel Bayesian formulation has demonstrated superiority in handling weakly-supervised laparoscopic image segmentation compared to other non-fully Bayesian approaches, indicating its efficacy and innovation in this specific domain.
R1Q2: Comparison of testing time for various methods.
A: U-Net was applied as the backbone architecture for all methods for comparison in our study. Our proposed method incorporates Monte Carlo dropout (MCDP), necessitating T forward passes to model uncertainty estimation. Thus, with the increase of MCDP times T, our method would consume more time during testing than other methods that require a single forward pass. We acknowledge this limitation and recognize the need for future optimization efforts.
R1Q3: Inconsistency of symbols.
A: We apologize for any confusion caused by typos regarding the symbols used in figures and text. These inconsistencies will be corrected in the final version of the manuscript.
R3Q1: Clarification of label merging in Section 2.2.
A: We apologize for the initial lack of clarity. This process involves a binary mask operation. Specifically, we create binary masks (denoted by \Gamma) based on weak labels y^s, where the value â0â represents the labeled region and â1â represents the unlabeled region in y^s. The final labels y is then generated by y = (1 - \Gamma) \odot y^s + \Gamma \odot y (\odot indicates element-wise multiplication). This operation allows us to make use of these accurate sparse annotations from y^s to further improve the quality of generated pseudo-labels. More details will be provided in the final version of the manuscript, and our code will be available for further reference.
R4Q1: Simulation of sparse annotations.
A: Yes, we simulated sparse annotations due to the lack of real weak labels and recognize this as a limitation. Applying our method to more datasets with real weak labels is regarded as an aspect of our future work. Additionally, we encourage both ourselves and the community to contribute datasets featuring real weak labels to support subsequent research in this field.
R4Q2: Reproducibility.
A: We will release our codes and simulated labels to facilitate reproducibility and further research."
https://papers.miccai.org/miccai-2024/010-Paper2774.html,Thank you for approving our work. We are deeply grateful for your constructive feedback and will refine the article according to your insightful suggestions.
https://papers.miccai.org/miccai-2024/011-Paper2364.html,"We appreciate the reviewersâ efforts in evaluating our work and acknowledge the paperâs contributions, including its innovative method (R1) and promising results (R3). We address each reviewerâs questions as follows:
I.METHOD(R3)
(a)Unlike [5], we propose Multi-CL and design a hard sample mining method based on self-paced learning. This strategy dynamically selects hard samples based on the modelâs learning state, without requiring a pretrained teacher model, improving stability and convergence speed.(R3-3.Q2)
(b)Compared to general CL with random positives/negatives, CoMCL constructs positives and negatives based on quality and lesions, ensuring semantic consistency between positives and anchors, and differences between negatives and anchors. This reduces false negatives, better fits diagnostic needs, and provides clear interpretability. Clustering samples into four classes via CL is inspiring but overlooks the relative relationships and continuity of lesions under different qualities by focusing on absolute classes.(R3-3.Q3)
(c)Considering lesion regions are small relative to the entire image, CL based on image-level lesion labels struggles to identify lesion features. Hence, our method considers three level perspectives: 1).Quality: high-quality lesions (HL) vs. low-quality lesions (LL); 2).Lesion: HL vs. high-quality healthy (HH); 3).Lesion: LL vs. low-quality healthy (LH). Learning from both image quality and lesions enables the model to identify lesions in low-quality images. However, when contrasting low-quality lesions and high-quality healthy samples, since both quality and lesion are present, the model may not effectively distinguish quality and lesion features, affecting downstream task performance. It should be noted that Fig.1(b2) simplifies the complex to intuitively illustrate the impact of low-quality factors on feature learning, prioritizing problem visualization over reflecting all details.(R3-[7.Q2-7.Q4])
II.MISUNDERSTOOD(R1/R3/R4)
(a)We will provide more case analyses: 1).Analyzing CoMCLâs diagnostic performance on images of varying qualities. 2).Comparing CoMCL with other methods in identifying key features of more diseases. (R1)
(b)Many publicly datasets(e.g. EyeQ/DR2/HRF) already contain quality label. Besides, obtaining quality labels is inexpensive and can be easily labeled. (R3-3.Q1)
(c)Not all regions in low-quality images have poor quality[1], allowing patch construction for multi-level contrast. Thus, CoMCL does not degrade into basic CL.(R3-7.Q5)
[1]Hou Q. A Collaborative â¦[J].TMI, 2024.
(d)Kt adaptively varies with training steps (Sec.2.2) and is mainly determined by the number of negatives,Î´,as shown in the ablation study in the previous supplementary material.(R3-7.Q6)
(e)The baseline you mentioned is for quality assessment task, while our work focuses on DR grading. The former aims to evaluate image quality, while the latter is disease diagnosis. The research tasks are different. It is inappropriate to directly compare their performance. We kindly request you to re-examine the content and objectives of our work.(R4-Q2)
III.NOVELTY(R4)
(a)Our work differs in problem definition and solution: Motivations: We address low-quality factorsâ interference with disease feature extraction, while Che et al. utilize low-quality features for diagnostic stability. Methods: Our multi-level CL enhances lesion feature extraction through patch construction and hard sample mining, distinct from meta-knowledge joint embedding. (R4-Q1)
(b)CoMCL focuses on design philosophy,e.g. Multi-level CL in low-quality image, rather than a specific SSL method. We welcome integrating our multi-level idea into more SSL methods. In fact, incorporating multi-level CL can improve the performance of related methods (e.g., MoBY, DINOâ¦) on low-quality images. (R4-Q3)
IV.OTHERS(R1,R3)
(a)We have uploaded the code to the CMT system, code link will be included in final paper.(R1)
(b)We will resize figures in the final paper.(R3-7.Q1)"
https://papers.miccai.org/miccai-2024/012-Paper2514.html,N/A
https://papers.miccai.org/miccai-2024/013-Paper1668.html,"[Response to Reviewer #1]

[Response to Reviewer #3]

[Response to Reviewer #4]"
https://papers.miccai.org/miccai-2024/014-Paper1802.html,"We appreciate all reviewersâ (R1,3,4) constructive comments. We propose a novel approach based on domain adaptation (DA) to address inter-subject variability issue in EEG for seizure type classification. Moreover, an attention mechanism is proposed to extract temporal-spatial-spectral features from EEG. The reviewers found our work interesting and novel (R1,4), with convincing experiments (R4), crucial for clinical applicability (R1). All reviewers recognized the contribution of our work. We elaborate on the comments below. We will revise our manuscript as requested and release our code if accepted. 
R1:
10.C2&6.1,4: Model evaluation: Evaluating on more datasets would more comprehensively evaluate the model. But annotations of multiple seizure types are not provided in other public datasets. In fact, it is common to use TUSZ as the only dataset for seizure type classification. We did train all models with the same dataset partition, as suggested (6-4). Among these models, Wavelet+lightGBM (2022), CNN (2021), GNN (2022) are recent SOTA models in seizure type classification. CNN+Attention and Transformer-based are attention-based methods for EEG classification. These models are depicted in section 4.2.
10.C1&6.2: Analysis of attention: We used Grad-CAM to interpretate the modelâs decision. We found high overlap between sensors attended by model and annotations. We did not show these results due to page limits. We believe it is reasonable for an 8-page single-column conference paper. We can provide as supplementary materials if permits.
10.C3&6.3: Discussion of limitations: We will discuss the source of bias and model robustness as requested.
R3: 
10.1,2&6.1,2: Domain discriminator architecture and training: The feature extractor, domain discriminator and seizure classifier are jointly optimized. The domain discriminator and seizure classifier are two simple classification heads that take the output of the feature extractor as input and perform two different classification tasks. It is common to use similar architecture, e.g., fc layer. Here, we use a pooling layer, and a conv layer with size 1 (the number of kernels equals to the number of classes, i.e., 2 for domain discriminator, 4 for seizure classifier), which is equivalent to a fc layer. Please see âSeizure Classifier and Domain Discriminatorâ of section 3.2. We use a gradient reversal layer (GRL) between the feature extractor and domain discriminator such that the feature extractor is optimized to misclassify the domains (learn domain-invariant features) while being able to classify seizures. This is different from the adversarial training in GANs. With GRL, we do not train the seizure classifier and domain discriminator alternatively. 
10.3,4&6.3,4: Overfitting and using existing attention blocks: We are not able to show the loss curve during training due to rebuttal policies. The acc on training set is 93% (lower on test set since they are unseen during training). But we argue that with DA the model performs better on test set, with a much higher weighted precision (88.2 vs. 84.4), showing the benefits of DA. Please also see the confusion matrix in Fig. 3. As the reviewer pointed out, the two attention blocks in our work are inspired by ECAnet and spatial attention. Although slight modifications are made, they are not entirely novel. However, we combine them to extract temporal-spatial-spectral features from EEG, which is novel.
R4:
10.4&6.4: Better explain design decisions: First, seizures exhibit highly complex temporal dynamics, and vary in temporal evolution. Therefore, we designed a temporal attention to focus on the discriminative time stamps. Second, different seizure types present highly complex spectral distributions over brain. This motivates us to design a spatial-spectral attention. Third, we use DA to relief the inter-subject variability issue. We will explain this more clearly.
10.1-3,5: Manuscript should be enhanced: we will revise it very carefully."
https://papers.miccai.org/miccai-2024/015-Paper0143.html,"Reviewer 1 (R1), Reviewer 3 (R3), and Reviewer 4 (R4) are highly recommended to provide code for reproducibility. We have already prepared to release our code and updated the GitHub link in the paper, and will transfer the repository from private to public before MICCAI proceedings come out.

R1 is also concerned that the model does not explore routing mechanisms between experts. We would like to clarify that the experts here are designed for 3D brain image segmentation, distinguishing them from sparse Mixture of Experts (MoE) architectures for 2D natural images, where a higher number of experts can be afforded and routing is necessary for expert selection. Most of these route selection methods are implemented using top-K strategies, but are actually non-differentiable (Puigcerver, Joan, et al., ICLR 2023). Due to this and the smaller number of experts in 3D-based MoME, we do not explore expert routing in this work.

R3 wonders whether MoME can demonstrate its effectiveness by fine-tuning the model on unseen datasets with limited training samples to achieve comparable performance with task-specific nnUNets. We would like to clarify that the results in Table 2 demonstrate MoMEâs ability to infer directly under unsupervised settings without any fine-tuning, achieving performance that exceeds that of supervised task-specific nnUNets on 2 out of 3 datasets.

R3 expresses concern that the proposed approach achieves better results in most comparisons with other foundation models, but the differences are low when compared to task-specific nnUNets. We would like to clarify that the foundation model will benefit more from more training datasets. Although in the current experimental settings, MoMEâs performance does not surpass that of numerous task-specific models with significant differences, when the number of datasets increases in the future, MoME can potentially show superiority. Besides, MoME, being only one foundation model, is comparable to 14 task-specific nnUNets. It is more convenient for clinicians to use MoME in real clinical settings, where managing and applying too many task-specific models can be challenging due to patient bias. And we will better clarify this.

R3 indicates that it is unclear why there is only one input image yet multiple expert outputs are shown. Please note that different imaging modalities can be correlated and provide complementary information. For example, FLAIR appears similar to T2 but with fluid being attenuated. They may both contribute to lesion segmentation with different features, and the outputs of an expert that is not specialized in the input modality may still provide useful complementary information (also shown in Supplementary Figure I.b). We will better clarify this in the paper.

R4 suggests describing the hierarchical nature of the gating network in Figure 1c. Weâd like to clarify that itâs challenging to present complex details within limited space, but readers can refer to the specifics in the Section 2.2 âHierarchical Gating Networkâ. We will better clarify this in the paper.

We will also address any other minor concerns raised by the reviewers."
https://papers.miccai.org/miccai-2024/016-Paper1535.html,"CONFIDENCE VS. UNCERTAINTY 
Despite being conceptually different, confidence and uncertainty can serve a similar function in the clinic. We intentionally define the term psi to refer to both confidence or (negative) uncertainty to reflect their similar utility. We are not implying equivalence, e.g. a model can have terrible calibration yet a good uncertainty estimation.

ALPHA SELECTION
The problem of alpha selection is similar to selecting the operating point of a classifier. During our in-domain (InD) experiments, we found the alpha which yields peak validation performance to consistently yield nearly peak performance on the test set. For unseen hospital (OoD-H) experiments, the alpha that showed best joint performance on the validation set does not translate well to the test set â it underestimates the required doctor oversight (but the joint system still outperforms standalone AI or doctors). The extent of this discrepancy was smaller for unseen devices (OoD-D). Thus, we emphasized the need for a small set of cases and assessments for choosing the alpha for out-of-domain settings. Having few doctors for calibration is not a problem, provided that the joint system will be used by doctors with a similar level of experience.

GAMMA SELECTION
The selection of partial areas (gamma values) is arbitrary as this value will vary from center to center depending on demand and available healthcare resources. We tried to select values that address a range of realistic use cases. For example, a hospital with limited resources may rely on their AI model to diagnose the majority of patients, reducing the workload of doctors. In this case, the hospital would be interested in comparing models at high coverage ranges only using a partial AUJRC with e.g. gamma = 0.9. Looking only at the full AUJRC (when gamma = 0) score, a model which performs well at low to medium coverage ranges, but poorly at high coverage ranges could potentially be chosen despite it not being the optimal model for the realities of the setting.

FEASIBILITY
While the joint risk curves may be resource intensive, the point of our work is that ignoring the joint function of humans and AI in a diagnostic setting can lead to suboptimal performance and a misguided reliance on existing strategies. We must account for the interaction between doctor assessments and uncertainty/confidence if we are to have a clear understanding of how (or if) these quantities are helpful.

JOINT SYSTEM USAGE
We agree that in many realistic scenarios, doctors should have access to model predictions (and uncertainties) to revise their assessment. However, this can only be done prospectively at high cost, which we plan to do. However, the AI-as-first-reader scenario we discuss is a practical and valid use case which is worthy of study and likely to be deployed in the coming years.

DATASET
Our data contained Doppler and grayscale transvaginal and abdominal ultrasound images. Centers showed high variance in their diagnosis distributions, sonographer experience and device model. Standalone experts (median 17 years experience) had on average 5 points higher F1 score than non-expert (median 5 years experience) doctors.

BOOTSTRAPPING
We employed bootstrapping where we first sampled the case, then a single random doctor per case. While bootstrapping doctor assessments, we normalized the sampling probability with respect to the number of assessments done by each doctor.

TRAINING DETAILS
We used each methodâs default hyperparameters except for learning rate and schedule, both of which we selected manually by inspecting performance on the validation set. Similarly, we picked the 5-member ensembleâs diversity regularization strength using the validation set. We used 10 configurations for MCD (trained with p=0.2), 10 checkpoints for SWAG, and 10 particles for SVGD. For each method we selected an applicable confidence or uncertainty measure based on the peak validation F1RC performance."
https://papers.miccai.org/miccai-2024/017-Paper1165.html,"We thank reviewers (R3, R4, R5) for their constructive comments and acknowledgment of our novel method in addressing the challenges of incomplete multimodal learning tasks.

Q1: Model details and reproducibility (R3, R4, R5)
A1: Each subject is assigned a randomly initialized and trainable subject-specific latent representation. Then, this latent representation is optimized by a multimodal reconstruction module to integrate information from all available modalities, further refined by a subject-similarity graph embedding module and AD-oriented clustering to enhance diagnostic capacity. The multimodal reconstruction module consists of four layers of upsampling and residual blocks. The first two layers follow the sequence {Conv3D-Upsample-Residual-Upsample-Residual} with shared parameters across modalities, while the last two layers follow {Upsample-Residual-Upsample-Residual-Conv3D} with modality-specific parameters. We will upload all codes and subject ID lists once accepted.

Q2: Distribution consistent loss (R3, R4)
A2: To improve class separability in the latent space, the AD-oriented clustering module uses a distribution consistent loss to preserve the cluster structure of subjects in their subject-specific latent representations. Specifically, we employ a spectral clustering algorithm on the latent representations to obtain the clustering results. Then, we estimate the probability density distributions of the original subject space and the latent space using kernel density estimation. Finally, distribution consistency loss between these two distributions is employed to guide the update of the latent representations. The improvement in Table 4 (from 61.88 ACC to 64.43 ACC) validates the effectiveness of the proposed module.

Q3: Classification error and performance (R4, R5)
A3: The classification error is primarily due to two main reasons: (1) a large amount of irrelevant information misleading the classifier, and (2) significant similarities observed between some MCI subjects and those with AD or NC. To address the first issue, we employ a latent space learning approach to project different modalities of subjects into subject-specific latent representations, thereby reducing the generation of irrelevant information. For the second issue, our method introduces a subject-similarity graph embedding module and an AD-oriented clustering module to enhance class separability. The notable improvement over the second-best approach, i.e., with an increase of 2.14% in ACC and 3.03% in F1S, validates the effectiveness of our model.

Q4: The chosen similarity metric (R5)
A4: All PET images are preprocessed using a standard pipeline, involving (1) intensity normalization based on the cerebellum and (2) smoothing with a Gaussian kernel to reduce inherent noise. While cerebellar distributions may vary, this preprocessing ensures comparability among PET images. Additionally, as the chosen similarity metric assesses global similarity, the cerebellumâs influence on computing relative neighborhood relationships is minimal due to its small proportion. Future optimization efforts will target disease-relevant regions.

Q5: Hyper-parameters and competitive models (R5)
A5: We prioritize equal training importance for multimodal reconstruction and neighborhood relationship learning (lambda 1 as 1). For lambda 2, we set a smaller value, as the classification task converges more easily than the reconstruction task. Related research has shown that using synthetic PET can enhance performance, while its effectiveness is constrained by noise. Thus, in Section 3.3, our focus shifts to comparing different types of methods in addressing the incomplete multimodal diagnosis task. We ensure that the methods compared in Table 3 are implemented either using released codes or following descriptions in original papers and that all network sizes are within the same range. This consistency guarantees fair comparison of performance metrics. We will add these details."
https://papers.miccai.org/miccai-2024/018-Paper1214.html,"We are grateful to the reviewers for appreciating the convincing ablation study (all reviewers), clarity of explanation and comprehensive experiments (R5), clarity and conciseness (R6) and good organization (R7).

R5 and R6: missing metric in Table3. Sorry, the metric is Pearson Correlation Coefficient (PCC), which is commonly used to measure the agreement between human expert AAC scores and machine predictions.

R6: comparisons with other transformer-CNN hybrids. While hybrid transformer-CNN models have indeed been used for different medical image analysis tasks: like segmentation, classification, super-resolution, and landmark detection, it is important to note that each task necessitates a uniquely designed hybrid model. We have performed image regression. To the best of our knowledge, none of the hybrid models proposed thus far have considered image regression tasks. Models like TransUNet, UTNet, Unetr, and CoTR are crafted for image segmentation and involve up-sampling feature maps (using a decoder) to generate segmentation masks. In contrast, our proposed model does not require feature map up-sampling.  It combines features from different hierarchies and down-samples them and then eventually generates regression scalar outputs. Hence, a direct comparison with the aforementioned models is practically not feasible. To conduct a meaningful comparison, significant modifications would be needed for the above-mentioned models, essentially transforming them into significantly different architectures, diverging from their original design intent.

R5: improvements from DRSA and EFFM have not been highlighted. Firstly, DRSA and EFFM are novel contributions which significantly improve the results (Table3). The operations in both modules are Self-Attention mechanisms that improve performance by increasing the capacity to weight the importance of different pixels or image parts. They increase the complexityslightly, but the reward is significant (Table3). Our proposed model has 22.02 million parameters while the CNN backbone without DRSA and EFFM has 19.84 million parameters. The inference time of our model increases by 91 ms, however, the average improvement in performance (PCC) is more than 10%. It is important to point out that our proposed model performs much better than the EfficientNetV2M backbone (without DRSA/ EFFM), which is more complex than ours (52.2 million parameters, 534.4 ms inference time). We will add these details.

R5: more explanation on the relationship between different branches of DRSA. To supplement Sec-2 of the paper, we can stress that DRSA is a Self-Attention (SA) module that employs SA on two different resolutions of the same input feature map. The upper branch (Fig 1(b)) is the high-frequency SA path. It operates on the actual input feature map and considers fine details within the window size. However, its context is limited by the window size. To consider a broader context, like the shape and curvature of the spine, we either need to increase the window size or decrease the input feature map. As increasing the window size is computationally expensive, we reduce the spatial dimension of the input feature map using low pass filtering operation (average pooling) in the lower branch (Fig 1(b)). The reduced-sized feature map has low-frequency information with broader spatial context within the same window size.

R7: time complexity. Our model has 22.02 million parameters, and for the input image of size 320x320, the inference time is 408.08 ms. This will be made clear in the paper, thanks."
https://papers.miccai.org/miccai-2024/019-Paper4063.html,"We appreciate your positive comments on our work (e.g., âChannel attention and spatial attention refined the feature map which is interestingâ by R#1, âMethod is described with sufficient details to reproduce the networkâ by R#3, âNovel attention based architectural changesâ by R#4). We address the main concerns below.

Q1: Dataset Construction and Size. (R#1, R#4)
A: We randomly selected 8 consecutive B-scans from each OCT volume for manual annotation of HRF, carefully ensuring that they are in either the training or testing sets. Although we didnât constrain HRF presence in training B-scans, our network is well-trained. While time-consuming manual annotation limits the dataset size, we plan to expand and make it publicly available in the future.

Q2: Novelty of SEM and MEM Modules. (R#3)
A: Inspired by clinical practice, the SEM and MEM simulate ophthalmologistsâ use of local and global perspectives to detect lesions in OCT images. Variations in HRF characteristics such as shape, size and brightness within a slice, and their proximity to tissues of similar reflectivity, require switching between detailed and broader views for accurate detection. HRF lesions often span 2-4 B-scans, making adjacent slice examination essential.

The SEM uses 4 branches to analyze regional importance at varying scales, simulating a mix of views within each slice. The MEM combines the SEMâs multi-scale attention with channel and spatial attention across multiple slices, enhancing multi-dimensional information extractions. These innovative modules, presented here for the first time, may offer new insights for future medical image analysis research.

Q3: Joint Loss Function. (R#3, R#4)
A: Given HRFâs small size and major background-foreground imbalance, minor imperfection impacts segmentation results greatly. Alternatives like weighted CE, focal and soft Dice loss are partial solutions. Dice loss relies on overlap struggling with small objects, CE lacks difficult sample weighting, and focal loss uses unreliable early predictions. We opted Dice plus Perceptual Optimization (PO) losses. They dynamically adjust the weights for hard examples in early training using a decreasing cosine annealing function and focus on challenging samples as confidence grows. Extra foreground pixel weighting might address class imbalance but risks neglecting boundary samples. Experiments show this joint loss function, with gamma at 1, improves HRF segmentation, outperforming other settings (0.5, 1, 2, 5).

Q4: Model Structure and Training. (R#3, R#4)
A: We used the 2D nnUNet with the standard Dice+CE loss function as the backbone, but MUSE-Net processes three consecutive slices through the SEM module to generate three feature maps per layer. These maps, as inputs to the MEM module (X_rem), are integrated and then fed into the semantic decoder to produce the prediction map. This configuration makes our model as a 2.5D segmentation network with a 2.5D attention module. We converted three slices into 3D NIfTI format for the comparison of 3D methods.

Concerning SEM and MEM module implementation, as R#4 highlighted, the matrix multiplication of W and P in the SEM module calculates region-sensitive weights [C, H, W] by averaging P[C,K,H,W] on the second dimension using W[K,1]. Further details on the matrix multiplication (Equation 3) in MEM will be provided upon public release of the code.

Q5: Necessity of Figure 5. (R#3)
A: Figure 5 aims to emphasize its significance in clinical applications. Our research on the segmentation and visualization of HFR in OCT images has substantial implications for the management of disease processes and the evaluation of treatment efficacy. Figure 5 showcases HRF subtypes obtained through subsequent quantization processing, illustrating the distribution of HRF in the retina. This highlights the clinical applicability of our research in understanding and managing retinal diseases, beyond just methodological contributions."
https://papers.miccai.org/miccai-2024/020-Paper4180.html,"We would like to thank all reviewers for their insightful feedback. R1 acknowledges the significance of our research via the challenges and important suggestions addressed in our paper, particularly emphasizing its relevance to the field. R2âs recognition of the reproducible results further validates the robustness of our findings, enhancing the credibility of our study (R3). Additionally, the acknowledgment of the valuable system design and the clarity in exposition by R2 highlights the effectiveness of our approach in communicating complex concepts. Furthermore, R3âs positive remarks on the sufficiency of qualitative and quantitative comparisons, along with the significant improvement over state-of-the-art methods, provide valuable validation of our contributions (R1, R2, R3). We are committed to thoroughly addressing each reviewerâs comments:
K-fold cross-validation (R1): 
Below we share the 3-splits result. The only condition we followed was that all the categories are present in train and test sets, for all the splits. For splits, 70% of data is considered for training, and 30% is used for testing. Three splits are:

Regarding selection of parameters, we have decided to keep the default settings for simplicity and to maintain consistency.

The naming of a subset of the collected dataset (R3):
R3 recommended adding a description on the naming of subsets of the collected dataset. Name assigned to subsets as:
H_100x_C1 = [Microscope Cost][Resolution][Camera Type], H stands for high-cost microscope, namely OlympusCX23, 100x is the resolution, and C1 is the camera type. Similarly for other microscope types, camera types and on different resolutions.
Supplementary Information (R3):
Generation of Cutoffs for Subgroups in Fig. 2b (R3):
In Fig. 2b of supplementary materials file, the cell sizes are mentioned as small, medium and large. As the dataset is annotated by  hematologists, they have assigned the cell size attribute to the WBC cells.
Explanation of Terms and Artifacts (R3):
In Fig1, part b, we have shown the None type of WBC with box level annotation. It includes the artifacts (Staining Artifacts, Smear Artifacts, Debris,  Air Drying Artifacts etc.) and clamp cells.
Clinical Feasibility (R4):
Diagnosing leukemia is expensive due to the necessity of using microscopes and relying on expert hematologists, who may not be readily available in all locations. Additionally, advanced and costly tests such as flow cytometry and genetic analysis are often required. Our method can help reduce these costs by providing detailed morphological analysis through AI, potentially decreasing the reliance on these expensive tests."
https://papers.miccai.org/miccai-2024/021-Paper0148.html,N/A
https://papers.miccai.org/miccai-2024/022-Paper1213.html,"We would like to thank the reviewers and the area chairs for this constructive and critical review of our paper.

As a CAI-focused submission, our main contribution lies in developing a novel methodology to create a paired in-vivo dataset that greatly benefits laparoscopic image desmoking. As indicated by the reviewers, the strength of our paper lies in the cleverness, effectiveness, and significance of our work. Reviewers also raised constructive criticisms regarding 1) the lack of methodological novelty and 2) the lack of comparison with other synthetic/ex-vivo datasets. In the following section, we would like to clarify these concerns.

1) Although each individual sub-step may not seem novel, the main novelty of our work lies in the overall workflow to systematically create such a dataset, allowing us to construct the first-of-its-kind in-vivo paired dataset for laparoscopic image desmoking. To the best of our knowledge, this is the first time such a methodology has been employed to address the well-known challenge in clinical translation for laparoscopic image desmoking, meeting the contribution standard for a CAI paper. Another novelty lies in the proposed pre-processing in the red-channel image, which is the main technique that can address the problem of motion correction through smoke. This discovery and methodology are novel and significant for the MICCAI community.

2) The ultimate goal of any clinical transnational method is to demonstrate its effectiveness in a real in-vivo setting. As suggested in our title and abstract, our dataset was created to satisfy this need, allowing us to evaluate and compare existing methods, reveal limitations, and offer insights for future research, which is also one of the main focus of MICCAI. Although synthetic/ex-vivo databases may facilitate training, they may not truly reflect algorithm performance in real in-vivo environments. Thus, we believe that comparing with synthetic/ex-vivo data for training purposes would be an interesting research direction for expanded future studies but is beyond the scope and focus of this paper.

Aside from these two major criticisms, we would also like to address other constructive comments.
1) For derivations of structural maps S_I and S_G, these are obtained by performing texture removal on the max-intensity color channel image (bright channel image) as a rough estimate of the image illumination. Since the red channel is almost always the max-intensity channel for laparoscopic images, S_I and S_G are computed as texture-free images of the red-channel images.
2) Example matlab code for motion correction will be provided with the dataset.
3) Another constructive question is about the potential difficulties in expanding the dataset for training. Existing de-smoking methods typically use 2000â20000 pairs of images for training (mostly synthetic). While our current dataset may not be sufficient to train a network from scratch, it is suitable for fine-tuning existing models or joint training with synthetic/non-laparoscopic data. To bridge the data size gap, a video content recognition-based method could be developed in the future to automatically identify candidate clips, significantly reducing manual work. With the in-house video data we have, we aim to expand the dataset to 2000 pairs and this number can be further expanded by using online database (eg. hamlyn database) or establish collaboration with other institution.
4) For R4âs concern 1.2, sorry for this oversight. Fig.2(c) is the result of equation 4 without red-channel pre-processing. The result of equation 4 with red-channel pre-processing isnât shown as it looks too similar to Fig.2(b) in a side-by-side comparison due to very small motion. A very similar case (with worse smoke) is showcased in a motion picture in the supplementary material (second row).
5) We will also address reviewersâ minor comments as best as we can in the revision to improve the quality of this paper."
https://papers.miccai.org/miccai-2024/023-Paper2715.html,"The authors thank the four reviewers for their critical comments and overall ratings (5 4 4 3) of our paper, and our rebuttal focuses on the weakness listed by them. 
[R3; R4; R5] Comments on the clarification of computation of the tongue dorsum motion. In fact, our method, inspired by reference [11], is designed for better automation and robustness. We now provide a detailed explanation of step 1 in Section 2.1. 
The are two major stages. (1) We establish a coordinate system for frame alignment. (i) Since the mandible is acknowledged as a crucial and primary support structure for tongue movement, we set the lower margin of the attachment point of the genioglossus muscle on the inner side of the mandible as the origin [11]. (ii)   The X-axis is determined by the second cervical intervertebral disc due to its fixed position [11], and the coordinate system is constructed. (2) We then mark the upper border of the tongue dorsum with integrity and accuracy.
[R3; R5] Comments on prior studies of swallowing analysis, including deformable registration-based methods. In Section 1, paragraphs 2 and 3, we addressed the limitations of prior quantitative studies on swallowing analysis. Additional insights into prior research are as follows.
(1) Prior studies on Cine-MRI may be either inaccurate or less effective. For instance, (i) Young et al. (2023,DOI:10.1002/hed.27309) studied four specific boundary points on the mid-sagittal plane of the tongue. But the four points cannot adequately capture the complexity or describe the diversified features of tongue movement. (ii) Yang et al. (2020,DOI: 10.1371/journal.pone.0228652) studied the characteristics of tongue root during swallowing with an improved deformable registration algorithm to track its motion in four directions using deformation vectors. However, they only focused on local tongue root movement without fully reflecting the entire tongue motion. (2) Unlike tagged MRI, which is less accessible due to its requirement for specific scanning sequences, cine-MRI entails more manual intervention and complex algorithms for accurate deformable registration. While deformable registration methods can estimate overall tongue movement, post-registration error validation is indispensable. Appendix E of DRIMET (Bian et al, PMLR, 2024) states these errors resulting from border mismatches should be corrected manually, otherwise they may undermine downstream tasks.
[R3; R4; R5; R6] Method novelty and effectiveness. In Section 1 paragraph 4, we elucidated the motivation and the novelty of our method. Further elaboration on the novelty and effectiveness of our method is provided below.
(1) Our method efficiently aligns 10 feature points with minimal annotation, enabling automated extraction. These 10 feature points (P1~P10) can provide a more complex and precise description of the tongue dorsumâs displacement field. (2) The accuracy of our approach depends solely on the ROI precision, eliminating further manual intervention. (3) Our latest clinical statistics from oral physicians reveal significant differences in the motion amplitudes between the squamous cell carcinoma (SCC) and normal control groups at points P8 and P9 (p<0.05). To follow the rebuttal policy, we will not provide our new experimental results (included in one authorâs master thesis recently), which can illustrate the effectiveness of the proposed method. 
[R2; R5; R6] Reproducibility. All the codes and datasets, including 32 swallowing cycles (with 8 images per cycle) labeled by oral surgeons, are uploaded to GitHub for public access. The link will be made public later.
[R5] Novelty of our temporal relative position embedding (RPE). We have cited earlier work on RPE ([18] Wu et al.). In CV tasks, the RPE is typically applied to 2D image patches, but our method adapts RPE to the temporal axis for exploring correlations among image frames. 
The rebuttal and revision policy be strictly followed with NO substantial changes to this paper."
https://papers.miccai.org/miccai-2024/024-Paper4098.html,"The RCEUV-207 was imaged radially at 10-20 Mhz by a Hitachi ultrasound imaging system using pathology biopsy T-grade results as video labels.
1.Differences Between Frame-by-Frame Analysis and Video Analysis: The main distinction lies in the consideration of temporal context information. Andrej et al. [1] discussed the differences between frame-by-frame and holistic video analysis, noting that while frame-by-frame analysis is simpler, it lacks inter-frame contextual information, which could reduce the robustness of model deployment. Similarly, Geert et al. [2] noted that conducting video analysis in ultrasound is closer to clinical reality. Recent trends in ultrasound analysis for breast cancer benign-malignant classification have moved from static 2D images to video analysis [3], suggesting that dynamic ultrasound analysis using video formats is more aligned with real-world scenarios and has greater research potential.
2.Method and Experimental Detail Additions:
(1) In RCVA-Net, we introduced two modules (NSEM and RSEM) to link temporal context information and enhance the global recognition capabilities of video-based models. In NSEM, we used the ViT patch method for tensor dimension transformation, which can currently be implemented using the Hugging Face transformers method with the following code: âinputs = ViTFeatureExtractor.from_pretrained(âgoogle/vit-base-patch16-224-in21kâ)(images=dummy_image.permute(0, 2, 3, 1).numpy(), return_tensors=âptâ).â In the RSEM, the tensor size transformation is consistent, but the selected video frame and its adjacent frames (M-1, M+1) are sampled without replacement along the time axis. This means that the selected frame M and the frames immediately before and after it are extracted only once and not reloaded for training within the same video feature extraction process. We will add these details to Sections 3.1 and 3.2 to clarify.
(2) Addressing the question, âWhy do image-based classifications seem to perform better than video classifications?â Table 1 shows the baseline capabilities of different data loading methods and model approaches on RCEUV-207. Apart from the ease of feature extraction and pattern recognition in image-based analysis, where training and testing data are usually divided proportionally by category, video analysis divides each case as either training or testing. We believe this is a key reason why image-based classification methods can achieve high-performance levels. Video-based analysis methods not only need to consider information from the temporal dimension but also require the model to have robustness against unknown cases to perform well in tests. Our paperâs proposed RCVA-Net achieves leading baseline performance in this regard. We will add additional metrics such as precision, recall, and specificity to Table 1 and include the aforementioned reasons in Section 4.1 to provide a comprehensive understanding of the experiments.
3.Clarifications on Other Comments: Some comments have pointed out the need for a comparison with datasets related to this paper. To our best knowledge, this is the first video analysis dataset proposed in the field of colorectal cancer endoscopic ultrasound. Although there are a few statistical reports, none have disclosed their datasets or corresponding machine learning methodologies. We are willing and hope to add citations to address any shortcomings in clinical descriptions objectively.
[1] Litjens, Geert, et al. âA survey on deep learning in medical image analysis.â Medical image analysis 42 (2017): 60-88.
[2] Karpathy, Andrej, et al. âLarge-scale video classification with convolutional neural networks.â Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2014.
[3] Lin, Zhi, et al. âA new dataset and a baseline model for breast lesion detection in ultrasound videos.â International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2022."
https://papers.miccai.org/miccai-2024/025-Paper3658.html,N/A
https://papers.miccai.org/miccai-2024/026-Paper1953.html,"We sincerely thank R1&R5 liked our clear writing and R3 agreed with the contribution of our work. We address the concerns raised by the reviewers below.
Q1: Scanner Difference [R1]: Thanks for your suggestion. Actually, the used datasets involve the setting, where four clients of RIF use 4 different scanners.
Q2: Fairprune [R1]: Thanks for your suggestion. Although this work indeed differs significantly from ours, the insight of pruning by saliency is very interesting, and we will discuss it in revised manuscript and explore its feasibility for FL in future work.
Q3: High Variance in Results [R3]: There may be some misunderstandings. Avg. and Std. are the average and standard deviation of the model performance across all clients. Therefore, we cannot directly add them together to evaluate the performance of the method. The goal of performance fairness (see Definition 1) in FL is to maintain a high Avg. while achieving lower Std.. Thus, the results on two datasets demonstrate that Fed-LWR achieves the better and fairer performance compared to existing methods. 
Q4: Lack of Complexity Analysis [R3]: We quantify the complexity of different methods based on the training time/round[3], and the results on ProstateMRI are as follows: FedAvg(239s) < Ditto (305s) < qFedAvg(322s) < CFFL(341s)< Fed-LWR(383s) < FedCE(394s) < CGSV(487s). This indicates that the training overhead introduced by Fed-LWRâs fairness mechanism is acceptable, considering its superior performance. 
Q5: Statistical Testing [R3]: Based on your suggestion, we conducted two additional independent experiments on ProstateMRI using different random seeds for all methods. Then, we conducted paired t-tests between the 3-trait results of all baselines and our Fed-LWR. The results indicate that the p-values for the Avg. and Std. of all methods are less than 0.05, demonstrating the statistical significance of the improvements yielded by our Fed-LWR over previous methods in terms of accuracy and fairness.
Q6: Clinical Integration [R3]: Thanks for the comments. We plan to employ Fed-LWR on multiple cooperating hospitals to collaboratively train breast ultrasound lesion segmentation model. The fairness mechanism of Fed-LWR will ensure that the model performs fairly on all hospitals, promoting long-term, healthy collaboration. 
Q7: Relationship and Difference [R5]: In FL, fairness issues are associated with non-IID data problems as different data distribution can lead to unfair model performance [16]. However, fair FL and non-IID FL are two different research directions for the diverse needs of users, i.e., better fairness or better accuracy. Non-IID FL focuses on addressing the performance degradation caused by non-IID data; thus they will overfit local data to improve their performances (high Avg.). Such improvements may decrease the fairness of the model (high Std.). This conflicts with the requirements of fair FL, i.e., high (Avg.) and low (Std.). Finally, we must clarify that the goal of Fed-LWR is not to address the domain shift issue. Instead, it perceives feature differences to adjust the weight of clients to achieve better global consideration, thereby improving fairness of global model.
Q8: Advantages [R5]: Compared to the aforementioned methods, Fed-LWR is specifically designed to address fairness issues. This ensures the model to perform fairly across different clients and better meet the userâ requirements for performance fairness.
Q9: Significance of Fair FL [R5]: Fairness is a crucial evaluation for FL systems, and previous studies [14,16] has confirmed the importance of fairness. An unfair FL system can diminish usersâ enthusiasm, thereby hindering the sustainable and healthy cooperation.
Q10: Comparison [R5]: We compared Fed-LWR with FedProx and FedBN on ProstateMRI dataset, and their results (Avg., Std.) are as follows: FedProx (87.27, 3.78) FedBN (91.62, 5.41). Our Fed-LWR outperforms them in terms of accuracy and fairness, demonstrating its superiority."
https://papers.miccai.org/miccai-2024/027-Paper2689.html,"We are grateful to the reviewers for their valuable comments to improve our paper. The following are our responses to the reviewersâ comments. If paper is accepted, we will add these changes in the final version. 
R#: Reviewers | comments order

1.Method 
(A) Novelty: replacing GNN with HGNN (R#3: 3-2)
Our contribution lies in formulating novel and efficient HGNN methods for constructing Hyperedges. Conceptually, we did not replace GNN with HGNN. Instead, we introduced a novel approach based on node degree using KNN, which has not been utilized previously and offers an improved scheme to construct hypergraphs. Our results validate the effectiveness of the method.
(B) Patch number and its effect (R#4: 3-1)
We construct hyperedge by considering one pixel as one node in feature space. A node is considered as a patch. We will conduct further experiments with different numbers of pixels per patch.
(C) Technical details & parameter adjustments (R#4: 3-3)
We will emphasize node degree construction and its implications in hypergraph construction, along with providing visualization results. We have also provided experiments using tuning parameter (alpha) to adjust effect of nodeâs degree. We observe that a low value of alpha is needed to avoid redundant information.

2.Experiments
(A) Additional dataset to compare (R#3: 3-1, 6-3; R#4: 3-4)
We validated our method using two widely used benchmark public datasets. Our method surpasses SOTA methods and achieves competitive result. For one dataset and compared to only one method, we observe a marginal and almost negligible drop in accuracy. Additionally, we have conducted experiments on another dataset (MoNuSeg), achieving SOTA results. The results validate the effectiveness of our method, highlighting the novel Hyperedge construction mechanism.
(B) Detailed comparison with SOTA and other baseline methods (R#3: 3-3, 6-2)
We will add additional comparisons with methods such as HGNN+ and Graphormer.
(C) Comparison: nnU-Net and nnFormer (R#4: 7-1)
Accuracy in nnU-Net and nnFormer is tested using 3D volumes, while our method is evaluated on 2D slice images. So, the evaluation methods are different. In the final paper, we will compare the proposed method with these methods using the same evaluation method.
(D) Analysis: existing hypergraph modeling methods (\epsilon ball) (R#1: 3-2, 6-1)
Our method models feature and local level hyperedge construction, merging these hyperedges to form a robust representation. This procedure operates simultaneously. While the \epsilon-ball approach uses a fixed-size radius to include nodes, resulting in redundant information. In contrast, our hyperedge construction method is adaptive based on the degree of nodes, efficiently capturing information. We will add a visualization comparison of hyperedge construction using these methods.
(E) Computation complexity (R#3: 6-1, R#4: 3-2)
Hypergraph construction process does not increase parameter count and maintains low computational cost by using low-resolution feature space. We calculated parameter count and our method requires fewer parameters. Further, increasing number of pixels in a patch for higher resolution and using a sparse matrix for hypergraph modeling (as in PyTorch Geometric library), may further reduce computation cost. Our method is efficient and computational time is comparable.
We will include the parameter counts in the relevant table.
Table-1(c): Transformer: 55.55M, Graph: 69.75M, Ours: 43.08M
Table 2: ScaleFormer:113.81M, MAXFormer:88.93M, Ours: 43.08M
(F) Model scalability (R#4: 3-2)
Our method can be easily integrated into other encoder-decoder frameworks such as TransUnet and ResUNet.
(G) Hyperparameter analysis: K neighbors and others (R#4: 7-3)
The hyperparameter values are selected based on our experiments. We will add analysis in relevant sections.

Figure Quality (R#1: 3-1; R#4: 7-2)
We have redrawn the diagram.

Discussions (R#4: 7-4)
We will add discussions accordingly."
https://papers.miccai.org/miccai-2024/028-Paper2053.html,"We thank the reviewers (R3, R4 and R5) for their feedback and R5 for noting the novelty of our approach. Please find clarifications for the main points raised below:

Lack of comparison to particle trackers (e.g., PIPs++) (R4): Particle tracking methods are trained on synthetic datasets of natural images using modules inspired by optical flow. They require large amounts of point correspondences between frames to learn unique features for various scene points, which is impractical in our interventional setup. For example, pips++ trained on PointOdyssey consists of 18,700 trajectories. While these methods can technically be trained with sparse points, our early experiments with CoTracker (claimed to outperform pips++) struggled to converge and performed 4x worse than our method for catheter tip tracking, likely due to sparse trajectories in our data. Additionally, all particle tracking methods include a refinement stage that smooths trajectories temporally, making them not real-time. Despite pips++ claiming 55 FPS, they use non-overlapping windows (36 frames) in time to process the entire video. In a practical real-time setting, overlapping sliding windows in time are used (as shown in Fig 1 supplementary) as we cannot wait to accumulate frames. In that case, pips++ results in a runtime of 2 FPS. Due to these impracticalities and to have a fair comparison, we focused only on feature-based tracking methods, which have proven suitable in addressing our problem.

Details on comparison with self-supervised learning methods and novelty (R3): One of the novelties in our SSL approach is introducing weak supervision forcing the network to learn richer features. This method can be applied to any latest SSL techniques, such as MAE and DINO. We chose FIMAE, which showed superior results compared to other recent video-based SSL methods, cited as [1] (recently published in a medical journal) to demonstrate our claim (Table 2). Our paperâs major novelty is the downstream tracking framework, designed to effectively leverage SSL features for tracking under complex angiography scenes. A naive use of the SSL encoder gives inferior results, as shown in Table 3âs first row, where motion-aware feature matching and past trajectory are absent. Moreover, we show that matching in space-time feature space (Cross-Attention) is more effective than traditional spatial feature matching using asymmetrical crops. This approach also results in flexible tracking, extending to multiple object instances, unlike existing tracking frameworks.

Details on ablation analysis (R4): We refer to the baseline SSL as FIMAE and the version incorporating weak supervision as FIMAE-SC. Table 2 compares these models across downstream trackers (SimST and ours), showing that weak supervision-based SSL enhances performance for both trackers. Table 3 presents ablations on downstream tracker components, including space-time aware feature matching (appearance tokens) and past trajectory. Supplementary Table 1(a) explores how varying weights of reconstruction and mask segmentation (from weak supervision) impact performance and supplementary Table 1(b) highlights the effective size of the features for matching. In conclusion, weak supervision requires less weight than reconstruction, and matching with a proper feature size is required to balance accuracy with noisy predictions.

Other Clarifications (R5, R4): During inference, the tracker uses 5 frames; the current frame and the last 4 frames. As time progresses, the oldest frame in the sequence is dropped and the newest is added, ensuring the sequence maintains 5 frames. This inference strategy is illustrated in supplementary Fig 1. We empirically determined that using 5 frames provides sufficient temporal context while maintaining the runtime for practical applications. The results presented were conducted on test sequences that can go up to 200 frames. We will fix the typos and make the figure interpretable in the final version."
https://papers.miccai.org/miccai-2024/029-Paper3962.html,"We thank the reviewers and the editor(s) for their careful reading of our paper, and constructive suggestions. We have found that all comments were to the point. In the following, we respond to the major comments and respond to the main questions posed by the reviewers.

Novelty and Contribution (R3, R4): R4 comments about the novelty of our research, referencing papers that use robotic systems with rigid instruments for autonomous brain biopsies or drilling through hard tissue. However, our paper introduces a unique steerable drilling robotic framework using a continuum manipulator with flexible instruments for creating J-shaped (not straight) trajectories in spinal fixation procedures. To our knowledge, no other robotic system can autonomously create smooth curved drilling trajectories in the complex anatomy of the spine. Please see Figure 3 in our paper to fully appreciate the planned and obtained curvature of the drilling path.

We reviewed the articles mentioned by the reviewers and found that they all use rigid instruments limited to straight trajectories. The core contribution and novelty of our framework lies in using a flexible robotic system to enable J-shaped trajectories in complex anatomies like the spine. The flexibility of our system adds many benefits, but also significant complexity, which we have successfully addressed.

We acknowledge that several modules of our system are adapted from existing calibration and registration algorithms commonly used in robotics. Nonetheless, this does not diminish the novelty of our work. As R3 noted, our key contributions lie in âthe system integration of a series of complex steps for image-guided spine surgeryâ and the first-time use of these modules in a novel steerable drilling system with flexible instruments. More advanced calibration and registration algorithms could always further enhance the performance of our framework.

Phantom Realism and Clinical Translation (R1): R1 had concerns about âthe difference between a cadaver and the utilized phantom in the experiments.â We should clarify that the phantoms used in this study were designed and fabricated based on CT scans of a real patient obtained under an approved IRB protocol, to ensure realistic scale of the used vertebra. Regarding the bio-properties, the vertebral body section of the phantom was constructed from Sawbone PCF 5 block, which simulates the properties of an osteoporotic bone as experimentally evaluated in [4] of the submitted manuscript. Therefore, the utilized phantoms closely mimic realistic size, dimensions, and tissue properties and we believe the presented approach and framework can be translated for performing a cadaver study in the future.

Performance Accuracy (R1, R3): We appreciate the important comments from R1 and R3 regarding the accuracy of our studies and the explanation of the obtained errors. We especially thank R3 for their thoughtful comments and the referred review paper [A]. As mentioned by R3 and reference [A], medial pedicle perforation exceeding 4 mm may endanger neural elements, causing neurological deficits. Therefore, the overall errors we obtained (i.e., 3.5 mm) could still be clinically acceptable, considering this reference and given that this manuscript presents our first attempt using a novel robotic steerable drilling system for pedicle screw fixation.

Niche Application (R3): While we evaluate the system on itâs performance for spinal fixation, this system is capable of being used for other applications such as: ACL repairs, pelvic fixations, etc. The key feature of the framework is to enable access to regions within hard tissues that are inaccessible with current rigid instruments. Such applications are numerous and not having the choice of using steering instruments instead of rigid one is a major shortcoming we suggest to overcome in this paper."
https://papers.miccai.org/miccai-2024/030-Paper1279.html,"We sincerely thank the reviewers for their thoughtful reviews and are pleased they appreciate the significance (R#1, R#3, R#5), novelty (R#1, R#5), and effectiveness (R#1, R#3, R#5) of our paper.

[R1-Q1] Other baseline. Thank you for the constructive suggestion. We will provide other related baselines, such as the LLava series and RadFM, in the open-source repository on our dataset.

[R1-Q2] Hallucination. Hallucinations are linked to both the model and training data. Given our primary contribution is the dataset, we included negative samples to mitigate this issue. We plan to further investigate this at the model level in future work. For more information about this, please refer to our response to [R5-Q2].

[R3-Q1] The meaning of referring and grounding. We apologize for any confusion caused by the definitions of terms. We will include this information in the revised manuscript as follows: âReferringâ refers to a user input process where, in addition to inputting a image and a question, the user also specifies the exact image area they wish to inquire about, as seen in tasks like ROC and RC in Fig. 1 and the second turn in Fig. 2. âGroundingâ indicates the modelâs ability to pinpoint the specific location of the object queried by the user, as shown in the VG task in Figure 1 and the first turn in Figure 2. As reviewers R#1 and R#5 have commended, âThese capabilities are essential for intelligent biomedical assistants.â However, these capabilities that previous biomedical large models did not possess. Similar to the evolution of LVLMs in the general community, the primary challenge here isnât the novelty of LVLM structures, but rather the lack of detailed interaction datasets. Hence, weâve developed this innovative dataset to address this gap in the field.

[R3-Q2] The novelty of the model. As mentioned above, our primary aim is to address the gap in fine-grained interaction data for biomedicine. Our model serves as a demonstration to the community of the successful application of this dataset, like LLava-med, PMC-VQA and so on. We are urging more researchers to empower LVLMs with this diverse data to enhance more capabilities.

[R3-Q3] We sincerely appreciate your selfless and meticulous review. I apologize for our oversight in the diagramming, and we will correct it.

[R3-Q4] Reproducibility. Section 3 describes the data construction pipeline, and the appendix provides detailed prompt templates refined. This information should suffice for replication. Additionally, our data and code are being organized and will soon be open-sourced.

[R5-Q1] Quality of data. We have adopted the following three strategies: a. Input structured information and specific details about image objects (category, coordinates, quantity) to GPT, unlike LLaVa-Medâs coarse-grained descriptions. b. Each time we generate data, we randomly apply some templates customized by professional doctors to GPT, maximizing the use of GPTâs in-context learning and instruction following capabilities. c. We adjust the input prompts to generate 500 data points for each of the eight modalities, which are then reviewed manually to ensure their quality. Only when the data from all modalities are deemed satisfactory do we finalize the form of the input prompts.

[R5-Q2] Hallucination. We are deeply appreciative of your constructive critique concerning our research. Despite ChatGPTâs limited knowledge of medical imaging, it has a substantial foundation in medical knowledge. Given the shortage of biomedical multimodal data, using multiple strategies to ensure high-quality data generation is worthwhile. We agree with you and consider that augmenting the dataset with additional clinical data and leveraging the LoRA fine-tuning technique for continuous learning could provide a viable solution in future works. For more information about this, we recommend referring to our response in [R1-Q2].

We sincerely thank all reviewers again for your kind attention and thoughtful reviews!"
https://papers.miccai.org/miccai-2024/031-Paper3756.html,"We want to thank the reviewers for their time and their thoughtful comments. All of their remarks are insightful and our future work will tackle these points. We apologize for the scarcity of details due to the restriction of the number of pages. In future work we will try to address the concerns that were raised.

R4 noted that the biomarkers used for prototype initialization are only specified in the caption for Figure 2. We will try to move them in the text body for the final version. R1 noted that our initialization strategy needs labeled data. This is indeed a limitation towards the interpretability of our method. However we do not need a large amount of labeled data to obtain high quality prototypes (MAPLES-DR for example is only 198 images), making this labeling relatively easy.

Our ViT baselines from [22] and [14] do indeed operate at a lower resolution than the CNN baseline and our proposed method. Our goal with this work is to propose a new method that allows for Transformer-like models to operate effectively at high resolutions through the superpixel tokenization and clustering, whereas existing methods are constrained by the quadratic memory cost of the attention mechanism. As such we donât think that this comparison is unfair, because our method is what allows to operate at those higher resolutions.

R1 mentioned that we do not quantify the time and GPU memory necessary to train each method. The original code, published alongside the paper, relies on a naive (but much simpler to implement) interpolation of the whole feature maps to the original resolution. In practice, there is a lot of room for improvement: for example, we can use a custom CUDA operator which interpolates only within the area of each superpixels and directly reduce corresponding pixels in parallel. Since the original submission, we have worked on such an implementation, which can be found on a distinct branch of the same code repository. Inference has been considerably accelerated but the current state of this implementation does not allow proper benchmarking (in particular, the custom backpropagation of this operation remains to be implemented).

R3 noted that our work depends heavily on the SLIC algorithm. Our model is a functional proof of concept which we believe encourage the exploration of new tokenization approches. SLIC has the triple advantage of being versatile, fast and easy to implement. It is a natural candidate for validating the concept of superpixels tokens, but any improvement on the segmentation itself should lead to even better/more interpretable ViT.

R3 also noted a lack of clarity about the tokenization process. Each superpixel segment is indeed considered to be one token to be fed to the Transformer. We average the vectors from the feature maps at the corresponding pixel locations. Our approach does not use positional embedding.

Also, we should also emphasize that the order in which the tokens are considered in the adjacency graph is not important, as each of our operation (attention, pooling, etc.) is permutation invariant.

R4 noted the lack of a reference to Cohenâs kappa as a performance metric. This metric has been the de facto standard in DR grading for several years in competitions [5, 8] and in the literature [3, 12, 14, 17, 20]."
https://papers.miccai.org/miccai-2024/032-Paper1108.html,"We highly appreciate reviewersâ comments and constructive suggestions and recognising our work, particularly the new dataset for trustworthy multi-disease detection (R3, R4, R5), the consistency loss (R3), well defined problem and good figures (R4) and framework (R5). Below we address the reviewerâs concerns.

To R3:
Dataset&code&Metadata&Bias
Data and code will be released in June. Metadata wonât as they are sensitive personal data. No sex bias and exist age and ethnicity bias. Fair AI is important but beyond the scope of this work.

Is multi-image/eye common? Weight the results? 
2031 of 4102 eyes have more than one image (see Tab.1). Supposing the probabilities of ME for 3 images in Fig. 1 are 0.4, 0.95, 0.45. Average(0.4, 0.95, 0.45)=0.6, indicating positive. Average(0.4, 0.45)=0.425, indicating negative. Weighting does not guarantee consistency.

TrustDetector
L_EyeCon enforces different images from the same eye to have more similar representations compared to different images from different eyes. L_Rank enforces different eyes of same disease to have more similar representations compared to those of different diseases.

Differences of RNC&RankSim
RNC learns ordered features for samples based on labels via contrasting in feature space. RankSim learns features via matching the sorted list of sampleâs neighbours in feature space with the sorted list of the neighbours in label space.

Augmentation and baseline
Rotation is inevitable. They are widely used, e.g., in [1]. Baseline is ConvNeXt V1 in Table 2.

Means and stds in Table 2
The 2nd best ConvNeXt V1: 56.47+/-1.26 50.89+/-1.96 93.18+/-0.41 94.35+/-0.57 89.69+/-0.81. Ours: 58.96+/-0.89, 53.67+/-0.88, 93.42+/-0.30, 95.07+/-0.47, 90.58+/-0.47. The means/stds of ours are greater/smaller to the 2nd. So, our method is better than the 2nd.

Lack of statistical significance, large stds in Table 3
mAcc considers whether the prediction is correct. Standard deviations (std) in Tab. 3 are about 0.3, which are acceptable. mF1 and mKappa consider the extreme class imbalanced issue. Slight differences of predictions on the small number of positive samples of each trail would lead to large std. A larger scale dataset would alleviate this issue in future.

Table 3 shows how two losses affect results. We never claim L_Rank can contribute to consistency. L_Rank considers the severity levels and contributes to the rating agreement of predictions and the GT labels. In other words, it improves mKappa. L_EyeCon is responsible for the improvement on mAccCon. Together with the two losses, as Tab. 2 shows, ours achieves best.

Applying the proposed losses to SOTAs?
Applying the losses to SOTAs in Tab. 2 would improve performances. f. ex. setting alpha to 0.2 and beta to 0.1, performances of ConvNeXt V2 increase to 55.55+/-0.85, 50.09+/-0.80, 93.06+/-0.27, 95.23+/-0.40, 90.07+/-0.53 from 52.04+/-1.99, 47.11+/-2.41, 93.02+/-0.13, 95.17+/-0.90, 89.67+/-0.58.

Other
UWF: Ultra-widefield. Detailed description about Fig.3 will be included in caption.

R4: Results on another dataset, description of SOTAs, how proposed improves SOTAs, visualising tables.
Experiments on DeepDRiD [1], a dual-view fundus image dataset for 5-level DR grading have been conducted but not incldued due to the limited pages. The SOTAs will be well described in the revised. We will use t-SNE to visualise the feature space to illustrate how the proposed improves SOTAs and Radar Chart to visualize Tables.

R5: Comparable detection results
For balanced evaluation metrics mF1 and mKappa, our method surpasses the 2nd best ConvNeXtV1 by 2.49, 2.78. In mAcc, it surpasses the 2nd best by 0.24. The improvements are obvious. For the consistency related metrics mCon and mAccCon, ours surpasses the 2nd best by 0.72 and 0.89. In future, we will use diffusion models to generate multi-views of images to improve the consistency.

[1] Liu R, et al. DeepDRiD: Diabetic retinopathy grading and image quality estimation challenge[J]. Patterns, 2022"
https://papers.miccai.org/miccai-2024/033-Paper2509.html,"We thank the reviewers for their valuable comments, and appreciate that they found our method interesting and novel.

Methods
â¢ Improved clarity of implementation:

Experiments & results
â¢ (R1, R3) We will include visual/qualitative segmentation results in the main text. 
â¢ (R4) Metric: we acknowledge that the Dice coefficient captures only one aspect of performance. In response, in the camera-ready version we will include results using a previously validated [2] multi-metric composite score (including lesion wise true/false positive rate and volume difference) in Tables 2 and 3. We found that using composite score confirms the findings of the paper, with the proposed method outperforming dedicated models on 4/7 modality configurations in both datasets.
â¢ (R4) Statistical tests: we will include Wicoxon p-values (< 4e-4 for DSC, < 5e-5 for composite score, comparing MD vs proposed method for both datasets). 
â¢ (R4) It is stated that âthe main improvements in the results relate mainly to combining modalities rather than the novelties introduced in this workâ. We are not certain what is meant here by the reviewer. While itâs true that the performance of all models is still sensitive to the choice of input modalities (particularly when segmenting subtle MS lesions), our method shows clear improvements compared to modality-dropout across all modality configurations in both datasets.

References
[1] Gentile, G., et al. âBIANCAâMS: An optimized tool for automated multiple sclerosis lesion segmentation.â Human Brain Mapping (2023).
[2] Carass, A., et al. âLongitudinal multiple sclerosis lesion segmentation: resource and challenge.â NeuroImage (2017)"
https://papers.miccai.org/miccai-2024/034-Paper2390.html,"We would like to thank the reviewers for their time and insightful comments!

Reviewer 3:
Label Missingness: 
Thank you for the suggestion! While our focus was on missing data, we recognized the challenge of label missingness and will explore it in future work.

KLG:
We combined KLG 0&1 due to their difficulty in distinguishing, while the other categories have clearer distinctions. This is a common setup in previous works.

Different contrast of MR images:
Utilizing different contrasts of MR images could be an alternative to cartilage maps, requiring a shift from a 2D to a 3D feature extractor. This would increase computation but is feasible.

Predict with longer period:
As the prediction period extends, the task becomes more difficult, potentially reducing performance. Additionally, losing training data could make the model prone to overfitting.

Reviewer 4:
Thank you for your suggestions and questions. We believe this work represents an innovative ML solution that will be of interest to the MICCAI community, and thus we hope that you can recommend it for publication.

Multiencoder/Design/Comparisons:
There may be a misunderstanding between our setting and MultiMAE. MultiMAE âoptionally accept additional modalities of information in the input besides the RGB.â In contrast, we infer across multiple images and aggregate various data types, making a single encoder inappropriate. Additionally, MultiMAE uses multiple encoders (linear proj. as illustrated in its Fig. 2) for different modalities. 
Thanks for the suggestion on alternative model designs. RE aggregation: it has been shown that attention-based aggregation outperforms mean/sum-based aggregation (e.g., Set Transformer, ICML19) and is more general as they can learn mean/sum through constant keys/queries. RE graph NNs: GNN-based methods are appropriate when edges exist between nodes. In our case, there are no priori existing edges between modalities, leaving us with either fully connected nodes or learned connections, which is what our attention-based approach achieves.

Open source:
As mentioned in the abstract, we will open-source our code upon acceptance.

Cost concern:
We only consider tabular data and 2D images with either 5 or 6 timepoints based on the task. There are ~113M trainable parameters.

Ablation:
In our model, we compared results using different numbers of views (Attention Block) in Appendix C and different timepoints (Decoder Block) in Table 1.

Mask dimension:
The ânâ in the mask indicator represents the number of views.

Figure 2:
Figure 2 aims to show that our unified model can perform view drop during evaluation, and the results match with those from specific models, demonstrating the efficiency of our unified model. Detailed results can be found in Appendix C.

SAINT model:
The SAINT model uses transformers to extract features from tabular data, considering both column-wise and row-wise attention. Details can be found in our reference.

Reviewer 6:
Data imputation:
For data imputation, we can compare either zero/mean (likely ineffective) or impute from other views. However, imputing from other views is difficult for our multi-body-part setup, and hence, we chose to ignore the missing views.

Tabular filter:
We remove tabular data requiring extra measurements, e.g., 400m walk time, flexion/extension speed. Except for easily obtainable blood pressure, all attributes can be collected through questioning.

Additional result explanation:
In Appendix C, ROC is more mixed than AP and Marco ACC, mainly due to the imbalance of our dataset. Additionally, the pelvis contributes little as it offers minimal information for knee conditions. Similarly, tabular is less effective for KLG prediction since KLG is based only on knee radiography. This highlights a limitation: non-informative views provide limited contribution while increasing the computation cost. In the future, we aim to develop methods for automatically predicting useful views."
https://papers.miccai.org/miccai-2024/035-Paper1005.html,N/A
https://papers.miccai.org/miccai-2024/036-Paper2322.html,"We are glad that the reviewers have shown enthusiasm for our work, and weâre grateful for their constructive feedback.

R1: 
Weâll ensure that all references mentioned by this reviewer are included in the final version. Meanwhile, we will add more background introduction on fMRI studies to engage a broader audience. (And please refer to Q2 below for the in-depth discussion on our method.)

R3:
We will add more details on experimental setting and model parameters in the final version. And we will include GNN models such as GCN, GIN, GSN and GNN-AK (we can still outperform them by testing). In addition to homogeneous dataset like HCP, evaluation on clinical dataset is certainly part of our future work.

R4:
Q1: Overlook external data heterogeneity issue and intrinsic subject-to-subject variance.

A1: We completely agree with the reviewerâs observation that variations in neuroimaging data often arise from a combination of multiple sources. However, we would like to clarify that the effective solution for disentangling data heterogeneity is to use a data harmonization approach. Although both approaches share some common methodology components, we are addressing a different problem of learning replicability which is unique in fMRI studies. Our method is designed to establish a mapping between phenotypic traits and functional neuroimages(by characterizing inter-subject variations) and produce consistent results across different fMRI experimental settings. In this context, our proposed replicable deep model is an important piece (but not explored yet in MICCAI field) in fMRI research. Together with data harmonization approaches, we can deliver an integrated computational solution for performing data-driven studies using fMRI data.

Q2: Insufficient discussion on methodological insights.

A2: We will include the following discussions in the final paper. 
(1) Effectiveness of Wasserstein distance in fMRI data. 
Due to large inter-scan variance and substantial amount of external noise in the BOLD signals, the conventional measurements, based on the differences between two instances of time series, often have limited power to capture the intrinsic task-relevant variations. To address this issue, we formulate the problem into a distribution-to-distribution matching scenario. Our experiments also (empirically) demonstrate the advantage of Wasserstein distance on this problem.

(2) Compare with other DA models (such as DANN and MCD). 
Our method stands out by using two distinct feature extractors for the two domains, minimizing the distance at the feature level. Specifically, we observed that samples of different classes exhibit similar shift scales, indicating a relatively neat shifting pattern. This shift arises from (i)the different phase encoding directions for scan1 and scan2 during the HCP-task data acquisition; (ii) in the continuous fMRI acquisition, changes in the order of tasks will lead to different fluctuations in the processed BOLD. These factors suggest group-level differences rather than individual variability. Thus using the same feature extractor for two domains (used in DA models) might confuse the model. Meanwhile, DANN uses domain discriminators to close two domains, but with strong feature extraction (as we use Transformer), feature distribution matching becomes weak, leading to class mismatch. MCD relies heavily on the discrepancy discrimination by two classifiers. The neat shift makes it difficult for the model to identify challenging samples in the target domain, limiting MCDâs effectivenesses.

(3) Extensive evaluations. 
Our focus lies in benchmarking replicability specifically on HCP WM because of its widespread utilization. If page limit allows, however, we are committed to show more experimental results on (i)comparison with GNN models, (ii) task-related brain mapping for WM, (iii) tSNE plot of feature distribution before and after domain adaptation, (iiii) enhanced Fig.3 to improve clarity."
https://papers.miccai.org/miccai-2024/037-Paper1747.html,"We thank the reviewers for their valuable feedback. We appreciate they agree on the novelty and interest of our annotation and framework (R1, R3, R4), well written, easy to follow and reproducibility (R3, R4). Detailed responses are given below.
Q1: Reproducibility. (R1)
A1: We will release the code publicly once the article is accepted.
Q2: Sec. 2.2. (R1)
A2: Classification models can better discover target regions when using local views. We design a local classification branch to assist the global classification branch in locating more complete target. Random cropping of global view patches for the local branch input can lead to incomplete or missing target areas. TIA allows cropping positives S from labeled areas and negatives V from lesion-free regions. Unlike natural images, medical images rely heavily on anatomical structure information for accurate feature expression. Direct cropping of local views loses this crucial information. Therefore, we map the anatomical structure from the global view matrix A to the local view matrix O using an affinity coefficient P. P is normalized by softmax (Oâ) and added to O, yielding the final local branch input that preserves anatomical structure.
Q3: Types of weak labels. (R1)
A3: Several weak labels for segmentation have been proposed. The image-level label is the coarsest and easiest to obtain but lacks target location. Fig. 1(a)-(c) illustrates more refined schemes. Points and scribbles offer limited target and background information, suitable for interactive segmentation but less flexible in automated scenarios. Bounding boxes are the most informative, but have limitations. The area outside the box is definitively background, while the inside contains target and background, causing ambiguity for segmentation models, especially in medical images with blurred boundaries. Hence, radiologists must create tight boxes, especially for non-convex lesion shapes like crescents, where much of the boxes cover background pixels.
Q4: Single dataset vs methodâs generalization ability. (R1)
A4: We have tested our model on a multi-center clinical dataset, demonstrating its generalization. Besides, we are committed to making our dataset public, which will strengthen our research and contribute to advancements in multi-lesion segmentation.
Q5: Explanation of TIA. (R3)
A5: TIA involves annotating just one random lesion per slice (Fig. 1(f)), improving target boundary accuracy with low labeling costs. We investigated the impact of lesion labeling location on segmentation results using four annotation strategies: random, maximum, minimum, and salient target. Segmentation results from these strategies (Tab. 1 in the supplementary material) were quite similar, indicating that as long as the annotated lesion is complete, the result has minimal impact.
Q6: Adding new weak annotating ideas. (R4)
A6: Drag&Drop [1] was released on February 20, 2024, and MICCAI2024 closed registration on February 22. We apologize for unable to analyze this work in time. Literature [2] used image-level weak annotations. We have introduced this annotation in the introduction.
[1] Y. Chou, Acquiring Weak Annotationâ¦
[2] Z. Chen, C-CAMâ¦
Q7: The texture of each lesion area. (R4)
A7: 1) Different scales will not affect the texture distribution of lesions. Fig. 1 in the supplementary material shows the overall distribution of the fitted texture for different labeled lesions within a slice. 2) A patient must be in the same period, and the normal intestinal texture is quite different in different periods [3].
[3] Meng J, Intestinal fibrosis classification,â¦ ER
Q8: Visualization. (R4)
A8:  We acknowledge comment for further visualization of semantic feature distance and consider it for future experiments.
Q9: Cost-accuracy trade-off. (R4)
A9: Tab. 2 in the supplementary material showed the DSC and mIOU of different annotations and our designed annotation strategy. We will further consider the annotation time for future experiments."
https://papers.miccai.org/miccai-2024/038-Paper1674.html,"Q1: Performance Improvements (R1, R4)
A1: In the research realm of using text prompts for segmentation improvement, AT (MICCAI 2023) currently stands as the SOTA method on QaTa-COV19, surpassing previous methods by 3-5% in Dice. Compared with AT, our method further boosts Dice/MIoU by 1.25%/2.08%, with the Dice score (91.03%) exceeding 90% on this dataset for the first time. We also conducted paired t-tests to verify the significance of our improvements. Results indicate that p-values on three metrics are all less than 0.05, demonstrating the statistical significance of our method.

Q2: Evaluation only on QaTa-COV19 (R4)
A2: To ensure a fair comparison, we followed the same setup of AT, conducting experiments solely on QaTa-COV19. In the future, we plan to extend experiments to more datasets to further validate the effectiveness of our method.

Q3: Parameters Setting: gamma, token lengths, channel dimension, and Pretraining (R1, R3, R5)
A3: In our experiments, we explored various gamma values ranging from 0 to 1.0 with a step of 0.2, finding 0.4 yields optimal results across three metrics. We also tested the dynamic adjusting strategy suggested by R5, but did not observe performance gains compared to using a fixed value of 0.4. As for token lengths, we referred to LViT and set three candidate values (36, 24, and 18) for M1, and explored two descending ratios to determine M2 and M3: an arithmetic progression (1, 3/4, 2/4) and a geometric progression (1, 1/2, 1/4). We found that the arithmetic progression with M1 set at 24 is optimal. Aligned with AT, the text encoder (CXR BERT) is pretrained and fixed, while the image encoder (ConvNeXt-Tiny) is trained from scratch with its output dimension C1 set to 768. We will add these details in the final paper.

Q4: Unreasonableness of directly concatenating text and image tokens (R3)
A4: In fact, our method does not directly concatenate text and image tokens. Instead, we concatenate the cross-attention map with the interacted image feature (tokens) along the channel dimension. This aims to incorporate text-prior information to enhance representations.

Q5: Token pruning and fusion (R3, R4)
A5: For pruning, we sort tokens based on their attention values, which were obtained by averaging the cross-attention map from the image branch along the last dimension. For fusion, we have already conducted an ablation experiment without the fusion of unimportant tokens, as shown by Model-D in Table 2. Experimental results show that without the fusion of such tokens, the performance drops by 0.3%. We also tried average fusion and observed a slight decline compared to our weighted fusion. The above analysis verifies the effectiveness of our fusion strategy.

Q6: The necessity of bilinear interpolation for text-prior predictions (R3)
A6: Our auxiliary loss involves text-prior predictions with multiple spatial scales, necessitating their expansion to align with the scale of the ground truth. Therefore, we employ bilinear interpolation for this alignment purpose.

Q7: Clarification of MLP and cascaded projector (R1)
A7: Sorry for the confusion. MLP stands for multi-layer perceptron. The cascaded projector is a custom module constructed by multiple MLPs hierarchically, which aims to align the channel dimensions of text tokens with those of image tokens at each scale.

Q8: Typos of Equation number, Definition of QKV, small Fig.1, and Review ref. 26 (R1, R4, R5)
A8: Sorry for the typos. We will fix all the typos in the final version. Also, we will define QKV at their first usage and review Ref. 26 in the introduction. Fig. 1 will be enlarged for better clarity.

Q9: Limitation: Reliance on paired data (R4)
A9: We agree with the reviewer that such text-guided works often require paired data. However, radiologists typically provide medical reports for each patient in practice, naturally pairing images with corresponding texts. Additionally, these texts can be also obtained by medical report captioning methods."
https://papers.miccai.org/miccai-2024/039-Paper2570.html,"We sincerely thank all Reviewers for their constructive comments, especially the weak accept with very confident from R1 and R2. We will release the source code and model once the paper is accepted. Below please find the responses to the main concerns.

R1Q1: Unclear description of global effects caused by FFT? Why were FC networks rarely used to boost performance? Does FFT have a closer relationship with MRI data?
1)The Fourier transform decomposes an image into a sum of sinusoidal waves with various frequencies. Altering a single pixel in the Fourier domain affects the entire image once transformed back into the spatial domain, resulting in a global effect. Similarly, performing convolution operations in the Fourier domain influences the entire image, effectively providing an image-sized receptive field.
2)Although FC networks can capture global features due to their dense connections, they are limited by high computational costs and poor generalizability. The number of parameters in FC networks scales quadratically with the feature size, making them especially computationally expensive for high-resolution features. Furthermore, FC networks lack flexibility regarding feature size. In contrast, utilizing FFT offers a more computationally efficient and flexible alternative to capture global features.
3)The use of FFT to achieve global effects in natural image reconstruction is also a popular topic, as demonstrated by works like âFocal Frequency Loss for Image Reconstruction and Synthesis (ICCVâ21)â and âCatch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder (CVPRâ23)â. Furthermore, FFT has a closer relationship with MRI data, as MRI signals are typically acquired in k-space and then converted into images via inverse FFT. Therefore, using frequency signals in MRI data aligns with the inherent characteristics of the data.

R1Q2: how to perform IFFT with amplitude and phase components.
We first compute the real and imaginary parts from the amplitude and phase components via real = ampcos(pha) and imag = ampsin(pha), then perform IFFT to obtain the image = torch.abs(torch.fft.irfft2(torch.complex(real, imag))).

R1Q3-Q5: Unclear presentations.
The function f(.) in Eq. (5) represents several convolution layers. In the FS-fusion module, the output shown in Fig. 1(c) serves as the output of the spatial branch, while the output of the frequency branch remains unchanged from its input. We will clarify these details in the revised version.

R2Q1: Under-sampling masks.
We follow the official implementations of the fastMRI dataset. For the 4x and 8x AF, the central 10% and 4% of k-space lines are fully sampled, respectively. The remaining lines are sampled randomly for 4x and equidistantly for 8x AF.

R2Q3: Fairness of ablation study.
For the variants w/o the CMS-fusion or FS-fusion module, we added several convolution layers after feature summation to ensure a similar number of parameters in the ablation study.

R3Q1: Novelty and advantages of this work.
We do not claim the introduction of MCMR as our contribution but rather propose a novel method to address the MCMR problem. Compared to existing studies in MCMR, we are the first to utilize the Fourier transform to efficiently capture global dependencies and propose the novel CMS-fusion and FS-fusion modules to integrate information across different modalities and domains.

R3Q2: Discussion of computational complexity.
It is well known that the computational complexity of transformer layers is significantly higher than that of convolutional layers. We apply convolutional layers in the Fourier domain to extract global properties. The variant that replaces the frequency branch with transformer layers has 819.2M parameters and 675.1G FLOPs, compared to our method with 14.1M parameters and 147.6G FLOPs.

R3Q4&Q6: Explanation for Eq. (7) and qualitative results on the fastMRI dataset.
More explanations and qualitative results will be provided."
https://papers.miccai.org/miccai-2024/040-Paper2133.html,N/A
https://papers.miccai.org/miccai-2024/041-Paper3065.html,N/A
https://papers.miccai.org/miccai-2024/042-Paper2216.html,"â- A clearer statement of the goal of each dataset evaluation should be provided. For example, in the clinical dataset, why did the authors did not replicate the annotations (scoring system) as the method suggested.â For the public dataset, we evaluated our Acne AI framework for two different tasks : overall severity score of the image, and detection of acne lesions. In the clinical dataset, the acne lesions were labeled (detection) using only two classes : inflammatory lesions and non-inflammatory lesions, whereas our classification system is designed to be more precise with more than seven types of acne. 
â- The evaluation of the AcneAI system should not be confined to the Acne04 dataset alone, as this may not accurately reflect the range of acne presentations seen in real-world clinical settings.â Yes, we agree, and that is why we evaluated also our system on a clinical dataset (section 3.1).

The minor changes will be adressed for the camera-ready version of the paper."
https://papers.miccai.org/miccai-2024/043-Paper2040.html,"Thanks to all reviewers for their comments. We found your points valid and constructive, and we are grateful to you for helping us improve this work. Here we address your comments:
1) Comparison with Graph Neural Networks (GNNs) (reviewer #5):
We compared our method with GNNs under the conventional settings for all five datasets. GNNs had the worst average performance (like MLP): Neuromod 40%, AOMIC 36.6%, Forrest 24.5%, BOLD 30.2%, RSVP-IBC 27.2%.
We suspect that this is because such neural network approaches are particularly data-hungry. However, our focus here is on scarce data conditions (82 trials per class max), and it is these conditions that remain of interest for most researchers simply because not everyone can acquire several hours of fMRI data with hundreds of subjects. Under these conditions, the linear classifiers are still the SOTA. Previous GNN studies like Li et al 2023 were not operating under scarce data.
Further, as mentioned briefly in the discussion section, the proposed ensemble framework allows to stack GNNs trained on individual subjects and might improve its performance (like it did with MLP).
2) Clarity:
a) Motivation behind the selection of models (reviewer #3):
We include various model families all with distinct inductive biases. Linear SVC represents the linear classifiers that are still good enough for such classification tasks with scarce data availability. Random Forest represents the ensembling methods and MLP represents neural networks.
b)The benefit of the study (reviewer #3):
We have already mentioned the usefulness of this method in BCI applications in the discussion section. However, this method could also be useful in both basic clinical and cognitive research, where a given disorder or cognitive domain could not be decoded accurately due to small sample size or low signal-to-noise ratio in conventional settings.
c)Data splitting procedure (reviewer #4):
This could be rephrased to - âWithin each dataset, we kept 90% of data for training and 10% for testing. We varied the size of the training set over 10 geometrically increasing sub-samples of that initial 90% training split and always tested the trained model on the same 10% testing split.â
d) Predicted labels (reviewer #4):
Neuromod â body / face / place / tools images
AOMIC â negative / neutral emotion images and cue for negative / neutral emotion images
Forrest â ambient / country / metal / rocknroll / symphonic music
BOLD5000 â animal / artifact / food / plant images
RSVP-IBC â type of text: jabberwocky / complex / simple / word list / pseudoword list / consonant strings
3) Cross-dataset generalizability (reviewer #5):
We had this in mind but were constrained because of the lack of availability of good-quality data where two different subject pools performed the same tasks. We touched upon this point briefly in the discussion section and agree that it would be interesting to investigate this in future work.
4) Hyperparameters of models, hardware/software specs (reviewers #3 and #4):
We used the default scikit-learn parameters for all models with a few minor changes:
MLP: 100 hidden layers, ReLU activation, ADAM solver, 1000 iterations
Random Forest: 500 trees, and for the maximum depth of a tree, the nodes are expanded until all leaves are pure or until all leaves contain less than 2 samples
Linear SVC: L2 penalty, the loss is square hinge, and the algorithm to solve dual optimization is set to automatic
Software: Python v3.11.4, Scikit-Learn v1.3.0, Nilearn v0.10.4, Numpy v1.25.2 and Scipy v1.11.1
Hardware: CPU-based cluster with 72 CPUs and 376 GB of RAM
5) Reproducible code and data links (all reviewers):
We did not provide the GitHub repo because that would have conflicted with double-blind reviews. We will make sure to provide all the details for the âcamera-readyâ version for final submission. All the datasets we used are publicly available and are accompanied by appropriate citations wherein the access information can be found."
https://papers.miccai.org/miccai-2024/044-Paper3963.html,"We thank all the reviewers for their constructive feedback. The reviewers find our paper novel (R3, R4), addressing a pertinent and challenging problem (R1, R4), clinically feasible (R4), complete (R1), and consistently better than the baselines (R1). We address some concerns of the reviewers below:

i) Novelty (R1): We respectfully argue that our method does make novel contributions. As R3 and R4 noted, this is the first work to employ VOG in an active label-cleaning setup for a noisy imbalanced dataset. [1] and [20] presented studies on VOG, but didnât use VOG for the LNL approach and, most importantly, they are out of scope for our setting.

ii) Motivation of VOG (R1): We emphasize that our method is well-motivated in Section 2.3, par. 1 and 2, detailing why VOG is crucial as a regularizer for imbalanced datasets. As explained in â â¦ underrepresented samples tend to exhibit high loss values because training is dominated by overrepresented samples, leading to their likely mis-selection as noisy samples â¦to avoid any potential bias, VOG estimates the change in gradients over epochs rather than makingâ¦â

iii) Improved Organization (R1): We decided to move some content from Section 2 to the introduction to ensure better clarity up front.

iv) Comparison with Recent Techniques (R1, R4): We agree that Co-teaching and CoreSet on their own may not be novel LNL and active learning approaches, respectively. However, our contribution lies in a unique solution that combines these two complementary approaches (LNL and active learning) for active label cleaning. Our approach is not directly comparable to only LNL or active learning methods; the closest state-of-the-art method for comparison is [2], which we have chosen as a baseline. We highlight that our approach is modular and can be replaced with any improved clean sample selection and active sampling function, which could further improve performance.

v) Pseudocode (R1): We agree that pseudocode could enhance understanding. However, we use text, equations, Fig. 1, and implementation details to explain our method, as the formatting space constraints hinder the inclusion of pseudocode.

vi) Grammar (R1, R3): We will address these in camera-ready version. We will incorporate the suggestion of R3 in the figure for better quality and organization, i.e., only use a single ânoisy labelâ block.

vii) F1-score (R4): Please note that we computed the macro-average F1-score (see italic text on page 6), which essentially averages the per-class F1-scores to capture class imbalance. Balanced accuracy, macro-F1 score, and MCC are typical metrics used to evaluate imbalance classification. We computed these metrics but reported only macro-F1 score on the graph for brevity and to avoid cluttering. Additionally, in Sec 4.2, we analyzed three extreme classes and showed that VOG approach doesnât ignore extremely under-sampled classes, as indicated by the recall and correct guess percentages.

viii) Assumption about Noise (R4): Without loss of generality, we followed common standard protocols in LNL. For reference, we kindly point you to [2], which also uses uniform noise distribution. We experimented with different rates within the uniform distribution (0.4 & 0.5 for ISIC and 0.7 & 0.8 for LT-NCT-CRC-HE-100K) to cover various noise ranges. While we agree about the importance of other noise distributions, we had to limit the study due to page constraints. We will investigate other forms of noise in future work.

ix) Varying class samples (R4): We will investigate with the varying quantities of pathological images as an additional ablation study in our extended future work.

x) Discussion section (R1, R4): Thank you for the suggestion regarding a more detailed discussion. We unfortunately had to limit our discussion to the Results section due to the original page constraints, but we will elaborate it into a stand-alone Discussion section in the camera-ready version given the additional space allowed."
https://papers.miccai.org/miccai-2024/045-Paper3895.html,"We thank the reviewers for their insightful comments. We are encouraged by the overall positive comments, e.g., important/interesting problem to medical AI community (R1/3/4), well written/excellent clarity (R1/4), thorough design/experiments/ablation (R1/4). There are also some conflicting reviews from R3, for which we will focus on clarifications. Our code will be published on GitHub.

R1
[adaptive moduleâs function] R1 is correct. The adaptive module brings an image representation x closer to a concept vector t if the image contains the concept, otherwise x is pushed toward -t. We will highlight this.

[doctor labelled concepts] Doctor/GPT (k=10) concepts are worded differently but have close semantic meanings, i.e., we found they have 0.66 to 0.72 mean cosine similarity for HAM, BCCD and DR datasets.

[text embedding ablation insight] The datasets do not contain text to meaningfully train the text embedding module, so it is not sensible to train text embedding to adapt the image characteristics. This is validated in Table 2-2.

R3
[novelty clarification] Our novelty lies in the re-examination of CBM as a linear classification system (LCS). This LCS characteristic is neglected in the CBM literature which instead focused on post-CBM modification using cosine similarity. Our work therefore is novel and important in demonstrating that the pre-CBM adaptation is the key to improving model classification performance while keeping explainability.

[lack of experiments/comparison, compare to fuzzy logic] We extensively compared with 6 non-interpretable/interpretable methods on 3 datasets and demonstrated 6 different ablations. Please see fine-grained comparison to [14, 23] (SOTAs, 2023) on # of concepts per class, concept selection, and CBM module in Suppl. Table 1 (Right). We compare to complex and SOTA Transformer (ViT, BioMedCLIP, etc) and ResNet architectures in Suppl. Table 2-3. Fuzzy logic systems implement rule-based logic handcrafted by the experts to encode human reasoning whereas CBMs are learning-based systems that endow semantic meanings to neural network decision logic. The different objectives make them incomparable.

[advantage to existing explainable methods clarification] AdaCBMâs advantage is its superior performance that closes the performance gap of CBMs to non-explainable classifiers. Whereas, for CBMs, lower classification performance is a significant issue as it inevitably harms user trust even if the model is explainable.

[AdaCBM interpretation and usage clarification] AdaCBM explainability is achieved by the quantifiable (and hence rankable) concept contributions toward the model outcome (see Fig. 2). Users can judge and intervene model decision by removing inappropriate text concepts.

[other similarities] CBM explainability relies on CLIPâs vision and language pretraining. Using another similarity needs to retrain CLIP but CLIPâs training datasets are not publicly available and out of the scope of our study.

[Fig.3-b AdaCBM not stable] R3 might have mistaken LaBo (blue lines) for AdaCBM (orange). The instability of LaBo is a validation frequency artifact (400 probes in 10K epochs vs AdaCBMâs 12 probes in 300 epochs).

[other issues] Paper format: we strictly used the MICCAI tex template without any change. Confidence = class probability.

R4
[replace linear classification system (LCS) with MLP] An MLP extending above the LCS will make decision logic nonlinear and hence not interpretable. One may extend MLP layers below the LCS, which is equivalent to our #-of-layer ablation in Table 2-3.

[out-of-domain evaluation] We only fed the class labels to GPT-3/4 and CLIPâs training data do not contain the evaluated datasets, hence our results are out-of-domain evaluation.

[domain specific LVLMs (Med-LLaVA)] We are unable to report on Med-LLaVA due to the rebuttal policy but will include in future work. Please see Suppl. Table 2-3, BioMedCLIP is Med-LLaVAâs image backbone, but it performs lower than CLIP."
https://papers.miccai.org/miccai-2024/046-Paper2585.html,"First of all, I would like to express my gratitude to you for your insightful comments and suggestions on this paper, which were delivered in a constructive and objective manner. I will provide a preliminary response to some of the review comments and make the necessary modifications to the paper in the subsequent camera-ready version.
For reviewer 1:
Thank you very much for your review. You have given us an objective evaluation and score on our work. We have noticed that your review content does not mention questions and doubts about our work. If you have any further questions that have not been recorded in the system or if you have any other questions, please do not hesitate to contact us at any time. We are happy to continue to answer any questions you may have.
For reviewer 3:
First of all, thank you for your review, you have raised a weakness in our article, as for âdifferent classes there are different number of images were generated (even if we count training+generated), there are few classes which has the same number of total training images others have more.â Your consideration is thoughtful. We set a different amount of generation for some classes in the datasets, because the amount of training data for these classes in the datasets is too different, which leads to a relatively low performance if a very strict setting method is adopted. We will add a discussion on this setting in Chapter 3.1 of the paper in the camera-ready version.
Other comments raised by reviewers will be modified in the camera-ready version. Bold line will not appear in Table 2. We will adjust the order of the datasets in Table 1 to match the order in Table 2-3-4. The âbccâ class from HAM10000 will be discussed separately in the discussion on page 7. We are grateful for your comments and suggestions.
For reviewer 4:
Thank you for your comments and suggestions. You have raised two weakness in our article. First you asked whether the MAGE and Adapter modules were simply combined. In fact, the method you have mentioned is merely a baseline for our proposed method, which can provide the function of image generation. However, there is still room for improvement under the condition of few-shot setting. Therefore, we have also proposed a quantified loss to constrain the intermediate features. The results of the experiment presented in Table 5 demonstrate the efficacy of the introduced VQ loss. This constitutes our complete method. We hope that this response addresses your queries.
The second identified weakness is the desire for further analysis to ensure the promotion is meaningful. First of all, in Table 5, we employ 5-fold cross-validation for each column, with the median serving as the final result. In addition to the aforementioned analysis, we have also conducted experiments with the objective of enhancing the classification performance of our method with respect to a number of specific sample classes. The experiment tested the classification results of each subcategory and found that the performance improvement after augmentation was significant in some few sample classes. For example, for HAM10000 dataset with 7 classes (â BKL â, ânvâ, âdfâ, âMELâ, âvascâ, âBCCâ, âakiec), the classification accuracy before augment (%) are 37.2, 31.2, 47.5, 52.3, 39.8, 92.2, 28.6, while the classification accuracy after augment (%) are 67.4, 43.0, 43.3, 75.0, 36.3, 90.9, 65.7. It can be observed that the improvement in the final classification performance of our method is largely attributable to the notable enhancement in the classification performance observed in several categories with small sample sizes. This growth is meaningful. We hope that this experiment will provide the answers you seek. We are grateful for your valuable comments."
https://papers.miccai.org/miccai-2024/047-Paper1423.html,"Reviewer 1: Thanks for the insightful comments.
1: Lacks discussion on differences from self-paced learning. 
R: We will include the discussion about self-paced learning in the final version.

2: A comparative analysis of the proposed AL approach with different pretrained encoders would be helpful.
R: Currently, we used a widely-used lightweight U-Net, with pretrained weights from ImageNet to facilitate faster interaction with annotators. Using self-supervised pretrained encoders can be investigated as future work, which we will discuss in the final version.

3: Lacks discussion of inter-observer variability.
R: We recognize that inter-observer variability relates more to inexact supervision and handling noisy label than reducing labeling costs - the primary goal of our paper. However, it is an interesting point for future exploring.

Reviewer 3: Thanks for the insightful comments and the recognition of the potential influence of our paper. The suggestion to include an additional baseline and explore other distance metrics provide further insights to boost our work.

Reviewer 4: Thanks for the detailed comments. But we have some different opinions in some points.
1: Missing some references.
R: Given the space constraints, we mainly focus on reviewing SOTA AL methods in medical areas rather than all AL methods in general.

2: Previous works on adapting AL strategy have not been cited. Thus, this paper lacks novelty.
R: We believe our paper differs significantly from the papers mentioned in the comments in terms of novelty, motivation, tasks, and approach. Our paper aims to adaptively switch AL query strategies during the learning phase based on the learning pattern of the DL model (e.g., querying easy examples first and harder ones later). In contrast, the paper Hacohen et al. (2023) focuses on selecting a proper AL method from a set of candidates for a given dataset based on different budgets for each dataset. This is more similar to AutoML method and is not a scenario considered in our domain of medical imaging. Paper Zhang et al. (2023) addresses how to automatically select an appropriate AL method from a set of candidates for imbalanced settings. Again the focus on imbalance and not a scenario applicable to us. So our novelty is completely different from these works.

3: Cannot find a fixed combination that always performs best from the experiments.
R: It is impractical to use a single combination of AL strategies to fit all datasets. It is incorrect, therefore to conclude from Table 1 that ACAL is sub-optimal in practice. Moreover, our studies on various datasets and base methods show good generalizability, suggesting that it would yield good results when applied in practice.

4: Unclear if the optimal combination was determined from validation or test results.
R: In Table 1, the optimal combination selection is based on test results.

5: Not all ACAL consistently outperform other AL methods.
R: ACAL allows the use of different AL methods as our base query strategy. We present six combinations to show how ACAL can improve the performance of different base methods. As noted in reviewerâs comment, âACAL with a random diversity-based strategy is worse or equal to random sampling 2 to 4 times.â This is because the base method is too weak, as shown in Table 1. Comparing ACAL with these weak strategies and the strategies alone shows a significant performance increase (e.g., LC only: 0.7, ACAL (Rand|LC): 0.860). ACAL with stronger base strategies always performs best under different labeling percentages.

6: Faster convergence claim is unclear, especially compared to random sampling in Fig 2.
R: In the early training stages, ACAL employs a diversity-based strategy (e.g., Rand), leading to similar performance to random sampling. Its advantage becomes evident later when it shifts to an uncertainty-based strategy, targeting challenging samples for quicker convergence and better performance (e.g., behind the dotted line in Fig 2)."
https://papers.miccai.org/miccai-2024/048-Paper3846.html,"We sincerely thank the reviewers for their thoughtful feedback and constructive suggestions.
R1: Why ASAU derivative provides benefits to the training?
Because they keep the gradient stable and differentiable, i.e., essential for backpropagation algorithms that use gradient descent. It also helps prevent large oscillations during training, making the convergence towards the minimum of the loss function more effective.
R1, R2: Missing comparison with other smooth functions (GELU or SiLU, LEAF). 
Due to space limitations, we did not put every available activation function result as a comparison, but please note that we have rich results presented already. It is also important to note that ASAU is derived from a family of maximum functions, and it has better results than other SOTA functions, including GELU, SiLU, LEAF, SMU, and Mish. This year, MICCAI does not allow the addition of new experiments in the rebuttal stage; therefore, the journal version of this article will include more applications and more comparisons.
R1: Missing discussions on ASAU performance on a different not-radiological use case.
We will include a few discussion points to clarify ASAUâs generalized role in analyzing both radiologic and non-radiologic examples.
R2: Applications (multiclass classification and segmentation) are not clearly explained, and the literature review should be updated with comprehension.
We will include a few more recent activation functions to address reviewersâ concerns in the related works. Further, we will paraphrase the application motivation sentences to make it clear that CAD systems require both classification and segmentation to be performed. We chose liver segmentation as a task due to its clinical relevance in cancer and chronic liver disease identification. Multiclass disease classification (with dozens of potential diseases) further emphasizes the improved learning facilitated by smooth activation functions, even in complex scenarios. This paves the way for developing more generic CAD systems in the near future.
R2: The claim of activation function susceptible to information loss in regions with negative inputs was very important, however, no references and further explanations were provided.
In medical images (such as MRI/CT scans), negative inputs can hold important information (tissue types, background noise). Â Non-smooth activation functions (ReLU family) can eliminate this data, hindering tissue differentiation and noise handling. The model might not learn to differentiate those tissues or properly account for background noise, potentially leading to inaccurate analysis. While pre-processing techniques exist (scaling/mirroring), theyâre suboptimal and limited. Our focus on smooth activation functions offers a more robust solution, providing valuable insights into learning at the neuron level. Weâll update the text to address this concern.
R2: What was the definition of C(K) in Proposition 1?
C(K) is the space of all continuous functions in the Real line. 
R3: In Figure 1, what is the c parameter for the ASAU plots? No c in equation 5. 
We apologize for this typo. âcâ is âbetaâ in equation 5. We will correct Figure 1 with beta.
R3: Regarding the hyperparameters a, b, alpha, and beta, how are they set? Similarly, for the leaky ReLU, what was alpha set to?
In our experiment, we set a=0.01 and b=1.0 (trainable parameters/hyperparams). For ReLU, alpha was set to 1.0. We will better clarify these parameters in the text. 
R3: Page 4: In (eqn 6), an (eqn 7), the alpha and beta factors are missing in equations. Were they omitted for a reason?
We presented the derivatives when alpha =1.0 and beta=1.0; that was the reason we dropped them in eqs 6 and 7. If suggested, we can include a more generalized form of the derivatives.
R3: Minor: tables with inconsistent font size; MCC is not defined.
We will fix the size of the table fonts; thank you for your attention. MCC stands for Matthews Correlation Coefficient. We will update it."
https://papers.miccai.org/miccai-2024/049-Paper3688.html,"Comment 1 (Reviewer 4): The method lacks sufficient novelty.
To begin with, SuStaIn is a pioneering, well-published model to capture spatiotemporal heterogeneity from cross-sectional data. However, SuStaIn has a fundamental limitation: it assumes the same progression rate for all subtypes, which underfits the data that incorporate potential subtypes with different progression rates. Therefore, we introduce dynamic and subtype-specific events to capture this important feature from the data. In particular, SSED is able to spot atypical subtype trajectories that have large differences in progression rates across ROIs, while SuStaIn canât spot such trajectories due to fixed events that constrain trajectories from deviating much from a uniform progression rate across ROIs.

Comment 2 (Reviewer 1,6): When data is sparse, noisy, or inconsistent, the modelâs performance might be significantly compromised.
Both SuStaIn and SSED use soft assignment and calculate the probability that a data point belongs to a particular subtype. This reduces the impact of noisy and inconsistent data since one single outlier wonât drastically alter the trajectories. We can also optimize the value of sigma in equation (1) in section 1.3, which accommodates the noise level of the input data. To improve upon SuStaIn, SSED leverages flexible events to fit more complex underlying data structures and make efficient use of all available data even if data is sparse, revealing atypical subtypes with less representative data not easily discernible by SuStaIn.

Comment 3 (Reviewer 1,6): The results are hard to interpret for clinicians.
We show the effectiveness of SSED to address SuStaInâs limitations through simulation and real data experiments. The fourth plot in Figure 1 shows that SuStaIn is unable to fit trajectories as separated as the ground truth data. Furthermore, all trajectories will inevitably bend in order to converge to a common final stage. In other words, fast progressing ROIs at early stages are bound to slow down at later stages to compensate for the fixed amount of progression, which is not clinically reasonable. We can see the same problems from Figure 2. The color scale represents the z-scored SUVR values. From the SuStaIn output, the three subtypes look pretty similar, especially from stage 10. This indicates that the three subtypes are not well-separated, and the difference between subtypes may not be clinically significant. In contrast, SSED captures three distinct patterns of trajectories: occipital, temporal, and parietal dominant subtypes. In addition, SuStaInâs subtype 1 has the temporal lobe not progressing much until stage 4, but progressing so rapidly until stage 10 that its SUVR value exceeds occipital lobe with already much higher SUVR values from the start to stage 7, which is clearly not reasonable. By contrast, from SSED subtype 1, the relative difference in the amount of progression between temporal lobe and occipital lobe is maintained for its clinical validity.

Comment 4 (Reviewer 1,4,6): The submission lacks information on how to reproduce the findings.
We will make the code open source upon acceptance.

Comment 5 (Reviewer 4,6): The paper lacks discussion of parameter selection.
SuStaIn selects 4 parameters in its original paper: the number of subtypes is 3; the assumed noise level sigma in equation (1) along the trajectories is 1; the z-score events are set to be 1,2 and 3, and z-max is 5 for each ROI. As for SSED, we use the same number of subtypes and sigma for consistency. However, SSED doesnât set z-score events as one of the parameters, as the events keep adapting to enable tailored characterization of data from each subtype. We first run SuStaIn to initialize the trajectories according to the events described in section 1.4. Then, SSED uses the same strategy to update events for each subtype for each iteration. Since SSED is incomplete without any of the parameters, ablation studies should be unnecessary."
https://papers.miccai.org/miccai-2024/050-Paper0173.html,"We thank the reviewers for their comments and thoughtful suggestions. Your feedback has provided us with valuable insights to improve the manuscript.

For reviewer 1:
Q: The samples are masked by a threshold in Equation (3), which is inconsistent. And a mistake.
A: Thank you for your comments. You are correct that there is a mistake in the equation. The value w_i was typed incorrectly and should be l_i. Thus, w_i=1 â 1/lambda*l_i. In the hard training paradigm, only a few samples are fed into the training process, while in the soft training paradigm, all samples are included and assigned weights. The primary difference between these paradigms lies in the weights assigned to the samples. As shown in Equation (2), the weight w_i is binarized into 0 or 1. In contrast, in Equation (3), the weight w_i takes on a continuous value. Additionally, we apply a threshold in Equation (3) to ensure that the weight does not fall below zero, as we threshold it to zero as a lower bound. Therefore, the equation remains consistent with the principles of the soft training paradigm.

Q: Why the proposed method is suitable for brain imaging task.
A: Deep learning methods for brain imaging often encounter high-dimensional features with limited samples, leading to overfitting and poor generalization. Therefore, an efficient learning paradigm is of practical value. Our study aims to address these challenges by proposing a novel approach to enhance model performance and robustness.

Q: Some experimental settings are missing. Fig. 1 should be revised.
A: Some important parameters are shown in the implementation part (12th line, 6th page), including alpha_w,0, alpha_fi,0, alpha_w and alpha_fi. The results in Table 1 were obtained using hard PCL and soft PCD training, as indicated in the sensitivity analysis. We apologize for the omission of this description and supply the description. Regarding Fig. 1, we will modify the number of samples in the figure to address the potential for misunderstanding.

Q: Novelty.
A: The integration is not a straightforward combination. Instead, it involves a carefully designed and innovative optimization process. Specifically, we introduce a novel self-paced strategy that enhances the modelâs ability to handle samples of varying difficulty levels more intelligently and efficiently, thereby improving overall learning performance. Furthermore, our approach demonstrates significant performance improvements, particularly in handling complex 3D datasets and enhancing model generalization capabilities. The experimental results highlight its effectiveness and practical value.

For reviewer 3 & 4:
Thank you for your astute comments. Regarding the parameter settings, there are two parameters for each curriculum setting (PCL and PCD), resulting in a total of four parameters. We believe that optimizing these four parameters is acceptable.
Additionally, we have validated these parameters using a grid search and found that two increasing rate parameters (alpha) do not significantly affect the results. Our method consistently outperforms others when these two parameters are set within a proper range, indicating that our method is not highly sensitive to the increasing parameters. For the other two parameters, i.e., the initial values of the pace parameters (lambda_0), all baselines also face the value selection issue. In this study, we determined these values by a grid search with a step of 0.1. To address this, we unified the selection rule of all baselines using the grid search. We would clarify this in the experimental settings in the revised version. And developing an automatic and robust initial value selection setting is still challenging for the paradigm of curriculum learning, which is one of our future works.

Others:
We would like to place a clearer visual example in the revised version for a better comparison."
https://papers.miccai.org/miccai-2024/051-Paper3227.html,N/A
https://papers.miccai.org/miccai-2024/052-Paper1045.html,N/A
https://papers.miccai.org/miccai-2024/053-Paper0165.html,"We thank the reviewers for the constructive feedback and for valuing our paper. To address the major weaknesses identified by the reviewers, we would like to clarify any possible misunderstandings as follows:

R3: Distinguishing real CXRs from limited bit-depth generated CXR.
We visualize the generated CXR by normalizing the modelâs output into an int8 image. However, since our model is trained in bf16, the model output can represent sufficient bit-depth to produce detailed CXRs. Furthermore, our goal is not necessarily to make real and generated CXRs indistinguishable. While future advancements in medical image generation technology might require creating CXRs realistic enough for clinical use where indistinguishability is crucial, our current intent is to use this technology for research and educational purposes, where such a requirement is not necessary.

R3: Reproducibility.
We have included our code in the supplementary materials and plan to make it publicly available. This will assist future researchers to fully grasp and build upon our work.

R3: Misunderstanding of MIMIC-CXR-JPG dataset version.
We utilize the MIMIC-CXR-JPG dataset for training and evaluating our model. This dataset is completely based on the latest version of the MIMIC-CXR database v2.0.0, offering JPG format files derived from DICOM images and structured labels extracted from free-text reports. Thus, we are employing the most up-to-date dataset available.

R3: Detailed explanation about multi-class AUROC.
As detailed in Tab.1(b) and Supp. Tab.1, we calculated the total AUROC by averaging the AUROCs from binary classifications for each of the 11 labels, which were determined by integrating the area under the ROC curve generated from model probabilities and ground truth labels.  This calculation follows the same evaluation metric as the baseline. Although sensitivity/specificity are not separately discussed due to space constraints, their values can be inferred from the AUROC presented.

R3: Details of Medical Expert Assessment.
âRadiology expertsâ with experience in CXR evaluated the images using three main criteria: Report Consistency, Image Completeness, and Factuality. Report Consistency measured alignment with the diagnostic report, Image Completeness assessed whether the image was fully displayed without cropping, and Factuality checked the imageâs resemblance to an actual CXR. Our model was compared and rated against existing baselines simultaneously and fairly on the same hardware/software configuration, ensuring impartiality.

R4: More information about baselines.
Due to space limitations, we were unable to describe the two recent state-of-the-art report-to-CXR generation models as baselines in detail.  First, RoentGen is a model fine-tuned on the MIMIC-CXR dataset using Stable Diffusion 1.5. The other baseline, LLM-CXR, generates CXRs from text-based radiology reports by instruction fine-tuning a pre-trained LLM with images tokenized using VQ-GAN.

R4: Ethical concerns.
Ethical concerns related to generated medical images are indeed crucial, and we fully understand and acknowledge the importance of addressing these issues comprehensively. Generated images are clearly marked for research or educational use only, not for clinical diagnosis. Conversely, when used solely for educational and experimental purposes, our generated images have the advantage of preventing patient privacy violations by not using actual patient images. However, we emphasize the need for strict marking to ensure these images are not used directly for diagnosis, and we are fully aware of this requirement.

R5: Hyperparameters for each feedback model in the final reward function. 
In the process of finding the hyperparameters for each feedback model, our focus was on ensuring that each feedback model provided balanced feedback without dominating the feedback from other models. In other words, we focused solely on scaling the values, simplifying the tuning process."
https://papers.miccai.org/miccai-2024/054-Paper2030.html,"Thank you for the valuable feedback on our manuscript. We appreciate the reviewersâ comments. We will carefully consider them in the final version of our paper to enhance its overall quality.

In particular, we will address the reviewersâ requests for additional details on our loss function and other areas mentioned. By incorporating these modifications, we aim to provide a more comprehensive presentation of our work."
https://papers.miccai.org/miccai-2024/055-Paper1749.html,"We thank all the reviewers for their valuable comments and advice. We appreciate the encouraging remarks such as âmotivation and application are very well definedâ (R1), âinteresting reconstructions are displayedâ (R3), and âgreat merit on novelty, writing and experimentsâ (R4). Our responses to the comments are as follows:
1.Clinical feasibility:
(R3) Accuracy: We understand your concern regarding accuracy. It is important to clarify that the bronchoscope and airway sizes given are measured radially (diameter-wise), whereas the depth errors presented are measured axially (depth-wise).Â Previous studies by Banach et al.[1] (Med. Image Anal.) reported an average MAE/RMSE of 7.8mm/10.6mm for depth estimation and a tracking error of 6.2mm for target localization, achieving success rates of 95%/86% in main/lobar bronchi. Our method was integrated into navigation systems, aiding pulmonologists in reaching up to 5th-generation nodules. The presented errors were found to be accurate enough for navigation while ensuring localization points stayed within radial boundaries, confirming our methodâs feasibility.
(R3) Speed: Acceptable speeds range from 20-30fps in bronchoscopic systems (Chang,SPIE,2023;Zang,IMIP,2023). Our method meets real-time demands for navigation, with inference/systematic speeds (for reference only: 57.2fps/26.3fps) feasible through clinical evaluation. Further speed results will be provided in our works on systematic integration.
2.Description of datasets:
(R1) Dataset creation: In the supplementary material, we visualized a three-step pipeline for generating image-depth pairs from virtual airways. We will enhance the details and release the code to support development in the field.
(R1) Choice of testing number: The 142 pairs were carefully registered with real images for quantitative evaluation. This number balances label quality and quantity, covering various in-airway locations with different shapes and scales.
3.Experiments:
(R4) Selection of comparison methods: We compared [6,15,33] (non-transfer models on natural scenes) to show the need for domain adaptation, and [1,18] (SOTA on endoscopic images) to show our superiority over clinical applications; these methods are more representative. Literature[3,20,27] was not chosen primarily due to their closed source.
(R1) Baseline confirming domain gap: Previous studies have highlighted the cross-domain gap[1,30]. In Tab.4, we compared three non-transfer baselines[6,15,33] trained on virtual images and test on real images, proving the necessity of domain adaptation.
(R3) Ablation study: The validation set includes virtual images, while the test set comprises only real images. The ablation study on test set is designed to ensure the methodâs robustness in real scenarios.
4.Clarity: 
(R4) Discussions of âdeclineâ: Compared to feature-level adaptive methods, the vanilla shows a 24.8mm decline in accuracy(Tab.1). Image-level adaptation is sensitive to domain differences, leading to incorrect translations[30]. In our case, perfectly real-like virtual images are challenging to generate due to limited rendering techniques in the field. Thus, our proposed model aims to learn domain-invariant features that are less affected by such visual disparities.
(R4) Latent diffusion: Latent diffusion encodes images into an information-dense space[24], recovering final images with higher level of details. We leveraged its condensed nature for refined depth generation at our resolution.
(R1, R4) Other details: Eq.1 represents a standard RMSE between GT&pred for pixel-wise loss. We cited the metrics[6,15] including accuracies and errors for readers to follow. We clarified the discussion of comparison against[1,18] and GT; the arrows highlighted these differences as described in Fig.3 caption. All the unclarities in Abs.&Intro. were carefully repolished to improve paperâs flow and clearness.
Once again, we express our gratitude to the reviewers and Area Chairs for their time and effort."
https://papers.miccai.org/miccai-2024/056-Paper0803.html,N/A
https://papers.miccai.org/miccai-2024/057-Paper3005.html,"We appreciate the encouraging comments like âinsightful analysis, novel centerline detection taskâ(R1), âvaluable for airway segmentationâ(R3), âdetailed experimental setupâ(R4) and âcomprehensive experimental resultsâ. Our responses to the questions are as follows.
1.(R1 R3)Novelty and motivation of topology preservation including Topology Enhanced Attention Module(TEAM)&Topology Embedded Interaction Module(TEIM) 
1)TEAM: The âProject&Exciteâ[9] in TEAM retains the spatial information of anatomical structures through feature recalibration. By merging low-level spatial and high-level semantic feature maps, TEAM is used to build cross-scale dependencies and enhance the networkâs response to various airway scales. Our previous paired T-test showed that Base-TEAM was significantly better than Base on DSC, BD and TD indicators. Theoretical analysis from [9] and our results(Table2) support its validity in segmenting small structures.
2)TEIM: Unlike the shared decoder strategy of Zhang et al.(BSPC,2023), we use the independent centerline detection decoder to ensure the networkâs learning ability for tubular topology. TEIM strengthens the topological interaction of dual branches through cross-channel feature fusion. Base-TEIM improves BD&TD by about 6% compared to Base. The previous paired T-test showed significant differences, demonstrating the effectiveness of our topology preservation method.
2.Introduction
1)(R1)âTopologyâ explanation: [11] showed that the centerline is the minimal point set representing the tubular topology. This inspired us to use it as prior knowledge to improve the networkâs topological awareness of the airway by introducing the centerline detection task and TEIM.
2)(R3)Structure of the introduction: Our method focuses on preserving airway continuity. The introduction reviews related work and finds that Ref[3-6] partly added topological priors but are still insufficient for bronchioles. We address this issue by introducing stronger anatomical priors. 3D-UNet serves as the backbone for [3-6], while the segmentation results of 3D-UNet typically exist the airway topology breakage problem (Fig.1). We then outline airway segmentation challenges and introduce our approach.
3.(R3)Comparison with clDice-loss
We only use the formula from Ref[12]. Different from R3âs view, our previous experiments showed that the Base-CLtask outperformed clDice-loss on all metrics, especially on BD&TD indicators. We found that the soft-skeletonization in [12] often results in broken centerlines, making it insufficient for accurate topology representation. Thus, we use the method in [14] instead to obtain an unbroken centerline, providing accurate ground truth(GT) for the auxiliary task.
4.(R1 R3 R4)Details of centerline GT
Lee et al.[14] proposed a robust method to extract centerline via 3D medial surface axis thinning algorithms. The method is widely used including Yao et al.(TMI,2023), Peng et al.(JBHI,2024). In our work, two doctors validated the accuracy of extracted results. Given the widespread adoption and expert validation, we think using the generated labels as GT is feasible.
5.(R3 R4)Dataset
The public BAS dataset, first used in [12], comprises 60 cases. In their subsequent work, Zheng et al.(TMI,2021) increased the number to 90 cases, which is used in most related work. We randomly divided the dataset after merging all 90 scans. For details about the dataset please refer to Zheng et al.(TMI,2021).
6.(R3 R4)Unclarity
We proofread Fig.2 and method variables for consistency. C Bold and S Bold are defined in the Fig.2 caption. We corrected the error in Eq5: the first-level TEAM inputs are E0 and E1, with subsequent levels using El and SEFl-1 (the previous levelâs TEAM output). We added details in the abstract, clarified the loss function in Fig.2, corrected the meta-parameters in Eq6&7, the error subplot in Fig.4, and syntax errors. We will carefully correct errors and release the code to ensure clarity and reproducibility."
https://papers.miccai.org/miccai-2024/058-Paper2017.html,"We thank all the reviewers for their detailed reviews and the insightful feedback on our manuscript, highlighting its ability to âgeneralize out of the boxâ, being âversatile enough to be of interest in a multitude of scenariosâ, providing a ânice comprehensive results comparisonâ, and having the potential to âmake a significant contribution to this fieldâ. Going forward, weâre going to address the concerns.

Application scenarios
(R1, R3, R5) {Can it be applied without modifications across scenarios, including other chambers, cine, multi-view inputs, and different diseases?} Our algorithm can be directly used without fine-tuning any parameters for other chambers, cine, multi-view inputs, and different disease morphologies. The input, being a point cloud and lacking explicit topological connectivity, allows our approach to adapt to various scenarios. We will do our best to include more detail.
(R5) {Can it deal with misaligned slices? Segmentation errors?} The misalignment in the slices originates from the imaging stage, while segmentation errors stem from the segmentation algorithm itself. Our future work will integrate methods for motion correction and segmentation quality control.

Experiment
(R1) {Comparing degradation rates under different resolution ratios} Thanks for having a consensus on the main challenge in this area. Graphics methods can reconstruct surfaces from sparse point clouds but struggle with the data in hand [page3]. Our method specifically tackles this issue. Additionally, cine imaging typically has a plane resolution of 8-10mm and in-plane resolution of <2mm [4, a]. Exp2 validates the performance under different resolution conditions, demonstrating the advantage of our algorithm in handling cine data. we will more thoroughly highlight this in a final version.
[a] Wang, C., Li, Y., Lv, J. et al. Recommendation for Cardiac Magnetic Resonance Imaging-Based Phenotypic Study: Imaging Part. Phenomics 1, 151â170 (2021).

Ablation studies
(R3, R5) {Lacking ablation experiments with Gaussian kernel function and comparative experiments with APSS} As mentioned in our Layered Adaptive Rational Kernel [page5], The Gaussian kernel is not capable of handling our data, which is used by APSS.
(R3, R5) {Lacking ablation experiments for normal estimation} Normal existing methods such as those in open3d/meshlab and [14, 15] produce failed results when directly estimating normals from point clouds. Therefore, we propose this module.
We acknowledge that the failure cases section is missing in the paper and will include these cases in the supplementary material.

Some implementation details
(R3) {Can the results of normal estimation be visualized} The visualization results of normal estimation can be seen in Fig2, represented by the blue small arrows on the point cloud. We will emphasize in the final version that these indicate the surface normals.
(R5) {Why use sphere fitting mesh instead of coarse mesh} The mesh obtained after normal estimation lacks proper topology.
(R5) {Why create a reference mesh instead of using the GT meshes of the dataset} MMWHS dataset does not provide GT meshes.
(R5) {No comparison to recent learnable methods in the introduction} These methods mentioned in the introduction differ from the problem we aim to address, and we have compared recently learnable method.
(R5) {How âcoarse-to-fineâ} We use DP for global normal consistency across adjacent layers, followed by smoothing to obtain adjusted normals.
(R5) {clarification of certain concepts} A âlayerâ in the point cloud represents a set of points belonging to a single SAX slice. Each pointâs nearest neighbors are within the same layer, which we called âhierarchicalâ.

(R1, R3, R5) We will do our best to include more detail. And we will release the code and test data on Github for reproducible research.
Finally, we want to thank the reviewers for their valuable feedback and hope we could clarify some uncertainties/ambiguities."
https://papers.miccai.org/miccai-2024/059-Paper2178.html,"[R1,3]We address skin lesion classification challenges due to data imbalance and limited diversity simultaneously, which is a gap in the literature. Existing approaches mitigate imbalance but donât adapt to diverse skin tones, making direct comparison difficult. Our approach can integrate with contrastive learning frameworks, which we plan to explore in future work.
[R1]To maintain consistency, we resized ISIC-2018 images to 100x100 pixels, matching the ASAN dataset, for consistency in analysis and comparison. ASAN has 12 classes, 5 of which are shared with ISIC-2018, experiments were done on both class sets.  After addressing imbalance, we obtained improvements of 1.63% for GM and 2.54% for Bacc for ASAN and improvements of 4.42% for GM and 3.49% for Bacc for ISIC, showing efficacy across different ethnic groups. Similarly, consistent patterns emerged when training on ISIC and testing on the same dataset or on ASAN with varying ethnicities.
[R1,4]Implementation detail for lambda is provided in the Preliminaries section. Lambda is sampled from the beta distribution B(Î±, Î±), with Î± set to 0.2 in the experiments.
[R3]The Fitzpatrick-17k dataset includes both pale and dark skin tones but cannot directly demonstrate our addressed problem due to mixed pale and darker skin images. Also, ISIC-2018 was used instead of ISIC-2019 since it has data from Argentina and Turkey (Wen et al., 2022), while we only want the caucasian sources of ISIC-2018 in order to contrast with non-white ASAN data.
[R1,3]When training with ASAN and testing on a combined test set of ASAN and ISIC, we found that our methodâs Eodd is 15.2%, nearly 3% lower than CBS and Mixup, which is approximately 18%. EOpp1 (the TPR difference for different subgroups) is 31.7% for our method compared to the nearest one, which is around 35% for CBS and focal loss. Additionally, the value of EOpp0 (the TNR difference) for our method, is 7.5%, closest to CBS at 7.8%. Comparing Bacc on the ISIC test set, our method is higher by 11% compared to CBS. These metrics demonstrate predictive fairness of the proposed method.
[R3]Unlike existing methods (CutMix, CopyPaste, MixUp), our adaptive sampler dynamically adjusts sampling probabilities based on the networkâs learning state and a meta-set, focusing on informative minority class examples and preventing overfitting. Instance sampling enhances minority class representations by leveraging context from majority class samples. Our heuristic-driven augmentation generates new training data, improving the modelâs understanding of minority samples and adjusting decision boundaries for different ethnicities, unlike the static nature of the 3 competitors. CutMix replaces parts of an image with patches from other images; this improves performance in standard image tasks like CIFAR & ImageNet (Yun et al, 2019)â. However, applying them to medical datasets like those for skin cancer detection yields less favourable results (Rao et al, 2023). Altering or removing image regions can lead to loss of diagnostic information, reducing the modelâs accuracy in identifying and classifying skin lesions.
[R4] ResNeXt-50, with its modular multi-branch architecture and improved cardinality over ResNet-50, was our primary model for simplifying image classification. We also experimented with EfficientNet. When training on the ASAN dataset, we observed an improvement of about 4% in Bacc compared to the closest methods, CBRW and Cbrt. Our Bacc was around 73%, compared to their 69%. When tested on the ISIC dataset, our Bacc was 42.3%, while the closest methods (CBS, Cbrt) achieved around 40.2%. Due to space limitations, detailed results and discussions on EfficientNet were omitted.  We will include them in the final manuscript. The blending acts as a form of regularisation, reducing overfitting and enabling the model to learn more robust features that are not specific to individual images but general across the dataset."
https://papers.miccai.org/miccai-2024/060-Paper1172.html,"To all:
We thank all for their valuable feedback. 
About open access and reproducibility: Due to anonymity requirements, we have not included source code links in our submission. The source code will be publicized and detailed algorithm descriptions will be provided if accepted.

To R1:
Q1: The statistical significance
A1: Thanks for your kind suggestion. We conducted our experiment on 100 image pairs and reported the average values. Box plots with p-values or confidence intervals will be incorporated later.
Q2: Descriptions of section prediction module
A2: Refer to Fig.2(a). I3 is the adjacent next section of the damaged section. 
Due to space limitations, we have not included further details. As an auxiliary part, this module follows the previous work STDIN (reference paper [12]), which is used to replace damaged regions by combining with the mosaic module and aid subsequent continuity reconstruction(Sec 3.2).

To R3:
Thanks for your comments. We will public source code to provide more details. Besides, we would like to first clarify technical soundness of our method and then show the validity of experimental evaluation as follows.

About Method
Q1: Eq. 1 and 2 may lead to a smooth deformation field
A1: The deformation field near tears or folds is not smooth actually. Considering this, we did not set di (in Eq. 2) as a linear distance directly passing through the fold/tear, which may lead to smoothness. Instead, we set di as a path distance that bypasses tears/folds. In our method, as an extreme case, if the tear/fold runs through the entire image, the path distance between the points on either side will be infinite. Then, the probability (in Eq. 2) that a point on one side of a tear/fold belongs to the other side will be 0. This reinforces that the deformation field near tears/folds is not smooth.
Q2: About epsilon (eq.2)
A2: Epsilon (e.g. 1e-10) is a constant defined to prevent Eq.2 from being meaningless when di=0. 
Q3: About segmenting image info fragments
A3: Refer to Sec 2.1. Image info fragments are categorized based on the deformations that different regions conform to. Specifically, we uniformly sample the coordinate points on the imperfect section and align them on the reference image. Points that conform to similar transformations are then grouped into the same category. 
Q4: About description of Sec.2.2
A4: As an auxiliary part, the section prediction module follows the previous method STDIN (reference paper [12]). We have not included more details due to space limitations but have cited the reference. 
STDIN [12] is designed to predict intermediate images from adjacent images and works well for EM images. We employed it to generate the missing data in damaged regions of imperfect sections by combining it with the mosaic module (Sec 2.3), which aims to help subsequent continuity reconstruction. Experiments in Sec 3.2 verify the effectiveness of adding this module.

About experiment
Q5: About ground truth
A5: While in the field of electron microscope image registration, the ground truth of the registration results is unobtainable. Therefore, comparing the registration result with the reference image (adjacent image) is a common practice. We measure the similarity between the registered image and the reference image using NCC and SSIM.
Besides, the deformation is real because the public datasets we used contain images of real folds/tears.
Q6: About Fig.3
A6: Refer to Sec 3.1. In Fig. 3, more artifacts in the visualization result and colors closer to blue in the heatmap indicate lower alignment accuracy. Our method, which shows fewer artifacts and less blue, demonstrates higher accuracy and effectiveness.

To R4:
Thanks for your high praise. Your suggestions are very constructive, and we will expand and modify them based on your suggestions in the future, including the largest crack/fold/starting-misalignment that can be handled, computational cost/scalability, how it works feasible on large-scale data, and so on."
https://papers.miccai.org/miccai-2024/061-Paper0117.html,N/A
https://papers.miccai.org/miccai-2024/062-Paper1358.html,"We appreciate all the reviewers for their valuable feedback. We have carefully addressed the issues and suggestions point by point, which will be elaborated on in the camera-ready version. Below we discuss some of the questions the reviewer raised. Abbreviations: Reviewer-R, Weakness-W, Constructive comments-C.

Visual Symptom Generator (VSG) (R1W1, R2W2). As written in the âVisual Symptom Generatorâ subsection, we explain that VSG consists of a text-only prompt to query the general feature of a disease and an image-text prompt to refine the visual symptoms based on the available data. This helps improve the accuracy of generated visual symptoms. For example, the generated visual symptom âparts of the spot look glossy and reflectiveâ in Fig. 1 is obviously not a visual feature of Fig. 3 (II)(d). By image-text prompt refinement, we discard this symptom from the description set generated by text-only prompt, resulting in more accurate descriptions.

Model diagnostic process (R2W1, R2C2). In Fig. 1, we first use an example of melanoma to demonstrate VSG is designed to convert disease labels to descriptive visual symptoms. Next, the generated visual symptoms are combined with context prompts (CoP), which will be used as input to the text encoder. The encoded text features are then aggregated using MeP. The aggregated feature in Fig. 1 corresponds to one of the s^c, and we present the visual symptom encoding process of melanoma for clarity. The final prediction is calculated based on the image feature and aggregated features of all classes (s^c_1, s^c_2â¦). We acknowledge that Fig. 1 might introduce confusion, and we will revise it accordingly.

Examples of learnable tokens (R3C7). CoP is designed to automatically learn the medical task context. We acknowledge that the learnable context tokens may appear abstract. There is several papers that explores the explainable prompt [1, 2], but it is beyond the scope of this work.

Importance of color features on CLIP learning (R2C6, R3C10). We appreciate the idea that the impressive performance boost observed in the Derm7pt dataset might be attributed to its similarity to natural image color pattern. However, the impact of different features on performance can vary across different tasks [3,4]. In future work, we will explore more datasets of different modalities to gain a deeper understanding of these factors.

[1] Visual-language prompt tuning with knowledge-guided context optimization. CVPR, 2023.
[2] MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level Image-Concept Alignment. AAAI, 2024.
[3] Artificial intelligence structural imaging techniques in visual pattern analysis and medical data understanding.Â Pattern Recognition, 2003.
[4] Local binary patterns variants as texture descriptors for medical image analysis. Artificial Intelligence in Medicine, 2010."
https://papers.miccai.org/miccai-2024/063-Paper0787.html,"We sincerely appreciate the reviewers for acknowledging our methodological contribution and providing constructive comments for further clarification. Our feedback is as follows.
Q1(R1) Limited CT dataset.
A1: We follow the approach of CTformer and Eformer by using the AAPM dataset, which includes approximately 400 images per patient, each with a resolution of 512Ã512. In the future, we plan to conduct our experiments on a larger CT dataset.

Q2(R3) Lack of related work.
A2: Routing techniques are predominantly associated with the field of dynamic neural networks [1]. These techniques often involve a routing network that dynamically selects network paths for the main network. Both SRM and CRM are variants of this approach. Our primary contribution is the introduction of a routing instruction network that generates task-relevant instructions and effectively applies SRM and CRM to address task interference in the all-in-one medical image restoration task. We will add this reference in our final submission. Additionally, the interference metric, DRMC, and AirNet will be addressed in the final submission.
[1] Han Y, Huang G, Song S, et al. Dynamic neural networks: A survey[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021, 44(11): 7436-7456.

Q3(R1) Statistical significance. 
A3: We conduct significance tests, and all our improvements over the comparison methods are statistically significant (p < 0.05).

Q4(R1) Computational complexity. 
A4: We measure computational complexity on 256Ã256 images. The computational complexity test results are as follows: AMIR: 127.0573 GFLOPs, Restormer: 141.5756 GFLOPs, and AirNet: 301.0390 GFLOPs. The inference time comparison results are: AMIR: 0.0882s, Restormer: 0.0721s, and AirNet: 0.1430s. These results demonstrate that AMIR has superior computational efficiency and a comparable inference speed to state-of-the-art approaches.

Q5(R3) Main results and figures in the supplement.
A5: I will move these results and figures to the main section of the paper in the final submission.

Q6(R3) Vague reference to supplement.
A6: We will revise this in the final submission.

Q7(R3) Figure 1 is not adequately mentioned and lacks caption. 
A7: I will revise this in the final submission.

Q8(R3) Only the training the best performing baseline for all-in-one task is unfair.
A8: I will retrain other baselines for all-in-one tasks in future studies.

Q9(R3): Concerns about the sampling strategy.
A9: We currently use a random sampling strategy. We will conduct ablation studies on different sampling strategies in our future research.

Q10(R4) Issues regarding I^{DF} and I^{RF}
A10: We will make a clear annotation of I^{DF} in the final submission. And we will make ablation studies on I^{RF} in further studies.

Q11(R4) Exploring more downscaling factors during training MRI SR. 
A11: It is a good suggestion and we will take it into consideration in our future work.

Q12(R4) Further experiments on task combinations. 
A12: We will explore our algorithm with different task combinations as more training tasks are included. Generally, one task may positively or negatively impact others based on their similarities and how the model handles their relationships. In our experiments, the CT denoising task has shown benefits to other tasks, but this requires further exploration.

Other issues will be addressed in our final submission."
https://papers.miccai.org/miccai-2024/064-Paper2801.html,"We thank the reviewers for their constructive feedback. We are delighted that the reviewers found the proposed dataset novel and recognized its diversity value by offering samples using African tissue slides acquired in a low-resource setting environment.
Dataset Value (R1, R3, R4): Since the annotated dataset is our major contribution, we acknowledge the reviewers concerns and will provide additional justification. Although the images were acquired using the 250X MF, it is important to note that by low-resource, we refer to the use of a digital microscope rather than a whole slide scanner. This makes our dataset the first AMONuSeg dataset, aiding the research community in adapting their models by studying the domain distribution with diverse population. We understand the importance of including the number of nuclei and will update our table accordingly. We chose to compare our dataset with the reported three public datasets due to similarity in terms of sample numbers. 
Additionally, the phrase âgeneralization across varied patient populations and pathology speciesâ will be modified by deleting the pathology species part. 
Data and Code (R1, R3, R4): The annotated dataset and modified Y-Net model will be made publicly available with ethical approval. Code for the methods we assessed is on the original papers GitHub repositories to support reproducibility and transparency.Annotation process (R3): We ensured that the annotation process was rigorous, as our main goal is to provide a well-annotated dataset that can benefit the research community. Two annotators were involved in this process: A data scientist (A1), and a postdoctoral researcher (A2) trained by an expert pathologist for nuclei cells annotations. Initially, the two annotators used the automatic solution FIJI in the software ImageJ to generate preliminary annotations of the tissue slides as a first step, providing a rough estimate of the nuclei locations and simplifying the initial annotation process. Due to the inaccuracy of the automatic annotations, manual ones were necessary by both annotators. A2 validated the annotations made by A1. To ensure the highest quality, three expert pathologists reviewed and validated the annotations, and a final validation was conducted by a fourth pathologist prior to the rebuttal to ensure that the annotations met the highest standards. Regarding Fig. 1, its purpose was to visually depict the difference between the automatic and manual annotations. 
Novelty & Analysis (R1, R3, R4): Our study focused on spectral feature and transformer-based segmentation models using the AMONUSEG dataset, excluding Cellpose and Stardist as they were previously compared [1]. We reported mean values from a three-fold cross-validation to ensure robustness, which is why standard deviations werenât provided. Our analysis focused on the Dice score for segmentation accuracy, the other used metric can be added in the appendix. We shared hyperparameters in sec. 4.1 and will add more details for reproducibility. We will revise our conclusion to ensure our findings are robust and well-supported. [1] 10.1016/j.cdev.2022.203806 
Stain Normalization (R4): Stain normalization is essential for removing staining variations and enhancing segmentation models generalizability in histology images. While the original dataset performs well without normalization, the experimentâs goal is to study the impact of different normalization techniques. We recognize the influence of parameters such as alpha and beta in the Macenko method, emphasizing optimization for optimal results. We propose moving these results to the appendix. This study initiates ongoing research on the effects of non-standardized staining protocols on model performance. Addressing these challenges is key for progress in histology image analysis and improving model adaptability. 
Minor Comments (R1, R3):We will revise the final version to correct typos and define any missing acronyms, ensuring clarity."
https://papers.miccai.org/miccai-2024/065-Paper3820.html,N/A
https://papers.miccai.org/miccai-2024/066-Paper1289.html,"We thank all the reviewers for their consensus on the value of this study and their valuable feedback. Below, we provide our responses to their comments.

More data information (R1, R2, R3):
The data used in this study was acquired from volunteers excluding major diseases over a span of 3 years (2019 - 2022). It was originally for studying the baseline statistics of multi-organ phenotypes. Later, we found this data useful for studying segmentation model fairness, which led to this work. In this dataset, a total of 1,056 volunteers participated, comprising:
Gender: 635 females and 421 males (a 6:4 ratio is considered normal in population study as females are generally more willing to volunteer). We will include the full population information in the revision.

Data being private (R2, R3):A major part of the data in training foundation models comes from the public. Using private data for model testing can ensure that the testing samples were never seen by the foundation model, thus ensuring an accurate assessment of their generalization capability and fairness. In the near future, we plan to set up a server system where researchers can upload their models and test them on our private data for fairness evaluation.

Object size & Dice coefficient (R1):
The Dice coefficient (DSC), by its design, does not inherently favor larger objects. For example, if both the ground truth object and the segmentation object are resized by the same factor (e.g., 0.5, half of the original size), the DSC score will remain unchanged. When comparing with objects of different shapes and topologies, the DSC might show a preference for one over another. However, this does not apply to our case, as the objects in our study are of the same type, shape, and topology but belong to different demographic groups. For instance, although females generally have smaller livers than males, the livers of both genders are consistent in shape and topology. Consequently, the DSC should introduce minimal to no bias when evaluating liver segmentation across genders. The main finding of this paper is that foundation models exhibit more fairness issues than specialized models. For this purpose, DSC is more than adequate for the organ types we studied.

Comparing to existing studies (R2): 
To our best knowledge, studying different BMI groups on segmentation performance is novel. Evaluating the fairness of emerging segmentation foundation models is unprecedented. Additionally, our study stands out by studying multiple organs and vessels, which is rarely addressed in existing research. Our findings corroborate previous studies indicating that UNet has fairness issues. Notably, our research reveals that foundation models exhibit more severe fairness problems compared to a specialized model.

On Pearson correlation (R3):
Indeed, Pearson correlation measures linear correlation, which can be more stringent than a gentler analysis. Following the suggestion of using ANOVA, we utilized ANOVA to re-evaluate the fairness of BMI and age in Table 2 and Table 3. We found that, most of the time, the significance (presented by p-values) obtained from Pearson correlation and ANOVA closely matched. We will include this information in the revision. As a side note, previous work utilizes Pearson correlation in fairness studies, e.g., Xu Z. et al., 2022.

BMI ranges (R1): Under Weight (<18.5), Healthy (18.5~24), Over Weight (ï¼24).

SAM and SAT were trained using a large collection of public datasets, most of which did not originally include demographic information. The original SAM generalizes better than Medical SAM on unseen data (i.e., the private data in this study). We will include references to papers that studied bias in UNet. We will place greater emphasis on the joint attribute analysis and distance measure visualization. Setups of foundation models and nnUNet training information will be further detailed in the revision. Code will be released to the public upon publication."
https://papers.miccai.org/miccai-2024/067-Paper1606.html,"We appreciate the thoughtful examination of our paper and valuable comments we have received. While R1/R4 considered the study valuable and interesting, R1/R6 point out the need for more detailed discussion of results. We plan to expand this discussion in the final paper along with the suggested improvements by R4/R1 (apologies for the missing annotation). Regarding R6âs comments, we would like to re-emphasize that the motivation for this study is to understand the differences between projectors in low-dose imaging conditions (and not to show that fewer views lead to larger condition numbers and RMSEs, which is expected from theory). To be specific: While existing studies in idealistic settings compare these projectors in forward error, our study shows that when the Beerâs law effects are taken into account, there are differences among projectors (Fig5) and these differences become more significant in few-view settings, where MBIR (instead of analytic methods) are actually used. Our study suggests that projectors with more accurate line integrals (compared to accurate areas) are more robust in these low-dose imaging conditions.

(R1/R6) Motivation For The Study, Difference from Previous Studies and Significance of Conclusions: (i) Existing studies of projectors (e.g., LTRI, SF, distance-driven) examine their performance in idealistic settings (i.e., linearized Beerâs law, noise free) where accuracy can be measured and projectors can be compared. But the nonlinear physics that is present in every real-world CT scan, as well as noise in measurements introduce additional sources of error to the error introduced by projectors. Therefore from a practical viewpoint, projector errors need to be put in perspective relative to errors due to nonlinearity and noise. This is precisely what our study provides (by simulating the Beerâs law and Poisson noise). (ii) The most significant conclusion in our study is that for low-dose CT applications, the projectors with most accurate computation of line integrals provide the most robust imaging results. CNSF calculates the line integral exactly and approximates the detector blur, whereas SF approximates the line integral over the detector cell, leading to differences in the results. Furthermore, LTRI that approximates the line integrals across the detector by an area calculation, introduce more significant errors that are observable in reconstructed images. We submit that these differences among projectors are not expected from what was previously known (R1).

(R1): We will clarify the perturbations in measurements due to noise versus perturbations in the forward model due to approximations introduced by (fast) projectors. Explicit formula for theta, justify resolutions and other suggestions will be incorporated.

(R4/R1): Figures 2 and 4 will be fixed to show x-axis (number of views). Further explanations on boxplots will be provided quantifying effects of Poisson noise in variations that it introduces to the condition number. We will  unify (and improve description of) the color coding. Sensitivity plateaus at a level depending on strength of the Beerâs law effect (detector/pixel sizes) as the number of views increases.

(R6): Fig1 shows the reference (ideal) projector showing the difficulty of the inverse problem from a linear algebraic viewpoint (kappa with the Beerâs law effects). Fig2 shows deviations of Fig1 when noise is added. Figures 1 and 2 show the upper bound of the backward error caused by the perturbations from the fast projectors compared to the Ref projector. Figures 3 and 4 show the corresponding plots of the difference between the three fast projectors and the reference projector, providing the actual backward error in this experiment.

We will make all code and experimental results publicly available from GitHub."
https://papers.miccai.org/miccai-2024/068-Paper3035.html,"We appreciate the reviewersâ valuable feedbacks. We are committed to revising our paper accordingly. 
(R1-1)In our finalized version, we will cite and acknowledge the contributions of the studies by [Prochnow et al 2013] & [Mraz et al 2003], which were pioneers in this area. Extended from the VR system in our previous studies, our MR.VRA system is now equipped with an MR-compatibility at 3.0/7.0-tesla using a high-quality LCD display to provide patients with a virtual environment with improved personal experience. Button box serves as an accessible interface that allows for real-time motion controls and interaction during the fMRI tasks. 
(R1-2)Clinical impact result. The reviewer is correct that we used only healthy subjects to test the feasibility, and thank the reviewers for their understanding and agreeing that this is an important first step to make MR.VRA system further useful for clinical uses, while the original non-MR-compatible version of this system was already clinically used in rehabilitation of stroke patients at two local major hospitals, which proved useful and effective based on resting-fMRI data. To report its clinical results with a hardware upgrade is our next step once this first report is accepted by the community.
(R1-3)Quantitative Analysis: We actually have conducted extensive quantitative analyses on the task-fMRI data. All activated brain areas, including Brodmann Area designations, MNI coordinates, and t-values have been meticulously recorded. Limited by the length allowance of this conference, we are unable to report the quantitative analyses in greater detailsï¼but will try to fit in as much as we can to the text in the camera-ready revision. Our full-length journal version of this paper will definitely delve into these suggestions.
(R1-4)Effectiveness Claim. Prior to the MR.VRA systemâs development, a non-MR-compatible immersive VR system for UE rehabilitation was already in use. Its effectiveness in stroke patients was validated through scales like the Fugl-Meyer Motor Scale and resting-fMRI. We therefore feel confident that MR.VRA will be capable of effectively functioning on UE motor functions following up this feasibility study. Although not emphasized in the manuscript due to the rule of double-blind review, we will highlight this part in the camera-ready version.
(R1-5)Balance in introduction. We recognize the need for a balanced perspective on the introduction of MR.VRA system. While we aimed to highlight the potential of MR.VRA, we understand the importance of acknowledging the current challenges and limitations. We now have revised the introduction to provide a more neutral and comprehensive overview, including the historical context and a discussion on the varied effectiveness of MR.VRA.
(R3-1)Minor English language problems. Thank you for your careful reading and pointing out the grammatical errors and ambiguous statement present in the text. We apologize for these defects and confusion. We will thoroughly polish the text and ensure its best quality in the camera-ready version.
(R3-2)Head coil mirror. The head coil mounts an angle-adjustable mirror directly in front of the patientâs eyes, at a distance of about 8cm with a comfortable viewing field of the projected scene.
(R4-1)The direction controlled by button box. Button box has four buttons for moving along four directions (front, back, left, and right). We have fixed the distance between the virtual arm and the virtual table.
(R4-2)Limitation of button box. We must acknowledge that the use of a button box may seem limiting in this first-stage report. As MR.VRA system has been proven effective on healthy subjects and in the past on stroke patients, we are conducting the next phase of experiment that will use a joystick to facilitate patient use and for the VR system to measure the proximal function of the stroke patients.
Reproducibility. MR.VRA will be available for free non-commercial uses once its software copyright is authorized."
https://papers.miccai.org/miccai-2024/069-Paper0233.html,"Dear Area Chairs and Reviewers,

We would like to thank you for your time, review, and the opportunity to clarify the points raised regarding our Generative Cellular Automata (GeCA). We appreciate your positive feedback that GeCA is âa very novel methodological extension that incorporates bio-inspired approaches and enriches the current schemes of generative models,â (R3) while clinically tackling âan interesting and important taskâ (R4) of 11 multi-label conditional OCT generation. Reviewers acknowledged the novelty in merging NCA [18] with diffusion [8], noting that GeCAs âmake crucial methodological modifications to the NCA architectureâ (R3). Furthermore, our âGene Heredity Guidance (GHG) technique significantly improves GeCAâs image sampling processâ (R4). There is also recognition of the clinical implications of GeCA, with the âEffective Dataset Expansion for Retinal Disease Classification,â (R4) and the âStrong Validation in multiple datasets using a wide array of qualitative and quantitative metricsâ (R3). Yet, there are reservations:
i) The AI model creates new images without clinical knowledge (R1):
We acknowledge your concern that images generated by AI models may contain unrealistic projections. In response, we highlight GeCAâs clinical implication not to show fake images for clinicians, instead, we highlight the aim of using these images to enhance AI diagnostic ability. Thus we kindly ask you to consider comparative results of GeCA with baselines for clinical and technical implications. 
a) We demonstrate the significant clinical implications in effectively expanding 11-multi-label OCT dataset, enhancing AI models significantly (*****).  The classification evaluation in Table 2 was conducted exclusively on the real non-fake OCT test-set, underscoring improvements to real-world clinical datasets. The code and the OCT-ML dataset will be released.
b)  GeCA demonstrates significant technical contributions over SOTA baseline, transformers with diffusion, DiT [23], while outperforming both quantitatively and qualitatively. (Tab. 1 and Fig. 4 for comparison)
ii) The classification and the super-resolution capability of methods (R1): 
To address any potential misunderstandings, GeCA does not propose nor employ super-resolution method, while âour generative model is evaluated on real-world clinical non-fake datasetsâ.
iii) Comparison with literature & additional priors (R4): 
Following R4âs advice, weâll update our manuscript to address why (R4:[1], [2]) donât compare well with GeCA. While [1,2] âleverage generative modelsâ with fundus segmentation priors, it is promising to replace their GAN with our baseline DiT (ICCVâ23) as well as our ânovel generative modelâ, GeCA. Note that [1,2] is inapplicable without vessel priors in our OCT-ML. We hope the clarifications help readjust the rating to accept GeCA. 
iv) The motivation behind using âpix-cellâ (R4):  âPix-cellâ refers to a unique time-state space representation for âpixâel or patch in our âcellâular automata. âPix-cellâ and their interaction capture and propagate fine-grained long-term dependencies, crucial for small structures in medical imaging.
v)  CFG &âMâ (R3, R4): We observed the same trends in CFG as [9,23] and opted for 1.5, defaulted in [23]. We set âMâ while training & inference to 12, DiT number of layers. Notably, our ablation studies on âMâ in Supp. Fig. 6 reveals that GeCA demonstrates novel zero-shot inference capabilities, while an optimal âMâ in inference can exceed the results presented.
vi) Future work (R1, R3, R4): Though we showed our novel GeCA in application of multi-label conditional generation of OCT, GeCAâs potential in medical imaging is yet to be explored. The scarce AS-OCT (Fu et al. MedIA 2020, Yang et al. Biomed. Opt. 2023) holds a great application (R4). Extensions for GeCA are twofold: selective metaheuristic guidance to further optimize the image sampling (R3) and schedulers for denoising strength, M, during inference."
https://papers.miccai.org/miccai-2024/070-Paper1781.html,"We thank all reviewers for their valuable and insightful reviews. They described our method as ânovel and efficientâ (R1&R4), with comprehensive experiments (R1&R4) and largely surpassed the compared methods (R3). Here we address their main concerns:

Motivation of Ti (R1)
Due to the imbalance between foreground and background, taking an average of pixel-level uncertainty across the image will be biased to the background. To obtain an unbiased global uncertainty estimation, we use an entropy threshold (Ti) to identify the uncertain region, and aggregate the uncertainty in this region as the slice-level uncertainty, which is more relevant to the prostate. To avoid manual tuning the value of Ti, we propose a self-adaptive threshold based on the histogram of pixel-level uncertainty.

Dataset (R3&4)
Collecting a large dataset is often challenging, and domain adaptation methods aim at transferring knowledge from a larger source dataset to smaller target datasets. We followed existing works of Yang et al. [27] to use the prostate dataset for SFDA, where a larger source domain and two smaller target domains   were involved. Due to the relatively small target domain, 4-fold cross-validation was performed to ensure robust evaluation. 
Though the prostate has a relatively simpler shape, the different domains of the prostate dataset differs a lot in image brightness, contrast, quality and resolution, posing challenges in cross-center adaptation settings. As shown in Table 1, the Dice of âsource-onlyâ was 42% to 45%, showing the large domain gaps. As this dataset has been widely used as a benchmark for SFDA, we used it for better demonstrating the improvement from existing methods. We agree with the reviewer that other datasets with more complicated shapes could be used, which will be investigated in an extension of this work.

Clarity and notation of the method (R1,3&4)
M_s in section 2 actually means the source model used for target-domain initialization 
The TST strategy is straightforward: 1) a model initialized with the source model is trained on active samples with manual annotation and remaining samples with pseudo labels, and then it predicts updated pseudo labels for unannotated samples. 2) The process is repeated once with the updated pseudo labels. Section 2.2 could be slightly revised to make this clarified. 
As described in the first paragraph of the method section, the input of our method is the source model and unannotated samples in the target domain, and the output of our method is the selected active samples and the adapted model. Fig. 1 can be edited a little bit to clarify this. The mistake in the notation in the upper part of Fig. 1 will also be corrected with minor modification.

Annotation budget (R4)
A low annotation budget is expected to keep annotation minimal, and Table 1 showed that only our method is comparable to fully supervised learning or finetuning in the target domain with only 5% images labeled. So, we did not further increase the annotation budget. In the future, it is of interest to further reducing the annotation budget.

Details for Fig.3(b) (R1) 
For the compared MC-Dropout, LC and Entropy for uncertainty estimation, we followed typical practice of averaging the uncertainty across all the pixels to obtain image-level uncertainty, which was then used for active sample selection.

Pseudo-code and code (R3&4)
Due to the space limit, we did not put pseudo-code in the manuscript, as done in most MICCAI papers. The code was not provided due to the anonymous reviewing process, but will be released in the final version, ensuring the reproducibility of the work.

Showing the worst case (R4)We agree that more analysis on where our method performed the worst would help better understanding its limitation. Fig.2 showed some average cases for comparison, and it can be slightly edited by showing some failure cases for more informative presentation in a final version."
https://papers.miccai.org/miccai-2024/071-Paper3665.html,"We appreciate the valuable feedback provided by the reviewers and will consider their suggestions when preparing the final version of the paper.

(R1, R2, R3) Reproducibility: All the reviewers expressed concerns about the reproducibility of the paper. The camera-ready version of the paper will include a link to the working code and trained models.

(R1, R3) Pre-training method design: The pre-trainer extracts the fundamental features from the B-scans to help the detector converge more quickly with less computation. The CSAT pre-trainer is trained to classify a pair of B-scans into two classes. Positive: the pairs are adjacent B-scans from the same patient. Negative: the B-scans belong to different patients (hence, they are not adjacent). This process ensures that while learning to identify adjacent B-scans, the model focuses on the inherent features unique to the adjacent B-scans. Those features are the shape, contour, and the presence of the same artifacts in both the B-scans. Hence, by pre-learning these features, the encoder provides the information that the detector needs i.e., the similar artifacts between two B-scans. To verify this theory, we extensively visualized attention maps between the positive and negative pairs, similar to Figure 2. We observed that the common features in the adjacent B-scans were highlighted in the case of positive pairs, whereas there were few to no highlights in the negative pairs.

(R1) We are grateful for R1âs comments and will address all the proposed edits in our camera-ready version.

(R1) How central is SCR to the paper?: SCR plays a central role in this paper as the method was specifically designed to detect SCRs. However, it can also be used to identify other pathologies with similar characteristics.

(R1) Choice of ânâ: Section 4.3 and Figure 3 (a) explain the importance of ânâ. Our experiments with ânâ = 1, 3, and 5 show that increasing ânâ improves detection accuracy but also increases the modelâs computational complexity. However, the rate of accuracy improvement decreases with higher values of ânâ, which makes it counterproductive.

(R1) âN classesâ: âN classesâ on page 5 refer to two classes, SCR and Fovea. 
(R1) Page 5, Advantage 2: Yes, the weights are updated every time a B-scan is input as a neighbor of other B-scans by construction.

(R1) Page 5, Advantage 3: Considering a B-scan, âPâ, we mean that during detection, the features from B-scans closer to âPâ are paid more attention to than those that are farther away from âPâ.

(R1) Loss functions: The loss functions are similar to those used in DETR, as mentioned in section 3.3. The loss weights were chosen after a series of experiments performed with varying values for weights.

(R1) âClass probabilityâ in Table 1: The âClass probabilityâ refers to the probability that the input pairs of B-scans are adjacent.

(R3) Comparison with existing methods: Jing et al. trained a YOLO network for SCR detection. In Table 2, we have compared our method with YOLO. Results show that CSAT is superior to YOLO in our dataset. Our method can be used in any DETR-like networks, such as CF-DETR and Deformable DETR by simply replacing the decoder."
https://papers.miccai.org/miccai-2024/072-Paper0442.html,"We sincerely thank the reviewers for their valuable and constructive feedback. We will make the amendments needed to satisfactorily answer all the reviewer comments (R1, R3, R4), especially the ones regarding the description of the technical details of ADDA and the implementation details conducted to successfully generate our models. The African dataset, the source code, and the models will be made publicly available upon acceptance. We focus this rebuttal on the most relevant concerns. All the typos and minor concerns will be addressed in the camera ready.

R1, R4. Incomplete description of the Adversarial Domain adaptation method.
R3. Concerns regarding the novelty of our work and suitability to Miccai conference
We selected ADA [26] as the best method in the extensive survey conducted in [31] for adversarial domain adaptation. We extended ADA with subsequent steps involving fine-tuning and the use of labels for supervised adaptation. We will improve the description of our extended ADA in the camera ready. We believe that the extension of ADA methodology, the use of ADA in a very particular medical imaging context, the application of ADA to cross-population domain shift in a relevant clinical task, our comprehensive evaluation with different populations, and the outperformance of the proposed method in our particular problem of classification may yield a contribution suitable for a conference such as Miccai.

R1. Lack of rigorous comparison with SOTA methods. 
We agree with the reviewer that there are other interesting domain adaptation techniques that may provide interesting insights into our problem. We found the citations suggested by the reviewer very valuable. We will include the most relevant ones in our Literature Review.
We intend to conduct a thorough comparison and evaluation of different domain adaptation techniques in an extended journal version, thus providing the rigorous comparison of the SOTA methods requested by the reviewer. We will also address the problems of small sample size and class imbalance, typically found in the clinical translation of AI models.
From our experience, continual or muti-task learning is better suited in environments with small sample sizes. We believe, that alone would not provide acceptable results for datasets with a great shift among them. It would be a good idea to combine ADA with CL or MTL for the improvement of our models.

R1. Take advantage of the labels in the African dataset.
ADA is designed for training with an unlabeled target dataset. However, we agree with the reviewerâs concern about utilizing available labels. Therefore, our method incorporates labeled data with a different loss function to fine-tune the models, enabling supervised adaptation. We will revise the manuscript to reflect this two-stage process better. A comparison with Madani et al. 2018 will be considered in an extended journal version.

In ADA, the target dataset contributes to the loss function through the domain discriminator but not directly through the classification loss. We use adversarial training to align source and target feature distributions. The domain discriminator loss is updated based on distinguishing features from both domains. Classification loss is calculated using labeled samples from both domains, penalizing incorrect classifications. This combination of domain discriminator loss and classification loss ensures robust adaptation and improved generalization across diverse populations.

R1, R3, R4. Confusing implementation details and results.
We will amend our manuscript to clearly describe the data-splitting process for cross-validation on the source datasets. Also, we will explicitly state that the African dataset is used solely as the target test dataset, not included in the cross-validation splits. Finally, we will clarify the distinction between validation and test sets derived from the source datasets during cross-validation and the target dataset used for final testing."
https://papers.miccai.org/miccai-2024/073-Paper3539.html,"We thank the reviewers for their thoughtful and generally positive feedback, with all reviewers recommending our work to be accepted unconditionally (R1) or otherwise (R3, R5). We appreciate that R3 and R5 praised the novelty of our model, and R1, R3, R5 noted the efficiency of our method.

Below we address the reviewersâ concerns.

[Comparison with MedLAM]
We appreciate R3âs suggestion to compare APE with MedLAM. However, we would like to highlight that, according to MICCAI rebuttal guidelines, we are not permitted to add new experimental results during the rebuttal process, and breaking this rule will lead to automatic rejection. Thus, we are unable to include a comparison with MedLAM in the current version of the paper. Nonetheless, we can mention that MedLAMâs part that produces anatomical embeddings is trained with the same loss as RPR, which we have compared with. Quantitatively, MedLAM reports comparable few-shot organ localization IoUs (though on a different dataset). Qualitatively, MedLAM predicts one positional embedding per patch, while APE produces embeddings for all individual voxels in one step.

[Comparison with SAM]
We acknowledge R1 and R5âs valid concern about the potential impact of different pretraining datasets on the comparison between APE and SAM. While APEâs pretraining dataset includes the unlabeled part of FLARE2022, and SAM was pre-trained on a different data collection, we believe that the comparison remains valuable and informative for several reasons.
First, the labeled and unlabeled parts of FLARE2022 used for testing and pretraining, respectively, do not intersect, ensuring that APE is evaluated on an independent test set. Second, the organ bounding box predictors based on APE, SAM, and other baselines (Tables 3-4) were trained on the same few-shot cross-validation splits of the FLARE2022 labeled set, ensuring a fair comparison. Third, FLARE2022 constitutes only 24% of APEâs pretraining data, mitigating the potential impact of domain-specific biases. Recall that FLARE2022 is a diverse dataset, with an unlabelled set collected from 22 different centers. Finally, LymphNode dataset used for SAM pretraining and the PancreasCT dataset (part of FLARE2022) are sourced from the same clinical center (The National Institutes of Health Clinical Center), suggesting similar domains.
Moreover, APE and SAM exhibit qualitative differences: APE embeddings are three-dimensional, explicitly encoding anatomical positions, while SAM embeddings are high-dimensional and purposely redundant.
[Overfitting to local textures]
R5 expressed concern that APE overfits to local image features, e.g. textures. As written in Section 2.1, our loss enforces APE embeddings to be equivariant w.r.t. augmentations, including masking out random image patches, which prevents overfitting to local features.

[APE specifications]
We appreciate R5âs request to clarify APE specifications, such as whether input images need to be aligned. The limitations of APE are already mentioned in Section 3.4: in its current version, it is not equivariant to flips and rotations, i.e., input images must be in canonical orientation, and it is trained only on chest and abdominal areas. However, in other aspects, APE is very robust, being equivariant w.r.t. input image shifts, crops, and changes in voxel spacing.

[Improving results presentation]
Following R3âs suggestion, we have supplemented the results in Table 3 with statistical significance tests. We have added âWe show that APE significantly outperforms all the baselines using Wilcoxon signed-rank test, with p-value < 1e-6 for all the baselines.â in the Results section.

Additionally, per R3âs advice, we have titled the projections in Figure 2 as âAxialâ, âCoronalâ, âSagittalâ, illustrated the directions of the main body axes (frontal, sagittal, and longitudinal), and added color bars to show the range of APE values.

We also added a column with average results to Table 4, as requested by R3 and R5."
https://papers.miccai.org/miccai-2024/074-Paper2014.html,"We thank the reviewers for their constructive feedback.
#R1 [ARSAâs Contribution]ARSA, as our key contribution, is the first to use anatomical regions and sentences as the minimum semantic units in MedVLP. Its design is in line with the reading process of X-rays, as validated by professional radiologists. AnaXNet and AGXNet only use single-modal information. RGRG uses Chest ImaGenomeâs annotations. What we want to emphasize is that we redesign an automatic alignment, which has advantages over Chest ImaGenome in â removing redundancy (Supp.Tab.1) â¡strict semantic alignment, e.g., in Chest ImaGenome, bbox left lung maps to sentence âlungs are clearâ, which is inaccurate and illusory. We provide two solutions: split sentence or merge bbox (Fig.1).
#R1,R3,R4 [ERLâs Contribution and Comparison with Previous Methods]We use Soft Label to further enhance our method. Comparison: â Methods in the natural image domain (e.g. SoftCLIP) generally rely on image tags output by object detectors as the pseudo-labels for contrastive learning, which could lead to error accumulation due to detectorâs inaccurate predictions. In contrast, thanks to the detailed nature of medical reports, the soft labels we use are directly parsed from reports, which is more accurate and computationally efficient. â¡For existing methods in medical domain, our method is superior in both accuracy and granularity of predefined categories, leading to better performance. For granularity, MedCLIP only adopts 14 categories to construct soft labels, which is insufficient to distinguish subtle differences in X-rays. In contrast, we expand them to 40 categories defined by radiologists. For accuracy, the pseudo-labels produced by MedCLIP include uncertainty(labeled as -1). In contrast, we ask radiologists to re-label these cases to obtain more accurate soft labels. The entire process is natural since we already obtain pseudo-labels in Sec.2.2. This is why we donât use other methods to reduce FN. We will add the comparison in the revised version.
#R1,R3,R4 [IRLâs Contribution and Zero-Shot Performance]IRL not only strengthens the image encoder but also aligns with the patterns of downstream classification tasks. This is an important reason why our 1% finetuning significantly outperforms previous methods. Similarly, we validate our model on zero-shot, surpassing SOTA methods. Due to space limits in paper, we follow the exp setup of MGCA(NIPS 22) and omit the results. We will add them in the revised version.
#R1,R3,R4 [Performance Improvement Analysis]We survey recent MedVLP methods and observe that our method achieves comparable performance gains with them on classification and much larger on segmentation (thanks to ARSA). Our model has appropriate standard deviations under different data proportions. We will add them in the revised version.
#R1 [Baseline Performance]The exp setup (Chestx-ray8:finetuning; Ours:linear probe) and data split are different, so the metrics differ significantly.
#R1,R3 [Training Details]We use the same settings and metrics as  MGCA. f_dec is a transformer decoder. Weâve clarified all details on our GitHub.
#R3 [Interpretability]In ARSA, we treat anatomical regions and sentences as the minimum semantic units, which is more interpretable than prior patch-word alignment as patches lack anatomical info. We provide heat maps (Fig.3) for further proof, superior to those in MGCA.
#R3 [GLoRIA]The sub-region referred to is a split of feature map (actually a patch).
#R3,R4 [Loss Design and Ablation]We parse raw reports into triplets, and all modules make full and reasonable use of this structured info, introducing corresponding losses. This is not a forced combination to improve the metrics. From ablation, each loss contributes to performance gains. Since bce loss and soft loss are both based on <find,exist> and bring similar improvements, we consider them together as IERL.
#R4 [Reference]In the revised version, we will compare with the strong baseline BioViL."
https://papers.miccai.org/miccai-2024/075-Paper0584.html,"We thank the reviewers for their helpful feedback. We will address your comments as follows and pledge to update the camera-ready version accordingly if the paper is accepted.

On our methodâs practical uses (for R3): one is that it can constrain specific organs/objects to have uncommon characteristics in generated images (e.g., an unusually large liver in an abdominal CT), or similarly generate counterfactuals by modifying one organ while keeping others fixed. This could be used to controllably generate synthetic data to augment a training set for some downstream task, to generate more of certain rarer cases to mitigate any dataset imbalance. Another related usage is cross-modality anatomy translation: e.g., given images+masks from one scanner sequence (e.g., T2 MRI), you can train our model on these images+masks, and then can use masks from another sequence of images (e.g. T1 MRI), to create new T2 images using the T1 masks.

Regarding R3âs concerns about our modelâs generalizability to other data, we note that generating images from segmentation masks of new datasets within the same modality should work in principle. Segmentation masks capture intrinsic anatomical content, the characteristics of which should not be significantly affected by any distribution shift due to a new image acquisition sites/new dataset, and so such new data should still be usable by our model. Similarly, for their question about our modelâs generalizability to imprecise mask delineations: our mask-ablated training (MAT) algorithm helps the model learn to be able to handle cases of incomplete masks, so this learned flexibility should similarly make the model work with imprecise/imperfect masks. Finally, we compared performance with and without MAT in the paragraph right before section 3.2.

Also, for R3: while we arenât allowed to add new uncertainty results for the tables for MICCAI rebuttals, we saw little variation in Dice score between evaluation batches when we ran the experiments for the submitted paper, so we expect the uncertainties to be small.

R4 had two questions about our MAT strategy: (1) alternative implementations and (2) its usability for other mask-conditional generative models. For (1), MAT can be easily modified, such as using different Bernoulli probabilities in Algorithm 1 for varied mask class removal. For example, a lower value than 0.5 results in training masks with typically fewer removed mask classes, and probabilities can also be varied by class, such as using a low removal chance for the breast class in breast MRI, but a high one for dense tissue. For (2), MAT can be applied to any mask-conditional generative model (e.g., SPADE [17]) without modification by just modifying/ablating training masks according to Algorithm 1 during training. Overall, we designed MAT to be simple and interpretable to allow for such customizability and generalizability to any generative model.

For R1âs numbered minor weaknesses: for point (1), weâll change the model name to âSegGuidedDiffâ; we will fix (2) and make (4) and (6) explained more clearly at camera-ready. For (3), we note that the network maps from c+1 to c channels because it takes in the noised image + the mask as input, and outputs the denoised image. We tried using one-hot mask encoding, but this makes the network scale poorly for large numbers of mask classes. For (5), we initially found that our model outperformed competing methods on both datasets by FID, but chose not to add this to the paper because FID uses features learned from natural images, and so may be inappropriate for medical images, and we canât add new MSE or SSIM results as this is not allowed for the MICCAI rebuttal. Finally for (7), the novelty of the latent space interpolation is that it has not yet been explored for medical image models, especially anatomy-constrained ones, and so traversing this latent space has a new meaning: the fine-tuning of the anatomies specified by the input masks."
https://papers.miccai.org/miccai-2024/076-Paper2508.html,"We thank reviewers for valuable comments. Below we clarify confusions.
Q1(R1): Necessity of parcellation and proxy labels of T2* scans. A: Parcellation and proxy label generation are done in T1 scans. T2* scans can differentiate cerebral microbleed (CMB) voxels. Thus, we performed registration to feed both T1 and T2* scans into a segmentation network.
Q2(R1): Is the network performed in a multi-task fashion? A: No. Our network directly outputs 3D prediction map for both CMBs and proxy labels.
Q3(R1): Lesion demographics? A: Local dataset has mean 2.98 lesions (vol 51.29mm^3). External dataset has mean 3.28 lesions (16.11mm^3).
Q4(R1&R4): Segmentation method was used rather than detection. A: We tried to use nnDetection as our backbone but predicting proxy labels as bounding boxes makes integrating our false positive reduction method difficult. Bounding boxes contained too many background voxels.
Q5(R1&R4): The reported Dice score (< 0.52) is weak, suggesting preference for detection measures and comparisons against detection works. A: We used nnDetection as baseline and report F1-score as detection metric in Table 1. Still, our segmentation-based model outperforms nnDetection in terms of F1-score. We cannot report other metrics due to rebuttal restrictions. We believe F1-score is a decent detection metric, especially under class imbalance.
Q6(R1&R4): Ablation of proxy task is missing. A: Ablations are reported as nnUNet baseline in Table 1. This baseline was trained without proxy task. We will clarify this in Results.
Q7(R1&R3): Minimal performance gain of clinically-derived false positive reduction (CFPR) module especially in internal dataset. A: We are puzzled by this issue. Perhaps internal training and validation sets are from the same MRI scanner, limiting the impact of CFPR. In external dataset, we observed that raw CMB predictions are unstable, while proxy label predictions are stable. This enables CFPR to improve overall performance in external dataset. We will mention this in Results.
Q8(R3): Less contribution in network and loss function. A: Networks and loss functions have been extensively studied in nnUNet, nnDetection, and DiceTopK papers. Our focus is to raise the significance of processing images and automatically deriving proxy labels on top of well-performing networks and loss functions.
Q9(R3): This paper has not taken measures to deal with possible false negative regions. A: We did not report direct measures of false negatives. Instead, we reported F1-score, which deals with both false positives and false negatives.
Q10(R4): Similar concept with ArXiv 2306.13020. A: Thanks for pointing out this excellent paper! We agree our concept of proxy labels is similar with theirs. However, ours is different from the mentioned paper in three regards. 1) Our proxy label generation is fully automated in white matter regions (internal capsule, external capsule, and deep white matter voxels), which was manual in theirs. 2) Ours was validated using a public dataset, while theirs was not. 3) We used the ratio of brain parenchyma for false positive reduction to follow the established MARS criteria, which is missing in theirs. We will mention the paper in the Introduction explaining the differences.
Q11(R4): Insufficient related works. A: We agree and will add more details in Introduction.
Q12(R4): Which views (sagittal or axial) are used as inputs in Fig 1? A: No 2D views were used. Our input is 3D images (Section 3.3).
Q13(R4): Verify the DiceTopK in ref [3]. Was the DiceTopK used for CMBs and anatomical region segmentation? A: The term DiceTopK was introduced in [17] and we will correct this. Yes. DiceTopK was applied for both CMBs and proxy labels with K=10 and lambda_TopK=0.5.
Q14(R4): Is VALDO dataset used during training? A: No, it is solely used for validation.
Q15(R4): Ground truth of sub-311 is incorrect. A: We agree but sub-311 is part of the public dataset, so we could not alter ground truth."
https://papers.miccai.org/miccai-2024/077-Paper1746.html,"We sincerely thank the reviewers for their thorough and insightful feedback, as well as their positive comments, including âclear presentation and easy to follow (R1)â, âpromising methods with potential advancements (R1)â, ânovel overall architecture (R3)â, and âinteresting and innovative ideas (R3)â. We appreciate R1 and R4 for accepting the paper directly, while R3 provides a weak rejection due to some misunderstanding. Below, we address the concerns raised by reviewers.
R1&R3&R4:Technical and experimental details
Due to page constraints, we cannot include further technical and experimental details in the manuscript. However, such information, along with datasets, hyper-parameter settings, and visualized results, will be available in the open-source code on GitHub. We also try our best to explain some technical details at the end of this rebuttal.
R3:Anatomy constraint
Edges identify significant transitions in intensity, often corresponding to anatomical boundaries. Many applications use edge detection and filtering to present anatomy in medical images [A-C]. In our study, edge detection presents the spatial density of anatomical structures: regions with significant fluctuations contain abundant information, and vice versa. The method is generalizable to CT scans of other anatomies, such as the abdomen [C]. While we believe edge detection can sufficiently represent the anatomy constraint, we appreciate the suggested simulation and segmentation masks, which we will consider for future work.
R3:Overall architecture and improvement
The overall architecture consists of three stages, following the motivation of âanatomy-constraint samplingâ(section 2.1), âsampling-guided image synthesisâ(section 2.2),  and âanatomy refinementâ(section 2.3), respectively. Each stage is clearly designed without redundant components.
Our image reconstruction improvement is significant: achieving performance comparable or superior to upper bounds with only 10% of the training data. We believe this improvement is useful for real-world applications.
R3:Training complexity
For the framework training, there are existing public models to follow, without training from scratch. We will make our code public to facilitate replication. The framework is trained on an NVIDIA A6000 in about two day, indicating small overhead in the training phase.
R3&R4:Details for generator and discriminator
We use three generators and three discriminators (Stage 2). The first generator synthesizes pseudo-masks from noise, with its discriminator distinguishing these from real-world samples. The second generator produces annotations from pseudo-masks, with its discriminator differentiating the generated paired pseudo-mask and annotation from natural ones. It follows Conditional GAN and loss functions. The third image generator synthesizes images from given pseudo-masks and annotations, which are concatenated along the channel dimension as the conditional input. It is first trained on real-world sampled pseudo-masks and annotations to make the generated images more realistic, and then fine-tuned on generated pseudo-masks and annotations.
[A]Sharpness-aware low-dose CT denoising using conditional generative adversarial network
[B]Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model
[C]Low-dose ct image blind denoising with graph convolutional networks
Bayesian uncertainty
Bayesian uncertainty gauges the reliability of registration, aiding anatomical refinement. There are two types of registration uncertainty: transformation and appearance uncertainty. We focus on the latter, which measures the intensity uncertainty of registered voxels or organ volumes. Details are in [23].
Reinforcement learning
We use reinforcement learning to avoid directly back-propagating uncertainty as a loss function, reducing computation and memory demands. Reinforcement learning treats appearance uncertainty as the reward and using policy gradient techniques[26]."
https://papers.miccai.org/miccai-2024/078-Paper2553.html,N/A
https://papers.miccai.org/miccai-2024/079-Paper1464.html,"We thank all reviewers for their feedback, which we incorporate into the final version. We are glad that reviewers appreciate the relevance of integrating anatomy into pathology segmentation models (R1, R3, R4) and rate our work as interesting (R3) and novel (R4). Here, we kindly address concerns, misunderstandings and discuss your interesting propositions.

Comparison to SOTA (Attention Unet, Unet++, SwinUNETR) (R1, R4)
In this work we focus on 2D processing, as we want to target both X-Ray imaging and CT scans with the same model architecture. Thus, we do not compare against 3D architectures such as SwinUNETR. The proposed experiments to evaluate the semantic seg networks Attention-UNet and UNet++ on the ChestXDet setting are not applicable. Here, we have to differentiate between instances of a class (like fractures) and thus fall under instance segmentation (cf. Tab 2). In this setting, we surpass the most competitive approach PointRend and the most recent MaskDINO by at least 2.06% mAP and a t-test (p <0.001) indicating a statistically significant improvement.

Paper Structure and Writing (R1)
We aimed to structure the paper as comprehensively as possible (acknowledged by R3, R4). Contrary to R3/R4, R1 states the concern to âPlease reformat the paper as it has a lot of mistakes in the sentence formationâ. We would be happy to integrate any specific suggestions.

Incomplete Evaluation (R1, R4)
We kindly disagree with the sentiment that our ablation studies are limited (R1), as we present a total of 14 different ablation experiments in Tab. 1. R4 notes that in the ablation studies our results âsolely presents validation scores, omitting test set performance, which hinders a more comprehensive evaluationâ. We adhere to a strict evaluation protocol by performing 5-fold cross-validation and developing our method exclusively on validation sets to avoid biasing test performance. Thus, we only evaluate once on the test set with the final model design. While interesting, the ablations on the X-Ray dataset would have entailed the training of an additional 70 models (14 experiments * 5 splits), beyond our computational resources.
R4 rightfully notes that the asymmetrical and symmetrical designs exhibit quite similar performances. In this paper, we chose the slightly better performing design (and simultaneously reflecting the pathologies being the target of interest), but agree that further exploration into this would be exciting future work.

Evaluation Metrics (R4)
The PET-CT AutoPet dataset, lacking instance-wise annotations, is treated as a semantic seg task. Following the recently published Metrics Reloaded [A] recommendations, we use an overlap-based Metric (IoU) and a boundary-based Metric (BIoU), shedding light on the performance on the boundary regions as suggested in the review. For instance segmentation on X-Ray, we report the recommended standard mAP metric. However, we acknowledge that reporting metrics which are more prevalent in the medical community (e.g. Dice) facilitates better comparability and we will follow the suggestion to add Dice to our work.

Limited Results Analysis (R3, R4)
The suggestion to analyze the relationship between anatomy vectors, pathology vectors, and their similarity pre- and post-mixing (R3) is intriguing, thanks! Sadly, due to the limited space, we could only include qualitative visualizations on the most influential anatomical structures chosen by the Cross-Attention Query Mixer (Fig. 2) but are looking forward to a more detailed analysis in the future.

Computational Cost (R3)
GPUs: 4 A100 40GB
#Params: DeepLab: 40M, UNet: 32.5M, M2F: 44M, Apex: 58M

Code availability and reproducibility (R1, R3, R4)
As stated in the abstract of the paper, we will release our models and the source code of our project, report the choice of hyperparameters and ensure reproducibility.

References
[A] Maier-Hein et al. âMetrics reloaded: recommendations for image analysis validation.â Nature methods (2024)"
https://papers.miccai.org/miccai-2024/080-Paper1595.html,"We appreciate the reviewersâ valuable time and comments. In the rebuttal, we summarize and clarify questions from reviewers.

C1. How do we generalize to real data? (R1/R4 Q3.1, R1 Q6.5)
We agree that it is important to generalize the proposed method to real data for testing clinical feasibility; however, labeling and obtaining enough training data from clinics is challenging. Some commonly used approaches, such as simulating real data by adding noise or increasing data diversity, can be applied. Note that our approach can still be useful to generate an initial model, which can be fine-tuned on the new datasets.
Benefiting from the capability of learning, the generalized learning-based imaging approach can be effective for real-world data where noise is present. This is evidenced by Supplementary Section 11 of the paper [1], where the robustness of InversionNet can be improved by adding Gaussian noise into the dataset.

C2. The details of waveform simulation. (R1 Q3.2 & 6.1, R4 Q7.1)
The waveform simulation parameters are set according to those listed in Table 1 of [2]. Our simulation considers heterogeneity only in sound speed, assuming constant tissue density without acoustic attenuation or nonlinearity. Using full-waveform observations, we consider wave scattering.
We thank R4 for pointing out the confusion on Equation 2. The notation c in the equation should be c(x), representing the speed of sound at location x. We used a finite difference method (2nd order in time, fourth order in space) for the simulation. We also applied an absorbing boundary condition on all boundaries.

C3. Comparison with conventional physics-based methods. (R1/R4 Q3.3)
Performance & Hardware: According to Fig. 5 of [2], the SSIM of physics-based FWI is 0.8, while our method achieved 0.8431 with a reduced transmitter number from 256 to 32.
Image Acquisition Time: According to the complexity analysis in section 3C(3) of [3], physics-based FWIâs imaging time is at least one order larger than the ML-based approach (used in our work).

C4. Performance on different types of breasts in Figure 3 (R1 Q6.2 & Q6.7)
Figure 3 shows visualization results for specific instances, not the entire dataset. We will clarify this in the revision.

C5. The cause of wave reconstruction by APS-wave in Figure 4 (R1 Q6.6)
In the raw waveform, there are both strong and weak events. During learning, APS-Wave prioritizes the reconstruction of strong events of very high quality. But, it also results in less accurate reconstruction of weak events, such as the vertical stripes observed by R1. Note that the strong events are important for imaging, making good inversion results from APS-wave.

C6. Dataset and writing (R3 Q3.1)
A part of the phantoms used in the experiments have open access [4]. We will follow your suggestions and check the whole paper to correct typos.

C7. AI-Physics synergy (R3 Q3.2)
ML-based APS-Wave is proposed to reconstruct dense waveforms from sparse data, enhancing the physical information. Then, we ML-based APS-FWI are developed for imaging. Both components exemplify how AI assists physics. Combined with the reviewerâs âphysics enabling AI,â we call our method an âAI-Physics synergy.â

C8. Diagnostic accuracy (R4 Q3.4)
We agree that our current manuscript lacks a discussion on clinical validation regarding diagnostic accuracy. Our paper focuses on image reconstruction from sparse data; diagnostic evaluation, system efficiency, and fairness will be addressed in future work. 
[1] âOpenFWI: Large-scale multi-structural benchmark datasets for full waveform inversion.â NIPS 2022
[2] âLearned full waveform inversion incorporating task information for ultrasound computed tomography.â IEEE TCI 2024
[3] âInversionNet: An efficient and accurate data-driven full waveform inversion.â IEEE TCI 2019
[4] â2D Acoustic Numerical Breast Phantoms and USCT Measurement Dataâ, Harvard Dataverse"
https://papers.miccai.org/miccai-2024/081-Paper2427.html,"We thank the reviewers for their feedback and appreciate the overall positive responses (6, 5, 2) with our comments:

R1: We correct the deficiencies and agree that this approach could be a very good one.

R3: We appreciate the reviewerâs enthusiasm and desire to see full-blown application to pathology (ââ¦explore how pathologist insights could mitigate these issuesâ). While we fully agree, this falls outside of our scope. 
There seems to be a misunderstanding in âneed of greater consistency between the problem and evaluationsâ since our problem (OoD and misclassified detection) is fully-aligned to its literature. Please see [1, 2] and the references in Tab. 2. Adopting OSR for OoD detection is also well-established [3, 4]. For misclassified detection, we direct to [5-7].  Also ââ¦instances where some fail to classify correctlyâ seems to refer to misclassified detection. 
We believe our OoD problem statement and its discussion w.r.t. generalization is sufficient in the intro (âââ¦clarify how it relate to OoD and generalization. The problem formulation lacks clarityâ). 
We are also surprised by the reviewerâs broad ââ¦novel insights proposed are not sufficiently supported by the experimental resultsâ¦â. We respectfully disagree. Results in S3 are quantitatively substantiated by Tabs. 3-5. 
Specifically for each point:

R4: Jaeger et al ref is now included.

[1] https://doi.org/10.48550/arXiv.2210.07242
[2] https://doi.org/10.48550/arXiv.2110.11334
[3] https://doi.org/10.48550/arXiv.2110.06207
[4] https://doi.org/10.48550/arXiv.2106.03917
[5] https://doi.org/10.48550/arXiv.2107.00649
[6] https://doi.org/10.48550/arXiv.2211.16158
[7] https://doi.org/10.48550/arXiv.2106"
https://papers.miccai.org/miccai-2024/082-Paper0271.html,"We thank your insightful feedback and constructive criticism and are excited about the early acceptance, for which our responses are optional. Nevertheless, we are committed to submitting a significantly improved camera-ready version that addresses all critiques. This includes new tables for pretraining and finetuning details, computational reports, standard deviations, explanations of baseline selections, clarifications of our setup, refined motivations and methods, and grammatical corrections for clearer presentation.
R1:
W:We briefly compare ASA with contrastive learning methods like SimCLR in the introduction. Moreover, ASA is extensively compared with, and shown to outperform, the SwinUNETR pretrained model, which incorporates contrastive learning as one of its three learning tasks.
C:(1,4)We will add a new table that details the pretraining and finetuning setups, along with computation consumption, to enhance reproducibility.(2)Beyond the 5 supervised and 2 self-supervised learning methods in Tab.1, we will explore more methods in future work. We chose to compare ASA with SimMIM and SwinUNETR due to their SoTA performance in the SSL domain.(3)The supervised baseline performances reported in Tab.1 established by Liu et al.(arXiv:2301.00785), were presented without standard deviations, we did so to maintain consistency in table format. However, we will add a new table in the appendix that provides these details from multiple runs.
R3:
W&C:Please see our motivation,Fig.1,and research question. Our method is driven by consistent patterns typical in medical images, often showing significant similarities due to the same imaging protocol. Our method leverages this uniformity to learn high-level anatomical structures and contextual relationships. To boost segmentation performance, the model needs to learn fine-grained features. Subsequently, it was found that training on local-level embeddings helps stabilize the pretraining process and boosts downstream performance further. Thank you for the suggestion to simplify the method. Weâre developing a new approach that integrates 2 learning stages and accommodates various body structures.
R4:
W:(1)For the 3 main results, ASA is pretrained on the AMOS22 datasetâs 240 training split for a fixed number of epochs, choosing the checkpoint with the lowest loss. Due to the target dataset for Tab.1 and Fig.3 being different from pretrianing, data leakage is not a concern. For the label-efficient experiment in Fig.4, ASA is both pretrained and finetuned on a subset of the 240 training split, but assessed using the non-overlapping 120 testing split, ensuring no data leakage during evaluation.(2)In our ablation study, we attempted training solely in phase 1 by adding T_gc into setup 3, but this caused the teacher model to collapse, leading to poor performance. Pretraining exclusively with phase 2 does not align with the objectives we outlined for R3. Nevertheless, to benefit the community, we will include results from p1 and p2 only in the next revision.(3)In the 3 main results, we divided each into experimental setup and result analysis to accommodate various target tasks and setups. Due to page limit, we changed the format for ablation studies, whose clarity will be enhanced in the next revision.
C:(2)Besides the somewhat intricate alternative pertaining, another limitation of our framework is the need for recurrent anatomy structure for the same image protocol, due to reliance on absolute position codings prediction, 1D or 3D.(3)Teacher student training approach, inspired by Antti et al(arXiv:1703.01780), Caron et al.(arXiv:2104.14294), and Ma et al.(arXiv:2310.09507), features both models sharing the same architecture without prior knowledge for the teacher. The teacher model is constructed from previous iterations of the student model, aiming to stabilize the pretraining and enrich knowledge accumulation from various perspectives. (4) We will proofread and fix all typos/grammatical issues."
https://papers.miccai.org/miccai-2024/083-Paper4128.html,"We appreciate the reviewersâ valuable feedback and are encouraged by their recognition of our methodâs clear motivation and promising results. To address their concerns, we offer the following clarifications:

A1: Novelty against(R#3)
Our method tackles two critical issues in polyp segmentation: (a) polypâs lesion-background differentiation and edge blurring from high-frequency detail absence; (b) SAMâs limited generalization due to the lack of training on endoscopic images. We introduce two innovations:

A2: Extend on other datasets(R#3)
Our ASPS trained on Kvasir-SEG and ClinicDB datasets and tested on five datasets, demonstrating its generalization to external datasets. Meanwhile, we are confident in its applicability to external tasks, such as skin lesion segmentation, which faces similar analogous issues of edge blurring and high-frequency information scarcity.

A3: Comparison with specific Foundation models(R#3)
We appreciate Endo-FM and will introduce it following R#1 in A4. We think fine-tuning Endo-FM differs from our method. Endo-FM is a video-level pre-trained model with rich endoscopic knowledge, while SAM excels in image-level segmentation tasks. They differ in input, generalization and task-specific ability. Despite this, ASPS outperforms Endo-FM in polyp segmentation with less data, demonstrating SAMâs generalization. We will integrate these works in future research.

A4: Absence of related work(R#1)
We will add the related work in Introduction section:

A5: Limitations without prompts(R#4)
We agree that removing prompts will limit the segmentation capabilities of SAM, but the interaction in polyp segmentation requires certain costs, such as experienced doctors. While automated segmentation in clinical practice can significantly aid both doctors and patients.

A6: Experimental results without UPR(R#4)
All models have been adequately trained, and CFAâs effectiveness can be compared to the initial two rows of Table 2. The first row of Table 4 is identical to the second row of Table 2, demonstrating that CFA enhanced performance.

A7: Computational complexity(R#1)
Most ASPS parameters are in CNN; Mask Decoder has a smaller parameter count. For Polyp-PVT[5], it trained in 3 hours on TeslaP100 at batch size 16, reaching ~60 FPS. Our Efficient-SAM model, at batch size 4 on RTX3090, also takes 3 hours, achieving ~20 FPS.

A8: Others
We will supplement more qualitative results and the citation of Table 1 in Results and Analysis(R#1&4). âMedSAM[14] and SAMUS[13] still incorporated prompts like SAM, but we removed themâ refers to the prompts in SAMâs encoder(R#1). Fig. 3 demonstrates CNNâs richer high-frequency information, aligning with ViT-Adapterâs study, and indicates features CNN_2 and ViT_2, as denoted by the left of arrows(R#4)."
https://papers.miccai.org/miccai-2024/084-Paper2993.html,"We are encouraged that reviewers found our work interesting (R1), experiments insightful (R3), our method effective (R4), and our paper easy to follow (R1). We especially thank R3 for their constructive comments, which we will include in the final paper.

R1: Model stealing (MS) and extraction attacks (ME). No comparison with [1-3].

Existing literature uses MS and ME attacks interchangeably. We compared our method to ME [23] as well as MS [21] techniques. We didnât compare with [1-3] as they are not directly relevant: [1] focuses on decision trees and MLPs, [2] is a defense approach, and [3] requires 5 million queries, which is unsuitable for medical imaging. Note: 1-3 are references provided by the reviewer, 21 and 23 are from the paper.

R1: 5000 is a large number for most medical imaging datasets; cannot be treated as âlimitedâ.

We refer to 5000 as a âlimitedâ query budget in comparison to existing MS attacks on general vision tasks [21,7] requiring millions of queries. We note that our method requires only âunlabeledâ images for querying, which are easier to curate than labeled images, even in medical settings.

R1: Similarity to knowledge distillation (KD), and ââ¦anchor model seems unnecessaryâ.

Our strategy is inspired by KD but extends it to include both labeled and unlabeled data. Knowledge from labeled data is embedded in a fixed anchor model, while the teacher model incorporates knowledge from both labeled and unlabeled data, and is dynamically updated from the student. Thus, the anchor and teacher complement each other.

R3: ââ¦ the novelty of studying MS attacks on medical imaging models is misleadingâ. â[32] performs text-based attacks on a model trained with lung CTâ.

The victim model in [32] is a 1D CNN trained on textual data, with the attack also using text. To our knowledge, our work is the first formal study on MS for medical imaging models under (a) realistic threat model of 5000 queries, and (b) hard-label access, unlike [32,21].

R3: Results in supplementary do not evidence the claim that MS defenses have been invalidated.

None of the evaluated defenses consistently reduce thief accuracy for all MS methods. Moreover, any drop in thief accuracy is usually accompanied by a drop in victim accuracy, thus questioning the utility of these defenses. Therefore, we advocate more research to develop stronger defenses.

R3: Key insight: âhard labels may lack meaningful informationâ¦â is well known.

The key insight is actually in the second/next sentence. That, soft pseudo-labels produced by anchor and teacher help a student capture the class structure better than the hard labels by victim model, which are of limited use in OOD data.

R3: insufficient information for reproducibility.

We shall release the source code post acceptance of our paper.

R3: Clarify how Logit Adjustment is achieved.

We use the logit-adjusted softmax cross-entropy loss from [19] during training.

R4: While authors emphasize their use of the labeled dataset compared to existing MS methods, it is not well demonstrated in experiments.

We want to clarify that our main contribution is in the use of unlabeled data along with labeled, unlike existing MS methods that rely solely on labeled data. Table 1 shows that our method (Random+QW / k-Center+QW) outperforms existing methods using labeled data alone (Random, k-Center).

R4: Ablation study is missing.

Due to space limits, we could not include our ablation study findings in the paper. Our CIFAR-10 experiments showed that removing the anchor model significantly lowers thief accuracy while removing the teacher model has a smaller but notable impact. Logit adjustment yielded mixed results.

R4: Impact on clinical practice remains unknown.

Our findings indicate that the IP of proprietary medical imaging models deployed in the real world is not secure. This has huge implications for the IP owners and underscores the need to implement more stringent security measures before deployment."
https://papers.miccai.org/miccai-2024/085-Paper0262.html,"We greatly appreciate the Area Chairs and the three Reviewers (R1&R3&R4) for their detailed review of our work, and we are honored to receive the feedback of being âearly acceptedâ. The constructive comments provided by the reviewers greatly improve the quality and clarity of our manuscript. In response to the comments, we have made several clarifications to address the concerns raised, as outlined below.

Q1: Elucidate the rationale behind our selection of specific architectural choices in CNN for data-driven feature extraction, as well as elaborate on the selection of network hyperparameters (R1).
A1: The architectural choices and hyperparameter selection in our CNN are based on empirical experience. Besides, we observed that modest changes in the architecture and hyperparameters do not affect the modelâs performance. For example, we currently employ four convolutional blocks, and the classification results show minimal variance while using three blocks instead.

Q2: Provide the rationale behind employing the Transformer for modeling temporal features (R1).
A2: In fMRI feature extraction, we utilize the sliding window technique to segment fMRI time series into multiple segments and leverage the Transformer to capture temporal fMRI features. The rationale behind this lies in the Transformerâs ability to capture long-range dependencies in sequential data, such as fMRI data. Given that neural activity captured by fMRI evolves over time, Transformer excels at capturing the dependencies across multiple segments by processing the entire fMRI sequence simultaneously.

Q3: Regarding the reproducibility of the proposed method (R1).
A3: Weâll release our source code to support reproducible research, and the GitHub link will be provided in the final version.

Q4: Regarding the suggestion to add new experiments, such as SOTA methods and attention-based methods (R3&R4).
A4: We thank the reviewers for their valuable suggestion. We intend to incorporate additional experiments, including comparisons with SOTA methods and attention-based approaches, in extended versions of our work in the future.

Once again, we express our gratitude to the Area Chairs and the Reviewers for their invaluable feedback."
https://papers.miccai.org/miccai-2024/086-Paper1932.html,"We thank all three reviewers for their constructive comments and appreciations of our strengths. We carefully consider each point raised by the reviewers and make the following clarifications: 
[R1] The role of dynamic noise kernel: 
The dynamic noise kernel is essentially a noise kernel that reflects the inherent uncertainty of the observations. In our work, influenced by factors such as dataset size, image complexity, and inter-annotator consistency, segmentation observations at different slices are considered to have varying degrees of confidence, with manual observation confidence set as 1. This dynamic noise results in a larger search space for edge optimization when automatic segmentation is less reliable, thereby ensuring robustness.
[R1] Details about the datasets:
The public dataset comes from the following paper which contains the linkï¼Burian et al.: Lumbar muscle and vertebral bodies segmentation of chemical shift encoding-based water-fat MRI: the reference database MyoSegmenTUM spine. (BMC Musculoskelet. Disord. 2019).The clinical dataset from Peking Union Medical College Hospital (PUMCH) comprises a total of 46 cases of spinal deformities (19 males and 27 females, age: 19.6Â±9.9 years). These cases were meticulously annotated using 3D Slicer. In each case, lumbar vertebrae were annotated in the T1-SAG sequence, while the erector spinae, psoas, and multifidus muscles were annotated on all slices in the T2-AXI sequence. 
[R1] Intermediate results:
We will supplement the intermediate results of pose, edge, and segmentation in Figure 3 of the final version of our paper.
[R1] Details about ablation experiments:
Ours-Dyn.Noise-Edge opt.: This group uses the results of pose detection and automatic segmentation as observations, with the confidence always set to 1, initializes the probabilistic shape model, and takes this as the final segmentation result.
Ours-Dyn.Noise: This group, building on the previous one, trains a muscle edge detector and optimizes the geometric parameters of the probabilistic shape model at each slice during inference.
Ours: This group, building on the previous one, assumes that the confidence levels of observations at different slices are different.
[R1] The methodological illustration of pose detection and segmentation model:
These models are based on the Ultralyticsâ Yolov8-s-pose and -seg architectures. The image size is set to 512x512, with the optimizer being Adam, an initial learning rate of 0.01, momentum of 0.937, for 400 epochs, a batch size of 16, without the use of data augmentation techniques, and all other parameters are set to default.
[R4&R5] Code release:
Our code is divided into two parts: pose and edge detection, along with preliminary segmentation, are handled in Python and saved as .mat files. These files are then read by Matlab, where the probabilistic shape model is built, trained, and optimized. We intend to clean up and perfect the code and open-source it before submitting the expended version of our work to a journal. For replication purposes, the muscle representationâs clear structure should make it easy to replicate using available Fourier transform and GP libraries in different programming languages.
[R4] The clarification for the multi-level context:
âMulti-level contextâ emphasizes the importance of integrating information from various sources and levels of detail. This phrase has appeared in works such as Li et al.âs âA Multi-Level Contextual Model For Person Recognition in Photo Albumsâ (CVPR, 2016) and Liu et al.âs âMulti-level context-adaptive correlation trackingâ (PR, 2019). In this work, we exploit three levels of context for robust muscle segmentation: a) spine-level, which consists of the musclesâ relative position and anatomical height features; b) edge-level, including muscle boundaries and the confidence of elective muscle shape; c) deep-level, which presents deep features of muscle images that are not easily established through intuitive cognition."
https://papers.miccai.org/miccai-2024/087-Paper1510.html,"We thank the reviewers for their comments and considerations for strengthening the paper.

R1: We agree that it would be beneficial to compare against other related works, specifically noting that a baseline would be useful to evaluate the modelâs performance on cancer detection. Since submission, a newly published paper [1] on detecting bone metastasis in the thoracolumbar spine reported a per-slice F1-score of 0.72 on axial CT scans, which we surpass (0.74), achieving a similar F1-score to orthopaedic residents (0.73).

R3: We would like to clarify that the manually labeled sets were a randomly chosen subset of the full data, which we split into testing and calibration sets using stratified sampling. We currently force the pipeline to output a positive or negative label without uncertainty. Since we run our LLM locally, we can get probabilities from our LLM to predict uncertainty, which we will explore in future work.

R4, W1: âMethod is simply a combination of existing techniquesâ. We do not claim that LLM summarization and LoRA are novelties of our work. However, our application of them is novel. We do not believe that R4âs references are directly comparable to our work as they are visual language studies that take both text and image as inputs. R4 states that text summarisation has been studied in references [a] and [b], but these focus on generating questions from single-sentence captions of natural images rather than summarising long-form text. In contrast, we demonstrate that open-ended text summarisation can be used to extract structured information from multi-paragraph radiological reports that contain specialised jargon and negation rules atypical in natural language (see Figures 2-3 in Supplementary Materials).

R4, W2: âLack of comparison to existing large vision-language modelsâ. This is an interesting future direction, but it appears to be out of scope for this paper. Our aim was to demonstrate a proof-of-concept that providing vision models with LLM-generated pseudo-labels can achieve performance comparable to models trained with human annotations, rather than creating a new visual question answering (VQA) system. Reference [c] is a medical visual language model in the same subject area as our submission, and we will cite and discuss it in the camera-ready. While it is an interesting study demonstrating multimodal VQA capabilities, it is distinct from our work in that it takes both image and text as inputs and aims to be a standalone visual chatbot.

R4, W3: âGains in labeling performance were minimal against GPT-4â. GPT-4 is a closed-source model that requires payment per token and the upload of sensitive medical data to remote servers for processing. Despite marginal performance gains, our method is more privacy-preserving and allows for local inference at negligible cost.

R4, W4: âTable 5 only presents classification results using labels generated by own method.â We would like to clarify that only the first and fourth rows of table 5 report results of models fully trained using our labels. Row 2 performs inference using SpineNetV2, which is trained using human annotations, and row 3 uses SpineNetV2 to extract encodings and train an SVM using our report-generated labels. We treat rows 2-3 as baselines for our stenosis classification method (row 4). We will make this clearer in the table in the camera-ready version.

[1] Motohashi, M., Funauchi, Y., Adachi, T., Fujioka, T., Otaka, N., Kamiko, Y., Okada, T., Tateishi, U., Okawa, A., Yoshii, T., Sato, S.: A New Deep Learning Algorithm for Detecting Spinal Metastases on Computed Tomography Images. âA New Deep Learning Algorithm for Detecting Spinal Metastases on Computed Tomography Images.â Spine 49.6 (2024): 390-397."
https://papers.miccai.org/miccai-2024/088-Paper1419.html,"We are thankful for the feedback. 
We provide our response in 5 main points.

1- Main Contribution 
One comment is that our contribution suffers from limited technical novelty with only marginal gains. This is not true as we pointed out in the paper motivation how the literature lacks a comprehensive dataset for pulp segmentation. We presented Pulpy3D, a unique and novel contribution, that will encourage dental research and new deep-learning algorithms. Our contribution is not using a prior state-of-the-art model and just adding gated attention to it. We clearly stated in the data collection section and showed in the Baseline comparison how our change aided in creating the seeding model from 150 scans which helped complete the remaining of the dataset. Therefore, the use of gated attention was backed up by its results on the validity of the seeding model. Other modifications in the architecture like using separate networks, one semantic network, and a multi-task network were highlighted in the model description to show how versatile the Pulpy3D dataset is and how different models, architectures, and tasks can run on the dataset. So, to say that the contributions are marginal and summarized by just adding an attention unit is underestimating our contribution.

3- Literature Review 
In the literature review, we focused on the segmentation task of CBCT, particularly the use of 2D and 3D segmentations.  One review commented that we did not cite some important papers that dealt with 3D segmentations. The review mentioned 4 refs. We already cited [1,2] of the 4 refs as refs [24, 5 ]in our paper. We missed the refs [3, 4] but we will add to the final version. 
As mentioned in the related work and dataset motivation [1, 2] work on 2D slices and used 2D-Conv rather than volumes and using 3D-Conv. The interpreted 3D arises from stacking multiple 2D slices which is not an effective 3D representation as it lacks the volumetric features in a pure 3D volume.

4- Experiments 
One of the comments is that our experiments were conducted for IAN segmentation and not enough for pulp segmentation. This is inaccurate. As explained in the Model and Experiments sections, we created a special seeding model for pulp segmentation only. We performed experiments using single networks for both pulp and IAN. We trained a single semantic network and a multi-task two-decoder network for both pulp and IAN with distinct labels for each. Training parameters and runtime environment were indicated in mentioned sections in addition to complete configuration files in the code which will be available once accepted.

5- Running time 
We thank the reviewers for asking about run time. We calculated the inference time of the scans. The mean time in (seconds) is as follows: 
Separate Network: 4.3 sec. Less than 5 seconds.
One semantic network: 3.84 sec. 
The common network: 5.57 sec."
https://papers.miccai.org/miccai-2024/089-Paper2202.html,"We would like to express our gratitude to the reviewers for their valuable feedback and constructive comments on our paper. We have carefully considered their suggestions and addressed their concerns.

With regards to the reviewerâs concerns about the reproducibility of our results, while we are unfortunately not able to share the patient data and the code due to the nature of the agreement we have with our industry partner, we want to emphasize the straightforward components of our approach with additional clarifications in the final version of our paper, so that the readers who have access to similar datasets can conduct their own analysis based on our detailed pipeline description.

Furthermore, we will also elaborate on our core contribution in the methodâs section of the final version, starting with our reasoning for working with a PCA model, which provides a space of smooth and natural skin and skull shapes, and allows the MLP network to operate in a smaller dimensional space. We will also explain why we have chosen to employ the MLP instead of directly fitting the PCA model to the face scan, enabling us to benefit from the non-linearity for a more accurate modeling of the skull and skin shape relationship. Finally, we will highlight the reason behind the creation of the second data matrix, where we have appended the same generic skull mesh to all of the input skin meshes, providing a better initialization of the PCA parameter vector.

Concerning the experiments with the compared methods, we first want to highlight that each compared method underwent fine-tuning tailored for our comparative analysis, optimized to predict a skull suitable for our dataset. Secondly, the observation that the OSSO predicted skull appears to be more detailed than our result can be explained as follows: The respective methods all use different template skulls to base their skull predictions on. For example, the detailed surface structures that appear to be on the OSSO skull prediction results are always there since it can be found in the methodâs template skull. This, however, does not reflect the actual prediction detail / variability of the results of the method. As is shown in the supplementary material in Figure 2, we actually notice a lack of shape variability in the predicted skull shapes generated by the OSSO method in comparison to our approach. This observation is mostly noticeable in the shapes of the estimated jaws.

Finally, the training was conducted on a Dell Laptop equipped with an Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz, 2.59 GHz, requiring approximately 28 minutes on average. Inference on the same device completes in less than 1 minute. Since we are training our MLP with PCA parameter vectors and not mesh data directly, training time on a CPU was reasonable and there was not a strong need for GPU."
https://papers.miccai.org/miccai-2024/090-Paper1135.html,"We thank all reviewers for their constructive feedback.
A. Innovation
R3 questioned the innovation of our method and its rationality (âthe proposed method appears odd to begin with as the auxiliary input is a big part of what the model is tasked to predictâ). We would like to clarify that our method was to facilitate learning a good mapping from x to y by introducing an auxiliary input z at the beginning of the training and gradually ablating it during the training. During inference, z is no longer needed but features related to z are incorporated into the network. y and z are different, but z can be inferred from x and used to predict y more easily compared to x. In our case, x is the image, z is the catheter mask, and y is tip heatmaps or phase feature vectors. To our best knowledge, the method was not published before. The innovation of our work is also acknowledged by R1 and R4. 
B. About learning the causal path x->y
R3 questioned if the actual causal path x->y was learned with AIT and suggested including âA robustness test under some domain changeâ. We want to clarify that the aim of our method was not to learn an actual causal path, nor to learn domain-transferable features. Our goal was to incorporate catheter features into the two tasks in the roadmapping application, and thus accelerate training and achieve better performance (as shown in our experiments). We also explained the intuition behind our method in terms of training convergence and shortcut learning, hoping that researchers in other fields could benefit from the paper.
C. Comparison with other methods
R3 suggested there was âno comparison with other methods and works only variants upon the proposed methodâ. We want to clarify that we compared our method with three other methods designed to incorporate features/knowledge into a network (FT, MTL, T-S), which were not variants upon the proposed method.
D. Implementation details
R1 pointed out the lack of details on the network architectures hindered reproducibility. We would like to clarify that we described the network architectures in the methodology but did not plot the network or specify the hyperparameters. We will include these in the supplementary of the camera-ready version. 
E. Speed in clinical deployment
R1 questioned the inference speed on larger images. We would like to clarify that the pixel spacings were normalized to 0.2mm before inference (âDatasets and Evaluation Metricsâ). For common data in roadmapping, image sizes are around 500-600. Even in cases of larger FOVs, we can crop the image around the catheter tip detected in the first frame and achieve real-time inference with PyTorch/V100 (doctors do not move catheters during roadmapping). However, we agree that it is important to benchmark our networks with TensorRT, which we will include in our future work. 
F. The impact of adding gaussian noise
R1 pointed out that âthe impact of adding Gaussian noise to AIT is not evaluatedâ. We agree that it is important to evaluate how sensitive AIT is to different noise-adding schedules. We will explore it in our future work.
G. Other issues
-R4 suggested adding an indication of the performance compared to SOTA approaches in the introduction without needing more experiments. 
We will add this to the camera-ready version.
-R4: the validity of using frame-wise distance given different frame rates
We want to clarify that we also used the ratio between the frame-wise distance and the cardiac cycle length of the case, which is a normalized metric regardless of frame rates. 
-R4: code and data availability
We cannot make code and data public as they are proprietary.
-R4: statistical tests when comparing methods
We would like to clarify that we did do paired t-tests in our experiments. The bold numbers are the ones significantly better than the others (p<0.05). We will add this clarification to the camera-ready version.
-R4: typos and introduction writing
We will address these in the camera-ready version."
https://papers.miccai.org/miccai-2024/091-Paper3075.html,"We  thank the reviewers for their insightful comments and constructive feedback.

NOVELTY (R5)
While the concept of average calibration error (ACE) is not new, our key contribution lies in adapting it as a directly differentiable auxiliary loss for semantic segmentation, a novel approach not addressed in prior literature. This provides a major distinction from the use of ACE as a metric in classification tasks. The transition from classification to segmentation is a key aspect which we are the first to identify, enabling ACE to be used as a loss rather than a metric. Classification training pipelines provide too few samples within a mini-batch to meaningfully compute its ACE (in contrast to segmentation where each voxel provides a sample). We will make this clearer in the final version.

DIFFERENTIABILITY (R4)
Equations 1, 3, and 5 are differentiable almost everywhere as the set of bin edges is of measure zero. In practice, we observe that discontinuities at bin junctions do not significantly affect gradient updates. These are based on averaging within bins rather than across bins and benefit from the large number of voxels per bin. Our approach thus ensures sufficient differentiability for practical purposes. Most neural networks, such as those using ReLU, contain gradient discontinuities. Training convergence for such non-smooth neural networks has been demonstrated under mild conditions [Allen-Zhu ICML 2019]. While we empirically observe convergence, we agree that demonstrating that our loss function meets these mathematical conditions would be insightful for future work. We further highlight that the calibration error calculated by our proposed loss is not significantly affected by the number of bins used, as evidenced in supplementary Fig. A1.

SOTA COMPARISON (R3, R4)
Direct application of DECE (R3) and other prior train-time methods were not performed due to fundamental differences between classification and semantic segmentation rather than difficulty in reproducibility (cf. NOVELTY paragraph). Previous methods cannot directly estimate calibration from a mini-batch and thus exploit surrogate approaches like DECE which relies on meta learning. This makes analysing comparisons more challenging as it implies many more changes than just the loss function.

Label smoothing (R4) applies a pre-determined ground-truth softening resulting in an overall reduction in confidence and a knock-on but uncontrolled effect on calibration quality. In contrast, our method allows us to monitor and train for calibration explicitly, providing a tailored approach to improving model reliability.

Despite this, we recognize the value of additional comparisons and plan to include them in a future journal extension.

STATISTICAL TESTING (R4)
We recognize that t-tests assume data independence and have limitations in this context. However, t-tests remain standard practice in the community and provide useful insight. We appreciate the suggestion and will consider Bayesian methods in future work to enhance statistical rigour.

RESULTS REPORTING (R3, R5)
Figure 3 focuses on the DSC loss because adding plots from other losses led to significant plot overlap, making them difficult to interpret (R3). The DSC loss results were visually informative and central to our paper and were thus highlighted. We nonetheless present all results transparently in Tables 1 and 2. We will revise Fig. 3 for clarity, ensuring legible tick labels and thicker markers.

The regions shown in Fig. 4 differ in size as they represent predictions from different models (R5). Each model has unique predictions, resulting in varying false positive and false negative regions.

MANUSCRIPT ORGANISATION (R5)
Conflicting opinions on organisation and clarity were provided, with R3 and R4 providing very positive feedback. However, we appreciate R5 comments and will revise the manuscript to remove unnecessary commas and improve readability. We will define sub-equations to enhance clarity."
https://papers.miccai.org/miccai-2024/092-Paper2248.html,"We thank the reviewers for their thoughtful comments. We are pleased that the reviewers found our work ânovelâ (R1), âeffectiveâ (R3, R5), âwell-writtenâ (R5), and praised the âsubstantial improvementsâ (R1) and âcomprehensive experimentsâ (R5) presented in our study. We are particularly pleased that the reviewers highlighted the improved âgeneralizabilityâ (R1) and reduced preprocessing at inference (R5), as weâd argue these are the main contributions of our paper which have clinical impact. We address their feedback below.

R1 & R3 noted that a single CNN architecture is used for experiments. The purpose of our work is to address a data-specific problem in echo, which is architecture independent. This differs from [7, 11] which propose generalized computer vision augmentations. Focusing on a single architecture allowed for in-depth validation using both i.d and o.o.d datasets and in a semi-supervised setting, which is of more relevance to our problem. Furthermore, because the data used in this work is typically paired with ResNet [a, b], this backbone is more relevant. We will add these arguments to Section 3.3.[a] Huang et al âTMED 2â ICML 2022. 
[b] Wessler et al âAutomated Detection of Aortic Stenosisâ JASE 2023

R3 questions the accuracy-focus trade-off. For o.o.d data, there is no trade-off as improved focus on the sector results in improved accuracy. Decreased classification performance on i.d data is expected as the spurious shortcuts aiding performance are not learnt due to improved sector attention (see Section 3.3). The discovery of background shortcuts which artificially boost i.d performance is a contribution of our work, rather than a limitation.

R3 suggests applying CutMix to the background only. We considered this idea but decided against it for two reasons: (1) the background is mostly black pixels, so a background-only CutMix algorithm would mostly mix empty crops, which is inefficient; (2) mixing full backgrounds maintains a realistic image, and we found augmentations that cause unrealistic disruptions to the image reduced performance (Section 3.3).

R1 comments that determining the best Î» value in wBackMix remains unclear, and the validation set favors Î»=0.  There may be a small misinterpretation of Table 2 by R1, as the optimum choice of Î» for f=0.05 is 1 or 2, suggesting that weighting the loss in favor of the augmented exampled has a positive impact on results, but the choice of Î» does not have a big impact.  A large Î» may restrict learning from unaugmented examples, while a small Î» does not place any emphasis on learning from the augmented ones. We recommend a grid search to identify the best configuration. Weâll add this to Section 3.2.

R3 asks what happens when applying BackMix when images have different shapes. This is a scenario we encountered as different scanners often acquire images at different resolutions. We followed typical preprocessing [17,18] by resizing all images to the same resolution. Weâll add this information to Section 3.1.

R5 had additional recommendations to improve clarity. Our responses below: 
(1) We will update Fig 2,4,5 to improve visibility of the ultrasound.
(2) We will provide examples of A4C in Fig 4.
(3) Weâll include a class distribution in Section 3.1.(4) We did not find any significant variance in performance between views. 
(5) In %F equation, we are counting the number of highly activated pixels.(6) Standard augmentations are applied to all methods.

R3 cites ânoveltyâ as the first reason for a weak rejection, however there is no mention of novelty within âweaknessesâ or âconstructive commentsâ, neither has this been raised by any other reviewer so we are unable to address this critique in any way. Our novel contributions are listed in Section 1.

R5 questions the resources needed to manually check the sector masks. Weâd argue that a human check is not necessary, however we wanted to verify that our masks were reliable, which they were"
https://papers.miccai.org/miccai-2024/093-Paper3885.html,"R1:1) The main aim of the paper is NOT to develop a SOTA denoising method, but to evaluate and suggest the best conditional samplers from the inpainting DDPM literature for the fluorescence microscopy denoising task. Therefore to show the relative performance of DDPMs we compared against two widely used methods N2V and CARE. However, upon suggestion by the reviewer, we compared the accuracy of our method with two additional methods namely SSID and IDR. Here we present SSIM metric comparisons for all datasets with methods mentioned in parentheses. Planaria - (0.270, 0.309, 0.296, 0.386)(SSID, IDR, N2V, Ours). Tribolium - (0.258, 0.301, 0.208, 0.458)(SSID, IDR, N2V, Ours). Flywing - (0.221, 0.163, 0.174, 0.290) (SSID, IDR, N2V, Ours). Indeed SSID and IDR performed better than N2V specially in Tribolium data that has structural correlated noise present in images. Still our method outperforms all these methods.
2) We will change the wording in the third contribution to reflect that we evaluated the support of N2V predictions for DDPMs in denoising (and not self-supervised methods in general).
3) Since the goal of this work is the first demonstration of DDPMâs efficacy for this task, we will leave comparisons with GANs as future work. We will also make sure to include a more comprehensive literature survey in the final manuscript.
R3: We thank the reviewer for the positive feedback. Due to space limitations we chose to respond to major concerns, but will incorporate suggested clarifications in the final manuscript.
R4: We thank the reviewer for positive comments however we do not see any weakness described in the main weakness question.
1) We use N2V outputs to condition DDPMs to test the hypothesis that initial denoising by N2V can further improve denoising during the generative process. However our results demonstrate that DDPMs conditioning on noisy images are better than conditioning on N2V predictions.
2) We split all datasets in a 80:10:10 ratio for train, eval and test sets..Each z-plane is treated as an individual image during training, resulting in 229120-Planaria, 198400-Tribolium, and 711200-Flywing train images. For evaluation, each z-plane is denoised individually, but max-projected for SSIM, MSE, and PSNR calculations, consistent with previous methods.Thus we have 1790-Planaria 1550-Tribolium and 1778-Flywing eval/test images. Size of input - 64x64 Training time: ~24 to 48hrs on a V100GPU, Evaluation time: <1second/image. 
3) Rather than a new algorithm, this paper focuses on the novel application of DDPMs to denoise fluorescence microscopy images, acknowledging the unique noise characteristics that differentiate them from other imaging modalities.
R5: We appreciate the positive feedback.
1) We would like to make a distinction between deconvolution and denoising. While deconvolution enhances spatial resolution, our method focuses on denoising fluorescence microscopy images, vital for removing photon and camera readout noise caused by low laser powers or brief exposures.
2) Denoising has several important applications such as preserving light-sensitive samples, reducing motion blur, neuron activity extraction [1,2], segmentation[3,4] and tracking, thus receiving considerable attention in the fluorescence microscopy field and therefore has broad applications.
3)We would like to clarify the training and testing procedure. We train the DDPM backbone using high quality clean images only from the train set. During testing/evaluation we DO have paired noisy and high quality images. Thus we denoise images using our DDPM method and evaluate the accuracy using high quality images in test/eval sets.
4)We were not able to compare against Deltavision as it is not open-source. However, we further demonstrate better performance of our method compared to 2 additional SOTA self-supervised denoising methods (see R1). 
[1]Buchholz,et al.ECCV,2020
[2]Stringer,et al.bioRxiv,2024
[3]Li,et al.Nat Methods,2021
[4]Lecoq,et al.Nat Methods,2021"
https://papers.miccai.org/miccai-2024/094-Paper3117.html,"We thank all reviewers (R1, R3, R4) for their positive feedback: well-structured paper and extensive experiments (R1, R3), practical setting (R3), and novel and efficient approach (R4). Our code will be publicly released.

R1
Detailed Explanation (Q1): We thank R1 for suggestion. We will further improve the clarity of our BAPLe algorithm in the Suppl. material.More Experiments (Q2): We thank the reviewer for acknowledging the extensive experiments in recommendation section. There are a few additional experimental visualizations in the Suppl. material that the reviewer may find insightful.

R3
On Effectiveness of BAPLE over Baselines (Q1): Our threat model, as discussed in the paper, assumes that the attacker has access to only a few-shot dataset. Baselines that fine-tune the entire model lead to overfitting and suboptimal performance with limited downstream training data. In contrast, BAPLe precisely tailors both the vision and text input spaces, enabling the frozen medical VLM to leverage its rich knowledge and achieve high attack success rates without compromising clean performance. While the text prompt is the main difference between BAPLe and the fine-tuning-based baselines, our primary objective is to demonstrate that the widely adopted prompt-tuning stage, where only the text prompt is updated, is vulnerable to backdoor attacks even with low data requirements (3rd para introduction). We believe that BAPLeâs simplicity in achieving high attack success rates emphasizes the vulnerability of the prompt-tuning stage.
Sleeper Agent Comparison (Q2): Sleeper Agent focuses on backdooring networks trained from scratch, which may not be feasible for large Med-VLMs. On the other hand, our goal is to backdoor the prompt-learning stage of pretrained Med-VLMs in a few-shot setting.
Patch Trigger (Q3): BAPLe uses natural-looking medical-related patch triggers (logo/text) commonly found in medical images (Fig. 1) instead of random patches.

R4
On Learnable Text (Q1): The learnable text prompts are optimized in the embedding space during prompt tuning and not to be provided by end-user during inference after the model deployment. User interacts with the infected model in the same way as the original clean model.
Clinical Scenario (Q2): In the case of real-world deployment of an open-sourced vision-language medical model, malicious actors can release an infected model version that would operate seamlessly with clean images but generate predetermined responses when presented with poisoned images. Our threat model, discussed in Sec. 3.1, highlights such attacks and strives to promote the safe adoption of such Med-VLMs before their deployment.
Defense (Q3): To our knowledge, we are the first to explore vulnerability of medical vision language foundation models towards backdoor attacks during prompt learning stage. We agree with R4 that defense for such attacks is an interesting future direction.
ASR (Q4): We apologize for the confusion. We have used the term âBackdoor Accuracy (BA)â which is an alternative term for ASR in context of backdoor attacks.
Finetuning (Q5): Recent backdoor attacks generally fine-tune the full model weights. We show that this setup is not effective in the case of our few-shot scenario (see Tab. 1),  acknowledging the reviewerâs point. Importantly, we also compare BAPLe with a prompt-tuning-based baseline by applying baseline methods to the Med-VLMâs image space. BAPLe outperforms this baseline (Tab. 1), highlighting its effectiveness in few-shot scenarios. Computational resource analysis is in the introductionâs last line.
Prompt Tokens (Q6): We used 16 prompt tokens, which we empirically found to balance performance and computational load.
Patch and Noise (Q7): The combination of patch and noise synergistically enhances the attackâs performance, as demonstrated in Tab. 3(c).
Epochs (Q8): We set the number of epochs to 50. 
Image-Text Pairs (Q8): Thank you for the suggestion. We will explore this in future work."
https://papers.miccai.org/miccai-2024/095-Paper2796.html,"We thank the reviewers for their valuable feedback. Below, we provide responses to the weaknesses(W) and comments(C) raised by reviewers R1, R3, and R4.

(R1.W1) Practical but Limited Innovation: We proposed an automated system to digitally transform high-precision tooth oral structures by integrating deep learning segmentation, 3D reconstruction, registration, and mesh stitching. We achieved significant improvements in registration accuracy and mesh quality, validated through quantitative metrics and scoring by seven dental experts (Table 1(a),(b)). Our approach demonstrates practical innovation in digitizing dental structures.

(R1.C) Suggestion for Deep Learning Integration: We appreciate your suggestions. Based on our method, we are creating a dataset for deep learning-based integration. We plan to expand our research to include deep learning-based registration and mesh generation.

(R1.C,R3.W4) Reproducibility. Lack of access to the source code and dataset: All cited works used internal datasets, making it straightforward to implement and validate our model. To enhance reproducibility, we will publish our code in the revised version.

4.(R1.W2) Comparisons Limitations(Registration): We apologize for the lack of sufficient comparisons in our paper. We believed comparing our coarse ICP approach with baseline SOTA results would demonstrate our methodâs coverage. However, we agree that benchmarking against several relevant studies would strengthen our validation approach.

(R2.W1,2,3) Initialization Details (Details of Orientation of the Two Modalities, initializing R coarse): The centroid alignment stage finds the center of mass for each tooth, creating correspondences between the IOS and CBCT datasets. We align the centroids of eight upper teeth (11-18) and eight lower teeth (31-38) separately for both modalities. This initial rigid transformation ensures proper orientation, providing a robust starting point for coarse registration.

(R3.W4, R3.C2,3) Details on the Mesh Stitching Task, Comparisons, and Limitations: Due to extensive content, including pseudo codes and images, we couldnât include everything. To enhance clarity, weâve visualized the process in Fig. 1(b), provided essential functions in pseudo code, and added descriptions. While there are existing tooth-based mesh registration studies, mesh stitching has only been explored by Qian et al. To facilitate further research and comparisons, we plan to publish our code in the revised version.

(R4.W1) Dataset Variability (not vary in spatial resolution). CBCT and IOS data were taken simultaneously within a year to create a high-precision oral structure of the teeth. Collecting such data is challenging, and there are no public datasets combining CBCT and IOS. Consequently, the spatial resolution does not vary. We acknowledge this limitation and plan to conduct further tests as more data becomes available.

(R4.W2) Image Artifacts and Statistical Analysis: Severe CBCT artifacts degrade segmentation, negatively impacting downstream tasks. Incorporating denoising and artifact removal would enhance our methodology. Our patient data sets are anonymized and lack demographics, limiting our analysis. We will consider more comprehensive analysis given appropriate datasets in future work.

(R4.W3) Clinical Feedback (may not serve as final proof of the quality of the produced results): We understand the challenge of numerically comparing the two modalities when generating a combined mesh, as there is no actual ground truth. Therefore, we employed a user evaluation method, like numerical evaluation metrics used in generative task. We consulted seven dental experts to evaluate the results.

(R4.C) Clarification in Table: We apologize for the inconsistency in Table 1(a). We mistakenly used âSurface(mm)â instead of âSurface Distanceâ in the table. We will correct this in the revision and review the manuscript for other typos to ensure clarity and accuracy."
https://papers.miccai.org/miccai-2024/096-Paper1336.html,"We thank the reviewers for their valuable comments and recognition of SAM tuning (R1, R3, R4), auto prompting (R1, R3, R4), promising results (R3, R4), and extensive experiments (R4). Major concerns are addressed as follows:
Q: Why and how is SAMUS designed for ultrasound? (R1, R3)
A: The primary goal of foundation models is to provide a powerful backbone for various downstream tasks. Given severe modality gaps in medical imaging such as WSI, CT, and US, we focus on one of the most frequently-used modalities namely ultrasound. To build a foundation ultrasound image segmentation model, we design/construct: (1) A large ultrasound dataset (US30K) for adapting SAM into SAMUS for performance improvement across various ultrasound tasks as shown in Fig. 3. (2) A CNN-branch image encoder and a cross-branch attention to supplement local fine features to address blurred boundaries and complex object shapes in ultrasound imaging. This is why SAMUS outperforms other foundation models on ultrasound image segmentation as stated in Tabs. 1&2. (3) Pioneer exploration in auto-prompting for end-to-end segmentation. It is motivated by the fact that ultrasound is frequently-used in clinicians while vanilla SAM relying on manual prompts brings heavy burdens. Following SAMCT, SAMedOCT, and other SAM-related models named according to modalities or tasks, we name it as SAMUS.
Q: The supplementary file cannot be opened. (R3)
A: It contains more details of datasets, visualization, performance comparison, GPU and FLOPs comparison, and ablation studies. To avoid possible issues, it will be published online for reference.
Q: More model details. (R1, R3, R4)
A: [For R1&R4] In SAMUS, as shown in Fig. 1, we apply a position adapter onto the positional embeddings of SAM to generate new position embeddings. As convolution excels at capturing local and fine features, the CNN branch is to transfer its detailed information to the ViT branch in vanilla SAM. In the auto prompt generator, as stated in the first paragraph of page 6, T0 is the output tokens rather than the combination of task tokens and output tokens. Specifically, the task tokens Tt are newly introduced and learnable while T0 denotes output token embeddings of the frozen mask decoder. It should be clarified that no point prompt is included in APG. When using APG instead of SAMâs prompt encoder to extend SAMUS to AutoSAMUS, no manual prompts were used during training/inference. [For R3&R4] For each downstream task, the number/length of corresponding task tokens is k. As stated in Tab. 3 of the supplementary file, the quality of generated auto prompt embeddings first increases with the increase of k, and then tends to saturation. Thus, the value of k is set as 10 in our experiments. [For R4] It should be clarified that SAMUS is a universal segmentation model and not restricted to any predefined segmentation task. Given a new category not included in SAMUS, it is addressable by fine-tuning SAMUS via zero-/few-shot learning.
Q: More experimental details. (R3, R4)
A: For a fair comparison, SOTA foundation models are re-implemented and trained for 400 epochs on US30K under the same settings using the same single-point prompts. For foundation models requiring resolutions of 1024x1024 (i.e., SAM, MedSAM, MSA) and 512x512 pixels (i.e., SAMed), images were resized accordingly. It is noted that though their input sizes are different, the output sizes are consistent (i.e., 256Ã256 pixels) with SAMUS for comparison. The GPU memory costs of SAM and SAMUS are 15.34G vs. 4.30G, showing SAMUSâs stronger deployability.
Q: AutoSAMUS- in Tab. 3 outperforms Tab. 4. (R4)
A: Methods in Tab. 3 are trained and tested on DDTI, UDIAT, and HMC-QU. Methods in Tab. 4 are trained only on TN3K and BUSI and tested on DDTI and UDIAT for generalization evaluation. They are not comparable. In Tab. 3, without fine-tuning the encoder, AutoSAMUS- still outperforms task-specific models on DDTI and UDIAT, showing the value of SAMUS and APG."
https://papers.miccai.org/miccai-2024/097-Paper3253.html,"Dear Reviewers and Meta-Reviewers,

I am writing to express my sincere gratitude for the early acceptance of my paper #3253 entitled âBGDiffSeg: a Fast Diffusion Model for Skin Lesion Segmentation via Boundary Enhancement and Global Recognition Guidanceâ. I deeply appreciate the time and effort you have invested in reviewing my work and providing valuable feedback.

I have carefully reviewed all the comments and suggestions provided. I am committed to incorporating these insights to enhance the clarity, quality, and overall impact of my manuscript. I will ensure that the necessary revisions are made promptly and thoroughly.

Thank you once again for your meticulous review and constructive feedback."
https://papers.miccai.org/miccai-2024/098-Paper0908.html,"We appreciate the positive comments from reviewers and constructive suggestions for improvements. However, there is still misapprehension and imprecision in the reviewerâs findings to be clarified, which may affect the assessment of the validity of the paper.

1) Significance of the study (detection vs. segmentation)
i) Reviewer #4: Brain tumor detection seems not so clinically relevant since we can do segmentation which is by far better in addressing such a disease.
ii) Reviewer #4: But no convincing in terms of clinical relevancy.
iii) Reviewer #6: It would be beneficial to include additional comparisons with other state-of-the-art object detection models, such as MaskRCNN or Segmentation Anything, especially considering that segmentation is a more detailed level of object detection.
Response: Due to low costs, brain tumor detection is useful for automated medical screening. If needed, further scans are required, and brain tumor segmentation will play a role in confirming the diagnosis of an abnormality. In clinical practice, patients/doctors probably choose single plane/modality MRI scans to save money because the prices of MRI scans vary in different types. The detection models are suitable for this scenario.
     Weâd like to clarify that Mask-RCNN and Segment Anything Model (SAM) are segmentation models, while the main task of this study/dataset is object detection not segmentation.

2) Novelty of the paper
i) Reviewer #5: Frankly speaking, this paper seems to be a combination of various existing modules. For example, as the authors mentioned: âwe develop a novel BGF-YOLO architecture by incorporating Bilevel Routing Attention (BRA), Generalized feature pyramid networks (GFPN), and Fourth detecting head into YOLOv8â.
ii) Reviewer #5: Limited in Novelty.
Response: We hope to clarify that our model is not merely a combination of various existing modules but introduces a new YOLO-based architecture to overcome the challenge of brain tumor detection. Moreover, we also introduced modifications in each individual module; for example, we designed a new enhanced GFPN-structure neck with CSP, Conv, Upsample, and Concat submodules. Furthermore, we also introduced an additional 160Ã160 detecting head aligned with the new structure of feature fusion networks in the neck part.

We group and summarize the major concerns of reviewers, which we carefully addressed as follows:

1) Figure quality improvement.
i) Reviewer #1: Readability of their main figure.
ii) Reviewer #4: Unclear graphical representation of architecture.
Response: We will improve the figure quality in the revision by enlarging the elements and adjusting the color tone to highlight the major elements in the figure.

2) Further comparisons
i) Reviewer #5: Without thoroughly comparing with brain tumor detection methods.
ii) Reviewer #6: Limited within the YOLO framework. Medical image detection models.
iii) Reviewer #6: Discussion of RT-DETR-X and YOLOv9-E.
iv) Reviewer #1: Some further comparison such as the time execution.
v) Reviewer #1: Results from the detection with other architectures.
Response: 
Weâve compared it with RCS-YOLO, which is the best-performing method applied to brain tumor detection in previous studies. Moreover, we have conducted additional experiments to compare with other non-YOLO-based state-of-the-art object detection methods such as RT-DETR and Co-DETR. The results have been shown in the anonymous GitHub, which we will include in the main text and Table 1.
   We have also conducted further comparisons in terms of time execution. The potential limitation of the proposed BGF-YOLO is mainly on increased computational complexity. The inference speed of BGF-YOLO is 223.0 ms, which is longer than YOLOv8 with 78.9 ms. However, the BGF-YOLO gives a substantial improvement in accuracy despite a slight increase in computational effort. We will report the parameters of each method in Table 1, for example, YOLOv8x | 68.2M, BGF-YOLO | 84.29M. 
We will include sample detected images in the supplementary material in comparison with other different architectures.

3) Generalizability of our model
i) Reviewer #6: Evaluate the proposed method on larger and more diverse datasets.
ii) Reviewer #1: Other brain tumor datasets to generalize their method.
Response: To demonstrate our modelâs general applicability to other computer vision datasets, we evaluated the proposed model on a facemask dataset in the Coronavirus Disease (COVID-19) situation. See the External Validation on anonymous GitHub which performs better than YOLOv8. We will apply our method to other types of medical imaging modalities or pathologies in future work.

4) More details for clarity
i)  Reviewer #4:"
https://papers.miccai.org/miccai-2024/099-Paper2799.html,N/A
https://papers.miccai.org/miccai-2024/100-Paper1194.html,"Thank you to all the reviewers and chairs for your comments. Our responses are as follows:
R3 & R6: Explain the rationale behind hyperparameter choices and model design decisions.
We selected hyperparameter $\alpha=1.5$ on the validation set, as it balanced the MSE loss constraining latent space similarity and the similarity loss affecting matching. Further tuning could improve results.
R3 & R6: Discuss limitations, set clearer expectations for future research.
Our approach has limitations, such as information loss from cropping or downsampling 3D volumes and restricted input text tokens. Future work could employ advanced techniques like GPT4-o for processing large 3D data and leverage updated language models supporting longer inputs.
BIMCV-R has many applications beyond retrieval, including long-tail classification, image-text pre-training, and grounding. 
R4 & R6: Clarify if the dataset underwent quality control, provide more dataset details.
To ensure the quality of the BIMCV-R dataset, we implemented a rigorous multi-step quality control process. For the textual data, we engaged professional translators, followed by GPT-4 refinement and simplification. Medical experts then provided diagnoses, identifying 96 distinct disease types with a long-tailed distribution. We removed excessively long or short diagnoses and will provide more details in the open-source files. Regarding image quality, we conducted manual screening and applied deep learning-based super-resolution and denoising techniques. 
R6: Conduct more empirical validation and comparison with existing methods, and discuss differences.
Our work is the first to address 3D medical text-image retrieval. Although many video-text retrieval methods exist, such as the CLIP4clip model we reproduced, there are significant differences between video and medical data:"
https://papers.miccai.org/miccai-2024/101-Paper0424.html,"We thank all reviewers for their valuable feedback. Below we address the main concerns and clarify some issues.

(R3, R4) Limited novelty: As R3 correctly states, we were inspired by [28], who used binary latent diffusion models for image synthesis. The novelty of our approach is to adapt their method to solve an unsupervised anomaly detection (UAD) task. To this end, we introduce a masked denoising process to recover the healthy reconstructions. Although other UAD approaches have also used masking schemes, our proposed pipeline is novel and specifically designed to exploit the binary nature of the Bernoulli diffusion model, allowing anomaly values to be obtained directly from the model output, i.e., the predicted flipping probabilities. We will include work [a] proposed by R4 in the camera-ready submission and briefly discuss the contribution of our masking scheme, which is not learned by the model but taken directly from the model output, highlighting the advantage of our binary architecture for this binary task.

(R3, R4) Evaluation metrics and comparison to SOTA: As R3 and R4 correctly point out, our method lags behind AnoDDPM in terms of AUPRC. As suggested by R3, we will highlight this in the camera-ready version. However, from the quantitative and qualitative comparison shown in Table 1 and Figure 4, it is comparable to a variety of SOTA diffusion-based approaches, while significantly reducing memory requirements and sampling time, which will be crucial for a successful extension to 3D problems in future research. Unfortunately, there are no ground truth anomaly maps available for the OCT dataset, so quantitative results are not possible for this dataset. We will clarify this in the camera-ready version. 
We agree with R3 that reporting [DSC] and AP would be beneficial. Following AutoDDPM [4], we additionally report the PSNR value between input and output image to indicate the reconstruction quality close to the input image. The aim is to show that the anatomical information is preserved in healthy tissue.

(R3) Experimental design: We thank R3 for the constructive comments on the training/validation/test set. We argue that training and testing on separate datasets introduces a domain shift, which may affect model performance. Omitting the lower and upper slices of the BRATS2020 dataset removes the bias of tumors being more prevalent in the middle of the brain. Although this 2D slice-wise approach can indeed be improved in its design, it serves as a good first step in model development before benchmarking as suggested by R3. As we trained only on healthy samples, no cross-validation was performed. The grid search presented in Figure 3 (left) was performed on the test set described in Section 3.

(R4) Meaning of subfigure 3: Taking advantage of the binary nature of Bernoulli diffusion models, we can extract image-level anomaly scores directly from the model output. In Figure 3 (right), we show that the flipping probabilities predicted by the diffusion model are indicative for out-of-distribution data. This can be seen by the clear difference between the healthy and diseased cohorts for different masking thresholds P and noise levels L.

(R5) Details on the masking scheme: We thank R5 for the positive feedback. The proposed masking scheme extracts entries in the latent space with a high flipping probability (indicating out-of-distribution data). While these entries can be changed during the denoising process, all other entries of the original input image are preserved. This step is implemented in Equation 7. This allows the anatomical information of the input image to be preserved, resulting in a higher PSNR value compared to the unmasked baseline (Table 1). The final layer of the diffusion model is a sigmoid activation function that provides probabilities in the range [0,1]. We will consider the suggestion to explore the calibration of the probabilities to further improve the estimation network in future research."
https://papers.miccai.org/miccai-2024/102-Paper0208.html,"R1: Reviewer#1 
We appreciate that this paper is technically dense and we have made every effort to clarify the details. We believe all questions from R1 can be addressed without significant changes to the paper and we summarise them as follows. 
Feedback to the main weakness. 
(1) The dataset is considered to include the information of LamÃ© parameters. LamÃ© is defined in Sec. 2.1. We will clarify that LamÃ© is part of the Eq. (1) and add LamÃ© in Fig. 1.(2) In the inverse problem of estimating material properties, we adopt the clinical-relevant assumption that PZ and TZ regions generally exhibit different stiffness properties. This assumption has been described in Sec. 2.2 (the third sentence of this paragraph), and has been incorporated (easily) into Eq. (2) (i.e., beta_k). 
(3) Table 1 includes more information, and shows that the nonlinear model can succeed in all test cases while the linear model fails in 1/3 cases, which has been described at the end of Sec. 3. The confidence interval is represented as shadow area in Fig. 4, which will be briefly introduced in the final version. 
(4) The choice of points over images.Most previous work (e.g., [14]) mostly used point clouds rather than image intensity directly for several practical constraints, such as the large number of voxels and unclear benefits in using intensity values for modelling biomechanics. In addition, we note that point clouds as robust features that can be directly extracted from raw medical images (e.g., voxels) in two spaces, are very suitable for multi-modality registration.  In contrast, there is still debate what the most suitable similarity metrics are to be used for the multi-modal medical image registration (i.e., the MRI-US registration in this study). This point will be added.Feedback to the constructive comments. 
The organization, description (e.g., mathematics, elaborations of experiments, the variability), and figures will be revised as requested without significant changes in the final version.

R3: Reviewer#3R3 is mainly concerned about the contributions compared to [14].We clarify that the main new contributions include (1) biomechanical constraints are generalised from linear to nonlinear elasticity;  (2) the inverse problem (and its solution) of estimating material properties with soft tissues is formally formulated using a completely different objective with [14]; (3) the influences of the nonlinear elasticity over performances of solving both forward and inverse problems have been explored.  We should note that formulations in governing equations differ significantly from those in [14] as second-order terms are considered, and the formulation of the inverse problem is entirely new (using both linear and nonlinear elasticities). Besides, Eq. (1) in this paper is a general framework for the forward problem, where both linear and nonlinear elasticities can be accommodated. 
On the other hand, we politely point out that codes will be released as stated in the Abstract.

R5: Reviewer#5(1) R5 is concerned about the saddle point detection procedure, which is done by finding the flat line in Fig. 4. We assume that âsaddle pointâ itself can convey this information, but this point will be clarified. 
(2) Estimating the ratio of the Youngâs modulus between two regions plays an important role in biomechanics-constrained non-rigid point set registrations. This is verified in previous published work [a]. We will emphasize this point in the final version. 
(3) R5 also asks the clinical value of estimating the ratio of the Youngâs modulus. First, the ratio of Youngâs modulus is essential for the biomechanical-constrained non-rigid registration, which is used in the ultrasound-guided prostate biopsy procedure. In addition, we note that a region with higher stiffness (e.g., Youngâs modulus) may have a higher probability of being cancerous. 
[a] Modelling prostate motion for data fusion during image-guided interventions, TMI 2011."
https://papers.miccai.org/miccai-2024/103-Paper2262.html,"We sincerely thank all comments.
[Reviewer #4]
Q1. Equations (5)

Thanks for the comment. This kernel is essential for preserving spatial information when computing the second derivative related to location. We reshape predicted cell density vectors into a 3D format (B, C, H, W, D) and use a 3D Laplacian kernel instead of the traditional central finite difference method. This approach allows for more efficient implementation via F.conv3D, reducing computational budget.  We will add a clarifying note.

Q2. Embed time t with the input x? And the benefit of t.

We clarify that in the 2nd paragraph of page 4 conceptualises each segmentation optimisation step as a progression in time. Time embedding is crucial for simulating tumour dynamics during training, ensuring the modelâs output aligns with expected growth dynamics. It also facilitates the calculation of the first-order time derivative, integrating temporal dynamics into our model. This will be clearer in the camera-ready.

[Reviewer #5 & Reviewer #6]
Q1. Single evaluation dataset and heterogeneity across cohorts and sites

The BraTS 2023 dataset comprises diverse, multi-institutional pre-operative structural MRI scans reflecting various clinical settings. This dataset is recognised for its heterogeneity in glioblastoma sub-regions, providing a robust model of variability across different cohorts and sites. It is widely used as a standard protocol for evaluating models in brain disease research. As future work, we plan to test as well in functional MRI.

Q2. The improvement in DICE

It is important to highlight the consistent improvement across all experiments. We conducted a Wilcoxon test on all results generated with and without the biophysics regularisation. The results demonstrated statistically significant improvements in DICE scores across each evaluated region and mean of three regions, with a confidence level exceeding 95%.

[Reviewer #6]
Q1. Detail of estimatorâs architecture

The feature maps (B, C, H, W, D) are initially flattened to vectors (B, C, H Ã W Ã D) with respect to C. These are then concatenated with a time matrix T of the same size, resulting in an input size of (B, 2C, H Ã W Ã D). The estimator includes three fully connected layers with two sine activation functions (nn.linear + sine + nn.linear + sine + nn.linear). Outputs are (B, C, H Ã W Ã D) to compute the first-order time derivative, and finally back to (B, C, H, W, D) for subsequent calculations of the second derivative related to location and boundary conditions.

Q2. Detail of the other compared networks

For our study, the nn-UNet model was reimplemented using MONAIâs DynUNet, adhering to the original paper with a kernel size of [3, 3, 3], five downsampling and upsampling steps, instance normalisation, and no deep supervision, matching the configurations used in the BraTS challenges. The UNet-TR setup follows the original design with a feature size of 16, hidden size of 768, MLP dimension of 3072, 12 heads, perceptron-based positional embedding, and instance normalisation. For SegResNet and SegResNet-VAE, while based on default MONAI settings, we adapted instance normalisation for consistency across all networks.

Q3. Regularisation be used during the beginning of the optimization?

For early in the training, lower-level features such as texture are more prevalent. We suggest applying the regularisation to smaller feature maps at middle stages of training. At these stages, the feature maps contain higher-level, more pathological representations.

Q4. Random seed and neural network initialization

Thank you for your suggestion. We are taking your recommendation to include multiple runs with different random seeds in our future work to better assess our modelâs performance.

[Other minor comments]

All typos will be corrected in our camera-ready version. More detail of networks will be provided in the supplementary file. The code will be released upon acceptance."
https://papers.miccai.org/miccai-2024/104-Paper0453.html,N/A
https://papers.miccai.org/miccai-2024/105-Paper0668.html,N/A
https://papers.miccai.org/miccai-2024/106-Paper2563.html,"We thank all reviewers for their valuable comments and appreciate them for affirming the novelty and contributions of our paper. We carefully addressed the issues raised by the reviewers and itemize our responses to major points as follows:
1)R1 concerned about the modest performance improvement.
Our method is an unsupervised approach. While the performance improvement over supervised methods like Deep-SLR is modest, our unsupervised method has strong generalization capability, enabling reconstruction under any sampling pattern and rate, which has a clinical impact and supervised methods cannot achieve.
2)R3 concerned about the the task of image and sensitivity estimation and Parallel MRI.
Parallel MRI is a commonly used imaging technique in clinical practice. It involves the simultaneous acquisition of data using multiple receiver coils to accelerate reconstruction. Sensitivity estimation refers to the quantification of the receiver coil sensitivities. By estimating the sensitivity distribution for each coil, it becomes possible to accurately disentangle signals from different locations, thereby improving image quality.
3)R4 confused about the lack of problem formulation in the method section and the confusion about sub/super-scripts after incorporating the diffusion model.
The content at the beginning of Chapter 2 is the problem formulation. If possible, we will rename that section as âProblem Formulationâ. In addtion, before incorporating the diffusion model, the iteration number was represented by a superscript in the iterative algorithm. After adding the diffusion model, the iteration number is represented by a subscript to maintain consistency with the diffusion formula.
4)R4 confused about the representation of x_t in Fig.1 and redesigned it.
In Fig.1, at the DDPM forward stage, x_t represents the noisy image. In the reverse diffusion, the input to the IMU is the undersampled image (x_t), and to maintain consistency with the diffusion formula, it is defined as x_t. In future, we will consider to redesign Fig 1.
5)R4 concerned about the diffusion models to generate the images and sensitivity maps following the DDPM training and DDIM inference, the noise profile and justification under this problem setting.
The diffusion model is trained by progressively adding noise to fully-sampled images and their corresponding estimated sensitivity maps. During inference, undersampled images (R_A= 4,6,8) and initial sensitivity estimates (estimated from ACS sampling line) are used as inputs for iterative MR image generation. It reduces the number of sampling steps and has no need to sample from noise by incorporating the diffusion model, significantly accelerating the process. Thus, the noise profile remains Gaussian. As indicated by Eq. 6 in the manuscript, both sub-problems assume Gaussian noise conditions and correspond to Gaussian denoising formulations. The optimization algorithm for each step is consistent with the sampling process of DDPM and DDIM.
6)R4 and R6 suggested to modify the spelling and formatting issues and improve the readality of method section.
We will make the corresponding modifications according to your suggestions and reorganize the method formulas in future.
7)R6 confused about the IUM importance in ablation studies and IUM and SMUM process.
The under-sampled MR images x_t are input to the SMUM, which performs gradient descent without proximal operator. During training, the diffusion models for image and sensitivity are trained only once. IUM and SMUM are inference stages, the inverse diffusion sampling is combined with the iterative algorithm.
8)R6 concerned about the lack of experiments at R_A=2 and the need for more validation.
Under R_A=2, traditional methods such as GRAPPA have achieved satisfactory reconstruction performance. To validate the superiority of deep learning methods, we conducted evaluations under R_A=4,6,8. In addtion, we will consider to validate our method on more datasets in future work."
https://papers.miccai.org/miccai-2024/107-Paper3335.html,"We appreciate reviewersâ positive comments on the novelty and superior performance of our method. Below, we clarify main issues of reviewers.

Common Questions
CQ1: Details of datasets. The two datasets were obtained from the axillary lymph node resection specimens of breast cancer from two hospitals, respectively. All patches were registered at 40x magnification. Further details will be provided in the final version.
CQ2: Cell region improvement. Cells are the primary structure in HE images. Our results outperform baselines at both SSIM and MS-SSIM, which are measures of structural similarity. Besides, we had assumed masks of HE images generated by Cellpose as ground truth and observed a higher IOU of our method than baselines.
CQ3: Writing suggestion. We appreciate the suggestions and promise to thoroughly revise the manuscript.

Response to R3
Q1: Details of dataset
A1: Please refer to CQ1.
Q2: Weight values of CSLoss
A2: We use the first four weights of VGGLoss for CSLoss, which is clarified in Section 2.3.
Q3: Cell region improvement
A3: Please refer to CQ2.
Q4: CSLoss is similar to deep supervision
A4: Deep supervision applies supervision at both the middle and final layers to help the network learn more useful features at the middle stages. CSLoss, simply and effectively, leverages the cell semantics of PCSM to provide supervision at the final layer.
Q5: Writing suggestion.
A5: Please refer to CQ3.

Response to R4
Q1: Fundamental difference between datasets
A1: CQ1 has provided more details. The fundamental difference is the data collection process, such as scanners and staining procedures of different hospitals.
Q2: Any potential batch effects in the datasets
A2: Yes, Factors such as data processing at various hospitals and varying train-test data splits. These prompted us to conduct the external validation.
Q3: Statistical analysis for close results
A3: The test and external validation results have demonstrated the effectiveness of our method. We will address statistical analysis for better clarity in future work.
Q4: Only align the encoder of PCSM
A4: Firstly, the decoder performance relies heavily on the high-quality features of the encoder. Besides, the decoder blocksâ output integrates their own features with those from the encoder blocks through skip connections. Aligning two sets of features simultaneously is more challenging, which was found unstable for training in early experiments. We are exploring potential ways to better utilize the decoder.
Q5: Is a multi-scale discriminator used?
A5: Itâs used in default pix2pixHD.
Q6: Downstream task. Why not use a pretrained model
A6: Please refer to CQ2 first. We didnât report the results since the Cellpose-generated mask differed somewhat from the ground truth. We are planning to conduct downstream tasks with ground truth.
Q7: The same guidance for both inputs to D could be redundant
A7: Itâs natural that the guidance for virtual images helps D identify fakes, while the guidance for real images also helps D identify real ones more certainly. Actually, for the label-to-image task, the same label guidance for both inputs to D is also adopted in pix2pixHD.

Response to R5
Q1: Comparison to other SOTAs
A1: Most studies on FFPE-to-HE virtual staining use unaligned cGANs, which typically perform inferiorly to aligned cGANs. Other studies are based on pix2pix, which has been compared. Other aligned cGANs like SPADE or OASIS are unsuitable for our task due to the demand for pixel category information.
Q2: Advantage of CSLoss
A2: L1 loss directly aligns every pixel and tends to incentivize a blur, as suggested in pix2pix. In contrast, CSLoss aligns high-level cell semantics and is advantageous in preserving structural details and robustness to noise (e.g., blots). Better performance of CSLoss than L1 loss is found in early experiments.
Q3: Research w/o adversarial loss
A3: This idea is constructive and worth further exploration.
Q4: Writing suggestion
A4: Please refer to CQ3."
https://papers.miccai.org/miccai-2024/108-Paper2020.html,"R1:
Thank you for your encouraging comments. We will make the modifications in the final version based on your two suggestions.

R3:

R4:"
https://papers.miccai.org/miccai-2024/109-Paper2888.html,N/A
https://papers.miccai.org/miccai-2024/110-Paper1779.html,"We appreciate the efforts and high evaluations from the AC and three reviewers. We have read all the constructive suggestions from the reviewers and will make the following revisions in the final version of this paper.

Reviewer #3:
(i). The performance is actually slightly improved after using 30% of the training data. However, we will locate the bottleneck that prevents the improvements after more than 30% of the training data was used and revise our method to achieve significant and continuous growth. These future works will likely appear in an extended journal version.
(ii). Using only the largest site from the dataset could avoid the site-effect but could also significantly reduce the sample size for testing, which is not in line with our initiative to test the model generalization.
(ii). The main focus of this study was to improve the transfer of the general foundation model to multiple brain tasks. In Ref. [2], most of the methods are specifically designed for AD. As a result, we may face a trade-off between generalizability and specificity, which demands further studies. We also note that Ref. [2] found that more than half of the surveyed papers may have suffered from data leakage and thus reported biased performance. Considering other deviations, such as data pre-processing and validation scheme, the baseline performance of MCI identification in our dataset and setting may not be as high as the intuition. This is indeed a significant issue in method comparison and development. We appreciate the sharing of Ref. [2] and would try the proposed validation scheme for a reasonable comparison.

Reviewer #5:
UniFormer and Med3D are based solely on vision information (T1), while BLIP-2 is based on both vision and text information (identical to BrainSCK). We will supplement more details about the comparison methods in the camera-ready version.

[1] Kunda, M., Zhou, S., Gong, G., & Lu, H. (2022). Improving multi-site autism classification via site-dependence minimization and second-order functional connectivity. IEEE Transactions on Medical Imaging, 42(1), 55-65.
[2] Wen, Junhao, et al. âConvolutional neural networks for classification of Alzheimerâs disease: Overview and reproducible evaluation.â Medical image analysis 63 (2020): 101694."
https://papers.miccai.org/miccai-2024/111-Paper1955.html,"We thank the reviewers for their kind words and valuable feedback. Below, we address the key points raised.

R4 raised concerns about the bias introduced to our dataset since we source data from one center. We are aware of this limitation and are currently setting up a larger multi-center consortium to validate our results. With this initiative, we also aim to obtain additional objective target outcomes in consultation with clinical partners. Indeed, our current labels (treatment or not) might be influenced by patient-specific factors that cannot be extracted from CT. Nevertheless, we expect that our unsupervised feature extraction approach might provide valuable information for novel targets.

R4 questioned the use of aggregate measures derived from the deformation fields. Indeed, it is likely that the ârawâ deformation fields contain additional information that might be extracted using, e.g., a separate CNN. However, we chose not to train a model directly from the deformation fields due to the limited size of our dataset. However, we agree with R4 that exploring more complex classification models using deformation maps is a valuable future study.

We agree with R3 that an evaluation of our method in a public benchmark would provide a better way to assess the generalization of the method. Unfortunately, to our knowledge, there are no public datasets of 3D CT images of patients with cSDH, for neither surgery prediction nor segmentation. We have contacted authors of related segmentation works but found that they were reluctant to share data.

R3 noted that our model is not trained end-to-end. There are practical benefits to the current modular approach: By considering segmentation a task that is independent of pseudo-healthy brain synthesis, we are able to easily swap out trained segmentation models. On the other hand, end-to-end training of the segmentation, pseudo-healthy brain synthesis, and even the classification task might improve performance in all stages. Hence, end-to-end training is interesting for future work. We will add this to the Discussion.

Following questions from R1 and R3, we would like to further clarify the terms in the loss function (Eq. 5). Each term in the loss function aims to guide the deformation to a more uniform and anatomically plausible form. R3 noted the ventricle symmetry loss might cause an unrealistic ventricle shape. This is partially true, as the final ventricle system in Fig. 3 is not perfect, and the effects of the âforced symmetryâ can be seen. However, we found that the inclusion of the ventricle loss in our model had a positive effect. Most of the compression due to cSDH occurs within the ventricles. By explicitly forcing this volume to be regained by the ventricle system,  we prevent excessive deformation elsewhere in the brain. We will include this information in the Discussion. In future work, a detailed ablation study could help shed further light on the contribution of each loss term. The open-source implementation of our paper will include all the coefficients in Eq. 5, which are generally kept at 1.0. We used these to balance the spectrum of our loss functions (keeping them between 0 and 1 as much as possible), rather than to rank them in importance.

Following the comments of R1, weâd like to clarify our MLS measurement protocol. The MLS was measured following the clinical protocol from our clinical center and validated through in-person consultations with our clinical partners. We will include this in the final version of our paper.

R1 asked about how our model would work with regard to different types of conditions. This model is developed for cSDH so will likely work for acute subdural or epidural hematomas. â¤â¤However, hemorrhagic strokes like SAH or IPH have different characteristics. â¤â¤Applying our findings to these hemorrhages would be interesting, but would also require caution, as their diagnosis and decision-making processes differ significantly from cSDH."
https://papers.miccai.org/miccai-2024/112-Paper1241.html,"We would like to thank the reviewers for their helpful comments.

#3, 6) Rationale for selecting the complex Morlet wavelet
The Morlet wavelet is preferred due to its Gaussian shape in the frequency domain, which minimizes ripple effects that could be misinterpreted as oscillations (Cohen, 2019). Its effectiveness has already been demonstrated in data requiring critical temporal resolution, such as EEG studies (Herrmann et al., 2005). Additionally, the Morlet wavelet has an optimal ratio between the Fourier period and wavelet scale, facilitating interpretation in the frequency domain (Torrence and Compo, 1998). This is why Chang and Glover (2010) used the Morlet wavelet to measure rs-fMRI functional connectivity. Given the characteristics of the rs-fMRI BOLD signal, which includes rapid fluctuations where temporal resolution is critical, we selected the Morlet wavelet. We will add and clarify these rationales of selecting the complex Morlet wavelet in our revised paper.

#3, 6) Rationale for selecting the CWT
We chose CWT due to the limitations of FFT and STFT. FFT cannot observe the frequency over time, making it unsuitable for analyzing non-stationary signals. STFT has a tradeoff between frequency and temporal resolution by selecting the window sizes. In contrast, the CWT captures both frequency and temporal information simultaneously, using scalable wavelets, providing a multi-resolution analysis that allows for capturing various scales and frequencies of signals. This makes it more suitable for non-stationary signals and better at capturing transient features in biological signals than FFT-based methods.

#6) Comparison with Fourier transform
To our knowledge, there is little research on applying FFT or STFT to rs-fMRI with deep learning, particularly in separating the real and imaginary parts. Also, a fair comparison is complicated because FFT is unsuitable due to the lack of temporal information, and in STFT, the real or imaginary feature can be zero in a real-valued signal depending on the frequency band (Sorensen et al., 1987). This requires additional research to find an appropriate representation. It should be addressed in future work.

#3, 5) Fig.2, interpretation and clinical implications
The ROIs in Fig. 2 are extracted by averaging well-classified subjects across groups and selecting 5% of the 200 ROIs to highlight the most significant regions, which are identified as meaningful biomarkers based on preliminary ASD research. Thus, our method extracts significant brain signal features, enabling the discovery of important biomarkers and enhancing diagnosis. However, as reviewers mentioned, detailed explanations are needed. We will add (1) individual-wise analysis and (2) the existing related studies in our revised version, including generalized analyzed ROIs with their relation to symptoms of ASD.

#5) Performance comparison
While the performance improvements were marginal, we believe that our novel approach of utilizing CWT-based real and imaginary values as input and designing a Transformer-based network architecture deserves its originality and is comparable to traditional approaches such as ALFF and FFT-based fMRI analysis methods. 
In the meantime, considering the modelâs efficacy, we conducted experiments on the ADHD-200 dataset and achieved meaningful improvements compared to the existing methods. However, we think it is not appropriate to add the results in the revised paper because the main focus of our current work is to develop a method for ASD diagnosis. Therefore, we will officially publicize the results on our GitHub page with the codes.
Regarding the use of mentioned SOTA network architectures with CWT as input, it is not possible because those networks fundamentally require functional connectivity as input.

#3) Eq. 8
We also used imaginary parts as a query and real parts as a key and value, combining these features for the classification. We will clarify the relevant part in our revised version."
https://papers.miccai.org/miccai-2024/113-Paper1458.html,"We thank all reviewers for their overall support of our paper âimportant topicâ (R4&5), âa novel methodâ (R1), âImprovements remain consistentâ (R1&4&5), âablation study showing the contributionâ (R1&5). Below, we address 2 general concerns followed by specific responses for each reviewer.

G1: Parameter k. (R1&4)
We set k=6 (the number of teacher model heads), with the input including the five augmentations mentioned in section 2.2 and the original image. Performance improves with increasing k, plateauing at k=7. Considering performance and memory trade-offs, we opted for k=6. Details will be included in the revision.

G2: Deep and shallow layer in cache mechanism. (R1&5)
For encoder-decoder architectures like U-Net or DeeplabV3, we define âshallow layersâ as those near the encoderâs input and the decoderâs output end (a layer consists of Conv2d, BN, and ReLU). The threshold for dividing layers into âdeep and shallowâ can be adjusted for dataset compatibility. We determined that setting one shallow cache layer is optimal, with details to be refined in the revision.

R1Q1: Comparative analysis of 2D, 2.5D, and 3D.
The baseline in the first row of Table 3 is the 2D approach. Introducing 2.5D improves performance, but 3D requires significantly more computational resources.
R1Q2: The fairness of ensemble.
STTA utilizes the ensemble predictions of the teacher model, following the effective multi-branch ensemble approach in TTA. Similar practices have been adopted by comparative methods such as UPL-TTA [22] and URMA [11].
R1Q3: Training procedure.
We will add the overall loss to Algorithm 1 in section 2.2.

R4Q1: More comparisons.
Apart from the non-TTA method in [2], [1] and [3] are strong baselines for future comparisons.
R4Q2: Optimize equations.
The inverse transformation is displayed in Fig. 1, and we will update Eq. 2 to include it. We will also reorder Eq. 6.
R4Q3: More slices.
Testing indicates that accuracy improves with an increase in the number of slices. However, given that most source domain models have 1 or 3 input channels, we opted for 3 slices as input during adaptation.
R4Q4: Cache application.
The cache mechanism enhances the student modelâs long-term adaptability without compromising the teacher modelâs stability. Our tests indicate that applying it to both models disrupt the teacherâs weight update and reduces performance.

R5Q1: Novelty.
STTA is the first TTA method to incorporate 3D spatial information and implement a multi-head ensemble with data augmentation. It introduces a novel cache mechanism (recognized by R4 as innovative âApplying cache mechanism in TTA is a great ideaâ), addressing catastrophic forgetting, unlike [9] where caching accelerates inference in AIGC. While we adopted the deep and shallow cache division from [9], STTA has devised a unique cache access method, validated effectively in experiments. Additionally, we adapted the MT [13] for TTA tasks and used L_Ment [22] to enhance the ensemble effects.
R5Q2: Unclear math.
We will make equations simpler and clearer according to your insightful suggestion. Also, to be clear, the quotation mark â is used to distinguish between the teacher and student models; we compare the confidence matrix with the threshold p_th element-wise; âpâ with a bar represents the ensemble output; âconcatâ will be changed to âmergeâ to avoid confusion.
R5Q3: Clinical environment design.
Our method is a form of TTA research, which fine-tunes models during testing in clinical settings to adapt to diverse medical data from various devices, operators, and patient groups.
R5Q4: Confidence hypothesis.
The prediction confidence originates from the source model. Whether the low confidence is due to domain gaps or the inherent uncertainty of medical images, the pseudo-label enhancement strategy (Eq.2) is applicable. We will refine this statement.
R5Q5: Clarification on Eq.6.
The deep cache is activated only at the last slice of each 3D volume among the continuous input volumes."
https://papers.miccai.org/miccai-2024/114-Paper3582.html,We thank the reviewers for their insightful comments and suggestions. We address critiques below:
https://papers.miccai.org/miccai-2024/115-Paper0640.html,"Common Question: Why choose only MISS as the baseline modelï¼
Common Response: Thanks for the question! In related research, most medical small-scale VLMs have been classification or ranking models. But in real medical application scenarios, there are often no answer candidates available. MISS was the only small-scale generative Med-VLM at the time of writing, showing excellent performance on VQA benchmarks. So, we conducted extensive studies on different PEFT methods (46 experiments shown in Tables.1-4 and Appendix-Tables.1-2.) based on MISS, adjusting the model structure, combining PEFT units with each components, thereby proving the effectiveness of our method.

R4:
Q1: Baseline
A1: Please see the âCommon Responseâ.
Q2: Evaluation Metric
A2: Due to the special requirements of medical tasks, the accuracy (ACC) is the ONLY GOLD STANDARD for evaluating model in the Med-VQA, which is different from the general domain. Incorrect diagnostic results will bring catastrophic results for patients. Whether for closed-ended or open-ended questions, ACC is the most reliable gold standard.
In the recent research on Med-VQA, ACC is used as the sole evaluation metric, such as the studies accepted by [MICCAI 2022] and [MICCAI 2023], proposing M3AE and MUMC. To maintain consistency with prior work and facilitate comparison, we use ACC as the sole evaluation metric.
However, we acknowledge the reviewerâs suggestion to use more metrics. If our paper is accepted, we will include these metrics in the APPENDIX.
Q3: Insufficient Experiment?
A3: In Sec.4 and Appendix-Sec.2, we present the results of experiments that use different PEFT methods and model architectures on two datasets, totaling 46 experiments, as shown in Tables.1-4 and Appendix-Tables.1-2. As you mentioned in the âDescribe the Contributionâ section, through extensive experiments,.
Specifically, (i) we demonstrated the effectiveness of PEFT methods, as shown in Fig. 2(a); explored the optimal combination method of PEFT and Med-VLMs, as shown in Tables.1-4, and (iii) explored the impact of instruction-format data on fine-tuning Med-VLMs.
The sufficient experiments contradict with your major factors of the score.
Thanks for your review. We hope you will reconsider your evaluation of this study based on the responses provided above.

R5ï¼
Q1: Guidance for Future Research
A1: Thanks for the comments! In Sec.3, we constructed a unified model, MILE. Based on experiments, we proposed the most effective fine-tuning method, MILE-LoRA, and explored the optimal combination of PEFT units and Med-VLMs. As [R4] noted, we point out an effective way to maximize performance gains. As [R6] noted: âThis work is significant as it demonstrates how such methods can be applied in the medical domain, where data is scarce and fine-tuning large models is challenging.â This provides valuable guidance for future research. 
Q2: Baseline
A2: Please see the âCommon Responseâ.
Q3: Need Validation on Popular General Domain LVLMs (LLaVA & BLIP-2)
A3: We would like to clarify that this paper focuses on small-scale medical Med-VLMs rather than general LVLMs.
(i) For many researchers, there are limited resources for LVLM fine-tuning. Our pioneering work is of great significance;
(ii) Recent studies accepted by [MICCAI 2022] and [MICCAI 2023], proposing M3AE and MUMC, have both shown that Med-VLMs must be pre-trained on medical domain data. Otherwise they perform badly. 
So, validating our method on general-domain LVLMs such as LLaVA and BLIP-2 would be meaningless.
We sincerely hope that after reading our response, the reviewer will consider raising the score.
R6ï¼
Thank you for your approval and advice on our study!
Q1: Effect of Instruction Fine-tuning
A1: In Sec.4, we showed fine-tuning Med-VLMs using only instruction data significantly degrades model performance, while the data can improve a global fine-tuned model that has already converged on the original data, and achieve the SOTA results.
Thank you again!"
https://papers.miccai.org/miccai-2024/116-Paper3097.html,"We appreciate the thoughtful analysis provided by the reviewers (R1, R3, R4). Below, we address their main concerns regarding our paper point by point.

Mask Generation Description (R1, R4): We acknowledge the condensed expression of the mask generation process. We will provide a more detailed explanation in each step and add appropriate references in the revised version. Specifically, the artifact mask was generated by computing the difference between the input and output of G_f, zero-filling negative parts to highlight positive discrepancies as artifacts, and applying Otsu thresholding [N. Otsu et al., 1975] to exclude minor changes that are not considered artifacts. The bone mask was produced using a standard graph-cut technique [Y. Boykov et al., 2006] to delineate the bone area, and the intersecting area of both masks was used as additional input channels for G_a.
Comparisons with Recent Methods (R1): We carefully selected models based on the unsupervised GAN framework for attribute editing tasks. Although suggested models such as U-Net GAN and Res-UNet GAN are more recent, they differs fundamentally from our studyâs approach manipulating images based on a user-specified attributes.
Loss Weight Determination (R1, R3): The preservation of the outline and internal structure of the bones is crucial for our generated images. We evaluated 100 randomly selected images from the training set, choosing final weights that produced the most realistic images while maintaining bone structure and edema patterns. We provided quantitative evaluations for various weight configurations in Supple. Table 1. Further details will be elaborated in the manuscript revision.
Target Artifacts and the Purpose of MRI (R3): Our focus is on unique artifacts that appear in DECT but not in standard CT, resulting from errors in extracting spectral information. These artifacts often resemble edema patterns, hence the use of MRI taken concurrently with DECT to meticulously annotate these artifacts. Our primary interest is not in streaking artifacts, which are irrelevant for bone marrow edema (BME) detection. The red box in Fig. 2 highlights the artifacts we aim to remove. The artifacts can occur in soft tissue, but our focus was within bones for helping BME detection.
Artifacts Overlapping Disease Areas (R3): Preserving the edema pattern is our top priority. In regions where artifacts and edema patterns overlap, we consider these as edema areas.
Risk of Deleting Unknown Patterns (R3): Our model incorporates identity mappings, highlighted in yellow boxes in Fig. 1, to prevent excessive alteration or removal of important patterns, particularly edema.
Evaluation with Simulated Results (R4): Thank you for the suggestion. We considered using simulated artifacts for evaluation. However, if the artifacts are artificially generated and deviate significantly from real patterns, the evaluation may not be reliable for real-world applications. Simulation could be beneficial for comparative analysis with other deep learning models.
Artifact Detection Accuracy (R4): The metric âartifact eliminationâ is graphed in Fig. 3 alongside the edema detection rate. This naming convention was chosen to emphasize that a higher value indicates better performance. We will replace âartifact eliminationâ with âartifact-free detectionâ in the revision to avoid confusion.
Generator G_a conditioned by Mask (R4): Indeed, G_a was fed by the mask as well as X_f as a two-channel input.
U() function (R4): The target artifacts are bright patterns within the bone that may resemble pathological changes. Thus, we regarded positive discrepancies (X_a-X_af) as artifacts.
Otsu Thresholding (R4): We omitted to specify that the threshold \theta was adaptively determined by Otsuâs method. This will be clarified in the revision.
Eq. 5 Error (R4): We will correct the expression ây=C_e(X)â to ây=C_e(G_f(X))â in the revised manuscript.
Incorrect Sentences (R4): We will make the necessary corrections. Thanks."
https://papers.miccai.org/miccai-2024/117-Paper3954.html,"We appreciate the reviewersâ (R1, R3, and R4) comments and insightful suggestions. The value of our proposed new approach to formulating the cardiovascular artery segmentation task as a disconnected components detection task was acknowledged. Our methodologyâs effectiveness has been validated on two public and one private dataset, with a comprehensive evaluation (R3). Additionally, we have made the synthesized dataset public, benefiting the entire community (R1). Below, we address the reviewersâ concerns.
R1:
Small disconnected components versus noise scatters: To mitigate the impact of noise scatter, we implemented a noise filter (excluding components smaller than 4 voxels) in suppl. algorithm 1, line 2. If accepted, we will revise the algorithm in the final submission to further clarify this point. Additionally, our focus on the relatively large disconnected components for reconnection was driven by clinical considerations. The clinical impact of very small disconnected components on cardiac flow dynamic calculations and/or plaque detection might be very limited due to the uncertainties and low signal-to-noise ratios, so we excluded these small components.
Fig. 2 revision: We thank R1 and will improve them in the final submission.
R1 and R3:
R1 (computation cost) or R3 (Multiple disconnected components): To bridge the gap between training and real-world inference scenarios and achieve relatively high detection and reconnection accuracy, we iteratively detected and repaired one pair of points at a time. We acknowledge that this is a limitation of our method due to the relatively high computation cost. While we did explore the method of detecting multiple disconnected components simultaneously, this resulted in severe missed detections. In the future, we will further our investigation to find a balance between detection accuracy and efficiency. In the final submission, we will add more discussions on this topic.
R4:
Comparison with other methods: We thank R4 for the suggestions and will include all recommended references in the Intro. of the final submission. The paper by Zhao et al. (PACMCGIT 2023) proposed a Ball B-Spline Curve method for modeling free tubular objects, which requires knowledge of the disconnected pointsâ location. In contrast, our method introduced an automated framework that includes both the detection of disconnected components and the connection using an open curve snake approach. Compared to DeepVesselNet (Tetteh et al., Front. Neurosci. 2020), which focuses on direct vessel segmentation and addresses bifurcation detection, our approach emphasized the post-processing step once the vessel tree with disconnections is available. Our method therefore has the potential to apply to all vessel segmentation tasks as an additional step to address topology concerns after leveraging any SOTA methods for the CNN-based segmentation.
More results and technical details: Our current disconnection detection network identifies one pair of disconnected points at a time. Therefore, there is indeed a gap between this setup and real-world scenarios. To address this, we proposed Algorithm 1 in the suppl., which includes a step to iterative run the network until all disconnected components are reconnected. The number of required iterations depends on the number of disconnected components.
Evaluation improvement: Our primary goal for this paper is to propose a novel framework to improve vessel segmentation and share our real data, the data synthesis pipeline, and the resulting dataset with the community. While our current disconnection detection method has achieved good performance (OKS = 0.859), we strongly agree with the reviewersâ comments regarding the need for further experiments and analysis on this algorithm. We will run more ablation experiments on these aspects in our future studies including comparisons of different methods. We will revise the final submission to point out these directions in the Discussion section."
https://papers.miccai.org/miccai-2024/118-Paper0053.html,We are grateful for the positive and insightful feedback received from the reviewers and the AC. We would like to answer the following points raised in the reviews.
https://papers.miccai.org/miccai-2024/119-Paper0918.html,"We sincerely thank all the reviewers for your valuable comments. Special thanks to R1 for the strong accept and highlighting the âHigh clinical relevance and impact,â âNovel machine learning method,â and âClear presentation.â We appreciate R3 for the weak accept and the praise of âgroundbreaking techniqueâ and âsignificant advancementâ. Additionally, we thank R4 for your detailed review, noting the âInteresting methodâ and âimproved results,â as well as for pointing out the issues in our writing and presentation.
â¢ Concerning issues of writing and presentation, such as 1)âuncommon and too many acronymsâ[R4], 2)âtypos in the titleâ[R4], 3)âunusual step namesâ[R4] and 4)âunhelpful sentencesâ[R4].
A: We have double-checked the entire manuscript and had it carefully proofread by a native speaker. 1) We have revised and reduced acronyms, for example, replacing âMIEâ with âMyocardial Infarction Enhancementâ. 2) We have corrected typos in the title. 3) We have corrected unusual step names, for example, replacing âCardiac mechanics-guided kinematics interpretation moduleâ with âCardiac motion moduleâ. 4) We have removed unnecessary sentences, for example, removing âVNE was published in Circulationâ.
â¢ Concerning issues of implementation and model, such as 1)âImplementation/model details are not clearâ[R4], including ânumber of data samplesâ[R1] and âIs the system trained end-to-end?â[R4]; 2)âWill the code/models be made available?â[R4]; and 3)ârelevant literatureâ[R1].
A: Due to space constraints, some implementation/model details are included in the Supplemental Material of our submission. For example, each data sample consists of 25 CINE frames, 8 T1 images, and 1 LGE image. Additionally, other missing details, such as âthe system is trained end-to-endâ will be included in the final manuscript. 2) We have open-sourced our code on GitHub. 3) We will include these VNE-related citations in the final manuscript.
â¢ Concerning issues of model design and performance, such as 1)âadaptability to highly variable individualsâ[R3], 2)ârobustness to unforeseen scenariosâ[R3], and 3)âinherent complexity and interpretabilityâ[R3, R4].
A: 1) Highly variable individuals are a recognized challenge in the field, and our knowledge-and-data-driven method is motivated to address this. Our method relies on myocardial mechanics and cardiac imaging atlases to flexibly infer myocardial function and cardiac structure, rather than solely on the training data. The experimental results also demonstrate the superiority of our method. 2) As mentioned above, our method also exhibits robustness to unexpected variations. This robustness is achieved by understanding and adapting to variations from both kinematic and morphological perspectives. 3ï¼Our knowledge-and-data-driven method does not significantly increase complexity and enhances interpretability. On one hand, the introduction of knowledge only adds the myocardial strain calculation formula, rather than adding new layers of complexity. On the other hand, the modelâs decision-making becomes more transparent. For example, the calculation of myocardial strain can directly show myocardial function. 
â¢ Concerning issues of experimental analysis, such as 1)âquantitative analysis of clinical metricsâ[R1, R3, R4], and 2)âdifferences between image-level and region-level metricsâ[R3].
A: 1) The clinical quantitative results (i.e., Fig. 5 in the manuscript) are a correlation coefficient (R) of 0.91 and an intraclass correlation coefficient (ICC) of 0.95 for scar size (P<0.001), as well as an R of 0.85 and an ICC of 0.90 for transmurality (P<0.001), demonstrating a high consistency between the synthesized myocardial scars and LGE images. More quantitative results are available on our GitHub. 2) As stated in our experimental setup, image-level metrics assess the entire image quality, while region-level metrics focus specifically on the MI areas.
All other minor comments will be carefully considered and addressed."
https://papers.miccai.org/miccai-2024/120-Paper1314.html,"We thank the reviewers for their constructive comments and are pleased they find our work novel, significant, convincing, and well-presented. We now address the main issues.
R1,R4: Note that in the submissionâs supplementary material, we also attached the full soon-to-be-published chapter on the technical details of the data generation process, as we indicated in Chapter 5. We will emphasize this paperâs availability in the revision.
R3, R4: Evaluation based on synthetic data: Our experiments utilized real case data with realistic synthetic deformations. As always, the challenge lies within motion validation. MR tagging is not dense enough in the thru-slice direction, and for CT, ground truth data for tangent movement is lacking. Therefore, we generated movement for validation purposes.
R1,R3: Applying method to arbitrary timesteps: Our method can be applied to any timestep in the cardiac cycle. We will clarify this in the revision.
R1, R4: Model training: Our model was pre-trained on a large motion-based dataset and fine-tuned individually for each of the 300 pairs, as described in Section 4.2 of our manuscript. We will clarify this in the revision.
R1,R3,R4: Figure clarity, math formatting, missing introduction to FMs, ZoomOut and ARFlow: We will incorporate all the suggestions in the revision. Thank you for bringing this to our attention.
R1,R4: ZoomOut model accuracy, modelâs reduced performance on lower angles: We conducted our evaluations in the voxel space. The average mEPE of the constraints from ZoomOut, including errors from converting voxels to mesh and back, was 3.7 mm, affecting all torsion angles, but relatively more at lower angles. We will add the constraints mEPE plot to the supplementary material.
R4: Regularization of the displacement field: Our current approach outperformed other methods without standard regularization. We agree that a more sophisticated approach could further improve and is worthy for a follow up.
R4: Missing Hyperparams: Hyperparams detailed in Section 4.4.
R3: Methodâs performance on healthy and diseased hearts: Our motion model is learned using a geometric preserving constraint on the surface, and not on pre-defined parameters of a movement model. As such, it can cope with unseen large deformations. By construction it can handle healthy and diseased hearts.
R3: Potential failure modes: We can generate synthetic deformation with multiple solutions based on symmetries where our approach can fail. However, due to the geometric structure of the LV, there is no rotational or reflective symmetry structure, so this is hypothetical in our study case.
R4: How to calculate sparse displacements from mesh vertices: conducted by subtracting corresponding vertex pair locations. We will clarify this in the revision.
R1: comparison to FFD methods: We prioritized SOTA optical flow methods and selected ARFlow as our baseline. VoxelMorph uses a U-Net architecture, while we use a more recent one. However, we compare to segmentation-based loss similar to VoxelMorph and will add the missing citation. TDFFD and similar B-spline-based methods are tailored for sequences, whereas our method addresses motion between two specific timesteps.
R1: Assessment via strains: The choice of window size for strain calculation impacts the trade-off between accuracy and robustness. Since we canât validate it directly, we did not add it.
R4: Performance on image similarity, dice score, and displacement field smoothness: Standard optical flow metrics may not gauge our methodâs performance accurately, especially for tangential movement. Methods relying on radial displacement score higher on image similarity and dice but miss tangential accuracy. We prioritize precise surface motion, resulting in 8.7% higher MSE image distance and 13.6% lower dice score compared to the baseline. Radial displacement methods produce smaller displacement magnitudes and smoother fields; our focus on tangential motion increases L1 smoothness by 48.5%."
https://papers.miccai.org/miccai-2024/121-Paper1149.html,"We are grateful to the reviewers for acknowledging our contributions and considering our work as âa significant contribution to the field of medical imagingâ. The reviewersâ questions mainly regard the method comparison, chest X-ray generation, method clarification, paperâs reproducibility, and clinical applicability. This response letter clarifies these questions.

Using Chest X-ray for cardiovascular disease risk prediction is a very new research topic and thus there is few related work. The most relevant work [1] on this topic appeared after the submission deadline of MICCAI 2024, which uses convolutional neural networks to analyze chest X-rays to predict 10-year risk for major adverse cardiac events. We have been contacting the authors for their implementation and datasets to set up experiments to compare their methodâs performance with the proposed BI-Mamba. Comparison with this cutting-edge method will be included in the camera-ready version.

[1] Weiss, Jakob, et al. âDeep learning to estimate cardiovascular risk from chest radiographs: a risk prediction study.â Annals of Internal Medicine 177.4 (2024): 409-417.

Our study generated frontal and lateral chest X-rays to perform cardiovascular disease risk prediction, bearing in mind that these two views are the most commonly ordered screening tests in clinical practice. We acknowledge that with the inclusion of more projected chest X-rays to draw a closer proximity to CT, the prediction AUROC will presumably increase. However, this setting loses its connection with clinical practice.

The Tri2D-Net method in CT [2] used a detector to extract the heart region from a chest CT volume and a three-branch network to combine features extracted from 2D slices in three orthogonal views (axial, sagittal, and coronal) for cardiovascular risk prediction. In this study, instead of limiting the ROI to the heart region, we preserved a complete chest X-ray for cardiovascular risk detection. Such implementation follows our observations that imaging features outside the heart, such as blood vessel problems, are also critical factors to determine cardiovascular risk.

[2] Chao, Hanqing, et al. âDeep learning predicts cardiovascular disease risks from lung cancer screening low dose computed tomography.â Nature communications 12.1 (2021): 2963.

The link to the open-source code for BI-Mamba will be included in the camera-ready version to ensure the reproducibility of this study.

The traditional method for cardiovascular prevention computes the atherosclerotic cardiovascular disease (ASCVD) risk score based on the demographic attributes, smoking history, blood pressure, cholesterol level, etc. Our proposed method represents a cardiovascular risk estimator applicable to the cohort of patients who undergo chest X-ray screen and whose measurements required to calculate the ASCVD risk score are not available."
https://papers.miccai.org/miccai-2024/122-Paper0391.html,"We thank the reviewers for their time and constructive feedback. 
Reviewers acknowledge our paper as novel, interesting, promising, simple, seamless (R6), improved performance (R6, R7, R8), practical and meaningful (R8), and easy to understand (R6, R7, R8).

Clarification in Setup[R6, R7, R8]: We clarify that in our setup, public data is open-source and can be downloaded to both clients and servers. We will also release the code and setup upon acceptance.

Heterogeneous setting[R6]: Please see our study on heterogeneous settings in Tab.1 and in Fig. 2 in Supp. involving public, uni- & multi-modal clients with data from three different institutions (MIMIC-CXR, CheXpert, Open-I), reflecting a real-world scenario.

Difference in test and public datasets and multimodal system [R7]:  Although there is no study on test samples largely different (e.g., domain adaptation) from public data, which is beyond the scope of this study,  the standalone performance in public data (Tab.1 in Supp) is relatively low. The 3.6% performance gain in heterogeneous settings is due to client data differing from the public/test set. Additionally, while text is more dominant than image, multimodal data provides complementary information, and without loss of generality, the contributions from each modality may vary. Thus, we assert that our method has comprehensive evaluation criteria.

Reliance on Public Data and its size[R6, R8]: We agree that we rely on a small number of public data. Our experiments on varying public sets (from 1000 to 250 patients~2.9% of the total patients) in both hetero- and homo-geneous setups (Fig 3(a) and Fig 2 in Supp) show the competitive performance. The trend does not decline sharply, and there is a wider gap in heterogeneous settings. Thus, we expect to achieve good performance even with fewer patients. As R6 rightly pointed out, our work is the first of itâs kind and exploring alternatives to public data is our future work.

Rare diseases in Public data [R7]: Yes, it is available but in limited quantities for augmentation. Achieving the same level of performance requires fewer multimodal data points compared to unimodal data, as multimodal data provide information from various sources. Our main goal was to investigate: since not all clients can collect multimodal data for rare diseases, how sharply does model performance degrade as the number of multimodal data points decreases in a system? And the role of cross-modal augmentation in mitigating the decline.

Effectiveness of Designs [R7]: While excluding label refinement in our method, we did not observe the performance gain consistently and also introduced unwanted computational overhead. In contrast, label refinement confined the search space to find the candidate for augmentation and demonstrated significant improvements without unnecessary computational overhead, leading us to adopt it as the default setting in all subsequent experiments. We also conducted a study on weight re-adjustment, with the performance results summarized in Tab. 4 in Supp.

Retrieved samples usefulness and random filling [R8]: Thank you for suggesting random filling using a public dataset. However, our experiments show that as the public data size increases, performance steadily improves. Our qualitative comparison (Figs 3 and 4 in Supp) also shows that augmented pairs are semantically more coherent and meaningful with the increase in the number of communication rounds.  We do not expect random filling to ensure such coherency and thus improve performance.

Retrieving is time-consuming [R8]: We agree with this point. However, our search space is limited due to the small size of the public data and the label refinement process. Additionally, we retrieve data at the start of each communication round, not every epoch. Precisely, we retrieved data 30 times during our training, which consists of 90 epochs. We believe the privacy protection provided by our method outweighs the computational overhead."
https://papers.miccai.org/miccai-2024/123-Paper2795.html,Thank you for the supports and the constructive comments. We will carefully consider them and make improvements in our camera-ready version.
https://papers.miccai.org/miccai-2024/124-Paper3127.html,"We sincerely thank the reviewers for scrutinizing our work and providing detailed and constructive comments.

Q1 Source code and dataset information for reproducibility issues. (R1&R3&R4)
We will make our code publicly available. The QaTa-COV19 dataset is a public dataset consisting of 5716 training images, 1429 validation images, and 2113 test images. The dataset split, along with the corresponding texts and ground truth annotations for the images, are publicly available [3].

Q2 The segmentation seems to be binary in all cases, which makes the âreferringâ part of the segmentation less convincing. (R1)
Referring image segmentation aims to segment specific regions based on natural language prompts. It is applicable to both binary and multi-class segmentation problems. For instance, some peer-reviewed works on referring image segmentation have focused on binary segmentation tasks [a,b].

[a] RRSIS, IEEE TGRS, 2024.
[b] Rotated multi-scale interaction network for referring remote sensing image segmentation, CVPRâ24.

Q3 The causal part of the segmentation wasnât clearly presented. Are the causal masks still there at runtime? (R1)
Yes, causal masks are generated during both the training and inference stages. These masks are learned and input-specific. We will clarify this in Section 2.3 of the final version of the paper.

Q4 Fig 1 - stronger color difference might be useful. (R1)
We will change the colors accordingly.

Q5 Lack of clarity and excessive jargon. (R3)
Confounding bias, spurious correlation, causal relationship, and causal intervention are fundamental terms in the field of causality in machine learning. For rigorâs sake, we cannot modify these terms arbitrarily. To address the reviewerâs concerns, we intend to add a PRELIMINARIES section providing background knowledge on causality and explaining these terms in the context of our task.

(a) Confounding bias refers to the interference of background factors and other extraneous variables in the modelâs segmentation of lesion areas.
(b) Spurious correlation is a related concept to confounding bias.
(c) A meaningful causal relationship is the ideal scenario where the model relies solely on visual features of lesion areas to generate lesion masks, without being influenced by confounding factors.
(d) Self-annotating confounders refers to our modelâs ability to adaptively extract confounders.
(e) The causal intervention module is a network module we designed to mitigate confounding bias.
(f) Excavating causal features refers to our module extracting visual features that satisfy (c).

Q6 Lack of ablation study. (R3)
Adversarial masking (AM) is an indispensable component of our causal intervention module, and the adversarial loss is specifically designed to guide this moduleâs learning. We have conducted an ablation study on the entire module (cf. Table 2). However, it is important to note that we cannot perform an ablation study solely on AM or the adversarial loss, due to their interdependence within the moduleâs design. We will clarify this in Section 3.4.

Q7 Lack of errors and uncertainties in the evaluation. (R4)
In fact, we conducted multiple experimental runs and observed minimal fluctuations in results. For instance, the standard deviation of Dice for our model did not exceed 0.002. We will report these uncertainty measures for all models in the final version of the paper.

Q8 It is not clear if the algorithm has been evaluated on a separate test set. (R4)
The dataset we use has predefined training, validation, and test splits. We performed model evaluation on the test set.

Q9 The authors could extend the cited literature to better position their work. (R4)
In the final version of the paper, we will cite and discuss the latest medical image segmentation models based on SAM. It is important to note that according to SAMâs code, it only supports point and bounding box prompts, not text prompts. In contrast, we chose CLIP, as it supports text prompts, meeting our needs."
https://papers.miccai.org/miccai-2024/125-Paper4133.html,"We thank all reviewers for insightful comments and are especially grateful to R1 and R4 for recognizing our work. The source code will be released in our final version.
1: Explanation of âthe first in body bradykinesia assessmentâ (R1)
To the best of our knowledge, we are the first team to explore video-based assessment of PD body bradykinesia (i.e., global spontaneity of movement), delineated in item 3.14 in MDS-UPDRS. Existing studies focus on the assessment of other tasks, such as gait (item 3.10) and leg agility (item 3.8). Following your advice, we will revise the term âfirstâ to avoid misunderstanding.
2: Multimodal methods instead of single modality (R3)
The key challenge in automated assessment of body bradykinesia is âabsence of specific examination tasksâ (1st paragraph of Introduction). According to the MDS-UPDRS, this scoring is based on the raterâs âglobal impression after observing for spontaneous gestures while sitting, and the nature of arising and walkingâ. Thus, multimodal schemes outperform single modality due to more comprehensive information. We empirically and experimentally determine leg agility (sitting) and gait (walking) as the inputs. 
3: Comparison with SOTA and low performance (R1, R3)
Compared to recent studies like TNSRE.2020.3039297 (67.59% for leg-agility) and TMM.2021.3068609 (65.66% for gait), this work tackles a more challenging task due to the absence of specific examination actions. As a pioneering attempt to utilize multimodal video information for indirect assessment of body bradykinesia, the performance achieved is acceptable.
4: Training steps and performance of h Ì_t (R3)
The training of h Ì_t is guided by L_cls (Eq. 2) and conducted in Eq. 10, which should be more clearly written as minâ¡{E,h Ì_c,h Ì_nc,h Ì_t}. h Ì_t is set to assist E in capturing modality-related motion features. Thus the performance of h Ì_t is not the main focus of this paper, so it was not reported. 
5: Why use EfficientGCN as the backbone (R3)
Additional experiments (Table 3) show the performance improvement when introducing the proposed modules (i.e., our innovations) into various backbones. EfficientGCN is selected as the backbone due to its advantages in deployability (1/9 FLOPs compared to 2s-AGCN and 1/12 FLOPs compared to MS-G3D).
6: The details of h Ì_c & h Ì_nc (R3)
They are both constructed based on a 3-layer MLP with independent parameters. The outputs of h Ì_c and h Ì_nc are prediction logits for the causal and non-causal parts respectively.
7: Each patient has differently labeled videos (R3)
Each patient was in distinct states (ON&OFF medication and/or ON&OFF deep brain stimulation) at different follow-up times, and scores were given independently by experienced raters. In cross validation, videos from the same patient are assigned to a single fold to avoid data leakage.
8: Subjectivity of the grader (R3)
The videos were captured in âreal clinical settingsâ and scored by the board-certified movement disorders neurologists with over 8 years of clinical experience in PD. Therefore, the label of each video is reliable.
9: The claim of âcausalâ is not sound (R3)
Following your advice, it is better to term the features separated by M as causality-driven features. Here, features relevant to bradykinesia symptoms in each modality are considered causal, while the rest are non-causal (i.e., the confounding factors), as they fail to contribute to body bradykinesia assessment. For example, channels in F_GA representing the continuity of arm swing are causal, while channels represent the walking action itself are considered non-causal. The proposed scheme separated causal features by measuring the contribution of each channel, which is an effective approach of causal intervention at channel level. 
10: The explainability of our model (R4)
The explainability is reflected in the definition of âcausal/non-causal featuresâ (response to Q9). We will demonstrate the explainability through visualization in the future."
https://papers.miccai.org/miccai-2024/126-Paper0745.html,"Thanks to the reviewers for your valuable comments of our work. Especially, thanks to R4 for accepting the paper directly. We appreciate your recognition of:

We will release the code, datasets, and detailed implementation for reproducibility to address all constructive comments.

Novelty: This is the first time that asymmetric causality has been achieved in quality assessment, comprising two aspects and providing clinically interpretability:

Q1: Why discovering causal relationships is crucial (R1): A1: Because causality is the key factor in cross-domain topological consistency, domain mismatch with quality variations can hardly be resolved without it. Our experiments demonstrate its importance.

Q2: How it is not included in the standard CLIP fine-tuning (R1). A2: Standard CLIP optimizes statistical relationships but lacks asymmetric causal relationships.

Q3: Clarifications about multi-granular anchor and multi-granular loss function (R1): A3:

Q4: Clarification about the image labels, multiple distortions, negative vision prompt and domain differences (R4): A4:

Q5: Limited information on language processing (R5): A5: Language processing is crucial to our framework, establishing the first specific prompt pattern for echocardiography quality assessment and focus on key metric interaction. For example, image with âoffsetâ distortion is accompanied by âbadâ quality, as stated in the 11th sentence in Sect.2.

Q6: Clarification about kÃk causality map (R5): A6:

Q7: Clarification about discriminability and collaborative optimization (R5): A7:"
https://papers.miccai.org/miccai-2024/127-Paper1909.html,"R1

Q: The method [18] for generating lower- and higher-dose images is not validated on the dataset used in this work.
A: First, this method has been validated on an in-house dataset in [18], and in our work as there is no lower- or higher-dose image in the public BraSyn-2023 dataset, we have previously visually inspected that the generated images are sensible (e.g., see Fig. 1). Based on these qualitative/quantitative evaluations on two different datasets, we believe that this method [18] is reliable. Second, as our goal is to synthesize standard-dose images, rigorous validation of the method [18] itself is beyond the scope of this paper. It is possible that generated lower- and higher-dose images could contain small errors and noises, but our ablation results show that the use of lower- and higher-dose images generated by [18] allows improved synthesis quality for our task, and thus these potential errors and noises do not have a large impact on our method.

Q: A detailed description of the loss functions is needed.
A: We are sorry that we did not describe the loss functions in detail as they are commonly used ones in existing image synthesis works. Briefly, the L1 and adversarial losses are borrowed from ResViT, and the L2 loss is the typical mean squared error. We will provide a more detailed description and make our code publicly available.

Q: Ablation studies should take into account the effects of higher-dose images.
A: Thank you for your reminder. We hypothesize that the use of higher-dose images can reduce the changes needed by the individual autoregressive steps and thus they are beneficial. As new results in the rebuttal will lead to automatic rejection, we will include these ablation results in the extended journal version.

Q: Evaluation metrics for the ablation studies are incomplete.
A: We did not include the Dice scores and outcomes for healthy tissue in Table 2 due to the page limit and because they have similar trends to the SSIM and PSNR results in the tumor. We will add these results in the journal version.

Q: The impact of the tumor mask on the result is not validated.
A: We have empirically observed that the mask is beneficial to the synthesis quality for the proposed and competing methods. Therefore, for each method the mask is included as input and this allows fair method comparison. Note that without the mask input, our method still outperforms the competing methods. As new results are not allowed and there is no space for these previous results not shown, they will be included in the journal version.

R2

Q: It is not proven whether the tokenizers fDI and fDV truly produce dose-invariant and dose-variant components, respectively.
A: It is indeed interesting to experimentally confirm the assumptions above. We have previously observed that the tokens in the autoregression change after each step, suggesting that the tokens given by fDV are dose-variant. But as new results are not allowed in the rebuttal, this issue will be more comprehensively explored in the journal version. The insightful suggestion also motivates potential improvement of our method in future work, where the decomposition can be encouraged with modified model and loss designs.

Q: Inference speed needs to be evaluated, as well as the benefit of the modified self-attention design.
A: We have empirically observed that the proposed method has similar inference speed to the other methods thanks to the modified self-attention design. Without the design, the inference speed is much slower. As our main goal is to improve the synthesis quality and new results are not allowed, detailed results on speed will be reported in the journal version.

Q: Limitations should be discussed.
A: We will add discussions on limitations and future works that address the limitations.

R3

Q: More qualitative results and failure cases should be presented and discussed.
A: As new results are not allowed, we will discuss failure cases as much as allowed."
https://papers.miccai.org/miccai-2024/128-Paper0727.html,Thanks for all the valuable feedback.
https://papers.miccai.org/miccai-2024/129-Paper0458.html,"We appreciate the reviewersâ constructive feedback and would like to address the key issues as follows:

Novelty and Generalizability (R#3-3, R#4-3-(a)): 
We thank R#4 for recognizing our work as âa novel and improved strategy for domain specific vascular segmentation.â Indeed, we have made significant efforts to unify theoretical and practical challenges to spark greater innovation in the field. The proposed cl-X-Dice framework elucidates the evolution of clDice, advancing beyond a mere variant. Extending valuable existing work does not detract from innovation. For instance, the B-DoU loss (MICCAI 2023) is derived from the B-IoU loss (CVPR 2021). With the cbDice combination loss (alpha=1, beta=2), we secured second place in the TopCoW 2023 Challenge and ensured reproducibility for the community. The datasets were randomly split without specific selection, following the nnU-Net (Nature Methods 2021) frameworkâs default fold 0 settings. We will further analyze the impact of the hyperparameter beta across various datasets in the revised manuscript.

Advantages of cbDice and Clarifying Figures and Tables (R#1-3-1/2/3, R#1-6-1/2/3, R#3-7-1, R#4-3-(c), R#4-7-(c)/(d)): 
Figure 2 visualizes the evolution of clDice, showing FN and FP at the centerline and boundary. Figure 3 showcases the differences in various metrics under different conditions, highlighting the advantages of the Dice and cbDice combination scheme, consistent with the three scenarios in the cbDice_cal_demo code. The key challenge is that the combination of Dice and clDice favors larger vessels; regardless of whether itâs clDice or its variants (such as cbDice), they need to be combined with Dice. To address ânumerous notations,â we will simplify them by changing âSIâ to âI,â setting normalization as the default in cl-MS-Dice and cl-MI-Dice, and removing cl-MSN-Dice and cl-MSIN-Dice. Thus, cl-MI-Dice becomes cbDice in the text. We will enlarge and reposition images in Figure 2 and simplify notations. To avoid confusion, we will replace the abbreviation for Dice with (mathcal{D}). Certain metrics are calculated only on 2D data (mu^{err}) and multi-class data (Dice(L) and Dice(S)), and we will unify the (beta^{err}) metric across all datasets. We will include more binary segmentation images, enhance Table 1 with explanations for different variables, and include standard deviation in mean values.

Small Vessel Segmentation and Clinical Importance (R#1-3-4, R#1-6-2, R#4-3-(b), R#4-7-(a)): 
Regarding small vessel segmentation, binary segmentation data lacks explicit small vessel classes. In multi-class datasets, cbDice effectively identifies small vessels and removes small background noise (see Figure 4, SwinUNETR). This improvement is due to cbDiceâs enhanced recognition of small targets. Future work will further validate cbDiceâs performance on more imbalanced multi-class datasets (e.g., AortaSeg24 Challenge). On the DRIVE dataset, this experiment aimed to demonstrate our methodâs applicability to 2D tasks. Despite using only CE loss with the nnU-Net framework, which approaches the performance limit, improvements were less significant. cbDiceâs quantitative improvements translate to clinical value, as mentioned in the Supplementary Material on CoW variant topology matching performance, addressing bottlenecks in downstream tasks (e.g., CROWN Challenge 2023).

Radius Computation and TopCoW Dice (S) of 0 Performance (R#1-3-1, R#4-7-(b)): 
Regarding radius computation of skeletal points, our method uses the distance map calculation (code: loss/cbdice_loss.py, line 60 and 65). We will clarify in the manuscript that the radius R is derived from the distance map D. The Dice (S) score of 0 performance on TopCoW might be due to not using deep supervision (3.2 Setup), impacting recognition of small vessels with low sample proportions. Due to the computational cost of distance map calculations, we did not compare deep supervision results to maintain consistency."
https://papers.miccai.org/miccai-2024/130-Paper2874.html,"We thank our reviewers for their constructive feedback. Main criticisms are addressed below.

âââ

R1: âcomparison to other tube- or curve-specific methodsâ
We acknowledge that we researched generic interactive segmentation methods rather than specific ones. We found some overlap with Liao et al. (to be referenced), e.g. the start and end points being similar to the tip clicks. However, a direct comparison with our work is challenging due to the use of different datasets and the lack of publicly available code. Liao et al.âs method is limited to two initial clicks, so comparisons can only be made under this constraint. Additionally, their method operates with paths that can only be approximated with a static width, which is not suitable for objects of variable width, such as blood vessels.

R1: â..RITM with Tip1, Tip2 + connected component analysis..explaining how the comparative methods achieve very high Dice values while still having high HD errors..â
We agree that connected component analysis could reduce the HD of mask-based methods, especially in cases of distant false positive predictions. However, such an algorithm must be carefully optimized w.r.t. its hyperparameters: use it after each click or only as a final post-processing? how to process disconnected true positive predictions? how to select the thresholds? Moreover, connected component analysis will not address false positive predictions within a single component, such as at bifurcation points. Nevertheless, we consider this comment very important and have added a paragraph to the discussion to support mask-based methods and highlight potential areas for improvement.

R1: âdoes that mean that the simulator starts of with only Tip 1 and Tip 2 with no positive or negative clicks.â
Yes. The click simulator is probabilistic. If n_pos=0 and n_neg=0, it starts with only Tip 1 and Tip 2. We have refactored this section to make it more explicit.

R1: â..variability measurements in Table 1 are missingâ¦â
Reporting average NoC values is a convention in the interactive segmentation field (works such as RITM, SimpleClick, and FocalClick all do just that). Yet, given the request, we propose to add the statistics to the supplement.

âââ
R3: â..the inference phase could use more details and is only described.. wrong RITM citation.â
We added more details to the inference section and fixed the RITM citation. Thank you.

âââ
R4: ââ¦in Table 1, Dice values range from 1 to 10.â¦â
In Table 1, the Dice values do not range from 1 to 10. Rather, the number of clicks (NoC) metric, required to achieve 0.85-0.9 Dice scores, range from 1 to 10 (as stated in the table caption).

R4: ââ¦method is relatively simpleâ¦and not novelâ¦â + R1: ââ¦technical noveltyâ
In the task where powerful modern models fail to segment elongated objects, we believe the simplicity of the proposed method is the main strength of our work: plain fully-convolutional networks without any autoregressive blocks do the job. This simplicity facilitates easy reproduction and reuse of our model, as noted by the other reviewers. Moreover, to the best of our knowledge, the data structure based on the combination of the centerline and the distance transform is novel, and we presented its first use in the interactive segmentation task.

R4: â..Iteratively refining the segmentation with one positive/negative example during inference is labor-intensive and time-consuming..â
Indeed, itâs a flaw of all click-based interactive segmentation methods. However, other prompts may have different disadvantages as well. For example, bounding boxes are too coarse for approximating thin and long structures, and text prompts can be too ambiguous for precise segmentation. For future work, scribble-based approaches (coarse mask or centerline) can be considered a good alternative to click-based methods. Additionally, they too can be directly used with our centerline-diameter data structure. We have added these thoughts to the discussion."
https://papers.miccai.org/miccai-2024/131-Paper0748.html,N/A
https://papers.miccai.org/miccai-2024/132-Paper0737.html,"We thank the reviewers for their detailed feedback. We are glad all reviewers(R1,R3,R4) appreciated the importance and clinical value of the problem we introduced. Reviewers also appreciated the technical motivation(R1,R4), superior performance(R1,R3,R4) and the new benchmark dataset (R3, R4) of our work. We summarize the concerns as follows:

Q1(R1)Technical innovation
A:Our method is simple but effective. The technical innovation naturally aligns with our motivation to address cross-age detection. The prototypical network can faithfully capture anatomical property of each landmark, and the masked modeling mechanism can further enhance the correlation between different landmarks. All these techniques are first developed in the field of cephalometric landmark detection which achieved superior performance against the current SOTA.

Q2(R1)Alpha setting in EMA
A:As in Eq.(4) in the main paper, alpha is the weight for the historical value of EMA, and the current prototype data is weighted by (1-alpha). The alpha is set to 0.99, our holistic prototypes can be robust to outliers of current prototype data, because it only holds a weight of 0.01. We tested a series of alpha values, finding that our method is fairly robust to alpha larger than 0.8.

Q3(R1,R4)Details about masked mining
A:Masked mining is used on all instance prototypes. We computed the cosine similarity between all landmark prototypes and found that intrinsically related landmarks, like A and UIA, exhibit higher similarity compared to those without masked mining. This indicates that our mining strategy enhances the correlation between landmarks. In addition, our ablation results in Table 2 demonstrate that the mining strategy significantly improves detection accuracy.

Q4(R1)Hyperparameter analysis
A:We have provided ablation analysis of many important hyperparameters in the initial submission. The analysis of mask ratio R, lamda_1 and lamda_2 has been provided in Fig.3(b) of the main paper, Fig.1 and Fig. 2 in the supplementary materials respectively.

Q5(R3)Evaluated only on one dataset with limited improvement
A:To date, our collected CephAdoAdu dataset is the first and only one that includes both cases of adolescents and adults, allowing algorithm development and evaluation of cephalometric landmark detection across ages. From Table 1, our CeLDA achieves the best MRE result of 1.05 mm on all cases, where the improvement is 21.64% over previous SOTA method with MRE of 1.34 mm. Additionally, a one-sided paired T-test yielded a p-value below 0.05, showing significant superiority.

Q6(R4)Criteria for annotation of CephAdoAdu dataset
A:In clinical practice, the templates used by different regions and hospitals vary significantly in the number and definition of landmarks(e.g. Vienna template, Huaxi template). For CephAdoAdu annotation, we consulted experienced dentists and selected landmarks based on two criteria: 1) they are defined in all templates, and 2) they are challenging to detect across ages. Ultimately, we selected 10 target landmarks. We will detail this information in our paper, and further enrich our CephAdoAdu dataset by including more landmarks in the future.

Q7(R4)More descriptions of the dataset
A:Adolescents are defined by the presence of baby teeth, regardless of age, and adults by the absence of baby teeth. The percentage of malocclusion patients, patients wearing braces, patients undergoing orthodontic treatment are about 70% 10% and 20% respectively. We will provide this information in released dataset.

Q8(R4)Feasibility in clinical practice
A:CeLDA is evaluated on 10 challenging landmarks, showing its superiority over current SOTA in cross-age landmark detection. It can generalize to any number of landmarks by adjusting the number of prototypes during training. This precise localization benefits further clinical evaluations, e.g. angle calculation and anatomical classification.

Code and dataset will be made public."
https://papers.miccai.org/miccai-2024/133-Paper3315.html,"Below we summarize the main concerns of the reviewers and our answers to them.
*Importance and novelty of the Fourier transform analysis of the echocardiography signal:
Spectral decomposition of echocardiography signals is not novel, a main example being tissue Doppler imaging. However, these frequential analysis methods use exclusively the magnitude information of the Fourier transform. What is new in the present proposal is how phase information is integrated into the analysis and the importance of considering this at categorizing different pathologies. In this case, phase information helps us to establish when a particular frequency component occurs during the cardiac cycle and use this to compare the severity of different cardiac stages.  The combination of magnitude and phase allow us to refine the notion of distance between cases pushing similar cases closer and different cases farther away

*Significance of the results and usefulness of the findings:
The experimental results demonstrate the proposed approach is more effective than the LVEF measure in capturing temporal patterns that differentiate between different levels of cardiac compromise, in particular 100 real cases with different severity levels of cardiac disease established by the clinical condition after the Stevenson score (A, B or C define different levels of severity). However, this was not readily apparent in the included plots. An additional ANOVA analysis revealed significant differences among the distance established by the frequency descriptor herein proposed for different severities of cardiac pathologies.

*Comments about the presentation of the method and organization of the paper suggestions:
-Preprocessing: Section 2.1 describes the preprocessing undertaken to ensure comparability between the two databases utilized in the article. The preprocessing was semi-automated, with the echocardiogram coneâs region of interest being manually identified, while the remaining preprocessing stages were automated. This section will be clarified by adding this information.

-Segmentation: Segmentation accuracy on the clustering results, a great advantage of making the analysis in the frequency domain is that the accuracy of the segmentation is less important since the possibility these errors are accumulated during the whole cycle is negligible. In fact the temporal patterns of the ventricular area reflect not only the outcome ventricular capacity but rather the compensatory mechanisms which eventually could make simple volume relations look normal. Finally, measure of the Dice was included in the methodology, specifically in the sentence âIn a previous work, this network demonstrated outstanding segmentation results (Dice of 0.93), â¦â

-Frequency Analisys: Algorithm 1 describes better in terms of the problem the proposed methodology, as follows
Input: Temporal series of cardiac cases
Output: Assigned clusters with updated magnitude using phase information
Step 1: Mapping to the Fourier space
For Each case in the dataset
Apply Fast Fourier Transform (FFT) to obtain magnitude and phase spectrum
End For
Step 2: Finding out the frequential representant of the cardiac cycle
Perform spectral clustering of the magnitude with 7,835 cases from the Echonet database and 99 cases from a local collection with registered clinic state
Step 3: Re-estimating intra-cluster distances using phase information for those cases with known clinical history
For Each of the 100 cases:
Set case_i to a particular group
Apply inverse Fourier Transform using the phase spectrum and a unitary magnitude for both case_i  and the medoid
Compute dot product of the reconstructed temporal series between case_i and the group medoid
Recover angle (theta) = \arccos ( <medoid, case_i> / (|medoid| |case_i|) )
Set a new case_i distance as the norm of the sum of two vectors, one with norm 1 and the other  1 + (stored distance), and making angle (theta) 
Store this new distance for case_i
End For"
https://papers.miccai.org/miccai-2024/134-Paper3636.html,"We thank the reviewers for recognizing our workâs significance in case-based CXR retrieval. Our approach achieves substantial gains (upto 26% accuracy, 23% ranking) over SOTA, especially for less prevalent findings. We appreciate the recognition of thorough experiments and effective graph-transformer design. We acknowledge concerns about detail due to space constraints and will incorporate all suggestions, including related to presentation, in final version.

@R4 (3.2) AnaXNet vs CheXtriev: Existing approaches rely on global features. We argue that anatomy-aware local features better capture subtle variations. AnaXNet (designed to classify) uses hand-crafted heuristics based on label co-occurrence. It lacks clinically relevant location information. Its naive 0/1 edges fail to capture higher-dimensional heterogeneity and neighbourhood aggregation limits expressiveness. Classifiers prioritize discriminative features, potentially losing details crucial for similarity.

CheXtriev, our proposed graph transformer (GT) framework, addresses these limitations with: (i) Learnable location embeddings(spatial context), (ii) Learnable continuous edge attributes(latent relationships b/w regions), (iii) GT layers with edge-aware attention(interactions b/w any node pairs), (iv) Shared-source gated residual connections (selectively combine features at different granularities). GTs have shown superior and generalizable representations(favorable for retrieval) compared to Graph ConvNets(GCN), supported by statistically significant boost (p<0.05) across metrics.

@R4 (3.3) AnaXNet vs V0: V0 averages region features for similarity. AnaXNet employs GCN before averaging. We use AnaXNetâs official code with region-level classification; its optimization for classification influences its design. We noticed when V1 was constructed with GCN instead of GT (AnaXNet but graph-level classification), it achieved lower performance than V0 (50.2mAP, 37.6mHR, 52.2mRR). This indicates classification-optimized features may not be the most effective for retrieval.

@R1 (3.1) Motivation of anatomy-awareness: We contend that anatomy-priors are not naive. Our method mimics radiologistsâ systematic approach to reading CXRs [13,17,25], reducing missed findings in blindspots. Benefits are evident in lesser prevalent findings: FO/HF(+94%), PTX (+254%), CONS(+35%), PN(+20%). Obtaining automated anatomy annotations with high accuracy (>0.98 IoU for most regions[14]) is straightforward.

@R1 (3.3) Clinical value: MIR aids diagnosis and treatment planning [3], with retrieval augmentation [12] benefitting CAD. Even without query labels (basic assumption), it helps clinicians retrieve visually similar CXRs to explore differentials. Findings are not unique; underlying causes differ. For instance, two CXRs with same labels (PN, CONS) might show lower CONS with sharp border (bacterial) and upper CONS with blurred border (fungal), requiring different treatments. Label similarity might miss these subtle visual differences (share limitations with hashing). Reports (though not common) are more accessible than labels in clinics, but medical text similarity remains a challenge in NLP.

@R1 (3.4) Focus on features: Accurate relevance measurement (cosine similarity) hinges on quality of representations. Current multi-label retrieval methods often lose crucial disease and ROI detail [10, 6]. Clinicians rely on anatomy to identify pathologies, making it essential that retrieval representations are discriminative and informative. We focus on learning sufficiently detailed fine-grained features for effective similarity measurement, following clinical reasoning.

@R3 (3.2) Saliency maps: We adapted [20] with anatomy-occlusions to generate maps for query and retrieved pairs and show the interpretability of our results, often ignored in retrieval literature. Weâve shown these maps may not be meaningful for methods that donât use anatomy-aware features (in interpretability analysis subsection)."
https://papers.miccai.org/miccai-2024/135-Paper2095.html,"We thank the reviewers for their constructive feedback. We are happy they acknowledge this workâs novelty, and the paperâs high clarity and quality. Nonetheless, we would like to address aspects of the work that were not fully clear and respond to the few concerns raised by the reviewers.

â¢ Conditioning on Anatomical Features: While we outlined the theoretical process of conditioning on specific anatomical briefly in section 2.1., we will extend the camera-ready version to provide a more thorough and detailed understanding to the reader. We determine the anatomical property using quantities like volume of the lateral ventricles or gyrification index and add this quantity as additional dimension to the latent code during training. @R1 The benefit of this explicit modeling is the strict disentanglement between this explicit dimension and the implicitly learned dimensions of z. During inference (i.e., fitting CINA to a new subject), we initialize the explicit variable with the weighted mean value from the latent codes learned during training. The model then learns the quantities during test-time optimization accordingly.

â¢ Accurate capturing of subject age by CINA: CINA is inherently built to capture common domain knowledge in the INR while pushing any features that show high variance across subjects into the latent codes z. As age is the factor of greatest variance within the (healthy) data, it is well captured in the first principal component of latent codes. The age prediction is worse for data with pathologies present as these introduce dimensions of high variance unrelated to age. If we explicitly condition on these anatomical features however, we can improve age prediction as we push the pathological variance into a dedicated dimension which we can then ignore for age prediction.

â¢ Limited number of ROIs: There is no theoretical limit to the number of ROIs that CINA can capture. The number of captured ROIs is directly determined by the number of ROIs in the training data, like in any traditional method. The FeTA dataset simply comes with a small number of segmented ROIs.

â¢ Time complexity of Test-time optimization: This is an interesting point that we will include in the camera-ready version.  In our setup, fitting CINA in test-time optimization on a new subject from the FeTA dataset for 20 epochs takes about 10 seconds (GPU: Nvidia A6000) compared to around 8 seconds for the traditional registration approach using ANTs symmetric normalization algorithm (CPU: AMD Ryzen Threadripper PRO 5975WX 32-Cores).

â¢ Alignment to MNI Space: R3 is right in pointing out that CINA requires the subject to be aligned to MNI space to perform an analysis. We do not consider this a limitation, as this pre-processing step is relatively easy (most 3D reconstruction tools already project the 3D volume to MNI space) and required by any atlas-based analysis, including traditional methods.

â¢ We thank R3 for providing some interesting ideas for more elaborate experiments and more thorough latent-space analysis. The limited number of scans of sufficient quality of pathological subjects in the FeTA dataset has so far prevented us from performing more elaborative experiments within the latent space. Indeed, we are planning to test and extend CINA on an additional dataset comprising several hundred subjects suffering from various brain pathologies. This will allow for a more thorough analysis of the latent space and also for an investigation of scanner effects and domain distributions across different datasets."
https://papers.miccai.org/miccai-2024/136-Paper1527.html,N/A
https://papers.miccai.org/miccai-2024/137-Paper0494.html,"We thank all reviewers for your suggestions and positive recognition of our work!

To Reviewer #1: Thanks!
Head category: Thank you for raising this important question! Although we manually prioritize sampling from the tailed class, the input images are unpaired, leading to a high likelihood that tailed classes might still be pasted onto cropped images containing head categories. In Table 3, experiment #4, the addition of class-aware sampling significantly enhances performance both in the tail category (AA) and head categories (LAC and MYO).

Pair Data: We consistently use an unpaired data setting throughout our experiments.

Combination Explanation: We appreciate your attention to detail. Weâll add more descriptions at the end of Section 2.1 and improve Figure 3 for better clarity on the sampling strategy.

Figure Layout: Thank you for your feedback! We will revise the layout of Figure 3 to enhance readability and ensure it is easy to understand.

Typos & font: Thank you for pointing this out. We will thoroughly check for typos and revise any unreasonable fonts in the later version.

To Reviewer #3: Thanks!
Datasets: Thank you for your understanding. Due to the page limitations of MICCAI, we could only conduct experiments on two datasets. However, we plan to verify our method on additional SSDA tasks in a future extended version.

Code: Thank you! We are going to release the code later.

Combining modalities: Thank you for your feedback! Our paper approaches the alignment of domains from the perspective of data augmentation. CT and MRI mixup is a direct method to generate inputs that encompass both domain distributions, effectively bridging the domain gap. This technique has been widely validated in general computer vision tasks [5,11], with real and game images mixup. Our ultimate goal is to develop a unified model capable of processing different modality images with high performance simultaneously.

Optimizers: Thank you for raising this question! We referred to previous methods [17, #1] on these two datasets to select the appropriate optimizer. Through our experiments, we found that SGD achieved better results on the MM-WHS dataset, while Adam yielded better results on the MS-CMRSeg dataset.
Ref: #1. Fuping, Wu and Xiahai, Zhuang. âUnsupervised domain adaptation with variational approximation for cardiac segmentation.â TMI (2021).

Writing: We appreciate your advice. We will carefully review and correct the time tense throughout the entire paper, with particular attention to the method and experiment sections.

To Reviewer #4: Thanks!
Novelty: Thank you for your feedback. In our approach to SSDA, we address the limitation of one-way mixup among the source domain, labeled target domain, and unlabeled target domain by employing a two-way mixup with triple alignment. This maximally diversifies the data distribution. Additionally, we introduce a simple yet effective class-aware sampling strategy to improve performance on the tail class. Unlike the shared weight model and co-teaching model, we propose cross-knowledge distillation to further enhance model generalization.

Backbone: Thank you for raising this question! We acknowledge the importance of proving the model-agnostic nature of our method. In the future, we plan to verify our approach on an improved transformer architecture specifically tailored for the SSDA task. 
Our prior investigation indicates that U-Net and DeepLab still achieve state-of-the-art performance for segmentation during UDA task. However, the transformer backbones referenced in [#1] yielded less competitive results than the base backbone on the MM-WHS dataset. Therefore, for this study, we focused solely on our method based on the CNN backbone. Moving forward, we aim to design a competitive transformer architecture for transfer learning tasks.
Ref:
#1. Ji, Wen, and Albert CS Chung. âUnsupervised Domain Adaptation for Medical Image Segmentation Using Transformer With Meta Attention.â TMI (2023)."
https://papers.miccai.org/miccai-2024/138-Paper2541.html,"Thanks to all the reviewers for your valuable comments on our work. Especially, thanks to R3 and R4 for accepting the paper directly. We appreciate your recognition of:

Q1: Relatively weak innovativeness of Auto-FM (R1). 
A1: We improved ALFA-Mix[15] by making the mixing matrix Î± a learnable parameter, thereby simplifying the solving process and reducing computational demands. This is the first time that the strengths of learnable feature interpolation have been leveraged to identify informative samples in medical image analysis involving high-resolution images.
Q2: Suggestion about evaluation on imbalanced multi-class datasets (R1&R3). 
A2: Extending to multi-class tasks is valuable, but it does not undermine our approachâs proven effectiveness for investigated imbalanced binary problems, which are common in medical research. Additionally, the MPPS module can be readily extended by assigning pseudo-labels based on the predicted probabilities of multi-class data, sorting accordingly, and sampling according to Eq.5. This iterative approach updates class distributions, emphasizing tail classes while maintaining balance. We appreciate the reviewersâ suggestions and will evaluate multi-class performance in future work.
Q3: Not very comprehensive experiments (R1). 
A3: For comprehensive experiments, we validated our method across various data scales and class imbalance ratios. Specifically, we tested it on 4 subsets of our private dataset with different imbalance rates and the large-scale public dataset ISIC2020. (Sec3.2)"
https://papers.miccai.org/miccai-2024/139-Paper0704.html,"We thank the reviewers for noting our strengths (proposed modelâs efficiency while incorporating LLM, extensive experiments with superior results, clear paper) and respond to critiques/misunderstandings.

Implementation challenges (R1): We agree usability is critical, hence âAll code and pre-trained models will be made publicly available on GitHubâ (footnote, p.1) and we will include detailed training settings in the online documentation.

Dataset efficiency (R1): Please note dataset efficiency (finetuning with only 1% and 10% of the data) results were shown in Supplement Table S1 and S2 and discussed on p.7: âNotably, our model also outperforms the baselines even with much less training data. We highlight that for RSNA, our accuracy drops by < 3% when using 1% compared to 100% of the training data.â As seen in S1 and S2, ours performed best under 1% and 10% data settings for both chest datasets.

Baselines use different backbones/initializations (R1): SOTA baselines are from published studies that use the same datasets as ours, thus the different architectures. These prior works used smaller architectures, but also initialized with pretrained models (eg BioClinicBERT, DieT). Introducing a LARGE-scale pretrained model with PEFT is our main contribution, in which even with fewer trainable parameters than the baselines (Table S3), we show better performance (Fig. 1, Table 1).

Model explainability (R1): We will apply model explanation like GradCAM and show examples in the Supplement.

Compare fully tuned BioMedLM+DiNOv2 (R3): Please note this comparison was already shown in Table 3 (bottom row, only 0.29% better on CheXpert) and discussed on p.8: âThe fully fine-tuned model improves the performance even with a much smaller batch size; however, this improvement comes with the cost of â¼4 times more GPU memory cost and only 1/20 batch size with 2 times longer training.â

Include PubMed CLIP, BioMed CLIP baselines (R3): Please note PubMed CLIP and BioMed CLIP are general vanilla CLIP models, but trained with much larger datasets derived from PubMed, with multiple imaging modalities and anatomy targets. Our experiments focus on chest-specific tasks and are only trained with the CheXpert dataset. Following previous work (eg [12,26,27,29]), we only compare with methods trained on Chest data, since comparison with baselines trained with more and different data would be unfair.

Model architecture confusion (R3): In Sec 2.1, we described the model architecture, ie GPT-2 and ViT. In Sec. 3.3, we described the model implementation, noting âWe choose BioMedLM-3B and DiNOv2 to initialize our encodersâ (p.6). Fig. 2 is labeled with the implemented encoders.

Modules are known to the field (R3): While the individual components are adapted from prior work, we are the first to introduce LLM efficiently to this task. Our study shows a promising path of combining LLM with medical image pretraining when data and computational resources are limited, which is critical in the medical imaging domain. Also, our validation and superior performance on large open datasets (CheXpert, RSNA, EMBED) provide new baselines against which future work can be compared.

Proposed PEFT/LoRA confusion (R4): We clarify here we did not propose a novel PEFT method, but rather âwe introduce the parameter-efficient fine-tuning (PEFT) module to the frozen LLMâ (p.4), where âFor the PEFT module, we experiment with LoRA [11], IA3 [16], and prefix fine-tuning [15]â (p.5). This benchmarks different PEFT methods in the medical domain and shows LoRA performed best (Table 1).

Ablation/Why use GPT2, BioMedLM (R4): We will evaluate the influence of different backbones in future work. Our goal here was to utilize large-scale pretrained models, which can help reduce the need for large size training data and improve performance (Sec 1). Specifically, we chose BioMedLM as it was the best performing 3B level LLM according to multiple benchmarks (Chen et al., âMediTron-70B,â arXiv 2023)."
https://papers.miccai.org/miccai-2024/140-Paper1738.html,"Reproducibility. [R1, R3, R4]
We will release the source code and related data to the public upon acceptance.

Tumor grade performance.[R1] 
Determining tumor grade is challenging due to multiple slides per patient, each with varying grades. The final grade is based on the highest grade observed across all slides. Low tumor grade performance has also been reported in other work, e.g., Chan CVPR23.

Quality validation on generated reports. [R1]
As depicted in Figure 2 and Fig.S2, our in-house dataset exhibits a relatively structured report format rather than being entirely freeform (with considerable variation in expressions conveying similar meanings). Moreover, based on the observation in Table 1 showing a linear relationship between the CE metric and NLG metric, we anticipate that the NLG metric indirectly but effectively reflects the validity of the generated reports.

Cross-validation + Data splitting. [R1, R3]
Cross validation: Our dataset has a severe class imbalance, making it difficult to distribute minority classes evenly across each fold (as each sample has multiple tags and each tag has multiple inner classes). Consequently, we were unable to perform cross-validation and cannot provide standard deviations. 
Data splitting: Although we split the data randomly, we ensured that 1) no classes appear in the test set that are not present in the training set, and 2) as many classes as possible are included by adjusting the seeds. Additionally, to avoid inflated metric scores from biased predictions on our imbalanced dataset, we reported the macro-F1 score in Table 1 for a more accurate assessment. 
Organ quantity: Although the quantities of organs are not exactly the same, the ratio between kidney:colon:rectum is approximately 1:2:1 in each split. However, no single organ is excessively dominant, and we believe the macro-F1 metrics accurately represent the modelâs performance across multiple organs.

Lack of comparison between other slide processing models. [R3]
Existing multi-scale models (e.g., ZoomMIL, HIPT) have already demonstrated superior performance compared to single-scale models (e.g., TransMIL, AB-MIL) so we did not compare ours with them. 
We found that recent histopathology VLMs (Guevara MIDL23, Sengupta ML4H23, MI-Gen) employ HIPT without fine-tuning. We followed the same experimental setup, but it did not perform well on our dataset. We also tried to fine-tune HIPT but failed due to the extremely long training time. 
We observed that ResNet50 features outperformed VGG16 features for ZoomMIL, but VGG16 features were better for our model.

Lack of comparison in PMPRG.[R3]
Due to time constraints, we were unable to include the result of PMPRG with ZoomMIL on the entire dataset. However, we observed that our method outperforms ZoomMIL on the small dataset (kidney data) in PMPRG, in line with the encoder-only performance shown in Table 1.

Quantitative proof compared to HIPT. [R3]
HIPT took about 938 seconds while our model finished in just 3 seconds to train using a single WSI with similar computational resources, which is approximately 300 times slower (see Image Encoder in Section 3.1).

Loss weight parameters clarification. [R3] 
We empirically found that the combination of alpha (0.2), beta(0.6), and gamma (0.2) provided the best performance.

Fig.2 validation. [R1, R3]
As for the modelâs prediction for the IHC-related contents in Fig. 2, the ground truth full-length report also contains similar text in the note section (not shown in Fig. 2), so it is a valid prediction. 
The attention maps were generated based on the attention scores between each tag token and regional feature. Since each tag token plays a crucial role in generating sentences related to the corresponding disease, these maps directly indicate where the model focuses when generating sentences related to each disease. We briefly assessed the validity of the attention map with our pathology collaborator, and the initial feedback was positive."
https://papers.miccai.org/miccai-2024/141-Paper1493.html,"We thank reviewers R1, R3, and R4 for their reviews and recognition of our workâs novelty and strengths. We will update the response and correct minor typos in the camera-ready version. 
Reply to R1: #Q6-1: In the results section, we introduced OrdinalCLIP [21] and its limitations in DR grading. Both our approach and [21] aim to learn ranked feature spaces in CLIP [9]. While [21] uses linear interpolation, we make DR fundus order learnable via text-image pairs. Additionally, our SMS addresses data imbalance by smoothing similarity vectors, unlike the entropy-based method in [19]. #Q6-2: It is indeed observed and should be considered with longitudinal datasets for individual patients. Our datasetâs DR color fundus has natural rank information, which we used for diagnosis rather than prognosis. We will discuss assumptions for decreasing similarities in the camera-ready version.#Q6-3: While related, the weights in L_rank loss are in a self-adaptive non-linear fashion, unlike fixed weights in kappa loss which are linear or quadratic. #Q6-4: We have conducted a t-test comparing our model to GDRNet [14] (SOTA). Our model outperformed it with a statistically significant p-value of 0.02, where ours achieved the best F1 or AUC. #Q10-1: it was set as 1. #Q10-2: Both confusion matrices in Fig. 3 are averages of all samples, as described in Section 4 (page 7). 
Reply to R3: #Q6-1 & Q10-1 & Q10-2: Our model does not obtain rank information from learning optimal context prompts. Instead, we make the natural ordering of DR color fundus learnable in the CLIP [9] feature space. We designed an innovative rank loss to rank aligned text-image pairs in order. Similar in OrdinalCLIP [21], ranking information does not come from learning optimal context prompts. Instead, it uses linear interpolation of the base rank embedding of [class] to obtain a ranked feature. The text encoders in both our model and [21] are frozen during training, so rank representation does not come from the text encoder. Our input text format follows [21]: both use the same text and a specific class of images. For example, âage estimation: the age of the person is [class]â for [21] and âThis image is [class]â for ours. [21] provides performance for 8 initial prompts, with only a 0.8% difference (min 2.30, max 2.32). Please see their OpenReview rebuttal for details. Notably. [21] requires much fewer basic rank classes (e.g., 1, 10) than the actual number of classes (100). This limits their ability to learn rank information with fewer classes (e.g., five classes in DR grading). See Section 4 Table 3 in their manuscript for details. #Q6-2: Besides domain issues, insufficient training data to fine-tune the CLIP-based model is a primary reason. This is explained in the supplementary material (Section 1, page 1). #Q6-3: We will add them to the camera-ready version. #Q9: We provide an anonymous code link in the abstract, including the test code and pre-trained model weight. We stated that the training code will be released after the paper is accepted. #Q10-3: Thanks for pointing out this error! We used the Resnet50 version of the pre-trained CLIP model (see the official GitHub page of CLIP). The text encoder is a Transformer, and the image encoder is Resnet50. Sorry for the confusion; we will correct the typos in the camera-ready version. #Q10-4: We apologize. We will abridge our supplementary material to meet MICCAI Conference requirements. 
Reply to R4: #Q10-1: We compared the same methods as the GDRNet [14] benchmark for fair comparison and included [9] and [21] for comprehensive comparison. #Q10-2 & Q10-3: Yes, our model applies to any classification task with inherent rank information. We have tested it on the CORN1500 (Mou, et al. 2022) benchmark for corneal nerve tortuosity, achieving 3% better AUC. Thank you for your suggestions. We will test more eye diseases and imaging modalities in future editions."
https://papers.miccai.org/miccai-2024/142-Paper0670.html,"1) Acknowledgement:
We sincerely appreciate the time and effort that all the reviewers have invested in reviewing our manuscript. We are grateful for the positive feedback on the good idea (R3, R4), clinical impacts (R3, R4), comprehensive experiment and the good results (R1, R3), and good writing structure (R1, R4).

2) Concerns:
On the clarity of technical details (Reviewer #3), the logic in paragraph one, and the definition and clinical significance of MVO (Reviewer #1):
We thank the reviewers for the comments. Influenced by your valuable suggestions, we are committed to refining our paper for the final version.
On the effectiveness and generalizability of the method, and future scalability (Reviewer #3 & Reviewer #4):
The reviewersâ advice is highly insightful, highlighting specific areas for improvement. We will incorporate these suggestions and aim for broader experiments in future research."
https://papers.miccai.org/miccai-2024/143-Paper0498.html,
https://papers.miccai.org/miccai-2024/144-Paper3580.html,"We thank the reviewers for encouraging comments. We provide rebuttal to address their concerns. Novel contributions (R3): While our model draws inspiration from non-contrastive SSL, we introduce following novel components: 1) Discretization of continuous representation space using a codebook to enable quantization to obtain discrete codewords. Each codeword represents a group of similar attributes within the continuous space. (2) Introduction of DiversiFuse, a sub-module of Quantizer module (R4), for enhancing continuous representation with discrete features by employing cross-self attention. This allows to take the advantage of discrete and continuous features, resulting in informative representations (R3). Codebook details: The codebook is not binary. It comprises K codewords, which are randomly initialized vectors of size D (R3). Once trained, these become the representatives of the continuous features (y_Ï) in the quantized space. For quantization, a pair-wise distance is computed between each feature vector in y_Ï and each codebook vector e_k to find the nearest codeword for each feature vector in y_Ï (R3, R4). Following the common practices ([24,32] from the paper), we use the Euclidean distance, which is also motivated by the simplicity and efficiency of  K-means clustering (R3). This enables quantization of y_Ï space to be partitioned into discrete regions represented by the codewords. As a result, the model represents common anatomical features using a compact set of discrete codebook vectors (R4). For more clarification about the codebook, please refer to [24,32] from the paper. Codebook Learning: The model is trained in two phases, pre-training and downstream training. In particular, codebook is trained during pre-training phase where the loss L_cb containes e_k representing the quantized features in the form of the aforementioned nearest codewords. L_cb effectively results in MSE loss between the continuous and discrete representations (R4). Decoderâs significance: As mentioned in the Ablation Studies section, the results from classification tasks are comparable w/ and w/o decoder, however, in segmentation tasks, the obtained results demonstrate a superior performance w/ the decoder (Table 1 and 2).  By reconstructing the input image from the output of the DiversiFuse sub-module, the decoder encourages the model to focus on capturing fine-grained details which are critical for segmentation (R3, R7). Also, we thank the reviewer for highlighting the mixing of the best results for different implementations of our model. We consider the model w/ decoder as our best model. We will make the discussion of the results consistent in the camera-ready version (R7). Results: A paired t-test comparing our model with best baseline method DiRA (Table 2, SIIM dataset) yielded a significant p-value of 0.012, indicating performance differences. The 5-fold cross-validation on SIIM in Table 1 results into 57.7 and in Table 2 59.2 (R7). The diagnostic maps are obtained during the downstream phase with the help of Gardcam using the available ground truth details for a subset of NIH samples, including the annotated regions and labels (R3). Additional details on data and training: As mentioned in section 4, We pre-trained on NIH and EyePACS datasets, using official splits for NIH and 35,126 samples from EyePACS. In downstream phase, 7,000 samples are in ODIR and 2,208 in MURED, with 20% allocated as the test set. The SIIM provided 12,047 samples, using equal numbers of positive and negative samples and allocating 20% for validation. Downstream training involved 100 epochs, batch size of 16, and a 1e-3 learning rate with Adam optimizer (R4). Typos correction: We will address the citation overflow and correct all the typos in the camera-ready version (R4, R6, R7). In Fig 1, we will rectify the variable inconsistency, which should be y_c^k instead of z_c^k, and add the missing closing parenthesis (R6). Codes will be released upon acceptance."
https://papers.miccai.org/miccai-2024/145-Paper2599.html,"Thanks to all reviewers (R1, R3, R4) for constructive comments.

Q1: Clarification on Text-guided Segmentation(R1,R4)
A1: Sorry for any confusion. Hereâs further clarification on our task: The essence of text-guided segmentation lies in utilizing additional textual information paired with images to enhance segmentation accuracy. This entails inputting both an image and its corresponding text (termed image-text pairs) into the model simultaneously to obtain segmentation results. We conducted independent experiments using two pneumonia datasets: QaTa-COV19 (QaTa), comprising X-ray slices, and MosMedData (Mos), comprising 2D CT slices. Each slice is paired with text descriptions structured by [10], containing information about infection type, the number of infected areas, and the location of the infection. Detailed explanations will be provided in the final manuscript.
Despite superior performance compared to image-only methods, text-guided segmentation rigidly requires complete image-text pairs for both training and testing, which we identify as a limitation in our conclusion. To allow for missing text in partial cases, we plan to enhance our model by addressing missing modalities in the future.

Q2: Experimental details(R4)
A2: Train split: The train, test, and validate sets division followed the guidelines from [10], ensuring consistency for reliable comparisons.
CT Image Selection: In MosMedData, the dataset provider [10] has already selected multiple 2D images sliced from a single 3D CT volume of a patient.
Data Comparison: As discussed in A1, since our model was trained independently on two datasets rather than jointly, performance metrics naturally vary due to differences in label quality and modal distribution. Notably, we achieved SOTA results on both datasets.

Q3: More implementation details and reproducibility(R1,R4)
A3: Aligned with LGMS [12], our backbone uses pre-trained encoders (ConvNeXt for image and CXR-BERT for text) and the same simple data processing steps, making it easy to reproduce. We will add more implementation details to the final manuscript and release codes to ensure better reproducibility.

Q4: Comparison experiments and performance(R1,R4)
A4: The comparison to unimodal methods [3-5] aims to validate the motivation and benefits of introducing the text modality, as outlined in A1. To make the comparison more comprehensive, we will add CT visualization on MosMedData in Figure 3.
Since training on the QaTa dataset typically converges relatively easily, achieving further improvements becomes harder as metrics approach the bottleneck (near 90% dice). However, our model achieved a dice score of 90.6% on QaTa, surpassing the 90% threshold for the first time. Besides, remarkable results in MIoU on QaTa and two metrics on Mos underscore the effectiveness of our method. Paired t-tests between our model and the second-best LGMS on QaTa and Mos show p-values for Dice and MIoU less than 0.05, indicating statistical significance.

Q5: Model complexity(R4)
A5: Compared to some methods [10,13] using Transformer as the image backbone, we opt for a more lightweight ConvNeXt with fewer parameters. Additionally, our proposed multimodal interaction module comprises just a few simple linear layers, contributing minimal parameters. For inference, we only need milliseconds to generate results, thus fitting clinical application scenarios.

Q6: Ablation Studies(R2,R4)
A6: Due to space constraints, we only showed ablation results on QaTa, where the ablation effects are more pronounced. To ensure comprehensiveness, weâll include ablation studies on Mos in the final manuscript. Also, weâll incorporate normal attention as one ablation version to better validate our attention modules.

Q7: Illustration(R3)
A7: In fact, the skip connection is to concat the updated Fcom and the image feature from the corresponding upsampling level, as described in section 2.4. We will improve our illustration in Fig.2.C to avoid misunderstanding."
https://papers.miccai.org/miccai-2024/146-Paper0187.html,"We thank the reviewers for their comments. They highlighted that our method is with good novelty (R3&R4), clear explanation (R3), leading results (R3&R4), available code (R4&R5) and well-structured text (R5).

Q1: Quality evaluation of generated data (R3&R4&R5)
In addition to visualizing generated data (Fig. 3), we quantitatively evaluate their quality by comparing our methodâs segmentation performance to JointTrain (Table 1). Since JointTrain and our method train the same segmentation model with the only difference in using raw and generated data, their performance gap reflects how closely the generated data match the raw data distribution. Our method has comparable results to JointTrain, revealing the high generation quality that is close to raw data.

Q2: Extensibility to 3D image (R3)
Our BJD is a flexible diffusion pipeline without constraints on model architecture. Therefore, a potential way for 3D image replay is to adopt a 3D diffusion model as suggested, still following the BJD pipeline using loss Eq. (3) without modification. We will study it in the future.

Q3: Dataset and pre-processing (R3)
Given paper limit, we evaluated our method on fundus, cardiac and prostate datasets to include extensive ablation in submission. We used them given their challenging setting with diverse imaging conditions, objectives, and widely used. Our promising results show great potential on other data, such as suggested multi-organ or tumor datasets. We will include it in the future, and adopt the suggested cropping operation.

Q4: Number of generated samples per task (R4)
We empirically set this number to 3000, same to GAR. This is because the results on the validation set increase rapidly up to 3000 samples, after which it plateaus.

Q5: Computational cost of BJD vs. non-BJD counterparts (NJD) (R4)
While BJD incurs higher FLOPs than NJD during training, it does not involve extra learnable parameters, thus avoiding higher FLOPs during inference.

Q6: Why not use BJD for segmentation directly (R4)
While our BJD has segmentation potential, it leads to tremendously higher inference time compared to classical segmentation models, even with DDIM acceleration. Such high time consumption may be acceptable to generate training samples for model development, but it is prohibitive for deploying BJD directly on segmentation tasks, which often require a fast response.

Q7: Hyperparameter analysis on EWC, LwF and PLOP (R5)
The hyperparameters of EWC, LwF and PLOP were empirically set at 5000, 10 and 0.01, respectively. Notably, this setting involves a trade-off between the modelâs plasticity on incoming tasks and memorizability on previous tasks. Lower hyperparameters may improve results on the incoming prostate task but at the cost of huge drops on previous tasks (even complete failure). These methods cannot balance the plasticity and memorizability even with alternative settings, while our method can achieve both high.

Q8: Evaluation over time using BWT and FWT (R5)
-BWT is measured after each training stage to quantify forgetting over time. Since new experiments are not allowed in the rebuttal, we try to use the results in the last training stage (Table 1) to intuitively analyze the potential forgetting throughout the learning period, i.e., BWT over time. Our method shows the least forgetting in the last training stage, suggesting the best BWT in this stage, which has undergone gradual memory fading with tasks incrementally involved. When it comes to the earlier stages with less memory fading, the forgetting would be relieved inherently and our best BWT trajectory can be retained over time. We will quantitatively evaluate this in future work.
-FWT appears ill-suited in our task-incremental setting, where tasks vary widely without objective restrictions, making it impossible to anticipate future tasks in advance. For example, a model trained on fundus and cardiac segmentation cannot be directly applied to an unseen task of prostate segmentation."
https://papers.miccai.org/miccai-2024/147-Paper1272.html,"Thanks for the comments.

[R1-W1. Statistical analysis] Following previous works, we report the results as mean_std of three random runs for a fair comparison. In future work, we will add more statistical analysis as suggested.

[R3-W1. Purpose of this work] Our goal is to interpret representations in DNNs by aligning the axes of the latent space with known concepts of interest. For example, concepts such as âblue whitish veil (BWV)â and âatypical pigment network (PN_ATP)â are important for diagnosing melanoma skin disease. Given the global image feature F=[f1,f2,â¦,fd] of dimension d, we aim to assign the first activation value f1 with concept âBWVâ, f2 with âPN_ATPâ, etc. In this way, we can identify the concepts that significantly contribute to the disease diagnosis, making the decision-making process interpretable.

[R3-W2. Relationship of concepts] As our goal is to build an exact biunique association between a hidden value of the image feature and a predefined concept, we apply orthogonalization to ensure the disentanglement of concepts. In future work, we plan to incorporate the inherent relationship of concepts by utilizing a nearly orthogonal matrix.

[R3-W3. Solution with heatmaps] Thanks for your suggestions. We will add this comparison in our revised paper.

[R3-W4. Dataset] In our work, the term âdiseaseâ refers to skin diseases such as âmelanomaâ, while âconceptâ denotes high-level attributes of lesions such as âstreaksââ. In Derm7pt, we consider 2 diseases (nevus and melanoma) and 12 concepts from the 7-point checklist. These concept categories include pigment network (typical/atypical), blue whitish veil, vascular structures (regular/irregular), pigmentation (REG/IR), streaks (REG/IR), dots and globules (REG/IR), and regression structures. In SkinCon, there are 3 diseases (malignant, benign, and non-neoplastic) and 48 concepts, such as plaque, scale, and erosion. We select 22 concepts with at least 50 representative images. The number of concepts is consistent between dataset introduction and Fig 2. We will clarify this.

[R4-W1. Model interpretability] By replacing the BatchNorm layer with our CAW layer, we not only conduct feature normalization but also enhance the feature interpretability. Please refer to the response to [R3-W1] for a more detailed explanation.

[R4-W2. Concept mask] The optimal solution involves utilizing ground-truth concept masks to facilitate concept alignment. However, existing datasets lack such fine-grained annotations. To address this limitation, we employ a weakly-supervised learning approach to generate pseudo concept masks. Table 2 demonstrated the superiority of our concept masks in both disease diagnosis and concept detection compared to other alternatives in such a weakly supervised setting.

[R4-W3. Concept annotation] Our approach uses concept annotations for training in two aspects: 1) We use concept annotations to pre-train a concept classification network, which is then employed for generating concept masks. 2) At the disease classification model training stage, we rely on the concept dataset to conduct concept alignment, which is constructed by grouping the images with the same concept label.

[R4-W4. Classification performance] Table 1 shows that all compared XAI methods consistently exhibit inferior results than the black-box ResNet. The empirical finding in the accuracy-interpretability trade-off [1,2] suggests that more complex models, such as DNNs, sacrifice interpretability for higher accuracy, while interpretable models often exhibit inferior performance. Remarkably, our CAW method achieves comparable performance and even surpasses the black-box ResNet on Derm7pt (ACC, F1) and SkinCon (ACC), highlighting our modelâs ability in improving interpretability while maintaining accuracy. 
[1] DARPAâs explainable artificial intelligence (XAI) program, 2019.
[2] Explainable artificial intelligence: a comprehensive review, 2022.

[R4-9. Code] We will release the code."
https://papers.miccai.org/miccai-2024/148-Paper0595.html,"As suggested by the R#1, we will add additional description to the illustrations, defining each variable presented in the figure caption, and how they relate to the method. Descriptions of the loss function terms will be expanded, in addition to the weighted balance between each loss term. Finally, we will provide more details on the datasets which was used in previous studies for liver motion prediction using previous observations. While they seem to be limited in size, each sequence includes more than 42,000 volumes, covering several breathing cycles.

We agree with the R#1 that stable diffusion models are rather limited for real-time inference. This proof of concept shows that sufficient accuracy can be achieved in comparison to previous methods. In fact recently, Latent Consistency Models (LCMs) have been getting a lot of interest because they allow to generate images very quickly around 150ms (acceptable for system latencies), as opposed to 10 seconds with vanilla Stable Diffusion. This will be investigated in further studies.

We in fact report geometrical errors in the paper, as reported in Fig. 2.  This yields a geometrical error of of 1.05 +/- 0.53mm. Due to the page limits, we plan to reports RMSE and DVF errors, as well as evaluate other modalities in future work.

We will distinguish vectors M and 1-M as complementary in Fig. 1.

We will add a reference to the 2D in-plane decoder, as well as the tracker presented by (Romaguera TMI 2023).

The loss term in section 2.4 will be detailed.

In several previous studies, VoxelMorph was used to register liver images between several phases, showcasing the methodâs robustness to various types of breathing patterns and deformations. The reference model will be added as reference.

The reviewer is correct, the fine-tunning is made on the trained DDPM. This will be corrected in the revised paper.

The term online was aimed to design the near real-time performance of the model for prediction motion during radiation. However, to avoid any further confusion, we will therefore remove this"
https://papers.miccai.org/miccai-2024/149-Paper3196.html,"Thank you for highlighting the key unclear items.

Reproducibility (R1,R3)
Upon acceptance, we will provide our code, experiment settings, and preprocessing steps for reproducibility. We were unable to use only public data as it lacks the AIF data for PPM computation. We also plan to release our private data in the future.

Technical Novelty (R1)
Our main contribution lies in our modelâs unprecedented versatility, allowing inpainting of gaps of any duration without extensive prior knowledge. This is achieved by minimizing the required condition to simple embeddings. Complex conditioning methods offered minimal improvement, and incorporating advanced technical novelty was unnecessary for our research purpose.

Ablations on the Embeddings (R1)
Distance (Local Context): This is essential for specifying the exact position of intermediate scans. Ablating it would prevent the model from distinguishing between potential intermediate scans.
Categorical (Global Context): MPVF and MCVD are advanced models providing local context but lack global context. Our modelâs simpler and efficient incorporation of both local and global contexts outperforms these models, implying significant contribution of global context. Explicit ablation experiment would have been more informative if space allowed.

Categorization of Global Context (R1,R3)
Categorization is automatic, requiring only the time index of the peak mean intensity. During testing, we inferred the peak as the average time point of the two highest-intensity known scans.

Impact of Wrong Categorization (R1,R3)
Incorrect categorization likely occurred due to its automated process. However, generating various condition set combinations for the same target (Section 2.2) enhanced robustness and generalizability during training. The model learned from correct conditions to reconstruct clean images, evidenced by its high performance despite categorization errors.

Clinical Benefit and Validation of Motion Correction (R1)
In Fig. 4, motion-corrected maps show clearer localization of perfusion deficits, with clear separation from unaffected tissue. Before correction, maps exhibit obvious abnormalities hindering accurate diagnosis. After correction, these abnormalities are removed, and the natural rise-fall behavior of the curves is recovered. Fig. 4 sufficiently suggests the potential without formal clinical validation, given the auxiliary nature of this task.

CTP Acquisition (R4)
Comprehensive datasets are needed for training. However, once trained, our model can interpolate from reduced scans during testing, eliminating the need for full CTP acquisition at deployment.

Tangible Benefits Compared to SOTA (R1,R4)
Our methodâs merit over SOTA models lies in its versatility. SOTA models require separate training for different gap durations (2s, 4s, 8s). In contrast, our single trained model can interpolate any gap between 2s and 8s, offering greater clinical value by being adaptable to various interpolation needs.

High CBF Error Rate (R4)
CBF inherently has high values as it measures blood flow per minute, so it does not necessarily indicate performance deficiency. Table 2 includes Linâs CCC, which is less affected by scale and combines precision and accuracy measures. The CCC for CBF is comparable to CBV and Tmax, indicating consistent performance across different parameters.

Radiation Reduction at the Cost of Precision (R4)
While precision is crucial, especially for severe stroke patients, our model could still bring clinical benefit in certain scenarios. Some scanning hardware is incapable of achieving a sufficient acquisition rate. Also, for patients with mild symptoms (assessed by NIHSS) or those at the chronic stage, where focus shifts to long-term management and rehabilitation, clinicians may opt for greatly reduced radiation (up to 1/8) at the cost of slight precision loss. This is especially relevant for patients sensitive to radiation (e.g., pregnancy, high radiation exposure history)."
https://papers.miccai.org/miccai-2024/150-Paper3622.html,"We appreciate the reviewersâ comments & insightful suggestions. 
R1: Advantage of DDPM for segmentation: R: There is inherent ambiguity in medical image segmentation as the delineation of the same image differs among experts. Generally, the ground-truth (GT) label for an image is obtained by a consensus among experts. Here, we utilized the stochastic nature of DDPM to approximate this process & generate multiple predictions during inference, then take their mean & threshold to obtain MORE accurate masks compared to deterministic models as U-Net (Table 2). We will add this discussion.
R1: Categorical nature of segmentation variables: R: For categorical distributions (Table 3), we use one-hot encoding for GT labels (Sec 3.1). Thus, we DID NOT introduce any order among labels. The prediction from our model is a probabilistic map for each channel of GT labels & we used thresholding to convert it to binary labels for each output channel, like standard segmentation models. For a given pixel, we considered the class which has the maximum probability value. We will clarify this.
R1: Misleading âdiscriminatorâ term: R: As we use the âgenerated (fake)â & ârealâ noisy samples at each time step to train the discriminator to classify between real & fake samples, the term âdiscriminatorâ IS appropriate. GAN-based models generally are not used for segmentation as adversarial learning is NOT well suited for segmentation tasks. 
R1: Zero âAttentionâ map: R: If the attention map had been 0, the discriminator would predict only fake, which is clearly not the case. We have also visualized the NON-ZERO attention map in the architecture figure that shows dependency on both x_t & I. The attention maps A_D indicate the relative importance of pixels [20], hence the term âattentionâ IS appropriate.
R1: Disparate inference distribution of x_t: R: We utilized the attention maps during training to learn a better mapping of the reverse denoising process & identify the importance of different parts of the segmentation labels (x). Inference just involves sampling from noise. Cross-validation results show there is NO distribution mismatch between training & inference.
R1: No multi-modal distribution learned with z: R: We introduced latent variable z to reduce the training & inference time-steps. So, when larger steps are used, the reverse denoising distribution becomes multimodal [18]. Ablation studies (Table 1) show that WITHOUT using z leads to a significant performance drop when using extremely low time steps.
R3: Computational cost: R: Our method is not computationally expensive as in inference we remove the discriminator and perform sampling with a much smaller number of steps. Additionally, compared to SegDiff, we observe a 95% decrease in trainable parameters. We will clarify this.
R3: Hyperparameters & generalization to unseen data: R: We chose the standard hyperparameters used in the literature for diffusion models & performed an ablation study to choose the best setting. Cross-validations are performed to avoid overfitting.
R3: Preprocessing & Risk of recurrence: R: We follow the standard preprocessing steps such as normalization, random scaling & rotation. We didnât consider the risk of recurrence but will add the discussion to the paper and add the suggested references.
R3: Limitation of our model: R: One limitation is that our model can only be applied on a 2D slice & we are working on the 3D version of it. We will add this discussion.
R4: Clinical contributions: R: By introducing our diffusion model for medical image segmentation, we are modelling an important clinical process as addressed in the first point to R1. We will update Sec 2.1.
R4: Inference time: R: The inference time for our model is 1 sec (T=4) vs 60 secs for SegDiff (T=100).
R4: Statistical tests: R: We had performed statistical t-test before & found p-value < 0.0001 for all metrics in Table 2 & Table 3, therefore our results are statistically significant. We will update this."
https://papers.miccai.org/miccai-2024/151-Paper2542.html,"Dear Chairs and Reviewers,

Thank you for your valuable feedback on our manuscript, âConditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction.â We appreciate the opportunity to clarify the points raised and enhance the understanding of our research contributions. We have addressed each of the main concerns as follows:

Clarity of Data and Model Description
Response:
1) The demographics in Figure 1 include both age and gender variables.
2) Number of Biomarkers and Network Inputs Shape: We used 71 biomarkers: 68 cortical thickness (CTh) values for 68 brain regions, plus three non-CTh variables (i.e., age, gender, and diagnosis). These non-CTh variables, along with the time gap between scans, were vectorized by repeating each of them 68 times to match the CTh length and concatenated in the channel direction. The biomarkers and time gap were zero-padded to a dimension of 72 for model compatibility with the modelâs upsampling and downsampling layers.
3) U-net Architecture: Our U-net is a 1D network. For inference, our model took 6-channel inputs (Gaussian noise, initial CTh, age, gender, diagnosis, and time gap) of length 72. The model generated the change of CTh, outputting a single-channel size of 72 through the reverse diffusion process. We then discarded the zero-padded region to obtain the final result.
4) Inclusion of Converters: As Reviewer #4 noted, our sample includes some converters (individuals whose status changed during follow-up). Including these samples allows us to provide early warnings based on MRI data changes, potentially leading to earlier intervention. Research shows that in Alzheimerâs Disease (AD) progression, structural changes often precede clinical symptoms. For example, cognitively normal (CN) subjects at baseline who later transition to mild cognitive impairment (MCI) or AD (CN converters) have smaller baseline CTh compared to stable CN individuals. We plan to further explore early-stage conversion prediction with CTh features.

Open Access to Source Code and Data
Response: The data used in this study, TADPOLE, is publicly available and can be accessed through the TADPOLE Challenge (https://tadpole.grand-challenge.org/) or ADNI (https://adni.loni.usc.edu/). The source code will be made publicly available after the MICCAI review period.

Concerns about the Lack of 3D Images Utilization
Response:
1) Our study focuses on CTh from T1-weighted MR images, intrinsically linked to medical image data. Our 1D U-net for estimating the score function of 1D tabular data suggests our approach can extend to 2D or 3D images.
2) The 1D CTh measurements provide quantitative indicators for brain subregions and their atrophy. Studying the temporal progression of cortical thinning may better characterize disease progression and patient condition. These measurements offer more direct interpretability than voxel-wise values in 3D imaging data. Our strategy uses a diffusion model for longitudinal, region-wise prediction tasks. For 3D images, this involves generating whole images through voxel-wise prediction, which poses computational, evaluation, and interpretation challenges.
3) We recognize that converting images to a 1D vector will lose structural priors among regions. In our model, each element of the 1D CTh vector corresponding to a brain cortex sub-region, while the diffusion model handles data distribution through the score function. We will explore extending to a graph-based diffusion model to incorporate inter-regional structural connectivity from DTI/DWI data into the prediction modeling.

Thank you again for your consideration and the opportunity to improve our manuscript.

Sincerely,

All co-authors"
https://papers.miccai.org/miccai-2024/152-Paper3400.html,"We sincerely thank the reviewers for their support of our work and the valuable comments.

Response to criticism:

Restriction to DSC (R3): We fully agree that the reporting of a single metric is rarely sufficient (see also the Metrics Reloaded framework). In this study, we focused on the DSC because it is the most widely used metric in biomedical image analysis (https://www.nature.com/articles/s41467-018-07619-7) and was used by ~80% of the segmentation papers analyzed in this study. Other metrics would have resulted in  a much smaller sample size (e.g., the Hausdorff distance, the most frequently used contour-based method, was reported by only 11%) .

Broadening the scope beyond MICCAI (R3/5): Paper screening is highly time-consuming, requiring hours per paper (multiple screeners + conflict resolution). We thus deemed for a representative sample of papers and decided for the MICCAI proceedings, which are representative of high-quality work in the field of biomedical image segmentation and also an exact match to the MICCAI audience, which the paper will be presented to.

Stratification by entity/model (R3): This is an excellent idea. As it is not allowed to put new experimental results in the rebuttal, we will consider this remark for a journal extension.

Calibration of SD approximation method (R4): We can complement the figure with more textual quantitative information.

Discussion of aspects related to inter-rater variability (R3): Excellent recommendation, which we will incorporate in the final version if space allows.

All: We will further clarify missing details in the final version."
https://papers.miccai.org/miccai-2024/153-Paper1765.html,"We sincerely thank the reviewers for their valuable comments on our paper. Below are our collective responses to the concerns raised:
Technical contribution: We employ supervised contrastive features, which have not been utilized in prior works. The derived metrics are based solely on supervised contrastive learning (SCL), representing a novel technical contribution. Additionally, the uncertainty and memory components are designed to mitigate bias towards majority class samples. Our proposed solution integrates SCL with uncertainty-based transfer and is clearly outlined in Section 1.

Emphasis on CRM: While we acknowledge CRM as a novel component contributing to uncertainty, our overall novel contribution encompasses the integration of various components. Due to limited space, we aimed to highlight each component rather than focusing solely on CRM.

Figures & Labels, Complexity of the article: We sincerely apologize if the manuscript was challenging to follow. Additionally, the Figure s 3(a-c) in the manuscript utilize different color schemes because they represent different analyses. Due to the space constraints and the need to present extensive analysis, we had to condense the information, which required us to use more complex terminology and brief descriptions. Also, we combined the rationale and motivation for conciseness, which may have contributed to the complexity of the paper. Furthermore, the figures were adjusted to fit the available size, resulting in smaller labels. Based on the feedback, we will provide figures with enhanced label sizes in the final drafts for publication, if permitted. Regarding the suggestion to subsection parts for better description of Figure 1 and provide separate rationale and limitations, we respect the comment. However, for Figure 1, we have provided a detailed and stepwise elaborative description in section 4, along with an algorithm in the supplementary material and also as per guidelines, we are unable to change the content in the original paper.

Equation 8: We respectfully disagree with the comment regarding Equation 8. The equation explicitly lists three hyperparameters, denoted by âlambdaâs, which is considered an optimal number according to the literature. The remaining parameters are confidence parameters that are learned automatically through uncertainty during the learning process, as mentioned just above Equation 7.

Dark, useful and relevant Knowledge: We apologize for not providing a more in-depth description of these terms. âDark knowledgeâ is commonly used in contrastive learning to refer to complementary knowledge or secondary probabilities used for incorrect classes. Due to its prevalence in the literature, we did not elaborate on it. The terms ârelevantâ and âusefulâ knowledge are used in their literal sense, referring to the knowledge necessary for refining the student modelâs learning.

tSNE: The reviewerâs assumption is indeed accurate, also supported by Section 3.2.

Implementation: We adhered to the settings outlined in the literature, as detailed in Section 3.1, and substituted the algorithms with our own. To assess the efficacy in handling class imbalance, we sampled input data with varying distributions and ran all models separately on the same data.

Supplementary and Codes: We apologize for this unintentional error caused by formatting issues that resulted in content being moved to other pages. Since this content is not part of the main manuscript, we assure you that, in accordance with the guidelines, we will ensure it is correctly positioned within the specified range for the camera-ready version. Additionally, we respectfully disagree with the comment regarding an incomplete understanding of the supplementary results. The results presented in the paper suffice for our analysis. We have provided detailed headings in the supplementary material that reference the corresponding paragraphs in the paper for better understanding & algorithm for reproducibility."
https://papers.miccai.org/miccai-2024/154-Paper3485.html,"We thank all the reviewers for their valuable feedback. We appreciate the recognition of our novel contributions, including our unique approach to learning better scale-invariant representations and our innovative pseudo-label consistency regularization. We also thank the reviewers for acknowledging the value of incorporating distribution alignment during training for enhanced stability. We have addressed the concerns below.
Reviewer#4 ï  (1) Discussion on the clinical credibility: The clinical credibility of AnoMed is twofold. Firstly, our studies demonstrate that AnoMed, utilizing a semi-supervised framework, is remarkably efficient. With only 20% labeled training data, it achieves an AP50 of 76.8% (see Table 1), surpassing existing supervised methods trained with 100% labeled data (ViTb16 - AP50 72.4%, ResNet101 - AP50 58.2%, VGG16 - AP50 34.6%). This highlights AnoMedâs potential to reduce the cost of manual labeling in clinical settings. Secondly, the concept of learning scale-invariant features for anomaly detection is also applicable to multi-organ segmentation tasks. Furthermore, we envision AnoMedâs potential in generative models, where we aim to scale existing models with synthetic data and minimal manual annotation in future work.
Reviewer#5 ï  (1) Overstating âcardiacâ tailored framework: In our paper, we define the problem as learning scale-invariant features and refining hesitant pseudo-labels for smaller and minor target ROIs. To the best of our knowledge, chest X-rays are one of the most suitable benchmarks for this purpose. Although our proposed method generalizes well to out-of-distribution datasets (see Table 1), we plan to include additional datasets for OOD evaluation to enhance our assessment. We are particularly excited about the potential applicability of this approach to abdominal CT images, which involve both small tumours and large organs to segment or detect. We will tone down our claims to present AnoMed as a generalized method for scale-invariant learning in both semi-supervised and supervised tasks, with a focus on pseudo-label optimization and refinement.
(3) Typo (t1 and t2 are reversed in the PLO section) and clarity of paper: Thank you for pointing out the typo in the PLO section, where t1 and t2 are reversed. We will correct this in the revised manuscript. Additionally, as per your suggestion, we will ensure greater clarity throughout the revised manuscript. 
Reviewer#6 ï  (1) Evaluation between supervised and proposed semi-supervised method: We have compared AnoMed with supervised methods in Table 1, where shaded rows represent supervised methods. Evaluations cover all three categories: 2-stage anchor-based (Faster RCNN), 1-stage anchor-free (DETR), and 1-stage anchor-based (supervised counterpart of AnoMed). Due to space constraints in the MICCAI format, we only included the best-performing supervised method in Table 1. For the final manuscript, we will update with at least two baseline comparisons for each category in supervised training results. For reference, Fast-RCNN (2-stage anchor-based), YOLO-v8 (1-stage anchor-free), and RetinaNet (1-stage anchor-based) achieved AP50 scores of 40.6%, 45.2%, and 56.1% and AP50:95 scores of 26.4%, 30.2%, and 40.5%, respectively. With only 20% labeled training data, AnoMed achieves an AP50 of 76.8%, surpassing existing supervised methods trained with 100% labeled data (ViTb16 - AP50 72.4%, ResNet101 - AP50 58.2%, VGG16 - AP50 34.6%). Therefore, we have focused more on semi-supervised frameworks to ensure a fair evaluation, countering the claim that our method does not meet MICCAIâs standards.
(2) Effect of semi-supervised method: Table 1 clearly shows that the semi-supervised method outperforms supervised methods by significant margins. We have detailed the impact of AnoMed in both supervised (AP50 61.2%) and semi-supervised (AP50 76.2%) settings, with comprehensive quantitative and qualitative comparisons (see Table 2 and Figures 1-5)."
https://papers.miccai.org/miccai-2024/155-Paper3653.html,N/A
https://papers.miccai.org/miccai-2024/156-Paper1410.html,"We appreciate the reviewersâ thorough and insightful feedback on our paper. Below, we address the concerns and misunderstandings raised, grouped into key themes for clarity.
Q1: Regarding the relevant works (R1).

Q2: Regarding the practical relevance of the proposed method (R1).

Q3: Regarding the complexity and explanation of the method (R3).

Q4ï¼Regarding the use of âpseudo-prompt patchesâ (R3).

Q5: Clarity on Ablated Models, i.e., ResNet and DenseNet and the Training Procedures (R3), data sufficiency for model training and evaluation(R4)."
https://papers.miccai.org/miccai-2024/157-Paper2182.html,"R1+R3+R4) Choice of feature encoder: We mention in Sec.2a that a wide range of encoders can be used. As the ResNet backbone is primarily used in literature, it is also used here for consistency. The layer prior to the classification layer is used for various downstream tasks due to its rich discriminative abilities. As mentioned in Sec. 2a, last paragraph, fixing the encoder avoids the invalidity of stored generators over time. We agree that accuracy could be further improved with an encoder offering more discriminative features. In future work, we will explore more feature extractor backbones.
R3) Novelty: To the best of our knowledge, generative replay is not explored in digital pathology so far. Our solution is privacy-aware and performs better than buffer-free and comparable to buffer-based works. Additionally, it does not require large storage as in buffer-based works. We mention these novelties in Sec.1, but probably need to highlight them even more. We will revise Sec.1 to avoid any misinterpretation regarding novelty. 
R3+R4) Referred works: As suggested, referred works will be added. Our work is significantly novel and distinct from the referenced studies in several aspects, including (a) target research domain, (b) CL incremental learning, and (c) CL strategy. The referred works mainly focus on learning new classes from a single domain in subsequent episodes belonging to natural images, whereas we focus on learning shifts arising from domain (organ, stain, etc.) changes in pathology. Here, a complex architectural-based CL strategy, as followed for new class arrival (Yang et al.2022 focused on few-shot learning), is not necessary. It is evident from the literature that replay strategies outperform regularisation and architectural-based strategies. However, generating actual images for replay (Pfulb et al.2021) will cause critical privacy violations. Additionally, generating artificial whole slide image patches with GMM (Pfulb et al.2021) or GAN (Shin et al.2017) is challenging compared to digit-like images. GAN-based work would raise further challenges in its applicability in pathology, e.g., updating a single GAN generator may cause photocopy problems to old domains; domain-specific GANs would add model complexity; training GAN with limited training data may lead to unstable pathology image generation, etc. In contrast, our generator is lightweight, works even with limited training data, and offers privacy-aware replay, which is a key factor when applying CL in pathology. 
Further, in contrast to the work (Yang et al.2021), our framework facilitates continuous knowledge accumulation from novel domains in learnable layers after the latent replay layer, which ensures its long-term usage. Additionally, contrary to (Yang et al.2021), in our work, there is no requirement to pre-train the feature extractor with multiple domains before the CL sessions.
R3) More baselines: We cover a wide variety of baselines commonly used in domain incremental works. The considered buffer-based baselines (2017-2020) are from years similar to iCarL (2017). Although we also had results for more buffer-based works (GDumb, MIR, DER, iCarL), which performed similarly to A/GEM, ER, & LR (Sec4 Para1), we only show the most common ones as representative upper bounds. Adding more buffer-based works does not provide additional insight but would shift the focus of our paper away from privacy-aware, buffer-free approaches.
R3) Train-test split: The amount of stain shift slides is not sufficient for slide-level split. However, after consulting with our medical collaborators, our tile datasets include sufficient tissue variability across splits. Also, tile-level split is a common strategy in digital pathology.
R4) Augmentation: Following the definition of CL in literature, intensity or color-based style transfer are not performed (they cannot cover all possible variability arising from different shifts, e.g., organs, centers, stains)."
https://papers.miccai.org/miccai-2024/158-Paper0254.html,"We sincerely thank the reviewers for their valuable comments and suggestions. We appreciate the encouraging comments like âContinual learning (CL) in the report generation (RG) task is novel and important. Contrastive loss in LLM hidden space is novel and interestingâ (R1&R3), and âmove the RG task from 2D XR to 3D CT is interestingâ (R4).

Q(R4): How do we avoid forgetting?
A: We employ task-specific weights to differentiate between tasks, a strategy that has proven effective in CL and avoids forgetting (ICLR[27], MICCAI[a], TIP[b]). Specifically, we learn minimal âdistinct and task-specificâ weights for âq, f_t, P, LNâ for each dataset/domain.

Q(R4): The input image size and evaluation metrics?
A: The commonly used image size in RG for XR and CT slices is 224*224 [c,d], while BLEU, ROUGE, and CIDEr are prevalent evaluation metrics for RG [c,d]. To ensure comparability with prior studies, we maintain consistency in both input shape and the metrics. Our experiments (Table+Fig.) can demonstrate the effectiveness of our method, confirmed by a radiologist.

Q(R4): How does our method align with clinical settings?
A: We focus on CL for RG, aligning with clinical settings where multi-domain data at scale can only be acquired sequentially (not simultaneously) for LLM. We tailor the CL pathway to match real clinical applications (from online to private, from XR to CT), and meet the clinical needs of RG for both XR (using 1 or 2 inputs) and CT.

Q(R3): Guidelines for the tuned parameters?
A: Reuse P for similar tasks (both English+XR). For dissimilar tasks (English to Chinese, XR to CT), tuning P is necessary.

Q(R3): Fine-tune LLM on RG task.
A: We aim to optimize the overall performance on multi-domain RG tasks (English&Chinese, XR&CT). Tuning the multi-lingual LLM on the 1st dataset D_XE1 causes overfitting in an English setting, deteriorating the subsequent performance. Keeping the multi-lingual LLM frozen maintains its generalizability as a backbone for CL on multi-domain RG tasks, and reduces computational costs.

Q(R3): Log-likelihood in Eq.(1)?
A: Generating subsequent tokens is to predict the indices of words in a given vocabulary, like classification. The LLM outputs are probability vectors for possible word indices, where log-likelihood is optimized by equivalent cross-entropy.

Q(R1): Baseline methods.
A: To the best of our knowledge, no prior work on CL in RG for comparison. DER and EWC are commonly used baselines. ProgPrompt, identified as the leading baseline on ICLR[e] and Arxiv[f] recently (Apr. 2024), is a competitive option for comparison.

Q(R1): Generalizability of our method using disease graph.
A: In lungs, our method can learn new modalities (e.g. PET/MRI) by using the domain token and pulling similar findings to the diseaseâs class centers of CT (e.g. tumor). If no similar findings exist, L^SC allows for free exploration of diseaseâs class centers in the feature space, while keeping distance from other classes. In a broad medical context for CL, our method can map disease graphs used in screening to detailed exams with additional labels (e.g. XR to CT, fundus photo to OCT).

Q(R1): Detailed info.
A: The number of training parameters in ProgPrompt matches ours, as they use an MLP layer for prompt reparameterization. Other methods use much more training parameters by tuning encoder E. Details on each dataset will be included in the supplementary material.

[a]Adapter learning in pre-trained feature extractor for continual learning of diseases,MICCAI,2023
[b]Task-Specific Normalization for Continual Learning of Blind Image Quality Models,TIP,2024
[c]Radiology report generation with a learned knowledge base and multi-modal alignment,MIA,2023
[d]Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning,TNNLS,2021
[e]Scalable Language Model with Generalized Continual Learning,ICLR,2024
[f]Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language Learning,2024"
https://papers.miccai.org/miccai-2024/159-Paper1236.html,"R1: How the dimensionality of the latent contrast representation affects the performance.
A: We experimented with different dimensionalities (64, 128, 256, 512) for the latent contrast representation. The model performance was suboptimal below 256, while the improvement was marginal above 256. Therefore, we chose 256 in this paper. We believe the representation should not be over-compressed as it must sufficiently represent brain structural information. The specific dimensionality also depends on the model architecture, which is not the focus of our paper. Our aim is to explore the latent relationship between contrast and imaging parameters using a multi-sequence dataset and imaging parameters as conditions for the network.

R1: More evaluation metrics.
A: We agree that including more metrics would provide a more comprehensive evaluation. However, due to rebuttal rules, we cannot include new experimental results in this study. We are currently investigating generating images with novel contrast using our method for segmentation tasks, which would serve as a valuable evaluation of our modelâs capabilities.

R1: Influence of the number of sequences on the results.
A: Using more sequences in training leads to better results. We balanced scanning time while trying to cover the entire parameter space, selecting nine sequences. However, performance is not entirely proportional to the number of sequences, as certain sequences are crucial for training (e.g., sequence 8: TR 8000ms, TE 99.98ms, TI 1030ms). Without this sequence, the model becomes difficult to converge. Investigating sequence selection and underlying mechanisms is an important future research direction.

R1: Evaluation of pathology data such as MRI with tumor.
A: Evaluating the modelâs performance on pathology data is important. Unfortunately, we lack such in-house data, and public datasets like BraTS do not disclose imaging parameters, making them unsuitable. We appreciate the comment as it aligns with our future research direction. The presence of tumors will make the task more challenging due to their non-fixed contrast compared to normal brain tissues. Dealing with tumor characteristics is a difficult yet worthwhile problem to solve.

R3: Lack of test set.
A: We would like to clarify that we do have a test set. As mentioned in âImplementation Detailsâ, we used eight subjects for training and two subjects exclusively for testing. These two subjects used for testing were never exposed to the network during training.

R3: Generalizability concerning small dataset. 
A: While the number of subjects may appear small, there are 90 scans used in this study (each subject underwent 9 scans). During training, the network performs mutual image translation between any two sequences, effectively enlarging the actual training data size.

R3: Experiments restricted to TSE-based acquisitions.
A: We agree with validating our method on a wider range of techniques. Our study is a preliminary exploratory experiment, and we used relatively common TSE sequences. The success on TSE suggests potential for extension to broader MRI sequences.

R3: Lack of effect of echo train length (ETL).
A: In our acquisition, ETL significantly impacts scanning time, so we didnât acquire multiple sequences by varying ETL while fixing other parameters. This is a very constructive suggestion, and we will include ETL as a variable parameter in future dataset expansions.

R4: Structural consistency constraint.
A: We acknowledge potential structural differences across sequences for the same subject. To address this, we modified the SSIM loss function, focusing on a component that relates to image structure. This component computes the covariance of two images divided by the product of their variances. By emphasizing this structural component, we aimed to make the consistency constraint less strict while still encouraging structural similarity across sequences."
https://papers.miccai.org/miccai-2024/160-Paper0251.html,"We appreciate the reviewers for acknowledging the novelty and contribution of our research. We provide detailed responses to feedback below.

[R1, R3] Clarification: We will revise the minor errors and clarify the logic for better understanding.

[R1, R3, R4] Source code & data: We plan to release the code with links to the data along with a detailed explanation of the algorithm.

[R3] Details about method (C_pr and trainable layers of pretrained LDM): Regarding the implementation of the backward process with C_pr for label synthesis, as described in Section 3.2, we followed the source code of Imagen [21]. For the scalable components of the pretrained LDM, we employed a trainable copy of the encoder structure from PathLDM [29], with zero convolution layers, similar to the approach used in ControlNet [32]. While it may not be feasible to include all the implementation details due to page limitations, we will try our best to clarify this in the revised text.

[R3] Efficiency of LDM for the task: As shown in Table 3, our method achieved comparable or better performance compared to SDM. However, it is important to note that while SDM generates synthetic images from the pixel-level label constraints, our method can generate both synthetic labels and image data from simple text constraints. Considering our goal of improving model performance through data augmentation, achieving similar performance to SDM with our LDM-based image synthesis demonstrates a reasonable contribution in terms of reducing the time cost associated with data generation.

[R3] Downstream performance comparison (e.g., DiffMix and Tang et al.): 
Ensuring semantic alignment at the instance level is crucial when performing data augmentation on nuclei images. We compared our method to SDM because it has demonstrated effective performance and semantic alignment capabilities for nuclei image synthesis, as evidenced by NASDM. Although we did not implement specific downstream tasks like addressing class imbalance, which DiffMix focuses on, our results can provide insights similar to DiffMix since DiffMix is a framework based on the SDM. The work by Tang et al. is not directly comparable to our study because it does not guarantee instance-specific differentiation during image synthesis, which is a key requirement for nuclei segmentation tasks.

[R3] Generation time calculation: As shown in Table 1 and 2, we calculated the synthesis time for generating the output label or image using the optimal number of sampling steps (T). In this experiment, SDM achieved its best performance with T=1000, while our fine-tuned LDM obtained optimal performance at T=100.

[R3, R4] Details about data splits and experimental settings for downstream tasks: To train the generative model, we divided each dataset into training and test sets with the following ratios: 80:20 for PanNuke, 85:15 for EndoNuke, and the NASDM split for the Lizard dataset. The difference between the baseline (HoverNet) performance in Table 3 and the performance reported in the literature for the Lizard and PanNuke datasets is due to the different training data sizes used for the downstream task; Our baseline model was trained using the patches that were not used for training the generative model to avoid data contamination, which is actually the test set used in the literature. Since this is much smaller than the original training data in the literature, the performance was lower than the reported numbers in the literature. As long as we used the same data setting, the relative performance comparison between methods in Table 3 should be fine."
https://papers.miccai.org/miccai-2024/161-Paper1911.html,"We sincerely thank the reviewers for their time and effort in reviewing our manuscript. We appreciate the constructive feedback which has significantly contributed to improving our work. Below, we have addressed each comment in detail.

[R1]-Low novelty, justification of methodology
We acknowledge Reviewer 1âs concern regarding novelty and have clarified our methodology. We revised the statement âWe only modify the inference formulationâ to âWe control the generation results in the inference stage by tuning the guidance scale instead of hyperparameter tuning for weights of the regularization terms in the loss function during the training stage.â 
This change is intended to emphasize our focus on controllability rather than simplicity, consistent with our title âControllable Counterfactual Generation for Interpretable Medical Image Classification. Additionally, we expanded the Introduction and Conclusion sections to better articulate our justification, emphasizing our methodâs ability to handle the trade-off between image similarity and class difference and manage both global and local structural changes in counterfactual generations, which other methods struggle with. These revisions are reflected in the updated manuscript.

[R1]-Unconvincing results in Table 1 and 2
We have clarified the purpose of Tables 1 and 2. Table 2 demonstrates that only our method achieves a promising trade-off in both image similarity and class difference. We will highlight the worst metrics for each method using a deletion line and red color to emphasize that these methods fail to achieve a balance. In Table 1, we demonstrate the controllability of our method by adjusting the guidance scale (w). For example, w=6.0 yields the best image similarity, while w=4.0 is optimal for class difference.

[R1,R4,R5]-Inconsistent results with anatomical knowledge in Figure 3 and low quality in Figure 2 and 3
We understand and appreciate reviewersâ concern regarding the reliability of our results. We realize that this may be due to our inappropriate presentation in the figures.
We have reviewed the examples provided in the paper by Toews et al. (2010) and observe consistent anatomical features in our results, including the anterior poles of the enlarged lateral ventricles, regions of atrophied white matter, enlarged temporal poles of the later. We will replace Figures 2 and 3 with higher-resolution versions to ensure these features are clearly visible. The new Figure 3 will include 4 rows for original images and results from our methods and two competing methods, and 4 columns for significant AD-related features. Figure 2 will have 3 rows for guidance scales of 1, 3, and 5, and 4 columns for the same features.

[R4,R5]-Data splitting procedure
We acknowledge the concern regarding dataset splitting and have clarified this in the manuscript. After removing subjects appearing in both ADNI-1 and ADNI-2 from ADNI-2, there are 200 AD and 231 CN subjects in ADNI-1, and 159 AD and 205 CN in ADNI-2. This revision ensures there is no data leakage.

[R4]-Classification results
We agree that classification experiments on small datasets should be conducted under k-fold cross-validation. Our method focuses on conditional image-to-image generation and does not modify the DenseNet3D backbone for classification. Previous works with DenseNet3D on the ADNI dataset support its generalization and robustness. Although we cannot provide new experiment results due to rebuttal guidelines, we will offer a subject-level k-fold cross-validation in future experiments.

[R5]-Hyperparameter sensitivity analysis
We appreciate the concern regarding hyperparameter sensitivity analysis. We have conducted an ablation study for a qualitative comparison of generation performance in Figure 2, a quantitative comparison in Table 1, and a downstream data augmentation task in Table 3. We will include this classification hyperparameter sensitivity analysis as appendix information in the updated manuscript."
https://papers.miccai.org/miccai-2024/162-Paper0850.html,"We thank the reviewers for their positive feedback.
Reviewers 1,3,4 are addressed as R1,R3,R4, respectively.

We are glad that R1,R4 state our convexity-constrained DNN framework as novel (âcreativeâ) and robust to real-world OOD images in new clinical settings.

For a specific application, we can easily assess if convex constraints would be beneficial from biological evidence (e.g., [7,28] for the anatomical structures in our case) or empirical evidence (when the DNN performs better with the constraint on OOD data, which may be so even when the object deviates slightly from convexity).

Indeed, our boundary-prediction block increases model size slightly (only 6% for UNet, and even lesser % for larger baselines) and train/test times (over baseline models that it extends). Still, our per-image test time within 30 milliseconds (Nvidia RTX2080Ti) allows real-time processing.

We didnât find 2023-24 works that were significantly different/improved in spirit from our 9 baselines.

Section 2 provides a precise mathematical description of our method; we can improve its readability and share GitHub code upon acceptance.

The paper states (Section 2.2, Paragraph 1) that we obtain the final per-pixel segmentation from our frameworkâs predicted boundary. We do so simply by determining if each pixel location lies inside/outside the boundary. With this interior segment, we compute Dice and Hausdorff-95 from the per-pixel segmentation as is standard in the literature [9,12,16,27].

The paper states (Section 2, Paragraph 2) that we automate this much-simpler task by another DNN and use it for ALL methods for a fair comparison."
https://papers.miccai.org/miccai-2024/163-Paper4034.html,"We appreciate the thoughtful analysis provided by the reviewers. Here are our responses to the concerns raised by Reviewers R1, R3, and R5:
Resolution Improvement with Convolutional Layers (R1): As outlined in Chapter 1, a batch containing all pixels from a single image undergoes processing similar to multi-channel convolutions with a 1x1 kernel size in the MLP flow. Our auxiliary flow introduces 3x3 kernel size convolutions, engaging both the target pixel and adjacent positions, which effectively manage local spatial fluctuations. This concept is expanded in Chapter 2.2. Our intuition was that encoded vectors from adjacent positions provide additional information about high frequency components. We appreciate this comment and are working on linking this intuition with mathematical explanations.
Different Methods Look the Same in Result Figures (R1, R3, R5): We acknowledge that differences in reconstruction results are not distinct to the naked eye because the original size of the sections of WSIs is still big but the figure size in the manuscript is limited. To address this, we included difference images and spectral images as alternatives in the paper. We will add zoomed images focusing on specific local spots to highlight differences between methods more clearly.
Lacking Support from Large-Scale Datasets (R3): The INR differs from traditional frameworks that use large-scale images for training and testing. In INR, âtraining setsâ comprise pixels within a single image, which, due to the immense size and pixel count of a whole slide image, effectively constitutes a large-scale dataset. Furthermore, in INR, the concept of a âtest datasetâ is ambiguous since inference involves restoring an image used in training rather than a separate test image.
More Public Datasets for Testing (R3): Our study utilized a publicly available pathology image dataset from The Cancer Genome Atlas, known for its wide use in numerous studies. We selected five random WSIs for thorough evaluation of our model. Each WSI is substantial, and testing across five independent images provides a robust assessment of reconstruction quality. However, to address potential bias from using a single data source, we agree that incorporating additional datasets could further verify our methodâs capability.
Compression (R3): Image compression, introduced as a potential benefit of INR in Chapter 1, requires accurate reconstruction of complex WSIs as a precondition. Currently, our focus is enhancing image restoration, with discussions on compression and multi-resolution representations planned for future work.
Quantitative Evaluation Metrics (R3): We used reference-based metrics like PSNR and SSIM, standard in INR studies, to demonstrate our frameworkâs performance.
Encoding and CINR Module (R3): Position encoding and CINR modules are trained for each WSI, with parameters stored accordingly.
Training and Testset Details (R5): Training loss was MSE of pixel values, with tests conducted on five WSIs as shown in Fig. 3. These details will be elaborated in the revised manuscript.
Training and Inference Processes (R5): During training, each WSI was segmented into patches with overlapping, and each batch of 100 patches was processed through the CNN and parallelly the MLP modules. During inference, pixel positions are defined by the original resolution and segmented into patches without overlapping. This detail will be expanded upon in the revised manuscript.
Comparison with Existing WSI Representation (R5): Our method can reconstruct images quickly (around 1 min. per 10k x 10k image). For compression purposes, reducing encoding and network parameters is an ongoing challenge and beyond the scope of this current study, but it remains a focus for our future research. It would indeed be beneficial to detail the current state of our method regarding loading speed and storage efficiency, especially in comparison with conventional frameworks. Thanks for highlighting this aspect."
https://papers.miccai.org/miccai-2024/164-Paper2916.html,"We would like to thank all reviewers for their positive, valuable, and detailed comments on our paper. We would like to address their constructive feedback:

Reviewer 1
Novelty and Architecture: Our main novelty lies in proposing a novel training framework emphasizing continuous representations tailored for EF regression from 2D+time echocardiograms. Thus, we used an existing architecture and a simple MLP structure. We apologize for not providing experiments on CoReEcho compatibility with different model architectures due to the space limit.
Hyperparameters: The hyperparameters used in our experiments are detailed in Section 3, including the # of epochs required for the 1st and 2nd training stages. This ensures a consistent structure in our paper, where general methodology is covered in Section 2, and hyperparameter settings are detailed in Section 3.
Results and Comparison: We will add the exact number of the CoReEcho result in EchoNet-Dynamic in Section 4.1. Additionally, we want to clarify that in Tables 4 and 5, we implemented transfer learning experiments using the EchoCoTr and CoReEcho pretrained weights and then compared their performance. We will make this point clearer in the Tables to avoid any potential confusion.

Reviewer 3
Captions for Figures and Tables: We provided concise yet sufficiently informative captions for figures and tables due to space constraints. We hope the detailed explanations in the main text can clearly explain the figures and tables.
Training Hyperparameters: We tuned our training hyperparameters to maximize the R2 score whilst considering our hardware limitation of 24 GB of GPU VRAM, which restricts the maximum batch size we could use.

Reviewer 4
As mentioned by reviewer 4, the discussion on how to integrate CoReEcho into existing clinical workflows would have been beneficial for translating this work into a real-world deployment. Yet, we could not discuss this in detail in this work due to space limitations, but we will consider discussing this in the camera-ready version.

Reviewers 3 & 4
Training and Inference Time: The training time for CoReEcho is approximately 4 hours, and the inference time is 5.95 Â± 0.14 ms per clip, demonstrating our methodâs suitability for real-time processing, which is crucial for echocardiographic analysis. We will add this information to the paper in Section 4.1.
Clinical Impact: We have demonstrated the superiority of CoReEcho over other SOTA methods. Additionally, we hypothesize that the continuity in representations may be used to detect distribution shifts during deployment and then perform domain adaptation. Exploring these could enhance real-world diagnostic processes, which we leave for future work. We will add this information in the Conclusion.

We appreciate the reviewersâ suggestions for improving clarity on some points, and we will incorporate their comments in the final camera-ready version."
https://papers.miccai.org/miccai-2024/165-Paper0746.html,"We thank all reviewers for their comments. Due to space limit, we only respond to major comments here and will address all comments in final paper.
For R#1 and R#4
âaddresses potential biases in the dataâ¦â
âpotential limitations or constraintsâ¦â
âWhen only one view is âgoodâ and all other views are perfusion featuresâ¦â
== The potential biases mainly refer to view correlation inferred from collected data. From Fig. 2, we find it difficult to determine a definitive ranking of the views in terms of their diagnostic value. Additionally, we have set a hyper-parameter, temperature factor, to adjust the sensitivity to inter-view consistency for weight calculation. By setting a relatively large temperature factor, inter-view differences could be narrowed, avoiding an excessive emphasis on the single or certain modalities and  allowing the model to receive more gradient information from various modalities. Furthermore, the optimal value of temperature factor is determined by cross-validation on the training set.== We admit the limited discussion on the extreme case where only one view is âgoodâ and all others are âbadâ views with invaluable feature. In this case, the weight of the onlyâgoodâview could be reduced, and the final prediction could be biased. Nonetheless, we can identify the abnormal condition in a simple way. Specifically, the average distance among views, theta, which functions as a threshold for credible/incredible view split, is likely larger than those of more typical scenarios. As expected, the onlyâgoodâview is distant from the remainingâbadâviews, and the distances amongâbadâviews tend not to be small with nearly random predictions. Thus, we can perform a statistical analysis of the average distance among views (theta) and detect the abnormal condition when its value exceeds a predefined limit, such as the third quartile. Furthermore, clinicians are required to re-evaluate the model predictions for such extreme cases.
âwhy the DST metric of view uncertainty was used.â
âInsufficient explanation of the methodologyâ
== Towards trustworthy prediction, uncertainty quantification in deep learning can be roughly divided into Bayesian neural networks (BNN), deep ensemble learning, and evidential deep learning. The first line of method, based on BNN, replaces deterministic parameters with distributions over weights, allowing the model to output the distribution of predictions and their uncertainty. However, explicit modeling of distributions over weights is computationally expensive and multi-view uncertainty quantification tends to be hard to converge. Ensemble-based methods integrate multiple deep models to assess the uncertainty of predictions, resulting in an increased number of trainable parameters, which is particularly problematic in multi-view learning scenarios. Hence, we resort to evidential deep learning, which introduces Dempster-Shafer Evidence Theory (DST) to directly model prediction uncertainty using beliefs from different views. Subjective logic theory in DST allows for a much more efficient way to model the prediction with Dirichlet distribution. The parameters of the Dirichlet distribution (beliefs) can be obtained by replacing the softmax operator with a non-negative activation (ReLU) and optimized by a modified cross-entropy loss. For the application of liver cancer aided diagnosis, DST metric of view uncertainty is significantly more feasible due to its substantially reduced computational complexity.âadd the experiment of using as hyper-parameters the thresholds.â
âverify the effectiveness on publicly available dataset.â
âInadequate comparison with existing state-of-the-art methodsâ¦â
== We acknowledge suggestions for further experiments and consider them for future work.
For R#1
âAbsence of detailed informationâ¦â
==To present the process of feature extraction, model construction and training, we will clarify more details in the revision and publish the project code."
https://papers.miccai.org/miccai-2024/166-Paper2321.html,"We sincerely thank the reviewers for their thorough evaluation and valuable feedback on our manuscript. Below is our response to reviewers.

Thanks for highlighting the spelling errors! We apologize for the oversight and will correct them in camera-ready version.

Robustness:
1.Direction
[R1-3, R4-3] The FSTNet is designed for single direction and needs to be retrained for other directions. However, some other SR methods demonstrate robustness across directions. We will integrate our approach with more SR methods in future work.

In contrast, the proposed Stage 2 is robust to direction. Training solely by SR images reconstructed in axial plane allows direct application to the other two directions without performance degradation. We will provide more quantitative analysis regarding directionality in future work.

2.Segmentation
[R4-2,4] Regarding the segmentation model, we used pseudo tissue labels generated by the EM algorithm (released by dHCP) to train a simple U-Net. Despite the segmentation model in this study not being of high quality, we still achieved positive results. Additionally, Stage 1 and Stage 2 are relatively independent. As shown in Table 2, using only the FAGCSR (stage 2) with the original FSTNet still yields good results. Therefore, the overall performance of our method is not sensitive to segmentation quality.

Model Performance
[R3-1,5] Limiting two digits may not sufficiently validate model effectiveness. Compared to adults, smaller brain volumes of newborns result in relatively small geometric accuracy values. Table 2 in CoTAN shows average 0.009 improvements in ASSD relative to previous method published in last two years.

We apologize for the use of âsignificantâ without formal testing and we sincerely appreciate your correction. The results of paired t-tests on the data from Table 1 and Table 2, reveal significant improvements in CD, ASSD, and HD compared to the baseline with the proposed method (all p < 0.01). We hope this addresses your concerns regarding significance.

[R1-6] Both stage 1 and stage 2 use cortical information as guidance, so the improvement in SCSR is not as significant as in the model without stage 2 (the other three methods in Table 1). Addtionalyï¼as shown in Table 2, stage 2 results in noticeable improvements in bicubic, FSTNet, and FSTNet with stage 1, with increases of 0.016, 0.011, and 0.005 in ASSD, respectively. As SR image quality improves, the diminishing performance enhancement from stage 2 appears reasonable. This indicates proposed method effectively aligns features without underfitting or feature forgetting. In our future work, we will provide visualizations of features using PCA or TSNE to show the alignment between SR image and HR image features.

Implementation Details: 
[R3-2,3] In order to ensure consistent data distribution, we equitably partitioned the preterm infants, with 20.6%/23.2%/19.9% in train/val/test sets. 
[R1-1,2,4,5] The number of vertices for both input template and predicted surface is 141,471. All MRI images and cortical surfaces were aligned to MNI152-1mm spacing. Pseudo-ground truth surfaces were generated using the official dHCP pipeline. We computed CD, ASSD, and HD using 100,000 sampled points from both predicted and original surfaces.
[R1-6] The error map indicates ASSD. To address both concerns of significance and to prevent a few large values from overshadowing the majority of significant errors, we clipped values greater than 1 to 1. We sampled 141,471 points to calculate ASSD as the error map. Regrettably, we misalignment the indices of predicted surface and error vector, resulting in a lack of correspondence between artifacts and the error map in Fig. 2. We assure that we will correct this issue in the camera-ready paper.

Code Releaseï¼
[R3-4] We will release full code once the paper is accepted. Additionally, we will provide FSTNet checkpoints of three directions, along with stage 2 checkpoints trained on the axial plane."
https://papers.miccai.org/miccai-2024/167-Paper1495.html,"(R1)âThe model comparisons with state-of-the-art are limitedâ (R3) âdifficulty in comparing results with existing papersâ:
When designing our study, we reviewed papers that used the BrixIA dataset referenced in this study (see BrixIA in the âpaper with codeâ). Most existing studies did not use BrixIA scores as we did and focused on other research areas. Only BS-Net and Slika et al. (2024) used BrixIA scores, but Slika et al. had a different evaluation methodology, making direct comparisons difficult. They predicts a global score rather than local scores. During this process, if an image has both positive and negative errors in different regions, the âglobal score,â which is the sum of the local scores, will experience error cancellation. Consequently, the global score MAE will tend to be lower than the sum of the local MAEs and closer to the mean of the global score distribution. In addition, the global scores in the BrixIA dataset are close to normally distributed, so they are likely to be highly correlated with the global score predicted closer to the mean. Therefore, an approach that uses the average of the local MAEs may be more reliable. Unfortunately, Slikaâs recently published study did not directly compare its performance to BS-NET. However, in both papers, when tested on the Cohen COVID-19 dataset, BS-NET had a higher Pearson correlation than Slikaâs method, so we cannot conclude that Slikaâs method is more accurate than BS-NET. In that sense, BS-NET is best suited for comparing performance with local MAEs. We also chose CXR-CLIP (MICCAI 2023) for comparison with a more recent study.
(R3)âUnclear technical contributions: 1) integrating lung region information and 2) using COVID-19 chest X-ray data for pneumonia severity predictionâ:
1) There are studies on segmentation labels (Slika et al.) and localization (Frid-Adar et al. 2021), but none have applied our proposed lung region information to severity prediction. Without our proposed integration, localization information is not learned from datasets with different label positions during transfer learning, resulting in an inability to distinguish features from visually similar but different labels. Although there is still much room for improvement, the integration of lung region information, consisting of spatial normalization and PAFE, has demonstrated clear performance improvements in local severity assessment.
2) Past research focused on single diseases, either COVID-19 or pneumonia, using multiple datasets only for comparison between multi-center data or data augmentation, not cross-performance between COVID-19 and pneumonia. We demonstrated generalization by cross-validating COVID-19 to pneumonia datasets using multi-region score extraction and shared MLP methods. Our model maintained strong performance on different datasets with altered label positions using linear probing.
(R1)âunclear information about STNâ:
STN training uses Affine Augmentation with the CXR segmentation map as input. The target image is the CXR lung segmentation, and the moving image is the augmented version. The transformation matrix is calculated, and the Affine transform is applied to the CXR image.
(R4)âBenefits despite lower performance?â:
The BrixIA dataset has 4 severity classes (0-3) locally, with random guessing accuracy at 25%. Our method achieves 67.1% accuracy, significantly outperforming random guessing. Furthermore, BS-NET reports an MAE of 0.528 for radiologists, while our methodâs MAE is 0.35, indicating higher disagreement among doctors. Therefore, we believe our method will provide valuable information for doctorsâ judgment.
(R1)âStatistical testsâ:
At submission, we had repeated the experiment three times, confirming consistent results. ResNet18 and ResNet34 showed no overfitting and significant performance gains of up to 10% over BS-NET. No statistical tests were performed as gains and losses were clear. We will add details of the repeated experiments to the updated paper"
https://papers.miccai.org/miccai-2024/168-Paper3750.html,"We thank the reviewers for their valuable comments and their recognition of the strengths in our work:

Here, we address the main concerns due to chars constraints. We will incorporate all other suggestions in the revision and publish our code.

Is the choice of datasets representative? (R4)
No. To verify our method, we implemented extensive experiments on 5 publicly accessible datasets covering multi-organ and multi-center, which was appreciated by Reviewer R6.

The experimental results are not well presented and seems not very ideal (R4).
We would like to point out that the other two reviewers (R5/R6) think our paper is well-written and comprehensible. We would also like to emphasize that we are focusing on challenging zero-shot tasks. Compared to the baselines, our method shows promising improvements, which are appreciated by the other reviewers. We hope the reviewers realize that our task is zero-shot without supervision labels.

Table 3 should include more case analyses, as there are more than two datasets used in your experiments (R4).
We only have two tables in the paper. We believe the reviewer should refer to Figure 3. Please refer to the appendix for more cases.

Statistical analysis and more evaluation metrics. (R4)
MICCAI rules prohibit new experiments but we will add discussion on other metrics.

Limited Novelty. (R5)
We differentiate our method from existing methods of CP-ViT and CP-CNN in two key aspects. First, our method aims to achieve better feature alignment, while the other two aim to make networks sparse. Second, our CP guided neural network is added to CLIP as an auxiliary network, while the other two involve redesigns of the ViT and CNN.

Overstatements/ Comparison with CLIP+CP network without shared parameters. (R5)
The CP component helps map image and text features to a unified latent space, facilitating feature alignment. This component can be extended to other applications, and we will revise the statement in the revision.

There is a slight misunderstanding by the reviewer about the paper. Our key innovation lies in designing a single CP network that maps both image and text features into a unified latent space. Having two CP networks, each processing image and text features separately would contradict our design and purpose of mapping both image and text features into the same latent space.

More comparisons/ underperforms MedCLIP in ChestXray. (R5)
MICCAI rules prohibit conducting new experiments, but we will address this aspect in the discussion. Unlike MedCLIP, which utilizes multiple prompts in their experiments, we consistently employ a single text prompt for all experiments. This distinction contributes to the observed performance differences.

Experimental implementation details and other evaluation metrics (R6).
We will add more details and publish our code. Other evaluation metrics will be added to the discussion."
https://papers.miccai.org/miccai-2024/169-Paper0339.html,"Thanks to reviewers for providing insightful comments toward improving our paper. We will make a point-by-point response to all comments.
To R1:
About the novelty: The mentioned strategy with different supervision signals is not listed as our contribution. Our primary contribution lies in the Criss-cross Injection Strategy(CIS) with BEC and CEC, enabling the diffusion model to utilize multi-level features of the boundary and core areas. In terms of the pre-training method, we apologize for less clear description of it. Different from other pre-training methods, our method uses the results from the second stage as weighted coefficients for the GP loss, making the model focus on the areas that are more challenging in the second stage. This interactive pre-training not only enhances the fidelity of the generated images but also narrows the domain gap between the image conditioning network and the diffusion backbone. We will add more details about our GP.
Choose P* over P: There seems to be a misunderstanding. We choose the structure of P over P.P is our baseline by only injecting prostate features. Pis stronger because it injects more features, such as prostate, core, and boundary features generated from a simple FPN. P* validates the effectiveness of boundary and core features injection. Therefore, we design more powerful BEC and CEC rather than FPN ((5) in Tab.2), which on average reduces ASD by 16%.
The reversed structure of BEC and CEC: BEC and CEC both remain a top-down structure. BEC stacks more shallow convolutions, focusing on learning edge texture features. Conversely, CEC focuses on learning semantic features by using deeper convolutions.
To R3:
More validation and evaluation: Thank you for your valuable comments, this work is supported by a local hospital in which a set of prostate images for validation is being under ethical assessment. We also expect to extend our method for practical uses. The FID of our GP in four datasets is 4.25, 6.73, 5.21 and 8.63, respectively. As per your advice, we will add these metrics in the final version. 
About figures: As per your advice, we will provide more details for Fig. 1 and 2 and revise Fig.1 in more clear way.
Future work and limitations: As per your advice, we will shift the future work and limitations from the supplementary to the conclusion in the final version.
About the minor issues: We will carefully correct them.
To R4:
Thank you for your insightful comments. In medical diagnosis, aggregating interpretations from multiple experts reduces the frequency of misdiagnosis. Therefore, we propose a multi-guided diffusion model including the guidance of boundary, core and generative pre-training to simulate multi-expert predictions. Our model not only enhances multi-level feature utilization but also narrows the domain gap between the image conditioning network and the diffusion backbone.
To R5:
The motivation of GP: Please refer to the response (novelty) in R1. 
The motivation of BEC and CEC: Most diffusion-based segmentation methods overlook boundary information and treat all regions equally. Directly predicting boundary maps by hard labels ignores the context of surrounding pixels. In our method, we decouple the original label into soft boundary labels and core labels. This allows BEC and CEC to learn contextual coherence between boundary and core regions, helping the diffusion model better capture the shape and location of target objects.
Injection into the diffusion model: The outputs of independent BEC and CEC are deterministic, tending to be overly confident even on wrong predictions. This does not fit the uncertainty estimates required by clinical diagnosis. Feeding extracted feature is to incorporate multi-level feature utilization into diffusion model. 
Unfair experiment setting: We would like to clarify that all diffusion-based methods are conducted in 25 ensemble runs as described in Section 3.2.
About the minor issues: We will correct them as per your advice."
https://papers.miccai.org/miccai-2024/170-Paper0321.html,"We thank the AC and reviewers for their time and valuable feedback. We would like to clarify several questions raised by the reviewers.

The design of PCR is counterintuitive according to some self-supervised literature (R1).
(1) Consistency with intuition. Intuitively, in semi-supervised segmentation tasks, the key is to generate reliable predictions for unlabeled data. However, due to SAMâs high sensitivity to prompt positions, prompts with similar semantic contexts but different positions may produce predictions with different qualities and reliabilities as indicated by [1,2]. Without ground truth for the unlabeled inputs, the prompts generated from unprompted outputs can be inherently unreliable and noisy. The prompted output is thus likely to be unreliable as well, failing to provide meaningful guidance. To address this issue, we design a novel PCR strategy to reduce SAMâs sensitivity to prompt positions and enhance the invariance of the output, thus ensuring more reliable and stable results.
(2) Consistency with the self-supervised literature. As stated in the reference mentioned by the reviewer, representation learning benefits from a lower variance in target encodings and a higher variance in source encodings. Our proposed PCR method is similar to providing a lower variance from the target aspect since we aim to provide a more stable and reliable learning target, which is expected to be able to improve performance.
In response to the constructive suggestions of R1, we will improve the writing of related text such as the introduction section, section 2.3, and the caption of Fig. 1 in our final version to clarify the motivation of this design and how it works. Additionally, we will provide detailed elaboration on our ablation studies to enhance the comprehension of the effectiveness of each component in our proposed method.
[1] Samaug: Point prompt augmentation for segment anything model. arXivâ23
[2] Desam: Decoupling segment anything model for generalizable medical image segmentation. arXivâ23

Effectiveness on medical-specific foundation models such as SAM-Med2D (R3).
Our proposed dual-branch strategy can be easily extended to medical-specific foundation models like SAM-Med2D without any modifications. With more domain knowledge, we anticipate an enhanced performance in semi-supervised segmentation.
Also, we notice that the generalization performance of SAM-Med2D remains limited as indicated in its original paper, especially when using a single-point prompt. Our proposed method has the potential to serve as a label-efficient way to assist SAM-Med2D in adapting well to a new dataset with minimal labeled data and a plentiful supply of easily obtainable unlabeled data.
Moreover, our proposed method enables automatic segmentation and eliminates the need for expert prompts during inference, which cannot be achieved using SAM-Med2D. In the meanwhile, the promptable nature of these models is fully utilized in our training process.
We will include this discussion in our final version.

Clarification of the supervised part (R3).
Thanks for the valuable suggestion. In Fig. 1, we only illustrate the use of unlabeled data. The use of labeled images is depicted in Equ. 3, where we use the annotations to supervise all the prompted outputs and unprompted outputs for the labeled data. Notably, our proposed cross-prompting and PCR strategies are not applied to the labeled data. We will improve Fig. 1 or its caption in our final version.

Open access to the source code (R4).
We will release the source code upon acceptance as we have mentioned at the end of the abstract in our original manuscript."
https://papers.miccai.org/miccai-2024/171-Paper0714.html,"We appreciated the favorable comments on the novelty of our method (Reviewer#1, Reviewer#2), and high-quality experiments (Reviewer#1, Reviewer#2, Reviewer#3). Below, we clarify the main issues raised by reviewers.
Reviewer#1
Q1:  Lack of a summary.
Thanks for your suggestion. We will add a summary to the introduction section.
Q2: The MDN Training lacks training details and loss functions.
We use the same settings (input size=256, learning rate=1e-4, and batch size=12) for MRM, MDN, and C-UNet training, with L2 loss for MDN.
Q3: Figure 4 is not clear.
We will re-plot this figure.
Q4: The sampling number is not mentioned in methodology.
Like conventional diffusion models, MDN predicts the clear target distribution from random noise through iterative sampling. While bigger sampling number increases accuracy, they also raise computational costs. We found that the sampling number exceeds 30, improvement is marginal, so we set the sampling number to 30 to balance performance and speed.
Q5: Is CDM a one-to-one or a many-to-many image generation method?
Our CDM is a many-to-many image generation method. 
Q6: About writing.
Thanks for your constructive comments. We will carefully revise all related sections.

Reviewer#3
Q1: Current metrics may have problems catching loss of details (image fidelity).
Following the TMI22 paper âResViT: Residual Vision Transformers for Multimodal Medical Image Synthesis,â we use PSNR, SSIM, and MAE for comprehensive assessment. Based on your suggestion, we introduce metrics like LPIPS for more effective evaluation. For example, on BraTS2023 dataset, our method achieves (T1c: 0.054;T2f: 0.055), Diffusion attains: (T1c: 0.079;T2f: 0.066). A lower LPIPS represents better generative quality.
Q2: The âAvgerage scoresâ are not necessary. 
Following your suggestion, we will remove the average scores.

Reviewer#4
Q1: Proving the role of the diffusion model.
Thanks for your valuable guidance. Following your suggestions, we have ablated the role of the Diffusion model. The results on the BraTS2023 dataset are: (T1c: 32.02, 0.928,0.0106;T2f: 29.38,0.922,0.0152), which are lower than our SOTA. We think the reason is the existence of a domain gap between training and testing target images, and the diffusion model can adaptively learn the representation closer to the testing target images. 
Q2&Q6: Logical inconsistencies: 
We will meticulously review and revise all the highlighted sentences.
Q3: About random mask in MRM. 
The motivation of random mask is that it enhances the modelâs semantic understanding for the image by reconstructing the original image from its masked version. According to your suggestion, we remove the random mask operator in MRM and train corresponding MDN and C-UNet. The results on the BraTS2023 dataset are: (T1c: 32.52, 0.940,0.0104;T2f: 30.22,0.926,0.0142), demonstrating random mask can improve the representation learning capability. We also cited reference.
Q4: Including variance.
We have saved all the predictions, which allows us to easily derive the variance data and include it in the manuscript. Due to page limitations, we report a subset of the results (three metrics for two methods). For the BraTS2023 dataset, our method achieves: (T1c: 33.08Â±1.69, 0.948Â±0.008, 0.0098Â±0.002;T2f: 30.76Â±1.21, 0.934Â±0.014, 0.0136Â±0.002). Diffusion attains: (T1c: 31.98Â±1.75, 0.930Â±0.01, 0.0109Â±0.004;T2f: 29.22Â±1.27, 0.921Â±0.018, 0.0155Â±0.005). These outcomes demonstrate the stable performance of our method across each generation process.
Q5: Sampling number in training and testing phases.
For MDN training, the sampling number is selected randomly from the range [0, T] to add varying noise levels to the latent variables of the target images. Once the MDN is trained, it is frozen and applied to the C-UNet training and inference processes. Hence, we maintain a consistent sampling number during C-UNet training and inference. According to your suggestion, we will complement more settings for the sampling number."
https://papers.miccai.org/miccai-2024/172-Paper0308.html,N/A
https://papers.miccai.org/miccai-2024/173-Paper1228.html,"Thanks for the rebuttal invitation. We itemize our responses to significant points as follows:
(1) The role of combining Transformer and CGI and the computational cost (R1, R3, R4): The denoising UNet architecture combines CNN and Transformer to enable the DPM to capture global context information through bidirectional connection units. Transformer Bridge uses a multi-head channel attention mechanism to fuse different levels of features and capture long dependencies across channels. The decoderâs CGI module focuses on essential nodes and edges through projection and reprojection operations, capturing complex correlations between pixels and their boundary details. Although CGI is widely used in medical segmentation, its application in 3D CT images of lung nodules remains scarce. CGI employed for nodules cannot be directly compared to the segmentation of other lesions due to the heterogeneity among them. Diff-UNet exploits the sparsity of the graph structure and convolutional layers to reduce computational complexity.
(2) Description of Fig. 3 (R1)ï¼ Fig. 3 depicts cases of 3D visualization of surface distances between the segmented surface and the ground truth. The segmentation result is closer to the ground truth when the green area is larger.
(3) Ablation experiment analysis (R1, R3, R4): To ensure a fair comparison, we build a baseline model consisting of DPM, ResNet encoder, Bridge Transformer, and decoder. Then, we introduce CGI, FFM, and MCM modules into the baseline model and study their impact on segmentation performance. As seen from the ablation experiment results in Table 3, Diff-UNet benefits from DPMâs ability to improve image smoothness and reduce noise through Markov chains and Transformerâs ability to capture global contextual features. Therefore, Diff-UNet can produce accurate segmentation results even in low-contrast or blurry lung nodule areas.
(4) Dataset information (R1, R3, R4): The LUN16 dataset was collected from the largest public reference database of lung nodules: LIDC-IDRI. The database consists of clinical-dose and low-dose CT scans collected at seven participating academic institutions. This article discards scans with slice thickness greater than 3 mm, resulting in a final list of 888 scans. Our experiment also used CT images of lung nodules collected in the hospital from 2012 to 2019. The self-collected dataset consists of 1299 samples with a resolution of 1 mm. The datasets were ethically reviewed, and informed consent was obtained from the patients. Experienced radiologists used ITK-SNAP software for annotation based on surgical pathology. Compared with LUNA16, the nodules in the self-collected dataset show obvious texture contrast compared to the surrounding tissues. Unified annotation protocols and standards used in self-collected dataset can reduce the subjectivity of annotation and help models to segment nodules more accurately.
(5) Experiment details (R1, R3, R4): We implement Diff-UNet using PyTorch and calculate the standard deviation using 10-fold cross-validation, evaluating its consistency across different data partitions. The Diff-UNet and comparison models use the Adam optimizer with an initial learning rate of 0.00001 and a batch size of 2. Since the self-collected dataset has unique imaging characteristics, more comparative methods are required to verify its effectiveness.
(6) Medical implications and further directions of this work (R4): Considering that the structure of nodules in CT images is irregular and similar to that of the surrounding environment, manual inspection is time-consuming and relies on radiologistsâ experience. This paper proposes a segmentation framework, namely Diff-UNet. This framework utilizes CGI and DPM to weaken CT image noise and capture boundary features to segment nodules. Further directions will explore semi-supervised and unsupervised learning to reduce reliance on large labeled datasets, making models more practical in data scarce environments."
https://papers.miccai.org/miccai-2024/174-Paper2317.html,"We would thank the reviewers for the valuable feedback

Disentangling network {R3,R5} 
We appreciate R3 for acknowledging our disentangling network. To further clarify, we would add that spatial transcriptomics (ST) and WSI have shared and unique genetic and morphological information. WSI provides unique features of cellular patterns in high resolution, while spot-ST bears unique features of expression patterns that form the basis of ST super-resolution. Meanwhile, the shared features of tissue structure across ST maps and WSI can bridge the translation from image to spatial expression. Hence, disentangling shared and unique features could better facilitate ST enhancement. Specifically, we devise a disentangling loss to derive the shared and unique features of each modality, where the disparity among shared features is minimized while that among the unique features is maximized

Cell-to-tissue feature extraction & cell patching {R4,R5} 
To generate cellular level high-resolution (HR) ST maps, it is central to consider cellular heterogeneity, i.e., cells may contribute distinctly to expression profiles. Hence, we perform cell patching to extract cellular features. Specifically, as described in Section 2, we perform patching in the average scale (160Âµm region at 20X) demonstrating cellular organization. We then use cross-attention to identify key patches for certain genes. In contrast, a simple pooling averages the gene expression attribution of different cell patches, which cannot generate accurate ST maps at the cellular level

Dataset and evaluation metrics {R4.R5} 
For Xenium dataset, we have publicly available HR ST [9] as ground truth, described in Section 3.1. HR ST is unavailable in two external validation sets, so we follow [10] for model evaluation, where ST enhancement should retain the original spot level pattern while increasing resolution[10]. Specifically, the enhanced ST maps are first downsampled to LR, and then used to calculate metrics with paired LR ST. We will further clarify the dataset preparation and evaluation metrics

Downstream task validation (R4) 
The HR ST at the cellular resolution has inherent advantages over spot ST in downstream analysis, e.g., localizing cell types, studying cell-cell communications [3,10]. We appreciate the reviewerâs insightful suggestion to conduct downstream exemplar tasks for model validation. Due to the paper limit, we were unable to include these results, and future work will warrant downstream validations at the cellular level

Curriculum learning (R5) 
The complexity of spatial gene expression patterns varies with the cellular complexity and heterogeneity (Jiaren Lin,2023), rendering the unstable model training across different patches. Therefore, we first estimate the cell complexity of individual patches using Shannon entropy [1], before implementing Curriculum Learning on different patches, which can learn samples with varied difficulty, enabling stable training for better results

Comparisons (R5) 
We would thank the reviewer for providing the two representative methods. Despite their effectiveness in ST enhancement, both xFuse and iStar are unable to directly utilize the HR ST maps in training, and they only use the downsampled LR ST as weak supervision, thus less capable of reconstructing expression details. Besides, xFuse may suffer from low test speed (~1 day/WSI). We will add detailed quantitative comparisons of these methods in future version

Gene range(R5) 
We observed that highly variable genes (e.g. GATA3 for breast cancer) are easier to predict, consistent with [10]. We thank the reviewer for this comment and will investigate the gene range in future work

CIGC(R5)The CIGC-Graph network is proposed to model the correlation of the features of multiple genes. Fig1 and Supple Fig1 show its information flow and structure. We will follow the advice to better describe the model in the future version"
https://papers.miccai.org/miccai-2024/175-Paper1010.html,"Thanks for the valuable comments and suggestions. Below we respond to the comments.
Code: As MICCAI requirements, if our paper is accepted, we will publish the code for UKB data processing, model training, and inference.
1.To reviewer3&4&5 : Clarifications about ECG Phase 2 training and ViT model design:
We sincerely apologize for the potential misunderstanding caused by a typo in Equation 4 during the second stage of training. To clarify, the correct Equation 4 should replace fc and xc with fe and xe. This loss function is specifically for training the ECG encoder. In the second stage, training involves two types of loss: the contrastive loss between ECG and CMR, and the downstream task prediction loss for the ECG encoder, which includes four cardiac diseases and CMR-related cardiac metrics. Thus, the ECG encoder(ViT) is updated under both losses, while the CMR encoder(Swin) is updated solely under the contrastive loss. And only the trained ECG encoder is used in the inference phase.
Regarding the ECG ViT model, a batch of ECG data is defined as [b, 12, 5000], where â12â represents the 12 leads, and â5000â represents 10 seconds of data at 500 Hz. We reshape the data to [b, 1, 12, 5000], treating it as a 1-channel image with height 12 and width 5000. 
For the ViT model, we set the patch size to (1, 100), dividing the data into 600 patches, and then apply the standard ViT approach with standard positional embedding. Our results, reported in Figure 2b, show that smaller patches yield better performance. This method enables cross-lead and cross-length attention and allows for the automatic alignment of ECG and CMR patches using contrastive loss. If allowed, we will include an analysis of ECG and CMR patch correlations in the revised version for better interpretability. Additionally, Fig 1âs depiction of ECG patches might be misleading, as the actual data is a narrow, long strip image (12 x 5000), unsuitable for display.
2.To reviewer3: Results using ECGs only and method that fuses ECG and CMR: Table 1 Results with W/O CL in the ablation experiments are the supervised learning results using ECGs only, and W/ or W/O represents with or without self-supervised pre-training of ECGs. While we did not use a method that fuses ECG and CMR as a control, we report the results of supervised learning of diseases and metrics using CMR as our upper bound and approximation target in the last row of Table 1. This may provide some reference value.
3.To reviewer3&4: Regarding the issue of single evaluation metrics and the lack of statistical significance testing, we will include these in the next version if allowed. We are encouraged by the positive preliminary results.
4.To reviewer3: Individual results for each trait: In Figure 2c, we reported the regression coefficients for 10 indicators, and the full set of 82 regression coefficients can be found in the supplementary materials. Although intermediate type classification and aggregation were not performed, we believe that the individual regression coefficients can provide some helpful insights.
5.To reviewer4&5: Interpretability: Future extensions will consider using ECG to generate paired CMR images, enhancing interpretability. We have already used well-established pre-trained models for segmenting and determine the cardiac region of CMR in our data preprocessing. For interpretability, we used 10-second ECGs and 50-frame CMRs, covering both systole and diastole with temporal correlations. If permitted by MICCAI, we will report UMAP results classified by disease or cardiac metrics in the revised version.
6.To reivewer4: Research on ECG phenotypes: It is limited due to the lack of ECG phenotypic information in the UKB dataset, but we will explore this further with additional datasets in future studies.
7.To reivewer5: Positive samples in the UKB dataset are rare: As mentioned, we combine all positive cases with a random subset of negative samples to form a smaller set for iterative training."
https://papers.miccai.org/miccai-2024/176-Paper2986.html,"Thanks for all valuable feedback and concerns regarding our submitted paper.

[R3 - UDA-related] 
We acknowledge that our proposed method requires paired and registered CTPA and NCT image data for each subject during the training phase. However, it is important to note that our target is to transfer the knowledge from the CTPA network to the NCT network, which is a different objective compared to unsupervised domain adaptation (UDA) techniques. UDA aims to adapt a model trained on a source domain to perform well on a target domain without requiring labeled data in the target domain. In contrast, our approach leverages the available paired data to transfer knowledge from the CTPA domain to the NCT domain, enabling the NCT network to benefit from the rich information present in the CTPA scans. While UDA techniques can be useful in scenarios where paired data is not available, our method specifically targets the scenario where such paired data exists and aims to exploit it effectively for improved performance on NCT scans.

[R5 - additional context for results and discussion] 
We appreciate the reviewerâs suggestion to provide additional context and discussion regarding the significance of our results. We agree that the superior performance of our proposed method can be attributed to its ability to leverage information from CTPA scans during training, which the baselines do not have access to. We will expand the discussion section to highlight this point and emphasize the significance of our method in utilizing this additional information during training while not requiring it during inference.

Regarding the suggestion to provide an additional set of results demonstrating the performance of the baselines when taking CTPA as input, we acknowledge that such results would further strengthen the paper. However, due to the page limitations, we were unable to include these additional experiments. Nevertheless, we believe that the current results sufficiently demonstrate the effectiveness of our proposed method in leveraging dual-phase knowledge for improved performance on NCT scans.

[R5 - the discussion of the IFA module is quite hard to follow] 
We appreciate the reviewerâs feedback on the clarity of certain portions of the paper, particularly the discussion of the IFA module. We will revisit the explanation of the IFA module and strive to improve its clarity and readability. We will provide a more intuitive and step-by-step explanation of how the IFA module captures pair-wise spatial feature similarities and enhances spatial contiguity and mutual learning between the dual-pathway network. We will also consider including additional illustrations or diagrams to aid in the understanding of the IFA module. The code will be released asap.

[R5 - The FUMPE dataset] 
We apologize for any confusion regarding the FUMPE dataset results. Due to space limitations, we focused on the segmentation task when evaluating our method on the FUMPE dataset. The primary reason for this is to demonstrate that our proposed framework is robust and effective, even when applied to a single data stream (CTPA only). By comparing the performance of our segmentation framework with public methods on the FUMPE dataset, we aim to show that the advancements brought by CPMN are not solely due to the chosen framework but rather the effectiveness of the CPMN itself. The purpose of including the FUMPE dataset results is to provide additional validation of our method on a publicly available dataset and to showcase its generalizability. We will clarify this rationale in the revised version of the paper to better justify the inclusion of the FUMPE dataset results.

[R6] 
Thanks for your suggestions, we will do more analysis in next version.

[All reviewers]
We hope that the responses address the concerns raised by the reviewers and provide the necessary clarifications. We will incorporate the suggestions to improve the clarity and strengthen the paper. Thank you once again."
https://papers.miccai.org/miccai-2024/177-Paper0441.html,"We appreciate the valuable feedback from the reviewers. We thank R1 & R3 for recommending acceptance and hope that our responses to R4 & R5 will persuade them to upgrade their scores.

RESPONSE TO R1:

When tensors with different dimensions are added or concatenated, broadcasting is performed accordingly; e.g., if two tensors with shape 1 x h x w x 1 and l x h x w x 1 are concatenated along the last dimension, then the first tensor would be broadcast to l x h x w x 1 before the concatenation.

Including the zonal mask indeed provides information regarding the location of the prostate, but provides no information directly associated with cancer lesions. Actually, several prior works have proposed inclusion of the zonal mask in the input or use of a multi-step network to learn information about the zonal masks, such that the cancer prediction is constrained to within the prostate [9] and [a],[d] below that we will cite.

The uncertainty can be computed using Eq (3), where u is the uncertainty value. It demonstrates that increased evidence leads to decreased uncertainty, and no evidence indicates complete uncertainty; i.e., u=1.

RESPONSE TO R3 & R4:

Thank you for your questions. We agree that we should report thorough experiments related to the hyperparameters. Actually, the performance is not very sensitive to beta and gamma. Gamma in a +/- 0.5 range does not substantially change the detection performance as sensitivity remains within 0.01. This does not change the loss much for easily-classified pixels and the loss would focus on the hard-to-classify pixels. The beta hyperparameter emphasizes the loss at true lesion pixels and its value is from the weighting in focal loss provided in reference [a] below that we will cite.

RESPONSE TO R5:

Per cited prior work [4],[9], and references [a],[b],[c] below that we will cite, using segmentation models to perform PCa detection is common practice, with the local maxima in the output probability map being considered the cancer detection points. The true positive PCa detection point is defined when it is within 5 mm of any PCa ground truth. This is done to account for a potential mismatch between the real lesion and the label due to labeling errors. In other words, instead of lesion segmentation, we use a segmentation model to perform PCa detection. Thus, evaluation of the segmentation performance is unnecessary. We will revise the manuscript to avoid this confusion.

Agreed that 2.5D segmentation is not new. In particular, [15] formalized the 2.5D segmentation approach and their method outperforms other SOTA results. We have discussed this paper and compared our method with theirs.

==

[a] H. Zheng et al. âAtPCa-Net: anatomical-aware prostate cancer detection network on multi-parametric MRI.â Scientific Reports 14.1 (2024): 5740.

[b] R. Cao et al. âProstate cancer detection and segmentation in multi-parametric MRI via CNN and conditional random field.â IEEE Int. Symp. Biomedical Imaging (ISBI), 2019.

[c] A. Saha et al. âEnd-to-end prostate cancer detection in bpMRI via 3D CNNs: effects of attention mechanisms, clinical priori and decoupled false positive reduction.â Medical Image Analysis 73 (2021): 102155.

[d] C. De Vente et al. âDeep learning regression for prostate cancer detection and grading in bi-parametric MRI.â IEEE Trans. Biomedical Engineering 68.2 (2020): 374-383."
https://papers.miccai.org/miccai-2024/178-Paper0532.html,"We thank R1, R3, and R4 for the insightful feedback. All comments and the suggested reference by R4 will be integrated into the final draft.

Reviewer #1

Reviewer #3

Reviewer #4"
https://papers.miccai.org/miccai-2024/179-Paper1240.html,"We thank the reviewers for appreciating the structure and clarity of the paper, and for their constructive feedback. The reviewers identified two common concerns, which are a lack of algorithmic / method novelty (R3,4) and a lack of accuracy improvement (R1,3,4). Before detailing our view on the particular concerns, we must highlight that the major novelty of our CAI work is the conception, implementation and validation of a full pipeline encompassing planning and navigation, to improve the workflow of a cumbersome and complex procedure in terms of safety and accessibility. This is a notable difference to typical deep learning MICCAI works, where accuracy and algorithmic novelty are considered to be the most important values. In CAI, safety and accessibility are as important, and they go hand in hand: A simpler procedure enabled by patient-side planning and decision-making improves safety by reducing overall OR/anesthesia time and exchanges between OR and control room. Typically, surgeons would scrub many times per procedure for a single insertion relying on CT-guidance. Depending on the setting, the operators might not even be able to leave the OR, requiring the control CT to be taken with them inside the room, despite protection, exposing themselves to radiation. Cryotrack enables safe, automated patient-side planning that makes intermediate control CTs unnecessary or optional, lifting a barrier to safe and successful treatment. Hence, the emphasis of our work is not on improving accuracy only, but on improving safety and accessibility.
Though most parts of the proposed pipeline build upon existing methods and the algorithmic novelty is limited, Cryotrack innovates on automatic intra-operative risk avoidance with visual feedback, together with our reproducible clinical validation in phantoms. We are not aware of any other work that achieves this full integration.

Experimental Protocol (R3): R3 points out that the experimental setup is incomplete, as Cryotrack is merely validated on a phantom with virtual risks, rather than animals or complex phantoms, and there is no direct comparison of novices planning under CT. In CAI, a phantom experiment is usually the first step before attempting studies with animal or human subjects, validating the systems in animals without formal pre-validation would violate the 3R principles of animal experimentation, which is unethical at this state.
Adding larger physical risk structures to the phantom would be interesting, but the expected benefit for planning is limited. This would have no impact on the planning and would only increase the complexity of our setup, limiting the reproducibility of our validation.
Novice experiments are not done under CT as novices lack the proper medical training to interpret CT images for pre-op planning, and translate the plan to the phantom. Our intention behind the novice experiment is to show how our setup increases accessibility and ease of use, while staying at a safe distance to risk structures.

5 mm Accuracy Margin (R3): R3 noted that [2] does not explicitly state a 5 mm accuracy margin for cryoablation. We derived the 5 mm margin from [2] which recommends to ablate the tumor plus at least 5 mm around the tumor, i.e. within this margin, the ablation is successful. This is a simplification, though it is nearly impossible to find a well-founded upper bound for positional error in the literature.

Baseline Comparison (R4): It was noted that a comparison to other EM guidance systems is not given. As we need to plan with long booking times for the OR, we need to carefully gauge the relevance of evaluations, and a comparison to a system without intra-op planning would not add much value to our evaluation. Further, EM-guidance tools on the market are scarce, and research products are typically closed-source. We are aware of only one commercial solution (IMACTIS) which provides EM+CT guidance, but no intra-op planning."
https://papers.miccai.org/miccai-2024/180-Paper0747.html,N/A
https://papers.miccai.org/miccai-2024/181-Paper2185.html,"Dear Reviewers, we appreciate your constructive feedback. Please find our clarifications below:

[R1, R3, R5] Reproducibility, codebase, dataset: We understand the concerns regarding our dataset and reproducibility. From a hospitalâs electronic health records, we exported 3D chest CT volumes and corresponding radiology reports written by radiologists in daily clinical practice, receiving ethical approval for both the CT2Rep project and the open-sourcing of our training data. To maintain anonymity, we did not include specific dataset details in our submission. If accepted, we will provide the ethical approval code, data details, an open-source link to the dataset, and our codebase.

[R1] Comparison with LLAVA: We acknowledge the constructive feedback on lack of comparison with multimodal methods like LLAVA. But, LLaVA-Med uses a VQA dataset for training, and generating such a dataset is beyond our scope. Also, LLaVA employs a pretrained CLIP with frozen weights, whereas training a 3D CLIP on our dataset was not finalized. Also, our 3D tokenization results in 4096 tokens, compared to LLaVAâs 50 tokens, highlighting methodological disparities. So, integrating a 3D encoder into LLaVA introduces complexities. But, we recognize the value of such a work and are working on adapting LLaVA for 3D medical images.

[R1] F1 Score: We agree F1 scores in our paper are not yet high enough for clinical application. However, it is important that this is the first paper addressing report generation for 3D medical images. Our primary goal was to establish a foundational baseline.

[R3] Use of causal transformer: The causal transformer models attention between slices, capturing inter-slice dependencies, which is clinically significant as certain pathologies span varying axial locations. This captures essential 3D structural information. We removed the decoder and do not use auto-regressive or MaskGIT training, which are beneficial for generation tasks.

[R3] Comparison with 2D methods: Our radiology reports are written for whole 3D volumes, with pathologies residing on different slices. This variability makes random layer selection and 2D report generation ineffective.

[R3] Evaluation with LLMs: We appreciate the insightful suggestion. Inspired by this idea, we evaluated similarities between generated and groundtruth reports using ChatGPT-4 Turbo. We will include a comprehensive analysis in supplementary.

[R5] Name similarity: We became aware of this after submission. However, it significantly differs from our work as it is not a direct 3D CT-to-report method. It extracts 113 semantic features using Radiomics and SISN and employs a neural network to fill predefined templates. This approach does not generate true reports and only processes tumor-related features, without addressing the wide range of pathologies in CT reports. Our method generates comprehensive radiology reports directly from 3D volumes, covering a broader spectrum of pathologies. But, we acknowledge we could have chosen a different name.

[R5] Fig. 1 and temporal patches: X1:12 represents the first 12 slices patched along x and y dimensions with a patch size of 24, creating 400 patches per slice. For 240 slices, this results in 20 sets of 400 patches. These are reshaped to form a tensor (batch_sizeÃ20Ã400) for the first transformer, capturing attention within each slice. The tensor is then reshaped to (batch_sizeÃ400Ã20) for the second transformer to capture attention across slices, effectively modeling 3D spatial relationships. The temporal patch size refers to treating 12 slices as a single patch.

[R5] Our network: The transformer encoder uses two subsequent transformers with conventional multi-head attention. While similar to the Vision Transformer, it does not utilize CLS. The 3D volume transformer refers to this transformer encoder. The transformer decoder is a language transformer adapted from R2Gen, with modifications like memory-driven conditional layer normalization."
https://papers.miccai.org/miccai-2024/182-Paper1439.html,"We thank all the reviewers for their valuable feedback and respond to each of the comments below.

Through validation of the end-to-end approaches (the better setup), the superior segmentation results of the Unpaired Neural SchrÃ¶dinger Bridge (UNSB) over GAN-based counterparts such as CycleGAN and CUT is confirmed via two-sided, paired-sample t-tests (p<0.05). The benefit of UNSB over CUT and CycleGAN is more evident when assessing segmentation in 3D (Dice=0.73, 0.69, and 0.56 for UNSB, CUT, and CycleGAN). In addition, with improved training stability, UNSB also offers a better SSIM metric than CUT for MRI-to-CT translation (~6% improvement), which also contributes to the segmentation outcome.

Public paired MRI-CT datasets of the human head are rare, but are crucial for validating our proposed framework and the baselines. Here, we used the iDB dataset as our test set. We performed the evaluation both in a 2D slice-by-slice manner and in full 3D to take full advantage of the test dataset. As multi-centre data of unpaired MRI and CT scans were used for model training, the performance of the proposed method was shown to be great despite that no target-domain ground truths were used and the test set comes from another source. This showcased the robustness and adaptability of our model. Nonetheless, additional validation on multi-centre datasets would provide valuable insights into the modelâs generalizability and performance consistency.

As target-domain ground truths were not used for training, uncertainty estimation for ventricle segmentation via Monte Carlo dropout helps enhance the reliability and interpretability for the deep learning modelâs segmentation outcomes. With intuitive visual maps, the uncertainty map helps identify areas where the model is less confident. This is invaluable for clinical decision-making (e.g., planning ventriculostomy surgery). We also computed the correlations between model uncertainty and segmentation accuracy across different methods, further confirming the robustness of the proposed technique.

Our study uniquely addresses the challenge of CT brain ventricle segmentation in the absence of manual labels (partially due to poor soft tissue contrast and image quality), which is common in the clinical but often overlooked in the literature. So far, solutions to tackle this issue and particular application are limited. Typical SOTA methods use fully-labeled datasets. For instance, Huff et al. ( IJCARS, vol 14, 2019) achieved a high Dice score of 0.92 for the same task with fully labeled in-house datasets, but their method does not tackle the label scarcity issue that we intend to solve. Due to the obstacles in curating fully labeled CT brain dataset, it is difficult to compare our methods with supervised SOTA techniques. To validate the proposed method, we have compared two major SOTA methods, including CUT and CycleGANs with different variants to fully test and characterize our proposed framework, and showcased the superior performance of our method. We will further curate a larger dataset (with manual ground truths for CT scans) to fully validate our proposed framework with additional SOTA methods in our future studies."
https://papers.miccai.org/miccai-2024/183-Paper2832.html,"We thank the reviewers for appreciating the novelty (R4), the importance of our research (R1), and adequate experiments (R3). In this rebuttal, we clarify the raised concerns, i.e. the workflow of the finetuning process (R1, R4), the computational cost of our method (R4), and comparisons to nnUNet (R1) where our method still outperforms this benchmark.

Common concerns:

R1:

R3:

R4:
1.Point number
We use 8 edge points."
https://papers.miccai.org/miccai-2024/184-Paper1009.html,"We thank all reviewers for their encouraging and constructive comments. Our responses to the major concerns are itemized as follows. 
1) The specific meaning of the symbol tanh() in Formula 1 (R1): The symbol tanh represents an activation function that is widely used in the field of deep learning. 
2) The description of DGC should be improved (R1): The node is defined by the AAL atlas [14], where each node corresponds to a specific brain region. Additionally, the CRB module is developed to estimate the causal relationships between brain regions. This is why the adjacency matrix (i.e., a directed graph) is adopted to aggregate node information. The two parameters Î¸1 and Î¸2 represent the parameters of two fully connected layers. These layers are utilized to convert the node features into a more compact dimension. 
3) The method for transforming a 2D matrix into a vector (R1): This is implemented by the built-in function, namely torch.tensor.reshape, of the PyTorch framework.
4) The meaning of the brain map is not well explained (R1): Each node in Fig. 2 represents a brain region, and the causal relationships between brain regions are inferred from the CRB module. The directed edges in Fig. 2 demonstrate significant differences in the strength of connections between these brain regions among the two groups of subjects.
5) Explain why there will be a larger improvement after adding LG loss (R1): The directed graph G is a part of the modelâs parameters to be optimized, and imposing a specific constraint on G helps reduce the risk of overfitting. The LG loss acts as a regularization term, guiding the optimization process to favor simple models that generalize better to new data. As a result, the addition of the LG loss leads to a more robust model, resulting in a significant enhancement in performance.
6) Disclosing the source code of the model (R1): Yes, we will release the code when the work is formally published.
7) Does G always include self-connections (R4)? Yes, we add self-loops to the obtained graph when performing the directed graph convolution operation, a technique that was also frequently utilized in similar studies [8]. The addition of a self-loop is utilized to retain valuable information from the nodes themselves when updating node features.
8) Did the authors only test the RAB module in the ablation studies, while using Pearson in the CRB module (R4)? Yes, there is only one combination option remaining when testing the effectiveness of the CRB module, as indicated in the final row of Table 2. This is because, for this ablation experiment, the directed graph was replaced by a brain network/graph constructed using the Pearson correlation coefficient, which results in an undirected graph. Therefore, the DGC layer is currently unsuitable for aggregating node features. Instead, the graph convolution operation [8] is suitable for undirected graphs. In addition, the loss term LG is not included in this experiment because the Pearson brain network is not part of the modelâs parameters in this setting, and adding the constraint does not affect the modelâs performance. Therefore, this is the reason why there is only one combination of ablation experiments regarding the CRB module.
9) How was the retention of 1/3 of the original number of nodes decided (R4)? This is a hyperparameter of the model, similar to the learning rate, and is typically determined through empirical optimization.
(10) Please describe how to treat the imbalanced data in your experiments (R5): The ADNI dataset utilized in this paper exhibits some imbalance in terms of the number of subjects in the two categories, with a ratio of approximately 1:1.5. From the results in Tables 1 and 2, it is evident that our method demonstrates good performance even on unbalanced data."
https://papers.miccai.org/miccai-2024/185-Paper0674.html,"First of all, we thank all the reviewers (R1,R4 and R5) for the valuable and useful comments and appreciate the effort of area chair (AC) and meta-reviewer (MR) for the chance for this rebuttal. We are encouraged by the positive feedbacks from the reviewers regarding our overall organization and the figures, as well as our critical reflection on the simplicity and novelty of the data augmentation (DA) strategies for medical image segmentation. We hope that such reflection will be beneficial to the community in researching DA for segmentation.

As implied by R1 and R4, our contribution lies in evaluating and discussing the design of effective DA strategies, instead of proposing another DA strategy. The results in our paper show that the counter-intuitive but simple DA strategy, i.e. CutMix, outperforms other more complex strategies specially designed for medical image segmentation, i.e. ObjectAug, CarveMix and AnatoMix. As noted by R1, our research may inspire the discussion on future ideas for designing DA strategies, as dedicated strategies aiming for anatomical correctness may fail to improve segmentation performance. We hope to raise the attention of the community to the fact, that the effectiveness of DA strategies does not necessarily depend on the methodological complexity. In this case we share the opinion of R4, that such finding can be beneficial to the search for DA with actual benefit for medical segmentation systems.

In addition, we would like to take this chance to clarify some common concerns from the reviews.

First of all, R1 and R4 comment on the computation cost of each DA strategy in our experiments. We want to add that CutMix takes on average 0.3s for one output image, while it takes 15.7s for CarveMix, 20.9s for AnatoMix and 40.4s for ObjectAug on the same device. Because CutMix only combines two images by region-of-interest (ROI), it is much faster than other methods using slow operations, like background in-painting or object rotation. Regarding computation cost as well as segmentation performance, CutMix turns out to be the best performing DA strategy.

Secondly, R1 raised concern on the choice of hyperparameter of each DA. In our experiments, the hyperparameters of AnatoMix and ObjectAug are optimized on a subset of the training data, but not CutMix and CarveMix. CarveMix contains no hyperparameters to optimize and for CutMix, the default settings from the original paper already led to a large margin over other DA strategies and thus it is not further optimized.

Thirdly, R4 comments on overfitting due to limited original datasets. 
We want to argue that this is just the research question being answered in our paper: What simple but useful DA strategies could counter overfitting, addressing data scarcity for typical medical image segmentation. 
In addition to the applied DA strategies, we rely on the regularization methods as implemented in the nnUNet framework, for example the ensembling prediction from cross-validation models and saving best model during validation as output.

At last, R5 raises concern on the lack of novelty. We want to again emphasize that the novelty in this paper lies in the finding that preserving anatomical correctness is not a necessary prerequisit for DA for medical image segmentation. As was also noted by R1, this was not known before and could possibly widen the range of available DA methods.

Overall we thank for all the constructive reviews, such as extension to extra segmentation tasks and including more evaluation metrics, that helps improve our research. 
We believe in the same way as R1, that the community should find a balance between technical innovation and simplicity."
https://papers.miccai.org/miccai-2024/186-Paper1569.html,"Comment 1: Itâs not reasonable to compare to basic SAM. You shall compare medical versions of SAM, namely SAM-Med2D and MedSAM.

Response 1: Thank you for the valuable comment. We took the advice and ran them on our tasks. For (retina, ventricles, tumor) dataset, for SAM-Med2D/MedSAM/CUTS+diffusion-kmeans/CUTS+diffusion-B we got dice scores of (0.548/0.079/0.675/0.741, 0.736/0.053/0.774/0.810, 0.591/0.088/0.432/0.486). Note: according to MedSAMâs authors, point-prompt is an unstable feature which explains the bad results. CUTS+diffusion-B significantly outperforms on 2/3 datasets, even CUTS+kmeans performs better on 2/3 datasets. Other SAM prompting methods such as bounding box prompts give additional information and lead to an unfair comparison. We have all results ready and can include these during the revision.

Comment 2: If SAM models are demonstrating better performance, is it still necessary to design a method like CUTS?

Response 2: First, SAM models are not showing better performance than CUTS. This alone is enough reason to design a method like this. Further, a model like CUTS is lightweight, in that it does not require months of annotation and pretraining in large compute warehouses. Further, it is clear that models like SAM indeed require domain-specific fine-tuning for specific tasks that are not implicitly covered by supervised pre-training stage, and this could suggest objectives, modules and methods for pre-training or fine-tuning. More generally, we believe large foundation models will not automatically work for any task without such fine-tuning, so specific approaches with the correct inductive biases will still be important for these.

Comment 3: You need a label prior of some kind in order to extract the nearest matching cluster for any region of interest. Is this another limitation on its applicability or are there strategies to circumvent this issue?

Response 3: We can simply provide multi-granular complete segmentation (Fig 2), after which a particular region of interest can be chosen by the clinician or user.

Comment 4: You may want to consider instance/semantic-level segmentation tasks, such as STEGO, rather than a binary segmentation task.

Response 4: We agree that semantic-level segmentation could be an interesting application of CUTS. However we were only able to access medical datasets with binary segmentation of particular targets. Datasets such as the Berkeley natural image dataset have semantic segmentation, and indeed CUTS performs well on that, but that may be less relevant to MICCAI.

Comment 5: What are the criteria for selecting persistent structures, hyperparameters, epochs, training details of contrastive learning, etc.?

Response 5: Persistence is a measure that is defined in diffusion condensation as clusters that stay separated over many iterations. We use this to select persistent structures and there are no hyperparameters involved in this. For selection of hyperparameters in CUTS training, we have previously conducted ablation/tuning experiments and can include them in the revision.

Comment 6: Will you open-source the code?

Response 6: We have well-documented code ready to be released, and we will include the link to the codebase once the paper is accepted.

Thanks again to the kind reviewers for the constructive feedback, and hope we have addressed your crucial questions and can deserve an increase in the rating. We will address the minor suggestions including figure size changes, rephrasing the conclusion, etc."
https://papers.miccai.org/miccai-2024/187-Paper3476.html,N/A
https://papers.miccai.org/miccai-2024/188-Paper0141.html,"We deeply thank the constructive comments from all reviewers. Hope the responses can address your concerns properly.

R1

Suggestions on statistical tests. We have done the 5-fold evaluation for all experiments. However, due to limited pages, the statistical variance is not shown in the manuscript.

Separate experiments for impact on WL, NIR-I, and NIR-II. Considering previous work DLS-DARTS having demonstrated the advantages of multi-modal data to single-modal data in fluorescence imaging-based glioma tissue analysis, we directly adopt the multi-modal setting to analyze all experiments. Meanwhile, note that the purpose of this work is to show the effectiveness of EEA in fluorescence glioma boundary recognition. Therefore, although the impact of EEA in different data modalities is worth studying, it will not severely influence the conclusion of our paper. Due to the limited pages, this experiment is not included in our manuscript. We will do further evaluations to show the detailed impact of our EEA on different intraoperative imaging modalities in the future.

R2

Questions on valid ranges of Solarize. The range of our EEA directly follows previous work RandAugment that uses a maximum value of 256 rather than 255.

Confused descriptions on 3 and 9 channel data. We emphasize that WL images contain RGB channels, while NIR-I/II images only contain one channel. Therefore, the 3-channel data is the concatenation of WL after RGB-to-grayscale conversion, NIR-I, and NIR-II. The 9-channel data is the concatenation of original WL, and NIR-I and NIR-II after grayscale-to-RGB conversion.

Questions on negative impact of EEA. It is not specially investigated in EEA, while we think it is worth evaluating. An intuitive way is to show the t-SNE of feature distributions of the augmented samples, which may show samples lie on the border between non-tumor and tumor clusters, or even form unexpected small clusters as the over-transformed samples that have a negative impact on the model prediction. It will be considered in future works.

Settings for hyperparameter N. We select 11 and 31 because these are magnitude levels in previous work AA and RA, respectively. Value 2 represents either deform or not, and 3 adds a less deformed choice for 2 that can be a representation of the cases with different deformation levels. We agree further investigation around 11 is worth trying. However, the original purpose of this experiment is not to find the best N for glioma boundary recognition, but to show that the selection of N is important for EEA, meanwhile 11 is just a satisfactory choice.

R3

Unclear details about the pQ-table and reward. First, we clarify that the positions in pQ-table (with shape DxN) can not reflect the sampled operators. The pQ-table is designed for magnitudes (or deformations) at different depth. âFor positions that $O^{d,m^d}_i$ are not sampled, the reward is assigned 0.â refers to the magnitudes not sampled, rather than operators. Then, we adopt an epsilon-greedy strategy that always has probability epsilon to randomly select magnitudes rather than from pQ-table. Therefore, although in theory there exist probabilities that 0 rewards are in pQ-table, in practice it is hard to observe this phenomenon, especially where we have an epsilon decay strategy that starts updating pQ-table from total random selection.

Hyperparameter selection. Hyperparameters of different models are from original papers, and then tuned through grid search. Experimental setups are different to benchmarks because there are feature gaps between natural images and fluorescence images. A problem indeed existing is that no separate validation set is established to tune the hyperparameter values due to the limited number of patients. However, the goal of this paper is to demonstrate the improvement of EEA for glioma boundary recognition. We believe the hyperparameters for classification models will not make a significant impact to the conclusion."
https://papers.miccai.org/miccai-2024/189-Paper1123.html,"We appreciate the reviewersâ valuable time and comments. In the rebuttal, we summarize the common questions with the source (say, R4 C1 indicates comment 1 from reviewer 4, which is under item 6) and response below.

Q1. Is a baseline method lacking for comparison? (R5 C1, R6 C2)
In fact, we compared the BiaslessNAS with a state-of-the-art NAS in the original paper, but we missed the citation. Specifically, in Table 2 of this paper, the results of the FairNAS in line 4 are from paper [N1] Table 3 line12 named FaHaNa-Fair. In the revision, we will clarify the baseline and add the citation.
For R6, we are proposing a general framework with three components for data/algorithm/model co-optimization. Fairness-related methods, such as the mentioned adversarial training, can be integrated into the algorithm component, while the mentioned disentanglement method into the data component. This paper employs the widely used FairBatch [N2] and FairLoss [N3] in data and algorithm components, respectively.

Q2. What is the cost of the proposed method (R4 C3, R5 C3, R6 C1)
We agree that NASâs high search cost is a consideration. The search process for all experiments can be completed within one GPU day. This is achieved by two designs: (1) we use a block-wise structure to unify parameters in multiple layers, such that the search space can be reduced compared with layer-wise NAS; (2) with the consideration of the efficient and real-time inference for medical applications, we target small-size architectures (for R6 C1). As such, the training time during the search process can be reduced. For example, the baseline MobileNetV2 has 3.4 million parameters; Figure 3 shows that BiaslessNAS-Fair has only 162,851 parameters, and BiaslessNAS-Acc has 3.1 million parameters. 
We want to note two things: First, the search process is a one-time offline effort for a given application. Second, the main contribution of this work is not to develop an efficient NAS but to build a holistic framework for data/algorithm/model co-optimization. Different efficiency-improving techniques (such as zero-shot NAS, single-path NAS, and hot-start NAS) can be incorporated to further reduce the search costs.

Q3. What are the arguments of data/algorithm/model intercoupling on fairness? (R4 C1)
We agree that intercoupling is the motivation behind this paper. Demonstrating the importance of considering the intercoupling for co-optimization is one contribution of this paper. In the existing literature, most research works perform fairness optimization on one dimension. However, in this work, we reveal that all these factors affect fairness, as shown by the result in Figure 1(ii). Then, we showcase that co-optimizing these factors yields the best performance from the results in Table 2. In the revision, we will rewrite these conclusion-oriented statements to support them using results from Table 2.

Q4. Does the work explore hyperparameters, and whatâs the difference between BiaslessNAS-Fair and BiaslessNAS-Acc? (R4 C2, R4 C4)
There are two hyperparameters used in the framework: (1) Alpha is the scalable parameter for accuracy, and (2) Beta is for fairness. We explore two settings: BiaslessNAS-Fair has a larger Beta (0.8) and a smaller Alpha (0.2), while BiaslessNAS-Acc has a larger Alpha (0.8) and a smaller Beta (0.2). We will clarify this in the revision.

Q5. Why is accuracy used as the metric for evaluation? (R5 C2)
We follow the fairness research [15,N1] and the fairness-related competition [3] to use the commonly used accuracy as the evaluation metric for classification. We agree on R5 that adding other metrics, such as ROC-AUC, can further strengthen the evaluation.

New References:
[N1] âThe larger the fairer? small neural networks can achieve fairness for edge devices.â Proc. of DAC 2022.
[N2] âFairBatch: Batch Selection for Model Fairness.â Proc. of ICLR 2021.
[N3] âFair loss: Margin-aware reinforcement learning for deep face recognition.â Proc. of CVPR 2019."
https://papers.miccai.org/miccai-2024/190-Paper3303.html,"We would like to thank reviewers R1, R3, and R4 for their constructive feedback and are grateful that they found that the âoptimization problem is solved efficiently with the trained global hypernetwork, leading to optimal tissue parameters per subjectâ (R1), the âsubject-specific parameter estimation for each type of the tissue is an interesting ideaâ (R3) and that we propose a ânovel registration algorithm that learns subject-specific linear elastic regularization [which] stands out as potentially pioneering this approachâ (R4). 
Here, we respond to their comments raised:

1) Lack of comparison to SOTA methods (R4): While we do not extensively compare, we do compare to the HyperMorph framework (based on Voxelmorph), and [17]. Our main aim was to introduce and compare different regularization schemes (diffusion vs. elastic/global vs. spatially adaptive) within one consistent framework. We will clarify this in Sec. 3.3. Setting our work in the greater scope of SOTA methods is important, and we will include that in a future, extended version. 
2) Adaptive regularization (AR) in iterative registration (R3): We are well aware that AR is explored in many prior works, including in iterative methods, and would like to clarify how we differ from the works mentioned by R3. While we do not model sliding motion (see below), we do learn the effect of physical parameters on the deformation so that efficient inference can be performed. Also, we explicitly model physical tissue properties. We agree that iterative methods are important, and space permitting, we will include them in the introduction, as well as clarify the differences to our proposed method.
3) Missing evaluation of sliding motion (SM) (R3): As already mentioned above, our work does not address SM. Since the linear elastic regularizer penalizes the tearing of elastic material, it cannot support SM in its current form. We appreciate that incorporating SM when working with lung data is relevant. However, our work at this stage focuses on tissue-specific elasticity without targeting specific anatomies such as the lung, which is why we do not consider SM. We agree that this is important to clarify and will add this in Sec. 4.
4) Weighting of similarity term (R1, R3): Thanks to the comments by R1 and R3, we can see that the weighting is not explained clearly enough in Sec. 2.1. Since the LamÃ© parameters are the inputs to the hypernetwork, they are normalized. To balance the similarity (sim.) and regularization (reg.) terms accordingly, the sim. term is weighted. We do both analogously to [11]. Since we use an adaptive reg. term, we use voxelwise balancing. Hence we use the weight maps in the weighting of the sim. term. We will clarify this in Sec. 2.1.
5) Novel regularization despite adaptive sim. term (R3): We learn the effect of the regularizer on the deformation since the LamÃ© parameters are the input of the hypernetwork. Thus, we indeed propose a novel regularization even though the weighting of the sim. term is based on the weight maps of the regularizer.
6) Only modest improvement of Dice (R3): While the proposed regularization only slightly improves the mean Dice score, we want to highlight that the adaptive regularization leads to improvements also for the class-wise Dice and the TRE across all datasets (Tab. 1). On top of this quantitative performance increase, the qualitative results reveal that the proposed approach results in more meaningful deformations, which cannot be captured by the Dice alone.

We also appreciate the respective suggestions of R1 and R3 to explore gradient-based parameter optimization and to evaluate our method on age-related data (although currently, we do not have access to a suitable dataset) and will consider this for future work. Following the suggestion by R4, we will add more details to Fig. 1.
We again sincerely thank the reviewers for their time and effort and believe that their feedback will improve the quality of our final manuscript."
https://papers.miccai.org/miccai-2024/191-Paper1489.html,"We thank the reviewers for the positive feedback. Our code and models will be publicly released.

To Reviewer #1

To Reviewer #2
1.1 Handling z-dimension information
The reviewer raised an important point regarding the potential loss of information when segmenting 3D data due to our model currently accepting only 2D image inputs. Our current approach involves slicing 3D images into 2D slices, which does indeed omit the z-dimension(depth) information. This could potentially affect the modelâs ability to fully capture the spatial complexities of 3D structures.
To address this, we are exploring methods to adapt our model to directly handle 3D volumetric data without slicing. This will allow our model to utilize depth information, which we hypothesize could enhance the segmentation accuracy, especially for complex anatomical structures. We plan to incorporate this enhancement in future iterations of our model and will rigorously evaluate the impact of using depth information on segmentation performance.

To Reviewer #3
1.1 Choice of Convolution Branch Input Size
This image size of $256\times256$ was chosen for the convolution branch because it matches the resolution of the raw images in our datasets, which allows for direct processing without additional resizing. On the other hand, the raw image should be resized to $1024\times1024$ and then is fed into the ViT branch, since the ViT module of SAM is specifically designed to process images at this resolution. 
1.2 Potential Replacement of Lightweight Convolution with ResNet-18
The suggestion to replace the lightweight convolution block with a more complex architecture such as ResNet-18 is intriguing. ResNet-18 could potentially enhance the modelâs ability to capture richer feature representations due to its depth and architectural advantages. We plan to conduct experiments to compare the performance of our current model with a version incorporating ResNet-18 to determine if this change offers a significant improvement. This exploration will help us refine our model design and optimize it for even better performance across various segmentation tasks."
https://papers.miccai.org/miccai-2024/192-Paper1608.html,"We sincerely appreciate all reviewersâ comments. Weâll address the weaknesses mentioned.

R1Q1. Effectiveness of DI in the whole framework:  Thank you for your suggestion. Our results in the main submission have demonstrated that the performance without DI is significantly worse in the sinogram domain, highlighting the effectiveness of DI. Given the sinogram domain output is a crucial input prior for image generation within the entire framework, it is expected that the poorer quality of the sinogram domain output without DI will similarly degrade the performance across the whole framework. Unfortunately, due to the rebuttal policy, we cannot include these results now. We appreciate your suggestion and will incorporate these results in our future work.

R3Q1. Access to code or data: We apologize for being unable to provide an external link at this time due to the rebuttal policy. We will make the code and data publicly available upon acceptance.

R3Q2. Relative paper [WACV2024]: 1) Our work differs from WACV24 in the following aspects: a) Methods: The two conditional diffusion models in WACV24 are both trained in the image domain (one for metal-free prior image learning and the other for metal artifact prior image learning). In contrast, our approach involves training two diffusion models in both the sinogram and image domains, and we introduce a novel DI model for the sinogram stream and new conditioning methods for image generation; b) Tasks: WACV24 aims to remove metal artifacts from 3D CBCT, whereas our focus is on general 2D CT; c) Datasets: We used the widely recognized public dataset, DeepLesion, while WACV24 primarily utilized a private dataset. 2) This paper was published in January 2024, which coincides with the period when we had completed most of our work. During the preparation of our manuscript, we were unaware of any supervised diffusion dual-domain methods for MAR. We apologize for overlooking this concurrent work. Weâll include a discussion of this paper.

R3Q3. Formulas should be listed: Thank you for your suggestion. Weâll list the formulas separately instead of embedding them within paragraphs in the final version.

R3Q4. Not SOTA methods in Fig.2, such as DICDNet: a) As shown in Table 1, OSCNet is the current SOTA method. We have included the results of OSCNet in Fig.2. b)  Fig.4 of the OSCNet paper compares with other methods, such as DICDNet, demonstrating that OSCNet performs better. c) Weâll include DICDNet in the final version, as suggested.

R3Q5&R4Q1&R4Q3. Lack of clinical validation & additional visualization result: 1) Weâve evaluated our model on CLINIC-Metal and a private clinical foot CT dataset. However, due to the page limitation, we decided to follow the papers of DuDoNet [DuDoNet: Dual Domain Network for CT Metal Artifact Reduction, CVPR2019] and MEPNet [MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images, MICCAI2023], both of which evaluated their models exclusively on synthesized DeepLesion data. This allows us to focus on method interpretation and comparison using the most widely recognized dataset, synthesized DeepLesion. 2) Although we canât include results due to space constraints and rebuttal policy, we will provide detailed evaluations on additional datasets alongside the publicly released code. These results will also be incorporated into our future work.

R3Q5. Time consumption: As mentioned in the implementation details, we have used DDIM to improve efficiency. The time is about 3.1 seconds/image for training and 4.4 seconds/image for testing, around 15 times faster than DDPM.

R4Q2. Parameter number: The parameter number is the sum of parameter numbers in the two denoising networks (UNet in our proposed method), similar to other methods that consist of two UNets as well (eg, DuDoNet, DSCMAR, DuDoNet++), which is around 26M."
https://papers.miccai.org/miccai-2024/193-Paper2022.html,"We appreciate all the reviewers for their constructive comments. 
[Overall presentation and important concepts (R3)]:
We appreciate this comment and will check throughout the paper to clarify key concepts including:
1) The description âprevious models focusing on node features: we intended to identify the gap that existing GNNs typically process node features by aggregating connected nodes without updating edge weights. This could challenge the learning schemes based on brain networks with much edge noise. 
2) Conventional denoising in graph structure learning (GSL): GSL typically denoises the graph according to specific properties, e.g., the relationship of a community when optimizing a community network, where edges are optimized by masking irrelevant or noisy edges. 
3) Information bottleneck (IB): IB minimizes the mutual information between processed features and the input to optimize information compression while maximizing the mutual information between features and the output to improve the model prediction performance. Since calculating mutual information only involves data distribution and does not need specific restrictions on graph types, IB is selected as the theoretic basis to develop our method for denoising brain connectivity effectively. 
4) The concept of âpluginâ: we would appreciate this comment and will clarify it. 
5) Node features: vectors showing the properties of brain regions, including rows in the connectivity matrix and time series brain activity recordings. We will expand our explanations in both main texts and Figure 1. 
6) Learnable edge mask: the masking matrix is learnable, i.e., masks in the matrix are updated during model training for better model performance. 
7) We have introduced in 2.1 that the edge refinement is achieved by a masking process; in 2.2, we mentioned that Bernoulli is used to create a binary masking matrix to mask the original connectivity matrix; as Bernoulli is not differentiable, we need the relaxation of it. 
We appreciate the reviewerâs constructive comments and will expand all the above explanations further to improve the paper presentation. 
[Figure captions (R3, R4, R7)]:
We would like to thank reviewers for this comment. Figure 2 presents exemplar refined networks using different approaches for visualized comparisons. For a comprehensive comparison, we compared our method with widely used traditional filtering method and other SOTA GSL methods, such as VIB-GSL. We used brain age prediction as an indirect evaluation of the model performance. Our experiments show that our method outperforms other methods in quantitative evaluations. For a qualitative evaluation, we present an example in 3.4, where the refined network demonstrates network modularity, a vital property of the human brain network. This result implies that the proposed approach could effectively refine the network connections to further characterize the brain. In the final version, we will expand all these explanations in the main text and captions of Fig 1-3.
[Model performance (R4)]:
Due to the heterogeneity of the utilized dataset, it is acknowledged that the age prediction task is challenging. However, our experiments show that by adding the proposed method, the model performance has significantly improved, supporting the usefulness of the proposed edge refinement module. 
[Model efficacy (R4)]:
In 2.3, we have described the computational complexity of the efficient version of D-CoRP. In the final version, we will add metrics on model efficiency, including inference time, to Table 2. 
[Refinement evaluation (R7)]:
Since brain age reflects vital properties of the human brain and has been widely studied, we chose brain age prediction to evaluate model performance. Down to the model frame, we only replaced our method with comparison methods in network refinement step. Combined the above, better performance on age prediction task from our experiment results indicate better refinement from our method."
https://papers.miccai.org/miccai-2024/194-Paper0638.html,"Thanks to the reviewers for providing insightful and constructive comments.

[Reviewer #1-Q1] The TopNet + SAP method seems to perform better on the metric normal consistency. Can you comment on this?
Author response: Thank you for your insightful and constructive comments. TopNet proposed a decoder following a hierarchical rooted tree to generate a structured point cloud. Although TopNet+SAP showed the highest performance on the metric normal consistency by hierarchically learning the overall structure of point clouds with normals of dental crowns, it had a limitation in learning the local details of dental grooves and cusps and the relationship to antagonist teeth, proximal teeth, and a margin line in this study. This results in the generated crown exhibiting a somewhat smoothed appearance. On the other hand, the DCrownFormer reconstructed the complex occlusal surface structure in more detail.

[Reviewer #3-Q1] The term âindicator gridâ is used without a clear definition, leaving readers uncertain about its meaning and significance in the context of the study. Such terminological clarity is essential for a broader audience.
Author response: Thank you for your insightful and constructive comments. We use the Differentiable Poisson surface reconstruction method [19] to obtain an indicator grid, which implicitly determines the mesh surface through the Poisson equation [19] and can be converted to a mesh using Marching Cubes [23].

[Reviewer #3-Q2-3] Another limitation is the discussion on the generalizability of the proposed method. The paper focuses predominantly on dental crowns, with limited exploration of how this method could be adapted for other types of dental prostheses. âFuture Worksâ section of the paper is notably vague, lacking detailed plans or strategies for addressing the potential challenges or limitations identified in the current study.
Author response: Thank you for your insightful and constructive comments. We fully agree with your comment that our study has limitations in generalizability for applying other types of dental prostheses. Dental prostheses are generally classified into five types, including dental implants, veneers, dentures, crowns, and bridges. Also, each type serves different purposes and comes with its own set of challenges and considerations. We carefully think that our study has the potential to be extended to future studies for the mesh generation of other types of dental prostheses.

[Reviewer #5-Q1] The ablation experiment was incomplete and did not reflect the improvement effect of MRL and CPL over baseline.
Author response: Thank you for your comment. We evaluated the effectiveness of MRL and CPL in Table 2b and Fig. 3c, respectively. The baseline with MRL achieved higher performance than without MRL (Table 2b). Also, the proposed CPL (Î» = 1.0) outperformed CDL (Î» = 0.0) in terms of CD and SDE in Fig. 3c.

[Reviewer #5-Q2] There are gaps in the paperâs content in the Section of Evaluation Metrics, making it difficult to understand.
Author response: We will improve the Evaluation Metrics in future work.

[Reviewer #5-Q3] There are challenges to the correspondence of various methods with Table 1. and Fig. 3 In the Section of Comparison with Other Methods, which is not conducive to the readerâs understanding.
Author response: In Fig. 3, the color map denotes surface distance errors reflecting the SDE in Table 1, and the green and red indicate a low and high SDE, respectively. We will add generation results about points and indicator grids in future work.

[Reviewer #5-Q4] Why is SAP included in all comparison methods in Tables 1âs comparison experiment, and why is SAP removal not included in any comparisons?
Author response: The purpose of our study is direct point-to-mesh generation using point completion networks combined with SAP, where SAP is used to mesh reconstruction from generated points and normals of a dental crown. Therefore, a comparison of SAP removal is not provided."
https://papers.miccai.org/miccai-2024/195-Paper3200.html,"We would like to thank all reviewers (R1, R4, R5) for taking the time to review our contribution and give us this constructive feedback. The reviewers acknowledged the novelty. In the following, we would like to take the opportunity to clarify the points raised by the reviewers:
Q1: Existing literature, comparison with other work, and necessity for the study. (R1, R4) 
A1: R5 and R4 emphasized that the problem we outline, the mismatch in magnetization dynamics between retrospective and prospective undersampled data, is a crucial point (cf. Fig. 1), that has received little attention in the literature (R5), but there is a study by Shimron et al. that shows bias in learning-based reconstruction from public off-label data [Ref1].
We agree that suitable training data can be obtained by tailored acquisition [4]. However, this means that specific undersampling factors or patterns have to be all newly acquired. This would be best practice, but such training data can never compete in sample size with existing measured data, also limiting vendor-independent and multi-site studies. Our tool describes the correct dynamics, and offers a highly efficient alternative for data generation. Moreover, the provided reconstruction operator can be used as zero-shot model. Thus, it is highly relevant especially to be highlighted within the MICCAI society. 
Q2: Quantitative results and conclusions based on two to three slices. (R5,R1) 
A2: In the training and evaluation section, we mentioned that 5 subject volumes, each containing 70 slices, were utilized for validation/testing. Of these, two subjects were exclusively for test data, comprising 140 slices. In the submitted manuscript, we showed an exemplary test data slice with RMSE and SSIM, which is representative for the full dataset. However, we acknowledge the lack of quantitative results for the entire test set and, as suggested by R5, we will publish the table for SSIM, MAE and RMSE for prospective and restrospective test data for VN trained on retrospective data with the p-value for all three metrics preferably in a main article and, if not possible, in the supplementary material.Q3: Explanation of Eq. 1, references for the MR-XXX framework, B1,B0 inhomogeneities and generalization to actual data. (R4,R5,R1) 
A3: The simulation framework MR-XXXX and the function Bloch() in  Eq. 1 is our recent improved implementation of the extended phase graph algorithm [1, Ref2]. The main message of our study, that retrospective undersampled data is different from prospective undersampled data, is not affected by B0 and B1 inhomogeneities and as mentioned in the first paragraph of the methods, our operator can describe all cases including real data scenarios [1, Ref3]. We acknowledge the suggestions on application of our method on real data, which is what we currently perform to be published in future work. We will include Ref2 and Ref3 in our reference list.
Q4: Code and detailed explanation of methodology. (R5) 
A4: One can check the performance of VN trained on retrospective data for prospective undersampling by using âretro_model.pthâ and argument kspace_name = kspace. 
The sequence used in the study was IR-FLASH with FA of 10Â°, the recovery time after the FLASH sequence was TREC = 2 secs and 12 inversion times TI = [10.0, 5.0, 4.5, 4.0, 3.5, 3.0, 2.5, 2.0, 1.5, 1.0, 0.2]. We will include this description in the final article. We will also provide the open sequence standard Pulseq file detailing all the sequence parameters and further documentation of the code, so that all results can be reproduced.
If accepted, we will include colabs for creating training data and fitting to simulated and real MRI data, so that every MICCAI researcher can use this tool to investigate their own undersampling approach for potential data crimes, and especially how to avoid them.

Ref1. https://doi:10.1073/pnas.2117203119
Ref2.  https://doi:10.1002/jmri.24619
Ref3.  https:// doi:10.1002/mrm.27040"
https://papers.miccai.org/miccai-2024/196-Paper2733.html,"We sincerely thank the reviewers for their valuable comments. We are grateful that reviewers acknowledged that our method is interesting (R4), novel (R4 and R6), helps find potential spurious relations (R6), and in line with reality (R6). We address the concerns of the reviewers below.

FM API (R3)-As stated in the first sentence of our intro, our focus was on pretrained models that convert complex input data into vectors for downstream prediction tasks. The references to commercial APIs were intended as background context. At NO point did we state that our research would be based on these commercial APIs. An API is simply a way for components to communicate [3], and our approach of using vectorized outputs from a pretrained medical model as input for another ML model aligns with this definition. We modeled key API properties mentioned in the second last paragraph of intro (white and black-box), and proposed corresponding debiasing methods. We intentionally simulated the API with a publicly available FM for reproducibility, as direct experimentation with commercial APIs was impractical due to access limitations and costs. This practice is common in academic research.

The term of âUniversalâ (R4, R6)-We appreciate the feedback. We used the term âuniversalâ because a single UDE noise can be applied universally across all patients to de-bias images for multiple diseases simultaneously. As demonstrated in Table 1, the same UDE noise was applied to different patient images on three different diseases, consistently improving fairness. Also, the term universal aligns with its use in other work [4] where it denotes fair representation learning for various tasks.

Evaluation on ChexPert (R4)-Our study focused on ChexPert for several reasons. First, CheXpert is a large-scale, representative dataset widely used in both medical fairness studies and medical FMs training. Second, it contains multiple disease labels, which is ideal for validating the âuniversalâ property of UDE. Therefore, ChexPert enabled us to perform extensive and systematic studies. Last, given MICCAIâs 8-page limit, we prioritized depth over breadth to thoroughly explore our method on a large-scale, representative dataset. This aligns with similar works in the field, which typically focus on a single dataset for in-depth analysis [1][2].

Underrepresentation bias (R4)-The UDE training objective (Eq.1) masks Sensitive Attribute-related information and maintains the utility. Thus, itâs able to mitigate multiple biases.

Performance (R4)-We want to humbly point out that an effective de-biasing strategy balances utility (accuracy) and fairness (EO, DI). Sketch on Edema achieves 0.5 lower EOp and 1.7 lower DI, very small compared to its 8.1% accuracy drop. UDE and UDEZeGO provide the best overall trade-off, excelling in both fairness and utility, making them ideal when both are prioritized. Additionally, UDE offers good interpretability by visualizing the UDE noise vector (see Appendix).

MedMAE Training Data (R6)-The Medical MAE was trained on NIH ChestX-ray14, CheXpert, and MIMIC-CXR, totaling over 500,000 X-rays.

EO Magnitude (R6)-We use E(qual) O(pportunity) to quantify the disparity in true positive rates (TPR) between sensitive groups. Our setting of data distribution is strongly biased, we confirm that itâs resulting in a large disparity in TPR.

Writing Improvement (R6)-We appreciate the careful review and will incorporate your suggestions into our final paper improvements.

Reproducibility-Our algorithm is detailed in the appendix, and hyper-parms are detailed in the experiments (Sec 3.2). Code will be open-sourced upon acceptance.

[1] On Fairness of Medical Image Classification with Sensitive Attributes via Learning Orthogonal Representations
[2] Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in DL Based Segmentation
[3]https://en.wikipedia.org/wiki/API
[4] Generating Fair Universal Representations Using Adversarial Models"
https://papers.miccai.org/miccai-2024/197-Paper0394.html,"Pathologist viewing for cancer diagnosis is a complex and specialized cognitive task requiring years of training. We present two independent tasks: predicting pathologist attention (to guide trainee attention based on specialistsâ) and predicting expertise (for trainee evaluation), both essential technical components towards developing our AI-assisted pathologist training pipeline. We will clarify this in the Introduction.

While non-specialists focus on the upper G3 tumor regions, specialists also allocate more attention to G4 and G5 regions, explaining their higher viewing time.

Weâll add Genitourinary (GU) in abstract and fix all notations."
https://papers.miccai.org/miccai-2024/198-Paper2247.html,"We thank the Reviewers R3, R4, and R5 for their valuable feedback. All suggested changes and additional details will be included in the final draft.

1.Reproducibility(R3,R4,R5):A well-documented code, data splits and trained models will be made publicly available within one week of paper acceptance.

2.Why method works(R3):Our method works due to a novel decoupled training approach by (i)improving the quality of pseudolabels through separation of  psuedolabel generation and separation (ii)increased tolerance to incorrect pseudolabels with our  worst-case aware decoder.

3.Comparison with SAM(R3):For a fair comparison, we implemented SAM for medical imaging in three settings: (i)Fine-tuned SAM solely on limited labeled data available in our SS setting from target dataset, (ii)Fine-tuned SAM on both labeled and unlabeled images using a pseudolabeling semi-supervised approach,and (iii)combined SAM with our proposed decoupled training, worst-case-aware decoder, and adversarial training. While (ii)improved performance over (i), it still lagged behind our method. Finally, (iii)achieved the best performance, surpassing our method, highlighting the complementary nature of our method and SAM.

4.Distribution match(R4):The adversarial decoderâs output distribution matches pseudolabels for unlabeled data and actual labels for labeled data.

5.Adversarial learning Loss (R4):The loss term operates in same manner as highlighted by R4.

6.On strong augmentation(R4):a)We enforce strong augmentations to be close to a shared weak view, minimizing their distance and promoting interaction. The strong decoderâs loss integrates loss from both augmentations. b)Two augmentations differ due to the non-deterministic nature inducing randomization, ensuring inequality. c)d)Using two augmentations enhances feature space exploration; dropping one leads to a performance decline. It aligns with principles of contrastive learning, fostering discriminative representations. Continued addition of views saturates performance without introducing new information.

7.On experiments details(R4):We included threshold ablation in Fig. 3, with \eta set at 0.95. Results provided are average of 3 runs. We will add Friedman test in final version.

8.Different% of train data(R4):We have varied train data percentage following common practice in SS literature, which typically varies depending on the dataset size.

9.Novelty(R4):Most methods use shared architectures, leading to lower quality pseudolabels and decreased model quality. In contrast, we introduce a novel decoupled approach, separating pseudolabel generation and training to preserve pseudolabel quality, and propose a worstcase aware decoder to address unreliable pseudolabels. This results in higher quality pseudolabels, enhanced tolerance to inaccuracies, and better performance.

10.Comments on fig, eqn and tables(R5): We will address it in the final draft.

11.On weak and strong augmentations(R5):We agree that rotation may introduce strong change in some of our datasets due to their specific nature (anatomy). However, compared to stronger augmentations like CutMix employed, rotation can be considered as a relatively weaker augmentation since the corresponding ground-truth/pseudo-label also can go through the equivalent transformation. Note that we do NOT perform tweaking of augmentations or training parameters for individual datasets and use identical hyperparameters across different datasets.

12.Existing pseudolabels based methods(R5):SSNet(MICCAI22), DCNet(MICCAI23) are pseudolabels based SOTA methods.

13.On less number of training samples(R5):We use ProstateX and PROMISE12. ProstateX has very few images, while PROMISE has ~1000 images. In line with works in semi-supervised literature, which demonstrate the efficacy of models with fewer supervised data, we also use a reduced percentage of data for our experiments.

14.Minor comments on table, equations, and discussions(R5):This will be addressed in final draft"
https://papers.miccai.org/miccai-2024/199-Paper3323.html,"We thank reviewers R3, R4, and R6 for their (very strong) support of our work and will focus on R5âs comments.

Key criticism - Lack of comparison to [17] (proposed for non-medical scenes): We had actually tested the method prior to submission, but the methodâs illumination data was not accessible such that we were not able to fully reproduce it. We had also applied their method with our OR-targeted illuminations (note that this is already a core contribution of this work!) but did not obtain competitive results, which is why we discarded the work as out of scope (not designed for surgical requirements). We will add this important information to the discussion.

Let us further clarify our contributions (R5):
Novel insights:
1) We are the first to experimentally demonstrate that previously proposed calibration methods fail in in vivo surgical settings. Note that this holds true despite prior work [2] because the latter worked exclusively in an ex vivo surgical setting - a completely different setting especially in the context of HSI because tissue perfusion has a radical effect on the spectra. 
2) A secondary novel insight is the radical effect of the miscalibration on the downstream task of segmentation - also not investigated by any prior work.
3) One of the most surprising insights was that our method generalizes to completely different OR settings. The acquisition of data from different species in varying perfusion states was a crucial contribution to reveal this important finding.

Novel methods: 
4) We propose the first learning-based approach to automatically recalibrate hyperspectral images during surgery, thereby solving an important clinical workflow issue. A key domain-targeted design choice was the physics-based simulation of illumination conditions to be expected in the operating room (see below).

WHY does our method perform so well (also compared to AngularGAN R6)?

We agree with R5 that it would be valuable to integrate all of these insights into the discussion.

Generalization across illumination conditions (R3): Our method only requires the primary light source (affixed to the camera and thus not undergoing dynamic changes) to be similar to the one used for generating the training set. The rat recordings are out of distribution with respect to the illumination conditions in the training set, indicating generalization not only across species but also across lighting conditions.

Claim of the need for turning lights off (R5): We apologize for the confusion. The standard approach is indeed to turn off all external light sources before acquisition, as described in [24 p.4] (animal study) and [15, p. 7] (wound monitoring). However, implementing this protocol proves unfeasible in the human OR [8, p. 14]. Our work tackles this problem."
https://papers.miccai.org/miccai-2024/200-Paper0658.html,"We appreciate the reviewersâ positive feedback and insightful comments, which have provided valuable directions for enhancing our work. Due to space constraints, we were unable to fully address all comments in detail. However, we will incorporate concise responses to address your concerns in the revised, camera-ready version of our paper. Please find our feedback below.

R1:

Disentangled representation learning: StyleGAN training does not explicitly use disentanglement learning in the way that methods like Î²-VAE (beta-Variational Autoencoders) or InfoGAN do. However, it does incorporate several techniques that result in a form of disentanglement within the latent space, leading to more intuitive and controllable image manipulation. Such techniques include the two-stage mapping network, adaptive instance normalization (AdaIN), and progressive growing. In contrast, disentangled representative learning typically utilize specific terms in their objective/loss functions to encourage disentanglement. The primary goal of disentanglement learning is to achieve representations where each latent variable corresponds to a distinct, interpretable factor of variation. Disentanglement learning methods often use supervised or semi-supervised approaches with labeled data to achieve disentanglement, whereas StyleGAN can achieve a level of disentanglement without explicit labels, relying on its architectural design and training process.

Influence of lighting and patient position: Thank you for highlighting this important concern. In our work, all portrait photos were taken with the face facing the camera, ensuring standardized patient positioning and minimizing its effect. In future work, we plan to investigate the influence of lighting conditions and other image quality factors to further enhance the modelâs robustness and accuracy.

R2:
Ethnicities: We acknowledge the limitation of our work regarding ethnic diversity. Our study primarily utilizes patient portrait photos from our department (Department of Radiation Oncology, University Hospital Erlangen, Germany). Due to data privacy concerns, obtaining photos of more diverse ethnicities is challenging at this stage. In the future, we aim to pursue multicenter collaborations and apply our method to photos from a broader range of ethnic backgrounds.

R3:
Comparison methods: In our work, only the StyleGAN-based methods are trained in a two-stage manner, which allows us to fully use the editability and explainability of StyleGAN latent space. However, we want to emphasize that other comparison methods such as ResNet-18/VGG-16 + DeepSurv were all trained in an end-to-end manner."
https://papers.miccai.org/miccai-2024/201-Paper2136.html,"Response to Reviewer #1:
Q1: Strage to train a set of networks for confidence estimation.
A1: Obtaining reliable confidence estimates is crucial in risk-sensitive applications such as clinical diagnosis. This paper provides theorectical and empirical evidence to support that a set of neural networks can help provide more accurate estimate of confidence. While Deep Ensemble (DE) may achieve slightly better classification accuracy than individual classifier (IC), confidence estimates from DE is worse than our method for IC. In clinical diagnosis, reliable confidence estimation is often more concerned when DE and IC have comparable classification accuracy.

Q2: Why not use ECE to evaluate the method?
A2: The goal of our study is to obtain a reliable confidence estimate. Calibration and misclassification detection are two frameworks used to assess the reliability of confidence estimates. As pointed output in Ref [31], calibration focuses on aligning accuracy and confidence, while misclassification detection emphasizes the separability between correctly classified and misclassified samples. It is important to note that a model with the best ECE score may not effectively distinguish between correctly classified and misclassified samples [Ref. 31]. We consider misclassification detection as a more practical way to measure the reliability of confidence estimation. This is because human experts, such as doctors, can pay more attention to misclassified samples (e.g., misdiagnosed patients), thereby avoiding potentially disastrous consequences. Therefore, our primary focus here is on misclassification detection and ECE is not a proper metric under such a framework.

Q3: Why not compare the method to Temperature Scaling (TS)?
A3: As presented in Ref [31], Temperature Scaling (TS) does not improve misclassification detection. This is probably because TS divides the logit output by a temperature T, thereby increasing or decreasing the confidence for both correctly classified and misclassified samples by the same scale. TS is an effective method for improving ECE because ECE focuses on aligning accuracy and confidence, and the optimal temperature obtained from a validation set can make the average confidence match the average accuracy within a bin. Therefore, TS is a standard baseline under calibration framework but not a proper baseline under the misclassification detection framework.

Q4: DMR is more calibrated than DE because its accuracy is worse.
A4: Classifierâs accuracy is not correlated with estimated confidence. The estimated confidence by the MSP (Maximum Softmax Probability) for individual classifier is inferior to that of DE. In contrast, the estimated confidence by our DMR for individual classifier is better than DE.

Q5: The submission does not mention open access to source code.
A5: We have mentioned to relase the source code in Abstract.

Response to reviewer #3:
Q6: Add accuracy and computational cost.
A6: Thank you for the suggestion! We will add them in the new version.

Q7: Perform experiment on medical image dataset under distribution shift.
A7: Thank you for the suggestion! We will do it.

Response to reviewer #4:
Q8: Inconsistent Performance Assumption
A8: We would like to clarify that, our theoretical foundation assumes that not all individual members but only the randomly selected member shows lower or equivalent accuracy than that of the ensemble. This is often valid in practice. More discussion and relevant experiments will be performed for the randomly selected member with significantly lower or higher accuracy compared to the ensemble model.

Q9: Limited Scope of Ensemble Evaluation
A9: We appreciate the suggestion and will perform experiments on correlated ensembles formed by methods like MC-dropout (with positive results expected to be achieved).

Other comments will also be carefully considered for further improvement of the paper."
https://papers.miccai.org/miccai-2024/202-Paper0483.html,"We thank the reviewers for their constructive feedback. We appreciate the recognition of our approachâs novelty (R4), its significance for ultrasound (US) imaging (R5), the conducted experiments (R3, R4), ablations (R3), and the manuscriptâs clarity (R3, R4, R5).
Our main contribution is an unsupervised deep learning framework for enhancing US image analysis by providing semantically meaningful image segments (R3, R4, R5). Ablation studies show our method outperforms the baseline [1] on three US datasets(R3), leading to better or comparable Boundary Recall (BR) and Undersegmentation Error (UE) for segmentation tasks than classical unsupervised methods, providing solid experimental evaluation (R4).
Unlike [1], which uses natural images, our method is tailored for the US domain. We introduce US patch-wise affinities based on SSD and MI metrics, commonly used in US segmentation and registration tasks. This differs significantly from color-based affinity matrices for natural images, as US images are grayscale. Thus, we improve segment distinctiveness and achieve higher DICE scores compared to [1]. Our semantic clustering step incorporates sequential coherence, considering the sweep nature of US data, and uses shape and positional priors for better cluster label assignment. Additionally, we include preprocessing steps like denoising, histogram equalization, and blurring rather than standard image normalization. Regarding novelty (R3, R5), to our knowledge, this is the first demonstration of using an unsupervised DL-based pipeline to obtain semantically meaningful segments in US imaging, traditionally reliant on fully supervised methods. This is crucial given the challenges in acquiring US labels. Initial meaningful, automatic labels from our pipeline can open the paths for their extensive use in downstream tasks.
Regarding comparison with other SOTA methods (R4), we acknowledge several recent works. Methods like TokenCut [Wang, Y., et al.,2023] and CutLER [Wang, X., et al.,2023] are conceptually similar to ours, but focus on single-object and instance segmentation, respectively, making direct comparisons challenging, whereas DSS baseline [1] aligns with our goal of multi-class segmentation. Due to the impossibility of fair comparison with TokenCut and CutLER, we focused on zero-shot unsupervised methods that divide images into multiple segments (superpixels) without prior training, similar to ours. Approaches like SLIC and FZ, though older, are still used as baselines for superpixel evaluation in recent papers on breast ultrasound segmentation [Daoud, M.I., et al.,2019; Huang, Q., et al.,2020; Ilesanmi, A.E., et al.,2020], making them reasonable for comparison.
In response to R5, we acknowledge that unsupervised clustering is just one part of US image interpretation. We conducted extensive evaluations on three US datasets, all annotated by clinical experts. CCA and Thyroid datasets have images from 24 and 28 health volunteers, respectively; the public CAMUS dataset has 50 cardiac patients. Our evaluation encompasses both per-image mask evaluation and semantic evaluation post-clustering, using metrics such as DICE, BR and UE. These thorough evaluations on expert-annotated datasets demonstrate our resultsâ clinical relevance and potential effectiveness.
We thank the reviewers (R3) for notation suggestions. E^TE denotes the multiplication of an orthogonal matrix with its transpose, yielding an Identity matrix I, not a scalar.
Minor comments will be clarified in the paper, and the code will be publicly available upon acceptance.
Once again, we appreciate the reviewersâ acknowledgment of the methodâs novelty (R4), experiments (R3, R4), and purpose (R5). We hope our proposed âDeep Spectral Methods for Unsupervised Ultrasound Image Interpretationâ opens new paths to the MICCAI community.

[1] Melas-Kyriazi, L, et al. âDeep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization.â CVPR2022"
https://papers.miccai.org/miccai-2024/203-Paper0551.html,"R1, R3, R4: Thank you very much for the useful comments that have helped us to further improve our work.

[Q1] Code and Reproducibility. 
[A1] Our apologies for not releasing the code during initial submission. Unfortunately, the rebuttal rules now forbid adding new files or links. However, we will release a GitHub link in the final version.

R1:
[Q1] Clarification of modelâs usefulness. 
[A1] Kindly note that our method is not an image transformation approach such as regular image-to-image transfer or images-to-volume transfer by using U-Net. DIP using 3D U-Net is like a generative approach, directly generating the 3D target volume from the noise. Then the observed images are generated from the estimated 3D volume and the loss is calculated between the simulated images and observed images. The loss is back-propagated to train the DIP model iteratively. Hence, figure 1 correctly shows this pipeline.

[Q2] Synthetic vs. Real-world Data.
[A2] Please note that DIP is a generative model that is trained for sample by sample. The model is not pre-trained such as the image-to-image transformation model by regular U-Net. Hence there is no question of domain gap between the synthetic and real data.

R3:
[Q1] Classical iterative vs DIP method. 
[A1] Please note that we do not claim the âclassical iterativeâ method as our proposal. It is just an example of the classical explicit regularization model, TV prior, to be compared with the characteristics of the deep learning-based prior, DIP, in our proposed methods.

[Q2] Open-Scivis dataset clarification. 
[A2] Kindly, note that, to the best of our knowledge, there is no dataset for non-linear observation images with a bright field imaging model with ground truth voxel attenuations. Hence, we use Open-Scivis as the ground truth and assume arbitrary attenuations on the 3D shape, then apply our observation model to simulate the imaging of multi-focus images captured by bright field microscopy. The Open-Scivis dataset consists of a total 55 samples and we randomly sampled 13 samples for all our experiments.

[Q3] Discussion and comparison with generative AI baselines.
[A3] Thank you pointing this out. Please note that one of the major challenge of our work is that it deals with translucent microscopic objects. Unfortunately, we are not aware of any GAN / diffusion-based methods that deal with such objects, as they mostly deal with opaque objects. However, we do plan to identify and modify existing generative AI methods for our setting and compare with them in an extended work in future.

R4: 
[Q1] More thorough validation of the reconstructions using fluorescent images?
[A1]  Thank you for the suggestion. Kindly, note that, we can also apply DIP for fluorescent images. However, the main contribution of our paper is applying DIP to attenuation volume reconstruction from the bright field images.

The imaging model for fluorescent images is modeled by the linear convolution in which the voxels themselves emit fluorescence and the rays from the voxels form a blur of the captured images based on different focal settings. Note that most DIP-based papers deal with linear observation or degradation models, such as deblurring, denoising, and super-resolution in 2D and 3D reconstruction.

On the other hand, our attenuation voxel reconstruction from bright field microscopy is similar to CT reconstruction in which the light ray comes from outside of the object, and the rays are attenuated by the voxels along the ray direction. It is non-linear as modeled by Equations 1 and 2 in the main paper. Hence, our contribution is to show that DIP is applicable to such an non-linear imaging model also.

[Q2] About code and typos.
[A2] We will address these issues in the final version."
https://papers.miccai.org/miccai-2024/204-Paper2857.html,"We thank all reviewers for their insightful and constructive comments. We appreciate the positive feedback that our work is âinteresting and very relevant to the fieldâ (R1), âtechnical soundâ (R3), and that incorporating PCA and relaxometry losses in a DL framework is novel and âhas not been studied beforeâ (R4), supported by âa thorough set of experiments and significant gainâ (R3). Other comments are addressed below:

Evaluation metric (R1,R3): We acknowledge the reviewersâ concerns regarding our evaluation metricâs definition and clinical relevance. We apologize for the confusion caused by assuming the readerâs familiarity with the âfitting SDâ metric, which is commonly used in the CMR community to evaluate mapping quality [6,10]. Instead of evaluating tissue heterogeneity (SD across T1 in ROI), the fitting SD measures the quality of curve fitting at each pixel. It transforms the pixel-wise fitting residual error (of signal intensity) to that of fitted T1 value, using the inverse Hessian and analytical Jacobian of the signal model. Therefore, well-registered images yield low fitting errors thus low fitting SD, indicating good motion correction. This fitting SD is comparable to the pixel-wise R^2 metric in [7], but offers better clinical insight as it has the same physics unit (ms) as T1. Given the difficulty of obtaining in vivo ground-truth T1, fitting SD has been widely used as a surrogate metric of registration quality for CMR [22,24,Tilborghs(MEDIA2018)]. We will elaborate on this metric in the revision.

Contribution of PCA loss (R1,R3): Though initially introduced in [8], the PCA loss is integrated into a DL framework for the first time, to the best of our knowledge. This brings two major merits: first, the DL-based optimization of PCA loss is much faster (~20s) than conventional optimization on spline parameters (~600s); second, the amortized optimization in DL enables direct inference, as shown in our experiments.

Small dataset (R1): The dataset is a real-world clinical dataset of 50 subjects, where each underwent pre- and post-Gd scans at three ventricular levels, containing 300 slices (3300 images). Our one-shot and TTA tests showed the methodâs efficacy with limited data.

Method comparison, applicability to other sequences (R1): We mainly focused on benchmarking with established deep learning-based methods [2,28]. We used Elastix [11] as an initial baseline and tested our method on other qMRI sequences from our physicist collaborators. We observed better quantitative performance compared with Elastix and broader applicability to other sequences, but for brevity we did not include the results in the paper. We will modify the claim in the revision.

Registration quality (R1): We agree that the improvement is visually subtle as we did not intentionally select cases with large motion. However, we could still observe improvement at the LV septum and RV wall in Fig. 4b, where high fitting SD values indicate uncorrected motion. The effect can be better appreciated in moving mode (supplementary avi files).

Title (R1): Following the suggestion we will change the title to âPCA-Relax: Template-free deep-learning-based groupwise registration for motion correction in quantitative cardiac MRIâ.

Non-differentiability of LS (R3): The T1 loss [8] is itself differentiable, but fitting optimizers require up to thousands of iterations, lacking practical differentiability [5,27,Hallack,Tilborghs]. In contrast, our mapping module replaces the iterative optimizer and is directly differentiable. We will clarify and add references.

Pretraining of mapping module (R3): The self-supervision learning followed [29] (Eq. 5) without prior estimation of signal parameters.

Linear assumption (R4): The linearity of PCA on the signal model will be stated in the revision.

References (R4): Thank you for suggesting the references and we will include them.

Parameter N (R4): N is fixed during image acquisition and is independent of our pipeline."
https://papers.miccai.org/miccai-2024/205-Paper2088.html,"In this study, we share a lightweight framework called DeepRepViz for the neuroimaging research community. The goal is to provide practitioners analyzing large (n>10000) neuroimaging datasets, with a tool to debug, understand, and diagnose their predictive deep learning (DL) models. We offer the tool as a Pytorch wrapper that can be easily integrated with any ongoing Pytorch DL project to visualize the modelâs latent (penultimate) representation and draw qualitative insights, or compute Con-scores. The large neuroimaging datasets such as UK Biobank, ENIGMA, and NAKO contain a battery of psychometric, sociodemographic, and data acquisition information related to the subjects of the dataset. With DeepRepViz we offer a way to use all of this available information to systematically probe and visualize the DL modelâs latent space.
Therefore, for the research field using predictive DL models with large neuroimaging datasets, we provide a simple and easy-to-use add-on tool to train DL models transparently (please refer to Fig 1 where we demonstrate it with a clinically relevant application). 
We greatly appreciate the reviewers for their constructive and valuable feedback. We would like to acknowledge the key criticisms individually below and provide justifications where possible:"
https://papers.miccai.org/miccai-2024/206-Paper3887.html,"We thank the AC and reviewers for their time. Most comments are positive and supportive, highlighting our contribution to âadapting the Gaussian Splatting for deformable surgical scene reconstructionâ with a ânovel and inspiring designâ. Our experiments âdemonstrate the effectiveness and efficiency of the proposed method.â

To R3:
Regarding future research, current progress in using Gaussian Splatting (GS) to drive SLAM has drawn our attention because accurate endoscopic camera poses are unavailable but of great importance. However, existing GS-based SLAM solutions fail to model deformable tissue surfaces, making them inapplicable for medical use. Thus, we intend to explore how to integrate our deformable GS into GS-based SLAM pipelines to model wider and more complex surgical scenes.

Regarding reproducibility, we will release our source code and share the link in the camera-ready version.

Regarding the failure cases, unfortunately, we cannot add new experiments according to the official instructions. But GS, as an explicit 3D representation, is inherently vulnerable to occluded regions where no RGB(-D) ground truth can be used to supervise the model optimization. Therefore, failure reconstructions happen when significant and long-lasting occlusions exist during video collection.

To R4:
Regarding the formatting issues and memory consumption, thanks for pointing out them and the suggestions. Due to efficient parameterization of deformation, FDM merely takes 50%~60% of the memory cost compared to EndoGaussian during training. We will update them in the camera-ready version.

Regarding the number of iterations, we truly set it as 3000 and removed the coarse stage in the original 4DGS paper since it was found to be less contributing to the reconstruction performance. This iteration number is significantly smaller than the number in the original 4/3DGS papers as the latter focuses on monocular videos with longer duration and RGB images only. On the contrary, surgical clips in the dataset for endoscope 3D reconstruction are relatively shorter. Meanwhile, there exists stereo information that can be leveraged to initialize the Gaussian point cloud with rich 3D geometric priors and thus largely ease the 3D information modeling.

To R6:
Regarding the concerns about incremental performance gains v.s. the concurrent work EndoGaussian, we would like to highlight that our research goal is to make GS reconstruction as fast as possible. According to our experiment, the proposed solution reduces over 50% of the reconstruction time while maintaining commensurate reconstruction quality with EndoGaussian. As shown in Table II, our method yields higher quality within a limited training time, which makes it highly efficient and better caters to intraoperative use.

Moreover, for distinct advantages, our FDM benefits from the nature of globally modeling the temporal deformation. Per-Gaussian deformation at various timestamps shares the same parameters (i.e., weights and learnable basis functions). Thus, deformation at one arbitrary timestamp can be naturally interpolated by deformations modeled at other timestamps.  As a comparison, Hexplane-based methods (e.g., EndoGaussian and LerPlane) locally model the temporal deformation in grid structures without capturing long-range information. Thus, our method is more robust to missing frames since parameters updated at other frames still contribute to the overall deformation model.

Regarding the capability of dealing with long videos, FDM has flexibility by changing the number of learnable basis functions and therefore, has great potential to handle various video durations and deformation levels. Also, we are constructing more challenging datasets and will explore them in future work.

Regarding the order of sections, thank you for your suggestion and for pointing out the mismatch. We will correct the orders and make changes accordingly."
https://papers.miccai.org/miccai-2024/207-Paper1643.html,"(1) Model with 2.5D rather than 3D approach (R3, R6): Building a network that simultaneously performs three tasks (segmentation, motion estimation, and artifact correction) resulted in a complex and resource-intensive architecture. Due to limitations  in our hardware memory allocation, we opted for a 2.5D approach. Incorporating adjacent slices assists in the segmentation process by providing additional information to the model. Therefore, we combined three consecutive 2D slices to segment the center slice. To determine if the 3D volume is maintained, we computed the MS-SSIM value (0.9658) between the motion-clean and motion-corrected 3D images, derived from combining 2.5D images and the 3D motion-clean image. While a full 3D model would ideally capture even richer contextual information, the 2.5D approach offers a practical solution with good performance given our hardware constraints. 
(2) Scanning details about the dataset (R3, R6): The private dataset was acquired on a 3T MRI scanner (MAGNETOM Skyra, SIEMENS, Germany) with following parameters: echo time (TE) of 2.3ms, repetition time (TR) of 2400ms, flip angle of 8Â°, and FOV of 230 x 230 [mm]. The axial image resolution was 1 x 1 [mm] for the public dataset and 0.7 x 0.7 [mm] for the private dataset. To ensure uniform resolution in both datasets, we standardized the image size to 256Ã256 pixels during preprocessing. We will include these details either within the main manuscript or as supplementary material upon acceptance.
(3) The impact of the deformation field and its visualization (only R5): The deformation field measures the difference between the motion-clean image and the motion-corrupted image. The deformation field has higher intensity in regions where obvious differences occur, particularly at the boundaries of the brain. Due to space limitations in the current manuscript, we omitted the deformation field image. However, we will include it in the final version for publication. A multi-task learning strategy involves training the joint motion estimation and segmentation network simultaneously, optimizing the loss functions together. Additionally, the implementation of the cross-stitch module facilitated the exchange of feature maps between other activities, resulting in improved performance.
(4) Purpose of motion simulator and the usage of paired dataset (only R5): The private dataset consisted of unpaired motion-clean and motion-corrupted image. We employed the motion simulator to produce motion-distorted image for the private dataset. The objective of motion simulation is to construct a dataset that has been intentionally altered to obtain detailed information about the motion parameters.
(5) Comprehensive statistical analysis (p-value, STD) (R3, R5): According to the provided statistics, a p-value of less than 0.05 was deemed to be statistically significant. The standard deviations of our suggested network were as follows: 0.101 (CSF, Dice), 0.102 (Gray Matter, Dice), 0.0658 (White Matter, Dice), 0.0466 (MS-SSIM), and 4.172 (MSE). We will include these STD measurements in the revised version upon acceptance.
(6) Dataset utilized for Table 1 (only R6): The quantitative evaluation involved the use of both private and public dataset. We will make it clearer in the revised version.
(7) Explanation of the formula 10 (Ladv(Dis)) (only R6): The formula represents the procedure for training using adversarial loss. The adversarial loss is utilized to distinguish the real and fake images in translational mapping. Moreover, âDis(.)â denotes the discriminators, whereas âEâ represents the expectation operation applied to each distribution. 
(8) Comparison with other models (only R3): We have obtained additional comparison results by using model that involves segmenting brain tissue after motion correction. We will include additional comparison in the supplementary material. 
(9) Reproducibility (only R3): Our code is available on the GitHub website."
https://papers.miccai.org/miccai-2024/208-Paper3762.html,"We appreciate the reviewersâ feedback and have addressed their comments to improve our paper. As reviewers pointed out, our Deform-Mamba network is an innovative architecture that introduced the Mamba vision system to SR for the first time. We hope our revisions address their concerns and kindly ask a re-evaluation of our score.
R#1:
Thanks for your positive comments. We take your suggestion on how the proposed method outperforms transformer-based models into account in final version. Compared to Transformer-based methods, our approach is suitable for both small and large datasets. Transformers usually require large datasets, risking overfitting on smaller ones. Our method shares parameters between time steps, learning from less data and preventing overfitting. For large datasets, Transformers need substantial computational power and struggle with ultra-large datasets, while our method is more efficient, requiring fewer computational resources. Regarding your comment about extending clinical implications of our results, we plan to test our method on more clinical datasets with other types of imaging, such as PET and CT. We have close collaboration with radiologists and plan to conduct studies with our hospital partners, some of whom are authors of this article. We add this point in our conclusion. 
R#3:
1)Thanks for your positive comments. As you point out, vision Mamba model has advantages in modeling long-range dependencies effectively with linear complexity. Apologies for not highlighting this advantage in our paper. Mambaâs hardware-aware algorithm processes data linearly with sequence length, significantly boosting computational speed. The self-attention in Transformers has a computational complexity of O(nÂ²d), while Mambaâs complexity is O(nd), where n is the sequence length and d is the token embedding dimensionality. [9] has been proven that Mamba has shown superior performance compared to Transformers. In our experiment, we compared our approach to the Transformer-based HAT model in computational complexity and running time. HATâs cost in Multi-Adds for a 64x64 input is about 5x higher than ours, our method is 2x faster in training, showing superior efficiency. We add this information to our final version. 
2)Concerning the proposed MVC module and CELoss, the performance of the pure vision Mamba network has already surpassed most methods. The MVC module at the bottleneck layer can enhance the modelâs capability to interpret complex MRI data, while CELoss can enhance the reconstruction of edge-related and contrast specific content(Fig.2). Although each module contributes a little, the combination of different modules resulted in competitive performance.
3)Concerning the two MRI datasets, IXI dataset is used for brain, and fastMRI for knee. Knee images feature simple textures and fairly clear contours, with data evenly distributed. Brain images are complex, with indistinct white and gray matter contours both physically and physiologically, occupying 2/3 of the image center. The rest is noise-filled black background. The difference in the average values of these two zones is very large, thus slightly disrupting Mambaâs attention mechanism. We will add this discussion in final version.
R#4:
Thanks for your positive comments. For your two concerns, we clarify point by point. First, regarding the CELoss, we designed three convolution kernels to extract features where E1 enhances horizontal and vertical edges via neighborhood differences, E2 targets diagonal edges through diagonal differential calculations, and E3 boosts local contrast by comparing the central pixel with all neighborhoods. In this way, the CELoss enhances edge texture and contrast in MR images. More details are added in final version. Second, regarding experimental validation, as we cannot perform further experiments, we invite you to view our supplementary material in the first submission in which quantitative results on 4x and 2x upsampling are provided."
https://papers.miccai.org/miccai-2024/209-Paper0179.html,"We thank the valuable feedback and positive comments about the novel framework (R3, R4, R5); extensive experiments (R5) and good results (R3).

We noticed suggestions for additional results. MICCAI doesnât allow new results in rebuttal, but we are willing to include more details in the final paper or appendix if permitted.

More Quantitative Results (R3, R4, R5): Our primary results (Table 1) are to compare with the existing benchmark [5]. For a more intuitive comparison, we presented qualitative findings (Fig. 3) to show the generalization and the preservation of 3D details, showcasing the superiority in helping downstream tasks (R5). More quantitative results about 3D consistency could be included in the final version if permitted.

Baseline Selection (R3, R5): Flow-guided or flow-edge guided methods were not included initially because the typical brightness constancy assumption for flow estimation is often violated in endoscopy due to frequently changing lighting conditions [15]. Other depth-guided methods were not compared because [6] and [7] require LiDAR data and uncorrupted ground truth, respectively, which cannot be obtained by current endoscopic cameras. Additionally, [8] has not released its code and model.

Table 2 (R3, R5): In a deep neural network, shallower layers learn low-level features (e.g., textures), while deeper layers learn high-level features (e.g., semantics) [Ref1]. In Table 2, three variants are tested with the same number of layers but different layers for depth estimation. The results show that using all layer features achieves the best performance, demonstrating the necessity of our current STGDE design.

Ref1: Hu, H., et al. âLearning implicit feature alignment function for semantic segmentation.â In: ECCV, pp. 487â505. Springer, 2022.

For R3:
R3-1. Temporal Consistency: We respectfully argue that warping errors and VFID are not convincing in our task. Warping errors rely on flow estimation, which suffers from frequent lighting changes in endoscopy, making it noisy. For VFID, recent research [Ref2] has shown that it focuses more on individual frames rather than temporal dynamics. Thus, we qualitatively demonstrate temporal consistency by showing near frames with less corruption in Fig. 1. We will release video examples for further demonstration on our GitHub page.

Ref2: Ge, S., et al. âOn the content bias in FrÃ©chet video distance.â In: CVPR, 2024.

R3-2. Ground Truth: We used the pseudo ground truth provided by [5]. [5] establishes pseudo-ground truth by detecting corrupted regions and shifting the mask on the same frames.

R3-3. DED: It includes 3D convolutions for temporal assessment.

For R4:
R4-1. Reference Frames: The caption âReference frames â¦â in Fig. 1 refers to specific cases showing inpainted content consistency. In our inference, we automatically sample 10 reference frames around the corrupted clip (one every 5 frames) without manual selection, similar to [20]. Other corrupted frames provide varied uncorrupted regions due to frequently changing corrupted regions, giving the framework more context.

R4-2. Pre-trained Model: The corrupted regions on input frames are masked out, so the pre-trained estimator [15] cannot provide convincing depth maps (Fig. 1 of our appendix). In contrast, our STGDE estimates depth better.

R4-3. Adversarial Training: Our DED evaluates the fidelity of the entire inpainted frame against the ground truth.

R4-4. Ablation Study: We removed each module to analyze its influence on our framework and are open to trying your suggested ablation.

For R5:
R5-1. Online Inference: We modified the sampling to include 5 frames before the corrupted clip. [5] samples every 10 frames as a reference throughout the whole video, resulting in many more reference frames for longer videos than ours. Even if only sampling past frames, its inference speed cannot meet the needs of online inference.

We will also follow the reviewersâ other suggestions carefully."
https://papers.miccai.org/miccai-2024/210-Paper0310.html,"We appreciate valuable and constructive comments from all reviewers and we are encouraged by the positive comments including ânovel (R6) and interesting (R3)â, the constructed datasets âbenefit the community (R4&R6)â, and our method achieves âoptimal performance (R3&R4)â. Below, we address specific concerns."
https://papers.miccai.org/miccai-2024/211-Paper2444.html,"We thank the reviewers for their valuable feedback, comments, and questions. We summarize several major opinions below:

Currently we experiment with three different models: GPT4, Gemini-pro-vision, and LAVA fine-tuned. For additional context, this dataset was used in a shared task and a majority of models and complex systems did not outperform our three baselines/LVM-based solutions. The main intention of this paper is to provide dataset construction details and several baseline starting points (with reasonably high performance models) for which shared task papers/other systems can refer and compare to. We believe although there are more models that we can test, the current systems are reasonably strong general baselines for reference and that the creation of a more complex system may be better left for future work.

If accepted, we will use the additional space allocation to describe future directions of development such as employing other foundation models (e.g. BiomedCLIP, Florence) fine-tuned on a set of synthetically produced  dermatology VQA data. Furthermore, experimenting with adding image segmentation and normalization, and intermediate image characteristic extraction may provide additional performance gains.

The dataset was curated from two online sites, as described in Section 3. In the first site (iiyi), responses are from platform-enrolled doctors. For the second subset (reddit), the responses were written by certified US medical dermatologists. Additionally, medical annotators were employed to ensure images included relevant content, quality was sufficient to use, and did not contain inappropriate content (e.g. genitalia, or user-drawn markings). For the iiyi dataset, medical annotators ensured that posts had medically relevant advice.

In our manuscript, we provided high-level information related to the data creation methodology and statistics. In our dataset release, we will provide further details, including guideline instructions, related to the data curation.

Some related works discussion is relevant to the comparison of the final system scores in the results. We will move this discussion to the results section.

If accepted, we will use the additional page to add additional commentary on how we may employ certain techniques to overcome some of the challenges of this problem. For example, additional future directions include experimenting with other foundation models, such as BiomedCLIP, and fine-tuning on a set of synthetically produced  dermatology VQA data. Furthermore, experimenting with adding image segmentation and normalization, and intermediate image characteristic extraction are potential avenues of future work.

Other medical VQA specialties such as radiology, ophthalmology, and pathology have established datasets and more narrow labels.  For example, previous workâs labels were largely 1-2 words with little variation. Part of the contribution of this work is the creation of a new dataset specialty that has completely different expected QA output. However, we agree, this would be an interesting direction for future work.

We agree this would be an excellent area of interest for future work.

If accepted, we will use the additional space to provide more context for these issues."
https://papers.miccai.org/miccai-2024/212-Paper1496.html,"We sincerely appreciate your insightful comments and positive feedback on our paper. We are glad that you recognize the novelty and effectiveness of our method. We address the main concerns as follows:
ï»¿
Quantitative analysis (R1): We will include more metrics like Hausdorff distance to comprehensively evaluate the segmentation performance. The min/max values and ranges will be added to provide a detailed analysis.
PRIM vs. Mask Scoring R-CNN (R3): Thank you for pointing out the similarity between PRIM and Mask Scoring R-CNN in terms of decoupling mask generation and IoU prediction. We apologize for not making the connection in our original manuscript. However, it is important to note that our motivation differs significantly. The primary goal of our method is to decouple the mask generation process in SAM from the influence of prompts, thus improving its robustness to low-quality prompts in automatic segmentation scenarios. In contrast, Mask Scoring R-CNN aims to improve the post-processing step by adding an IoU prediction branch. Moreover, the IoU prediction in PRIM follows the same mechanism as in SAM, serving as a quality filter for generated masks. Thank you for the insightful comment.
Evidence of the âcoupling effectâ (R3): We will provide both theoretical and empirical evidence to support our claim. Due to the space limit, we did not show the quantitative results of prompt quality on output deviation but will include it in a future journal version.
Computational cost (R3, R4): We will compare the parameter count, inference time, and memory usage of DeSAM with other methods. Results show that the overhead introduced by PDMM is minor compared to the performance gain. DeSAM can be trained on personal devices with entry-level GPU since our approach does not rely on tuning the heavyweight image encoder. During training, the video memory usage was approximately 7.8 GB.
Discussion of related works (R4): We will expand the comparison between DeSAM and other SAM-based methods, emphasizing the novelty of our decoupling strategy in enhancing robustness and efficiency.
Typos and writing (R1): We appreciate your meticulous reading and apologize for the typos in our manuscript. We have carefully proofread the revised version and corrected the errors as follows:"
https://papers.miccai.org/miccai-2024/213-Paper0889.html,"We sincerely appreciate the constructive feedback from all three reviewers, who acknowledge that our paper addresses significant issues in the medical field (R1, R3), introduces novel methods (R1, R4), and provides comprehensive experiments (R4). In this paper, we employ VQA as the primary task for representation learning and utilize two contrastive learning losses to facilitate modality alignment. 
Differences between QFT and QFormer (R1): Thanks for your interest in our model. There are two differences: 1) We use a separate text encoder for better encoding rather than sharing weights with the adapter. 2) QFT is initialized with the same pre-trained weights as text generator rather than with a pre-trained BERT. We believe this will facilitate QFT to convert visual information to features that text generator can understand. 
Differences between QCL and CL (R1): CL uses global feature to represent text and images, which contains one visual or textual token. QCL uses the output from QFT instead of global visual features for calculation. These features contain multiple visual tokens, providing fine-grained information for answer generation.
Why QCL works (R1): Through CL, paired images and texts are mapped to the same regions. This method brings the distributions of two modalities closer to each other, thereby narrowing the modality gap and promoting alignment. This approach is widely applied in image generation. For instance, DALL-E uses CLIP to encode text, enhancing the textual understanding of image generator. We employ a similar manner: we utilize QCL, a modification of CL, to narrow the distribution gap between image and report. This approach is beneficial for the language model to utilize visual features as the language model is pre-trained on a text distribution. After removing QCL, the performance in report generation shows an obvious decline (five percent in BLEU4). We believe this is because QCL loss is calculated with quasi-textual features, which are used to generate answers, while CL only affects global features, which do not directly participate in answer generation.
Why there is less research exploring VQA for pre-training (R1): This is because building a targeted VQA dataset is cost prohibitive as it requires doctorsâ annotation. Public VQA datasets contain diverse questions, making it difficult for the model to focus on the targeted aspect. We construct our VQA dataset based on medical reports. Medical reports offer accurate interpretations of medical images, ensuring the VQA datasetâs correctness without doctorsâ involvement. We can also design the questions based on what is important in clinics.
Existence of similar works (R3): Thanks for pointing out works similar to our paper. We are sorry about the confusion. In the two papers mentioned in the review, VQA is used for train a question-answering system. In our paper, we use VQA to learn a generalized medical representation. Our paper differs in the following aspects: 1) After pre-training, the pre-trained weights of our model are transferred and fine-tuned to enhance the performance of various tasks, including object detection and segmentation, rather than through instruction tuning. 2) We construct a targeted VQA dataset based on medical reports rather than utilizing public datasets. 3) We design our model in a contrastive manner to facilitate modality alignment, rather than directly conducting language modeling.
Trainable parameters of the model and the use of text generator (R4): Thanks for the point. There is no parameter frozen in the training process. The text generator is used to generate answers and reports. 
We are grateful to the reviewers for their comments. Due to the limitations in the length of the paper/rebuttal and the restrictions in adding experiments, we will further explore more details in future work, such as QA pair details (R1), standard deviation (R3) and conducting doctor evaluation on the generated reports (R4)."
https://papers.miccai.org/miccai-2024/214-Paper2521.html,"We highly appreciate the reviewersâ insightful feedback. We are encouraged that they found out our self-distillation prompting strategy (R4, R5), the clarity of our writing (R5), and the promising proposals we have put forward (R6)."
https://papers.miccai.org/miccai-2024/215-Paper0372.html,"We are grateful to the reviewers for their time and constructive comments. We are glad to see that all reviewers agree on the importance of noisy label detection (NLD) for developing medical AI models and the strength of our evaluations.

Clarifications on noise settings for CIFAR10N (R3)
We would like to clarify that CIFAR10N is a variant of CIFAR10 with multiple sets of real-world noisy labels taken from crowdsourced annotations [25]. We selected the âaggreâ and âworstâ label sets to reflect different noise level settings. R3 also asked if the supervised feature extractor used in our CIFAR10N ablation study overfitted to label noise. We would like to clarify that the encoder is pretrained on the independent ImageNet and frozen, and is agnostic to any labels in CIFAR10N.

Datasets and evaluations (R1, R4)
R4 suggested that we introduce artificial noise to other datasets as in Mushroom to further strengthen our evaluation. We agree with R4 and will add the following previously omitted (due to space constraint) results and discussions as suggested. For CIFAR10N, we believe that the detection of real-world annotation errors serves as a better demonstration for the performance of fastReCoV (sensitivity of 93% and specificity of 99% for the âaggreâ noise level and sensitivity of 92% and specificity of 98% for the âworstâ noise level) as supposed to artificially introduced random noise. As for the real world medical datasets PANDA and HECKTOR, estimation of the detection accuracy of intrinsic noise (inter-observer variability and noisy survival labels) is inherently challenging. Without confidence in the correctness of existing labels, it is unclear how artificially introduced noise will interact with intrinsic noise. We seek to address this gap in future work, by presenting the identified noisy samples to domain specialists for verification, and investigating the best ways to introduce artificial noise, especially in time-to-event datasets. We hope this also addresses R1âs concern regarding the selection of datasets. We included Mushroom and CIFAR10N as sandboxes to lay out mathematical foundations and quantitatively evaluate NLD performance. We then included PANDA and HECKTOR to ensure that our method is practical and robust in large-scale real-world medical imaging datasets.

Comparison with SOTA, and ablation experiments (R3)
We would like to clarify that we had comparisons with SOTA for each dataset analyzed. In Mushroom, we showed that our algorithm achieved 100% accuracy and outperformed state-of-the-art general ML NLD algorithms (Table S1). Our method could also potentially rank 2nd place in the public leaderboard for NLD on CIFAR10N, but we left out this information because leaderboard algorithms fully tune the embedding model while fastReCoV froze it. In our manuscript, we described that âNaiveâ detection serves as the SOTA method for the PANDA challenge. We also mentioned that our method is the first explicit NLD algorithm for survival analysis (HECKTOR). We would also like to clarify that we evaluated the effect of different noise levels, different feature extractors, and tau in the experiments section.

Details of runtime (R4)
FastReCoV (all required runs) took 3.5mins for CIFAR10N, 3.5hrs for HECKTOR and 40.6hrs for PANDA on one Nvidia Titan Xp GPU. The algorithms are fold-independent and can be further accelerated by parallelizing multiple GPUs.

We thank the reviewers for their valuable feedback. In our revised manuscript, we will implement the suggested layout and formatting changes to better highlight key results, include runtime information, and reduce reliance on supplementary materials. As mentioned in our manuscript, we will make our code publicly available (datasets are public). We look forward to the application of our method as a plug-and-play NLD tool for the community to try out on various datasets and tasks. We are also really excited for the synergy between our algorithm and recent foundation models."
https://papers.miccai.org/miccai-2024/216-Paper3736.html,"One major concern of Reviewer #3 and Reviewer #5 is that the study (method) lacks novelty. But as Reviewer #3 pointed out, the novelty of this study lies in the novel application of an established method to established datasets. Previously, no studies have attempted to chart the effective connectome from infancy to young adulthood. Our work bridges this gap and reveals important insights about the nonlinear growth trajectories of effective connectivity (EC) during these critical brain development stages, which could serve to benchmark individual brain development and aid in the early detection of developmental disorders. Therefore, our study represents a novel investigation that is of great interest to the human connectome and brain development communities.

Reviewer #3 and Reviewer #4 have concerns about the utilization of two separate datasets, whose inter-scanner and inter-study differences may lead to the observed EC differences across age. Though such concerns are reasonable, we believe that the inter-study differences may not explain the developmental changes in EC. The BCP dataset covers the age range from 0 to 5 years old while the HCP-Development dataset spans the age range from 5 to 22 years old. As shown in Fig. 1D and Fig. 2, there is a significant difference within the same dataset (e.g., from neonatal to late infancy) and there may not be significant differences between age groups of two different datasets (e.g., neonatal and late childhood). Also, we observe a graduate increase in EC from early childhood to adolescence, which more likely reflects the age difference than inter-study differences.

Reviewer #3 has concerns about the potential side effects of connection pruning of rDCM. We inferred EC using rDCM for resting-state fMRI (Frassle et al., Human Brain Mapp. 2021) and did not use sparsity constraints to remove connections (Frassle et al., NeuroImage, 2018). Though as classical DCM, rDCM regularizes the estimates of individual connection strengths by introducing shrinkage prior of variance that depends on the number of regions, it does not remove specific connections. Thus, an analysis of how priori culling affects EC estimation is not necessary.

Reviewer #5 has concerns about missing scientific novelty. As mentioned above, our study represents the first attempt to chart effective connectome from infancy to adolescence, which reveals novel insights into the nonlinear nature of early brain development. Specifically, we find that rather than following a linear trajectory, the EC decreases first from neonatal to late infancy and early childhood before increasing to adolescence. Such a normative EC reference chart could be used to quantify normal brain development and detect developmental disorders at an early stage. Therefore, we believe our study has sufficient scientific novelty to be published by MICCAI.

Reviewer #4 has concerns that the study may not cover all potential networks of interest. As shown in the paper, we have covered all the networks in the whole brain.

Reviewer #4 has additional concerns about reproducibility. As Reviewers #3 and #5 commented, the manuscript has provided a clear and detailed description of the algorithm to ensure reproducibility. We intend to make the model source code and data accessible to the public if the paper is accepted for publication."
https://papers.miccai.org/miccai-2024/217-Paper0750.html,"We thank all reviewers for their comments. We are encouraged those reviewers found our work novel (R3,R4), clearly organised (R1,R3,R4,R5), and clinically significant (R3,R4,R5). Below we replied to their other suggestions.

To R1
(Novelty) We will highlight novelties in the revision, which include:

To R3
(Performance) While CCII and FLD are both small datasets, FLD is extremely small with 500 training cases compared to CCIIâs 2500. AG-Swin Transformer was comparable on CCII but fell dramatically on FLD. Our method stayed robust, showing better sensitivity and specificity. We will denote performance values in Fig.2 for easier observation.
(Diffusion) The consistent superiority of diffusion representation in both sensitivity and specificity across ablation studies highlighted its efficacy in small datasets. However, the size difference between the datasets suggested that diffusionâs advantages over contrastive learning were possibly limited by small datasets. We will explore if diffusion representation can yield greater improvements in larger datasets and perform t-SNE comparison of different representations in our future work.
(Training) Diffusion needs extensive training data. In our pipeline, it is used to extract 2D representation, and it is trained by a large 2D dataset of 93967 slices generated from a small 3D dataset. We will detail these in the revision.

To R4
Cluster-ViT calculated the centroid score of all the slices of the same cluster and revised their score by centroid value to reduce the calculation complexity, leading to similar slice scores within a cluster. Taking the mean value for fusion proved to improve fusion performance during our design. We will enhance the image quality in the Appendix.

To R5
(Subjective validation) Our model excels in identifying key clusters for decisions (Appendix Fig.1) and visualising significant features (Appendix Fig.3). While not directly tested clinically, it allows developers to verify if the modelâs high-performance predictions rely on biased, recognised, or novel features with experts, which is not feasible with unexplainable models. This adds significant value for further clinical AI evaluation and presents a more suitable AI for disease research. We will add discussions on the practical benefits and relevance for clinicians and researchers.
(Literature) We will strengthen our critical review.
(Data) We did not use data augmentation for all methods except for 2.5D methods, which inherently benefited from resampling augmentation. We generated eight inputs per patient without replication. We will clarify this in Section 3.2 along with the image details such as slice numbers and make the input samples available with the published codes.
(3D) A 3D CT scan was aggregated from a sequence of 2D slices. We referred to the method analysing all slices from a scan as 3D methods."
https://papers.miccai.org/miccai-2024/218-Paper1173.html,"We are pleased that the reviewers find our DiffDGSS to be novel and overall solid (R4), the paper to be very well-written with formulations that are easy to follow (R1), and the experiments to be comprehensive and demonstrating impressive performance (R1,R3,R4). There are our feedbacks for the major weaknesses:"
https://papers.miccai.org/miccai-2024/219-Paper1486.html,"We thank all reviewers for providing thoughtful feedback.

In our paper, we propose a novel approach for assessing motion severity in head CT images using the likelihood from a score-based diffusion model trained only on motion-free images. We show that our approach effectively compensates for motion artifacts by aligning the reconstructed image with the distribution of motion-free images and without using any motion-affected examples at training time. We appreciate that this contribution is positively assessed by all reviewers as being âintuitiveâ (R3), âinnovativeâ, ânovelâ, and âfree of explicit assumptionsâ (R4) as well as âvery interestingâ and having âcompetitive resultsâ despite its unsupervised nature (R5).

Complexity of mathematical formulation (R3):
We politely disagree with R3âs recommendation to reject our paper based on its perceived mathematical complexity. All equations are based on established work on score-based diffusion models that are widely used in medical image processing and referenced accordingly. Functions f(x,t) and g(t) are clearly defined in section 2.1 of the paper. We believe that MICCAI values contributions with both experimental and mathematical advancements, and that our paper aligns with these standards. Rejection solely based on mathematical complexity seems unwarranted, especially given that our work is supported by robust experimental evidence.

Missing training details (R5):
An overly concise description of the training settings limiting the reproducibility is a valid concern raised by R5. We will add the hyperparameters used for training the score network to the paper and our code will be released in case of acceptance.

Modeling of motion patterns (R4, R5):
We acknowledge the paperâs omission of key details regarding motion parameter modeling. Here is a summary of the missing points: Rigid motion parameters are directly incorporated into the projection matrices via matrix multiplication, defining the perspective projection for each acquired projection view in the CT scan. To ensure smooth motion patterns during optimization, we apply Akima splines with continuous 1st derivatives, but discontinuous 2nd derivatives to avoid overshooting. For motion simulation, 10 evenly spaced spline nodes are used across the 360 projections of a full scan. Motion estimation utilizes 30 nodes, equating to one node for every 12 projection images, which we believe adequately captures high frequencies and complex motion patterns given the frame rates of modern CT scanners. Code for motion simulation and integration into projection matrices will as well be released in case of acceptance.

Simulated measurements (R3, R4):
We use real reconstructed head CT images but simulate the corresponding measurements via forward projection. Unfortunately, there is no open-access data set containing clinical cone-beam head CT scans together with pre-reconstruction measurement data. Likewise, data sets with real motion-affected images are not openly available. However, our score network is trained on real reconstructed images, with simulated measurements used solely in the motion compensation optimization. Therefore, we anticipate minimal performance degradation on real measurement data since the network operates in image domain. Given appropriate data access, we are eager to validate this in future experiments.

Extension to 3D (R4):
The current study is focused on 2D images. Rather than applying the method slice by slice, the preferred way of extending it to 3D incorporates a full 6 DOF motion formulation. This requires a likelihood evaluation taking volumetric information into account. We hypothesize that this is possible, but leave the investigation to future work.

Missing related work (R5):
Thanks to the reviewerâs suggestion, we identified the paper âAccelerated motion correction with deep generative diffusion modelsâ by Levac et al. as a relevant reference. We will add it to the paper."
https://papers.miccai.org/miccai-2024/220-Paper0322.html,"We thank the reviewers for their time and constructive feedback on our paper. We are grateful for the positive comments on the originality and novelty of our contribution and have carefully considered the suggestions to improve our work. Below, we address the reviewerâs concerns:

Firstly we address the general comments from the reviewers:

Secondly we address specific points raised by Reviewer #4 :"
https://papers.miccai.org/miccai-2024/221-Paper0078.html,"We appreciate the time and effort the reviewers have devoted to providing their valuable feedback. We are encouraged by the reviewersâ recognition of our workâs motivation, novelty, clear presentation, and high clinical significance. Below, we present our replies to the reviewersâ comments.

Clinical Evaluation (R1 & R4): Thank you for your suggestions. We are currently collaborating closely with experts in lung diseases for the initial clinical evaluation of the proposed method. In the near future, we plan to conduct a more comprehensive evaluation of the proposed work, extending beyond CT modalities to include various tasks and assess efficacy in explaining different backbones. Given the constraints on new experiments this year, we will also include comparisons with XAI methods like LIME and SHAP in our future work.

Related Work (R3 & R4): We will address some overlooked related work and supplement the citations and discussion in the second paragraph. Reference [1] presents a novel approach that leverages an existing counterfactual generation method to produce an enhanced saliency map, aiming to improve the classification model and its attention map. This showcases another utility of counterfactual images. However, the authors did not report the outcomes of the counterfactual images they generated, their efficacy in achieving the desired classification decisions, or the quality of these generated images. We will refine our claim to being the first to âdevelopâ this kind of method for CT images to more accurately reflect our contribution and will cite this work to highlight different benefits that can be derived from counterfactual images.
[1] ACAT: Adversarial Counterfactual Attention for Classification and Detection in Medical Imaging.

Comparison to Explainable Models (R3):  Thank you for your question. We are highly inspired to compare our method to the interpretable models proposed in [2] across the modalities they use. We would like to conduct a comprehensive comparison of the two approaches in terms of their classification abilities, explainability, and the features utilized for decision-making. 
According to our past works, the classification performance of explainable models can be inferior to deep, end-to-end, but unexplainable models to some extent. This is often because explainable models are constrained to using features within the domain knowledge, sacrificing some performance for increased explainability. A significant advantage of using counterfactual explanations is the ability to achieve optimal performance and explainability simultaneously. Another advantage is that explaining deep models can potentially uncover novel features that are overlooked by humans but leveraged by these models to achieve high performance. We will further elaborate on this discussion in the final version of our paper.
[2] Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.

Minor Issue (R3): Thank you for pointing out the minor error. We will correct it and conduct thorough proofreading before submitting the final version."
https://papers.miccai.org/miccai-2024/222-Paper3391.html,"We thank all reviewers for their invaluable comments and approval that our method is novel and effective. Our code will be released to ensure reproducibility. Common questions are first answered, followed by responses to individual comments.

R3&R4: Hardware and computational cost
A: DiffRect is trained on a single 24G NVIDIA 4090 GPU and Intel Xeon Gold 5418Y CPU. The training time and GPU memory consumption of DiffRect, MCNetV2, and INCL are: DiffRect(0.24s/iter, 5612M, 71.85%Dice); MCNetV2(0.25s/iter, 3582M, 49.92%Dice); INCL(0.16s/iter, 19682M, 67.01%Dice). DiffRect shows superior performance with negligible training overhead. The low computational cost of DiffRect is attributed to (1) LFR is conducted in low dimensional latent-space with a lightweight U-Net, (2) the denoising process only requires 10 steps to achieve favorable performance, resulting in shorter training times. During inference, only a U-Net is needed thus DiffRect incurs no extra inference cost.

R1&R3&R4: Figure/table clarity, qualitative results, more relevant works
A: We will address the format and clarity issues in revision, and add discussion of relevant literature.

R1: Why consistency mask predictions do not fully grasp semantic content
A: Latent-space representations contain complex semantic content and class-wise relations, which may not be reflected in the output masks. Moreover, consistency on masks may cause overfitting to a single mode of the mask distribution, neglecting modes or variations expressed in the latent space.

R1: Noise schedule and relation to category-wise correlation
A: The noise schedule is aât=â_i=1^t(1-bt), where bt is computed from a cosine scheduler bt=cos((t+0.008)/1.008*Ï/2)^2. It enables the model to learn underlying patterns and category-wise correlations in a progressive and robust manner.

R1: Use diffusion model over flow-based model
A: Although flow-based models can estimate data distributions like diffusion models, they are not suitable for medical image segmentation as they (1) use deterministic trajectories, which have limited expressiveness; (2) require invertible architectures with topological constraints that cannot adequately capture complex medical data distributions.

R1: Dice score
A: DS=2*|Pâ©G|/(|P|+|G|).

R3: Limitations and future directions
A: Although DiffRect is efficient, it may be slow for extremely large inputs. It is also designed only for modeling data within the same domain. Future directions include exploring faster ODE solvers to improve sampling speed and investigating its robustness to out-of-distribution data.

R3: Choice of UNet
A: (1) For fair comparison, we follow [26][34] to use U-Net as the segmentation model. Due to the structured layout of medical images, both high-level semantics and low-level features are important. The skip connections in U-Net can integrate multi-level features, making it suitable for medical image segmentation. (2) Our method can feasibly integrate with other models, e.g., on UNeXt, our model also shows a higher Dice score on ACDC with 10% labels: Ours: 86.87%, Fixmatch: 79.34%.

R3: Statistical significance
A: The p-values of the improvement over INCL on ACDC and MS-CMRSEG19 are 0.0086<0.01 and 0.033<0.05, demonstrating the statistical significance.

R4: RGB mapping
A: We maximize the color difference between each encoded category to avoid semantic confusion, e.g., on ACDC with 3 fg classes, we use red/green/blue to encode.

R4: Module effect and feature visualization
A: We visualize features before LCC, after LCC, and after LFR with tSNE to validate the distribution transition of each module. The averaged class-wise variance decreases from 2.61 to 1.08 and to 0.29, indicating that LCC and LFR effectively learn the underlying class distributions.

R4: Diffusion steps
A: We ablate the diffusion steps in Fig.1 in Supp and set it to 10 for the best speed-accuracy trade-off. DiffRect is trained for 30k iterations and can be completed within 2 GPU hours."
https://papers.miccai.org/miccai-2024/223-Paper1193.html,"Initialization of deformable field (R1):
The initialization of the deformation field during the training stage follows the standard DDPM settings. In the inference stage, the initialization of the deformation field is pure Gaussian noise.

Comparison of smoothness (R1)
Besides higher dice, our method also achieves better smoothness than the compared methods. In terms of the percentage of the negative Jacobian determinant (NJD, the lower the better), our method achieves 1.74%, whereas FSDR and DM achieve 1.80% and 1.78%. A lower NJD metric is better. In terms of the standard deviation of the Jacobian determinant (SDJ, the lower the better), our method achieves 0.51, FSDR 0.54, and DM 0.53.

Difference between image vs. deformation space (R1)
For 3D MR images, image space is three-dimensional, describing the values of the voxels. Deformation space is five-dimensional, describing the sampling direction (3D) of each voxel in 3D space.

Long inference time (R3&4):
Our method is in parallel to the efficient diffusion sampling method eg DDIM, which can reduce the inference time from 6.5 mins to 0.5 mins without compromising the performance. For further acceleration, we can adopt a distillation-guided method to reduce the sampling time to 3s, which becomes competitive with the end-to-end DL methods of 0.20s without any performance decline. Importantly, rather than focusing on the inference speed, our major scope is to provide end users (eg clinicians, radiologists) with the opportunity to control the registration process by adding prior information to guide the sampling process via a classifier-guided diffusion scheme (eg control the smoothness of the deformation field, the general direction of deformation, the target organ for registration), where previous end-to-end DL model fails to achieve. Therefore, when the generated deformation field is relatively stable, the inference process can be manually stopped to further save the inference time. As shown in Fig. 3, the inference can be chosen to be terminated when the sampling has only been performed for 400 steps (T=1600).

More compared methods (R1&4):
For a fair comparison, we focus on comparing with diffusion-based or -relative registration methods, i.e. FSDR and DM. Both of them claim to be a diffusion-based method. Moreover, FSDR and DM have already compared themselves with non-diffusion-based DL registration methods, i.e. VM, VM-diff, and CycleMorph, and show significant improvements. Since our model can surpass FSDR and DM, it can also outperform those DL methods. We also didnât compare with iterative registration methods since the sampling stage in diffusion-based methods is different from the iteration of deformation fields. Specifically, the sampling process is a gradual denoising of a deformation field, and it does not involve deforming the moving image during the process. In contrast, iterative methods require deforming the moving image at each iteration and then using the warped moving image for the next iteration. In other words, iterative methods need to deform the moving image multiple times and aggregate multiple deformation fields, hence they are complementary to each other.

Transparency (R4):
We follow the previous work [1] by defining the transparency of our method as the ability that it can accept external control or intervention from users during inference (as elaborated in our fourth response). Non-diffusion-based end-to-end DL methods, such as VM, and CycleMorph directly output a deformation field, lacking the kind of process that diffusion-based models have, where external control can be applied. Therefore, traditional DL methods lack transparency.
[1] The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue 16(3). 2018

Implementation details (R4):
Swin Transformer patch size is 2, image size is 32128128, preprocessing steps is center crop and z-score norm."
https://papers.miccai.org/miccai-2024/224-Paper2125.html,"We thank the reviewers for their thoughtful evaluations and constructive feedback. We are grateful for recognizing our diffusion modelâs novelty and clinical applicability for generating ultrasound images, as highlighted by all reviewers. Reviewers found our work to be an âinnovative methodâ and a âsignificant departure from traditional approachesâ (R4) with âexcellent organization and clarityâ (R3), âclean writing style â (R5) and âcompleteness of the data analysisâ (R3), demonstrating âclinical feasibility by testing the method with different databases, from different anatomical regions and acquired with different scannersâ (R3). Such feedback underscores the potential impact of our work on the field of ultrasound image synthesis, with one reviewer remarking that our results âhave noticeably improved and could be groundbreaking in the futureâ (R4).

METHODOLOGICAL CLARIFICATIONS:
We have revised our paper to better explain the emphasis on areas close to the probe (R4&R5), ensuring the reasons for enhanced contrast and confidence in the generated images are clearly understood. Our focus on regions closer to the probe is due to their inherent clarity and detail. By emphasizing these areas, we capture the most reliable details with higher fidelity as we allocate more denoising steps to these regions. 
Further, we have also refined the definition of B-maps (R5) to more clearly communicate their role and benefit in our model so that readers without an ultrasound background have a clearer understanding of the formation of ultrasound images. Thereafter, we highlight that our proposed scheduler is designed to mimic the natural attenuation of sound waves interacting with tissues to generate more plausible B-mode images.

QUALITATIVE ASSESSMENT:
Acknowledging the suggestions from R4 regarding the qualitative evaluation, we have overlaid arrows on the images to directly highlight areas where our model has enhanced image quality. These arrows guide the viewer to specific improvements, facilitating a clearer comparison between the synthesized images with and without B-Maps.

EXPERIMENTAL DETAILS:

FID SCORES: 
The choice to use those specific layers for assessing the FID scores, rather than the final layer, is deliberate. The selected layers offer a more appropriate feature recognition capability for ultrasound images, which differ significantly from natural images. Thanks to the feedback (R5), we have modified the paper to explain that we compute the FID score using the entire validation set to ensure a comprehensive representation of the modelâs performance. Despite the apparent subtlety in visual differences, the statistical distribution captured by the FID scores confirms the efficacy of our model. We understand the concern and have made the qualitative analysis clearer, as suggested by R4 and R5.

MISC:

We trust that the revisions provided here effectively address the  reviewersâ suggestions. Our revisions aim to enhance the clarity, depth, and scientific rigour. We will incorporate all the necessary changes into the final version of the paper."
https://papers.miccai.org/miccai-2024/225-Paper1315.html,"ââWe thank reviewers R3, R4, R5 for their constructive comments and positive assessment: THOR, our novel diffusion-based method for unsupervised anomaly detection (UAD) in medical imaging âaddresses a critical issueâ (R4) in reconstruction-based UAD by enhancing coherence between input-reconstruction pairs. R3, R4, R5 appreciated our âvery strong evaluationâ across âdifferent modalities and tasks,â including brain MRI and wrist X-rays, with âeffective and convincing resultsâ, âcompared to relevant state-of-the-art modelsâ. Additionally, R5 recognized that our work âcan inspire more researchers using generative modeling for medical imaging to tailor their inference processes to the specific domainsâ.

Main points raised:

R4 asked about dataset splits, i.e., potential train/test leakage. We strictly prevent train/test leakage by using healthy slices from one group of patients for training and slices containing pathology fromdifferent patientsfor evaluation. Details are provided in our published codebase, which we will clarify further in the revised manuscript.

R3 asked about the motivation of our work. As also highlighted by R4, our main motivation is to maintain fidelity in healthy tissues during the synthesis process while effectively replacing pathological tissues with pseudo-healthy alternatives, as can be seen in Figs. 1, 2, 3, 5,  and Supplementary Figs. 1,2. We will emphasize this motivation more clearly in the revised manuscript.

R3 asked about the performance on small lesions. Measuring Dice scores (show in %) for small lesions is challenging due to the impact of small displacements and false positives. Although, our method shows substantial improvements for small lesions (103% with Gaussian noise, 44% with Simplex noise), we concur that the detection of small lesions is an important task that needs further research attention.

R4 asked about ablation studies: We conducted ablation studies on noise levels and noise type, demonstrating robustness to higher noise levels (Fig. 4), which is essential for effectively removing pathologies. Due to space constraints, additional details on sampling intervals and hyperparameters are available in our codebase. We did not optimize these for specific datasets to maintain generalizability across diverse setups. While LPIPS anomaly maps are also used in other SOTA methods like AutoDDPM, our improvements primarily stem from the novel integration of implicit temporal anomaly maps, which guide the diffusion process more effectively (see Figs.). Other components like the harmonization frequency, anomaly map computation, and dilation operations ensure no anomalies are restored during synthesis but have a less substantial impact. As suggested by R4 we will consider these ablation studies in future work.

Minor Points

ââ- R4: Domain shifts from training on different datasets are common and impact UAD performance. We included target domain images in training to mitigate these.

R4: We appreciate the contribution of Simplex noise and agree that the hypothesis about its self-supervision effect needs more validation. We will revisit this claim. However, our experiments highlight specific limitations, and acknowledging these will help the community to progress.

R3/R4: UAD only supports binary classification without detailing anomaly types. We use normal X-rays for training/validation and pathological ones for AD evaluation. All methods detect 3/4 foreign body anomalies. Soft tissue anomalies are subtle and challenging to detect on X-rays.

We sincerely thank the reviewers for their insightful feedback which we will incorporate in our final manuscript where possible."
https://papers.miccai.org/miccai-2024/226-Paper0195.html,"To all reviewers:
We thank all reviewers for the affirmation and valuable comments on our work, we have carefully revised the manuscript according to your suggestions. The point-to-point responses are listed below.

To Reviewer#1:
1). We agree with the reviewerâs opinion that using more different modalities of data can better support the generalization ability, it provides constructive insights for our future work. In this paper, we evaluated our proposed method on CT and T2-SPIR MRI for the purpose of fair comparison with existing UDA methods, as the most of existing methods reported their performance on CT and T2-SPIR MRI instead of T1-inphase or T1-outphase in the CHAOS datasets. For a fair comparison, the dataset, the amount of training and test data, and the training-test partitions are consistent with the existing SOTA methods. We partition the dataset based on the individual subjects (patient-wise) to ensure the training and testing subjects are fully non-overlapped.

2). We reported most of the SOTA results from the original paper to ensure the optimal results confirmed by the original author on the benchmark datasets. The SSM only reports the performances of the MR to CT in the original paper. And we didnât find the publicly available code for the SSM. Since we are unaware of all the implementation details such as all hyperparameters of this work, we did not reproduce and report the result of the CT to MR. And we have added the description of SSM in the section 1 (Introduction).

3). We agree that the capabilities of the diffusion models are important, many works have demonstrated the superior image generation capabilities of the diffusion models over GANs, such as [1][2]. Due to space limitations, in this paper, we mainly focus on the UDA performance. We have evaluated the impact of utilizing intermediate data distribution generated by the diffusion models on the performance of UDA in the ablation experiments.

4). We have enlarged the fonts in Fig.1, added the explanations of dashed and solid lines, and corrected the issue of singular and plural forms in the English words. We have revised the manuscript according to all points listed in the minors.

[1] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. âDenoising diffusion probabilistic models.â Advances in neural information processing systems 33 (2020): 6840-6851.
[2] Song, Jiaming, Chenlin Meng, and Stefano Ermon. âDenoising diffusion implicit models.â arXiv preprint arXiv:2010.02502 (2020).

To Reviewer#3:
1). We trained our models in stages, firstly, we trained the diffusion models on the dataset, enabling it to generate MR/CT images. Indeed, the training time for diffusion models is relatively long. We Trained the diffusion models with a batch size of 24 for 100000 iterations. It takes around 33 hours using 6 Tesla V100 GPUs with 32 GB of memory. Then, we use the trained diffusion models to generate corresponding MR/CT images based on the given CT/MR reference images. With the diffusion models, we obtain intermediate image sequences and use these images to train the segmentation network by generative adversarial learning. 
2). We have added more explanations for Fig.1.

To Reviewer#4:
1). If the space permits, we will include more visual results of the cases in the new version.
2). We have checked and revised our manuscript carefully to correct the formatting and spelling mistakes."
https://papers.miccai.org/miccai-2024/227-Paper0872.html,"We thank ALL reviewers for their valuable feedback and appreciate that they found our ideas intriguing (R1) and novel (R3) and acknowledged our qualitative evaluation by radiologists (R3) and our strong performance (R4).

R1:

We agree with R1 on the importance of analyzing the realism of the outpainted images by CT-Palette not only qualitatively by radiologistsâ evaluation but also quantitatively. Hence, we had already included the FrÃ©chet Inception Distance (FID) between ground-truth and outpainted images in the submitted supp. material. CT-Palette achieved the lowest FID, i.e., images generated by CT-Palette have the most similar distribution to the ground-truth compared to other models. 
Additionally, we will now also include PSNR and SSIM in the supp. material upon acceptance.

R1 suggested adding metrics like DSC and statistical information to Tab. 1. Please note that in the submitted supp. material, we had already included the DSC for muscle and SAT between ground-truth and outpainted images and the corresponding standard deviations. Adding the DSC scores to Tab. 1 would either require adding columns (thus using much smaller fonts) or rows (thus taking much more space). We therefore propose to summarize the DSC results in the main text more clearly. 
Weâve now additionally added standard deviations of the RMSE scores to Tab. 1 for the manuscript. In addition, weâve now conducted the Wilcoxon signed-rank test to compare the muscle/SAT area distribution of ground-truth images and that of outpainted images. We find that there is no significant difference between the muscle/SAT area distribution of ground-truth images and that of images outpainted by our CT-Palette, while there is a significant difference when the test is conducted on other models.

R3:

We agree that evaluating whether the generated slices exhibit characteristics of gender/age would be interesting. However, we highlight that weâve already included both qualitative evaluations with radiologists and quantitative evaluations. Considering the page limit, integrating these studies into the current paper would require removing some of these evaluations, which we consider more important. However, we plan to include the suggested studies in a journal extension.

R4:

While we acknowledge R4âs point that our approach is an application of an existing diffusion-based method, we argue that our work contains additional novelty through (i) the development of an efficient mask generation method (see 3rd paragraph of Section 2 âMethodâ) and (ii) a new inference scheme designed specifically for body composition analysis (see Section 2.3 âInferenceâ).

The reviewer mentions confusion regarding the multiple inference in the inference stage. We are sorry that we probably did not explain the multiple inference clearly enough. To clarify: The selection of the best image is not done by comparing each generated image to the ground-truth image, but by selecting the most representative image from the learned distribution, i.e., the ground-truth is not used in this stage. We chose to select the image closest to the median of the muscle and SAT area derived from the generated images as the best image because we found that compared to the mean, the median is a more robust metric against outliers among the generated images. Hence, the best image wonât be affected by outliers.
Regarding the reviewerâs point of the final output not being a distribution, itâs unfortunately impossible to sample all images belonging to the distribution. Therefore, weâve instead utilized the learned distribution by sampling multiple images instead of one. That way, we can select the most representative image among a larger pool of samples.
Without multiple inference, i.e., by generating 1 image, the performance of our method slightly drops in all metrics except FID but is still superior to the baselines. Weâve now added these results to the manuscript."
https://papers.miccai.org/miccai-2024/228-Paper1467.html,"Thank you for your constructive comments and recognition of our strengths, such as âDiffTCL can be promoted as a pluginâ (R1), âsufficient ablation studiesâ (R3), and âthe proposed method is novelâ (R4).

To Reviewer1: 
Our method is not limited to baseline methods and is effective on any backbone. We will compare more methods specifically designed for retinal image segmentation in future work. Additionally, we will enhance the readability of Fig. 1 and correct the citation format.

To Reviewer3: 
Thank you for your suggestion. We will clarify the technical contributions and motivations in the revised version. Additionally, we will rewrite the theoretical section on diffusion-enhanced pre-training to make it easier to understand. Finally, we will correct the OCT layer region names in Table 2 and Fig. 3.

To Reviewer4: 
We will explore extensions such as pre-training diffusion with multi-modal data and using foundation models trained on medical images as a base.

Thank you once again to the reviewers for taking the time to review our manuscript, recognizing its value, and providing suggestions that are highly instructive for our future research."
https://papers.miccai.org/miccai-2024/229-Paper1334.html,"Reviewer#1
A1: The MDM generates multi-scale spatial features S_j. Meanwhile, the TRM module extracts the multi-scale features frame-by-frame. They are averaged along the temporal axis and aggregated into multi-scale temporal features R_j, the same dimension as S_j. Then S_j and R_j are integrated by pointwise addition.
A2: The testing dataset is divided into seen and unseen categories based on data distribution. âSeenâ represents the visible case and âUnseenâ represents the invisible case. There is no intersection of âSeenâ and âUnseenâ. âSeenâ is divided into two parts, one part is used as the training set S_tr, and the other part is used as the test set S_te, i.e. case7_2 in the training set and case7_1 in the testing set.
A3: We used some of the data from the easy_seen dataset to prevent overfitting during training.
A4: Itâs impractical to extend the input to whole video frames due to insufficient memory. Our experiment uses two NVIDIA 3090 GPUs, each containing 24 GB of memory. Itâs just the right amount of training.
Reviewer#2
A1: The âbatch size of 16â means that each iteration of training will input 16 samples. The âpatch size of 224Ã224â means that the original image, for example, 1024Ã1024, is divided into small areas, which is the size of the local area used for feature extraction or analysis in image processing.
A2: L_TRM is the same as L_G. The purpose of adversarial learning is to induce the Generator, which is the TRM module in this paper, to learn the image features of a clip of frames during the reconstruction process. 
Reviewer#3
A1: Since there is still a high missing rate in the diagnosis process, this paper aims to mitigate this issue by improving the accuracy of detection. However, it is undeniable that improvements need to be further explored in the clinical setting.
A2: Thanks to the reviewer, we will improve the references in the final version.
A3: For reasons of space, we point to the reference from which the dataset originated and do not further characterize the dataset. The comparison methods chosen are classic for this task. The dimension of the mask image is a small typo and has been amended to 1ÃHÃW."
https://papers.miccai.org/miccai-2024/230-Paper3584.html,"We would like to thank all reviewers for their constructive comments and thorough evaluation of our work.
We are pleased to note the recognition of our paper as the first to consider a foundational model in hematology, leveraging DINOv2 for single-cell image analysis (R4). As R1 states, âthe impact of releasing these models should be valued as a significant contribution, especially considering that the data, models, and evaluation employed are clearly described.â Moreover, we appreciate the positive feedback on our extensive evaluation and the strong performance of our models, demonstrating robust generalization capabilities to external datasets despite batch effects (R3, R4). The effort to assemble the large-scale dataset from 13 diverse public sources has also been highlighted as a significant contribution (R3, R4). The clarity of our manuscript is rated as âvery easy to follow and to the pointâ (R4). All reviewers commended our commitment to sharing the trained models and code.
We acknowledge the questions raised by R3 regarding the methodological novelty of our paper. While our core model leverages the existing DINOv2 framework, our contributions lie in the adaptation and optimization of this framework specifically for hematology, making this the âfirst work to consider a foundational model in the hematology domainâ (R4). Extensive experiments showed that removing the global-local crop loss significantly improved the model, which has not been previously described in the literature. Our workâs contribution lies however not only in these technical adjustments but in particular in the successful application and validation of these models in a highly specialized domain. To put it into the words of R4: âIn fact, the adaptation of foundational models to different subdomains of medical imaging is indeed a critical topic to study.â Especially in hematology, where multiple instance learning is used to address disease classification problems, a robust feature extractor is needed.
Therefore, we respectfully disagree with the âStrong reject â major flawsâ rating (R3), as we cannot see any major flaws raised by R3. The given reason for this score âAlthough the assembled datasets and models might be helpful to the community, there isnât much novelty hereâ contradicts the MICCAI call for the submission of Foundation Models, which do not necessarily involve significant methodological novelties. Given that all reviewers acknowledge the value of our work to the community, we believe this should be the determining factor in advancing scientific progress. In other instances, models using DINOv2 or similar frameworks without any technical improvements have been published in respected journals [1,2,3].
We agree with R1 that some statements might be perceived as too strong and revised them accordingly to ensure clarity and accuracy. To clarify the content of Figure 2: it includes visualizations from the training set (initial UMAP) and unseen test sets (embedded in the fixed UMAP fitted on the training set). We acknowledge that a completely out-of-domain test would be favorable, however, we were not able to find an additional dataset with patient-level annotations. We added a reference indicating that the held-out Acevedo dataset exhibits a strong batch effect [4].
We believe that our work makes significant contributions to the field of hematology image analysis, both through the development of DinoBloom and the assembly of a large-scale, diverse dataset. We look forward to âthe high impact that the presented models can have in the communityâ (R1) and appreciate the reviewerâs feedback, which has helped us improve our paper.
[1] https://doi.org/10.1038/s41551-023-01049-7, Nat. Biomed. Eng. 
[2] https://doi.org/10.1038/s41591-024-02856-4, Nat. Medicine
[3] https://doi.org/10.1038/s41591-024-02857-3, Nat. Medicine
[4] https://doi.org/10.1007/978-3-031-45857-6_14, MICCAI 2022"
https://papers.miccai.org/miccai-2024/231-Paper2230.html,"We are thrilled to receive such detailed and high-quality feedback! Your comments and suggestions shed light on our path forward.

PCA clarificationBoth Reviewer #1 and #4 had questions on how PCA was performed in this work, and we hope to provide further clarification on this matter. Suppose the two images Ref and Mov are encoded into feature maps of size [H,W,D,C], we flatten each feature map into [HWD,C] and concatenate all features to obtain a [2HWD, C] matrix. PCA is performed to reduce the column size of this matrix so that the result is [2HWD, Câ], with Câ < C. The resulting matrix is then recovered into two images of [H,W,D,Câ].

Feature vs. Registration algorithmReviewer #4 expressed curiosity on the performance of recombining the proposed registration algorithm, ConvexAdam, DINOv2 features, and MIND features. Since the bulk of the proposed registration algorithm is plain gradient descent, we find it to perform similarly with ConvexAdam, which adds convex global optimization before performing gradient descent. For the abdomen MR-CT dataset specifically, rigid alignment in the axial direction might be more suitable.

Foreground patchesWe agree with Reviewer #3 that using thresholding to obtain foreground patches is not the most elegant implementation. We are actively researching better solutions. When left unmasked, the first principal component of the patch features should differentiate foreground/background. However, defining a threshold to automate the process is not as straightforward.

Again, we really appreciate the reviewersâ constructive feedback. Hope to see everyone at the conference!"
https://papers.miccai.org/miccai-2024/232-Paper1074.html,"We appreciate the reviewersâ recognition of our work as âinnovativeâ and emphasizing it is âsignificantly streamlining the diagnostic processâ. We address the reviewersâ concerns below:

Adequacy of Using MSE Constraint for Facial Reconstruction Loss [R5]
The MSE loss is computed over a dense set of 328 facial soft-tissue landmarks, which are sufficient to capture the complexity and diversity of facial geometry. This dense coverage ensures that the MSE loss adequately captures facial structural variations.

Performance on Prognathic Subjects [R3]
We acknowledge that the performance on prognathic subjects is slightly lower compared to methods using bony landmarks. However, the extraction of bony landmarks requires CBCT imaging, which poses radiation risks and is costly. Our method, leveraging facial soft-tissue landmarks from 3D facial images that can be captured by non-invasive 3D cameras, is safer and more cost-effective. Furthermore, our approach adapts an off-the-shelf 2D facial landmark detection model to a 3D scenario without needing CBCT images or extensive landmark annotations for training, significantly reducing the burden of data collection and annotation.

More Metrics and Datasets for Further Evaluation [R3, R5]
In the final version of the paper, we will include additional metrics such as F1 score, precision, and recall to provide a more comprehensive evaluation. We also plan to conduct further experiments on additional datasets to validate our methodâs performance in future works.

Gold Standard of Deformity Diagnosis [R3]
A senior oral and maxillofacial surgeon with over 30 years of clinical experience classified the subjectsâ anteroposterior mandibular positions as normal, retrognathic, or prognathic. These classifications were used as the ground truth (gold standard) for our experiments.

Open-Source Code and Data [R3, R5]
We commit to making the code publicly available upon acceptance of the paper. However, due to data privacy concerns regarding patient facial structures, the dataset cannot be publicly released."
https://papers.miccai.org/miccai-2024/233-Paper2009.html,"We thank all reviewers and are pleased that 2 of 3 recommended acceptance. We are encouraged to hear that they found significant implications (R1), incredibly cool, novel findings (R4), and interpretability (R5). Our responses to the main points are as follows:
Genotype-protein correlation (GPC), genotype-age interaction, and genotype-environment interaction (R1+4+5): The GPC module extracts meaningful genotype-protein patterns because genes and proteins are highly correlated. We separate the normal aging effect (irrelevant to AD) from the disease progression trajectory, while interaction between the gene and age still exists. The effects of SNP on AD to a certain extent depend on age, thus the interaction was included in our model. The value of genotype-environment interaction can be: (1) Choosing the best treatment for the individual to maximize response or minimize side effects. (2) Predicting potential changes to modifiable environmental factors. Thus, the direction of the top identified genotype-environment interaction will be analyzed to show the effectiveness and possible treatment.
Baseline status (intercept) and the changing rate (slope) (R1): Both the baseline status and the changing rate of disease progression for AD were influenced by genetic variations, and thus we used both of them in our model.
MCI subjects (R4): The MCI subjects in this study are all progressive ones (finally diagnosed as AD), and stable MCI subjects were removed.
Optimization and gene partition (R1+4): SNPs naturally form block structures, and the information are mainly carried by blocks, thus gene partition is reasonable for most cases. The ablation studies show that gene partition can alleviate computational challenges without sacrificing model performance.  We also conducted statistical analysis and post-analysis, and all p-values reached a significance level after correction. 
Dynamic task prioritization (R1+R4): DTP adaptively adjusts the weights of each sub-task to automatically prioritize more challenging sub-tasks. The dynamic task prioritization technique can ensure an overall optimal and this will be clarified.
Comparisons and statistical analysis (R4): All methods worked on the same inputs, and thus the performance gain was not only due to the data but also our newly designed method by combining disease progression and biomarker identification jointly. The Bonferroni correction is used to ensure robustness for multiple comparison issues in Section 3.2. We will add p values compared to the baseline method in terms of CCC and RMSE in Table 1. In Figure 1 (c), we compared every pair of groups rather than three groups (HCs, MCIs, ADs) together.  The parahippocampusâ results were similar to the hippocampus and were omitted due to space limitations.
Organization, dataset description, and preprocessing (R5): We will rearrange our paper as suggested by R5. The dataset description, preprocessing, and discussion with limitations will be spun off into separate paragraphs.
Further generalizability (R4+5): MA-DPxGE was evaluated by two different imaging data types (the grey matter density and surface area), which could demonstrate the good adaptability of our method. We also have conducted quantitative evaluation via a simulation study but only real results were presented due to space limitations. Additionally, we are working on additional ethnic groups to further demonstrate its generalizability.
Reproducibility (R4+5): The code will be released if accepted.
Figure fonts and typos (R4+R5):  We will enlarge the fonts of all figures, and fix all typos as suggested."
https://papers.miccai.org/miccai-2024/234-Paper0383.html,N/A
https://papers.miccai.org/miccai-2024/235-Paper2861.html,"We sincerely thank all reviewers for their time and effort in reviewing our manuscript. First, we reiterate our contributions: (1) We introduce DAGNN, a novel attention-based model enhanced by a disentanglement loss, designed to identify spatially distinct patterns within brain networks relevant to ADD/MCI classification. (2) Unlike traditional AGNN, DAGNN can learn decorrelated latent representations localized at the mesoscale. (3) DAGNNâs ability to generate spatially distinct patterns enhances its effectiveness and interpretability, making it a potentially valuable tool for other clinical applications involving brain connectomes. 
Below, we respond to reviewersâ comments.
Experiments: R1 suggests conducting more ablation experiments and comparisons with other models. We believe our presented comparisons with models tailored for brain networks (BRAINGB, BRAINGNN), disentanglement-based methods (DisenGCN, FactorGCN*) and AGNN (an ablated version of DAGNN without the disentanglement loss term, i.e., Î»=0) provide substantial evidence for the effectiveness of our approach. Additionally, we confirm that the same parcellation atlas was used for all datasets, addressing a concern raised by R3.

Disentanglement Loss: In response to all reviewersâ comments, we would like to elaborate on the disentanglement loss. We propose DAGNN as an improvement on AGNN which partially suffers from the homogeneity problem across heads. While this is a common issue in AI research that extends beyond the medical field (as noted by R3), our proposed resolution is specifically motivated by the desire to identify distinct and possibly overlapping brain regions whose degradation may reflect different aspects of disease progression. That said, as a novel solution to the homogeneity problem, DAGNN may well be useful in other domains. Note that, the loss function is simple yet more effective compared to DisenGCN, which uses the neighbor-routing mechanism, and FactorGCN*, which adds a classification head for disentanglement in latent space. By directly performing disentanglement in coefficient space, it achieves more differentiated latent representations in comparison, yielding superior experimental results. Also, DAGNN utilizes an L1-norm-based loss (well-bounded and promotes sparsity) and does not introduce a new head or routing mechanism, which makes it computationally efficient.
R3 raised concerns about the effectiveness of the disentanglement term in the loss function. The coefficient Î»=0.1 was indeed determined empirically. Although the details of this hyper-parameter optimization were omitted, its contribution to the performance is evident from our comparison with AGNN. Note that, AGNN is DAGNN without the disentanglement loss. Generalizability of the proposed disentanglement loss (a reservation of R1) to other multi-headed attention models, such as transformer-based AGNNs, is straightforward. Its effectiveness in different settings is a question we hope to address in follow-up projects motivated by the present proof-of-concept study.

Clinical applicability: R3 and R4 raised concerns about integrating our approach into clinical settings.Figures 2.A and 2.C attest to the promise of DAGNN as a biomarker for diagnosing AD and, in the future, possibly other neurodegenerative diseases that alter brain connectivity. Additionally, the distinct spatial patterns (subnetworks) identified may offer deeper insights into underlying mechanisms from a connectionist perspective.

Reproducibility: The preprocessing stage of structural network construction is crucial, as noted by R4.  While we summarized our custom preprocessing pipeline in the paper, further details can be found in the paperâs GitHub page which will be made available in due course. We regret that it had to be left out due to the anonymity requirement during submission.

Figures. We thank R3 for the figure corrections/suggestions. They have been implemented."
https://papers.miccai.org/miccai-2024/236-Paper3719.html,N/A
https://papers.miccai.org/miccai-2024/237-Paper2429.html,"Reviewer #1
Q1: In each batch, we form the context set (C) by selecting a subset of samples. The target set (T) includes Câs samples plus additional ones from the same batch. During training, we compute their Gaussian distribution parameters and measure the distributional divergence between C and Tâs distributions. This metric is incorporated into the loss function. This process simulates differences between training and real-world data, enabling rapid adaptation of DAML. See Reviewer #2âs Q2 for more details.
Q2: Thanks for pointing it out. Small-data regime may be more suitable for our setting. We will correct it in the final version.
Q3: Although C and T originated from the same dataset, the small sample sizes and inherent randomness introduce statistical biases between them. By minimizing differences between C and Tâs distributions, we enforce DAML to achieve fast adaptation among various distributions, thus addressing drifted statistical properties.
Q4: We note that the limitations raised by the reviewer impact all methods fairly. DAML and comparison methods share the same datasets and preprocessing techniques. Moreover, to guarantee thorough coverage of all subjects, we employ five-fold cross-validation, affirming the fairness of our empirical study. Our analysis was done without taking advantage of any datasets. Issues related to dataset size and generalizability are important but beyond this studyâs scope.
Reviewer #2
Q1: While sensitivity and specificity metrics are not included, we did report the F1 score, which is the harmonic mean of precision and recall. F1 score offers a balanced evaluation of these two metrics. Given brain classification tasks are often biased, F1 score is widely used in prior studies.
Q2: The Variational Distribution Encoder manages distributional discrepancies using a latent variable (z) that follows a Gaussian distribution. Data is firstly categorized into context and target sets to simulate experimental and real-world conditions. Brain graph representations (x_i) and labels (y_i) are concatenated to capture both individual features and their correlations. The encoder parameterizes Gaussian parameters for both context and target sets and minimizes the discrepancy between them, measured by JS divergence.
Q3: Fig.2(a) shows prominent connections between the right hippocampus (HIP.R) and the right fusiform gyrus (FFG.R) in health control. In contrast, Fig.2(b) shows a weaker connection in HIV patients at the same sparsity level. Patients exhibit atypical connections involving the left hippocampus (HIP.L), the left middle occipital gyrus (MOG.L), the right postcentral gyrus (PoCG.R), and the right angular gyrus (ANG.R)âconnections not found in healthy controls. These findings align with prior medical research and can be identified by DAML.
Reviewer #3
Q1: Thanks for your suggestion. During training, we did not observe significant impacts of this parameter; therefore, we chose not to include further discussion on this aspect.
Q2: Due to page limits, we omit the detailed analysis and prioritize to emphasize the superior adaptation ability. The optimal hyperparameters (as shown in section 4.1) are used for the results reported in Overall Performance Analysis. While eager to provide, the rebuttal policy prohibits us from reporting more results. We are happy to include this in the open-source version upon acceptance. 
Q3: Due to page limits, we primarily present visualization for HIV. ABIDE and PPMI, which show positive samples often misclassified as negative by other methods, can be recognized by DAML. On a group-level analysis, DAML achieves minimization of distribution discrepancies between context and target sets, addressing the data drift unaddressed by previous work.
Q4: Since DAML models the Gaussian for both the context and target sets, it allows us to quantify the likelihood that test samples are being modeled, serving as a measure of uncertainty and enhancing DAMLâs interpretability."
https://papers.miccai.org/miccai-2024/238-Paper4059.html,N/A
https://papers.miccai.org/miccai-2024/239-Paper1343.html,"We thank reviewers for their thoughtful comments and appreciating our work as âinterestingâ (R1, R4), and âinnovativeâ (R2), with a âgood set of experiments including ablation studiesâ (R1), which âeffectively demonstrate the superiority of the approachâ (R2). Reviewers also acknowledge that releasing the annotated dataset will âfacilitate further research in BCDMâ (R2) and is a âgreat contributionâ (R1).

(R1) âThe experimental setup is problematicâ¦in a UDA, the train and test sets should not be the sameâ¦ I wonder if the D-MASTER performance will be consistent â¦ on totally unseen test sets.â

We firmly believe that our experimental setup is not problematic. Standard UDA methods like DANN [1], ALDA [2], SAFN [3], MDD+IA [4], CADA-P [5], GVB-GD [6], HDAN [7], SPL [8], SRDC [9], HDMI [10], and SHOT [11], all follow the same setting by adapting on the whole dataset in an unsupervised way. Concern regarding performance on unseen test splits is also unfounded. Note that Table 3 in the main paper already shows results on âunseenâ cityscapes test split. Still, suggested experiments on custom split can be easily added in the camera ready.

(R1) âhow much domain shift existsâ.

We utilized 4 BCDM datasets with train and test split ensuring that no patient is common between the two splits.  First, we trained a model on the source (train split) and tested this model on the source (test split), establishing the source modelâs reliability. Then the reliable source model was directly tested on the target (test split) called âsource onlyâ. Next, the source model was fine-tuned on the target (train split) and tested on the target (test split) called âskylineâ. A significant performance gap between the source only and skyline models indicated a domain shift.

Clinically, the domain gap across the four datasets was verified by three expert radiologists. The DDSM dataset is composed of screen film mammography images, while our In-house hospital data consists of digital mammography images. Although some RSNA and INBreast images also originate from digital mammography, they exhibit noticeable differences in image quality. The RSNA-BSD1K dataset, notably, includes data from ten different vendors, contributing to its diversity. Furthermore, DDSM, INBreast, and RSNA-BSD1K datasets are derived from screening mammography distributions, whereas our In-house hospital data is sourced from diagnostic mammography distributions. This distinction underscores the variability and domain shifts present among the datasets, highlighting the robustness and adaptability required for effective cross-domain breast cancer detection.

(R2) âLimitations of model performanceâ and (R2) âFailure casesâ.

If small datasets like INBreast are used as source, then the trained model has severe generalization issues. Our method struggles in such scenarios. Using single views (CC and MLO) is another limitation but can be fixed easily.

(R1) Weak and strong augmentations.

Refer to section 3 (implementation details). We followed MT [12], and AT [13], to study different parameters in augmentations.

(R1) âMissing details on how the different hyperparameters were selectedâ

We followed a systematic grid search and empirical validation performance for tuning our hyperparameters. Additionally, we considered domain-specific knowledge and insights from previous works in UDA [12, 13, 11, 2, 6] to inform our choices. We will add further details in the supplementary section corresponding to Table S1.

[1] arxiv.org/abs/1409.7495[2] arxiv.org/abs/2001.01046[3] arxiv.org/abs/1502.02791[4] arxiv.org/abs/2006.04996
[5] arxiv.org/abs/1906.03502v2[6] https://arxiv.org/abs/2003.13183 
[7] https://arxiv.org/abs/2011.14540 
[8] https://arxiv.org/abs/1706.07522 
[9] https://arxiv.org/abs/2003.08607 
[10] https://arxiv.org/pdf/2012.08072
[11] https://arxiv.org/abs/2002.08546 
[12] https://arxiv.org/abs/1703.01780[13] https://arxiv.org/abs/2111.13216"
https://papers.miccai.org/miccai-2024/240-Paper2081.html,"We thank Reviewers#3,#4,#5 for their valuable feedback. All comments are carefully considered and will be reflected in the final version. In the future, we will create a homepage that includes our code, more details and additional quantitative analyses(box-plots, variance).
R3&R5: [Q]:DFE module, feature fusion, zero values [A]:The DFE might reduce the feature information of the Z-plane. However, since we concat the enhanced features with the original fused features. There is no sacrifice of any features, so there will be no blank areas during decoding. In the DFE module, we adopt element-wise addition for the fuse process. We also tried using cross-attention to fuse features, which not only did not improve performance but also increased training time. We will update Fig.2 and the description in the revised version.
R3&R4: [Q]:Results details, robustness [A]:The metrics shown in the tables are the averages of multiple results. Due to space limitations, we can not include the variance of these metrics in the table. Compared to other related studies, our model has a smaller variance, which also reflects that our model is more robust.
R3&R4: [Q]:Performance improvement, metrics [A]:Our work focuses on reducing the depth reconstruction errors caused by the occlusion of instruments. However, the generally used evaluation metrics compare the rendering results from the main view, which can not reflect the differences in depth. This significant visual contrast in Fig.1 is usually more meaningful than slight improvements in quantitative metrics.
R3&R5: [Q]:clinical relevance, quality needed for medical application [A]:The ENDONERF dataset is collected from real surgical scenes. The previous studies have demonstrated significant practical value in clinical medicine. We have further reduced the depth errors. This undoubtedly serves as a meaningful boost to practical medical applications. So, we believe the rendering quality has reached the âquality neededâ. Furthermore, we will collaborate with doctors to optimize our approach for practical medical applications in the future.
R4: [Q]:Limitation, a graphical abstract for method [A]:Our method is only applicable to networks that model dynamic and static fields separately. However, separating dynamic and static fields can reduce training time. A clear graphical abstract of Fig.2 and bar about different tensors will be provided in the revised version.
R4&R5: [Q]:Dataset, more information, rendering time [A]:The dataset is a sequence of RGB images containing surgical instruments. DnFPlane is used to render a 4D tissue structure without surgical instruments. So, the RGB image sequence does not need to be partitioned. Regarding the loss function weights, we made appropriate adjustments based on the baseline. The DFE and CIR modules only resulted in a slight increase in parameters, and the time cost is almost negligible. The entire model completes the training and rendering process within 3 minutes. Individual rendering meets real-time requirements. 
R5: [Q]:Comparison study [A]:To ensure fairness, we strictly followed the requirements and hyperparameter settings of the comparative works, replicating their results under the same conditions. We also reference the metrics provided in their papers (showing the better metrics in our tables).
R5: [Q]:unclear expression [A]:The âstrange symbol for Deltaâ indicates element-wise subtraction. The âstrange sentence in Section 2.2â aims to convey that we represent the surgical scene as a 4D volume consisting of spatial dimensions XYZ and time dimension T. This structure greatly reduces the computational cost of NeRF-based methods. The âdepth distortionsâ on Page 2 actually refer to depth errors. These will be uniformly revised in the revised version.
R5: [Q]:3DGS [A]:The 3DGS requires point cloud data to initialize the 3D Gaussians, resulting in a large number of parameters. The advantage of NeRF is its implicit representation, which has fewer parameters."
https://papers.miccai.org/miccai-2024/241-Paper3110.html,"We deeply thank the reviewers for their insightful comments. It is truly gratifying to see that they find this work rigorous (R1), extensive (R2), interesting and well-suited to the problem (R5). Below is our detailed response to the comments.

Experiment details (R1, R5). Due to space constraints, we couldnât encompass all the specified information in the paper (resolution, scanner, stains, prep steps, etc.). However, to provide a more detailed description, we will clarify that each institution scanned H&E WSIs using a standardized protocol and a single scanner, Rocheâs scanner Ventana iScan HT (for Hosp. A) and Philips Ultra Fast Scanner (for Hosp. B). For further clarification, we will include a citation to a paper that details the datasets used in this study, which was initially omitted due to anonymization requirements.

Latent variables (R2, R5). Because a single scanner was used per hospital, similar intra-hospital characteristics were observed. For this reason, we assumed that the latent variables were the hospitals and behaved as a normal distribution. This distribution can model the characteristics and variability of the specific institutionâs scanner.

Model details (R2). We utilized the reparameterization trick for our model, which is very popular in some probabilistic deep learning models, e.g., VAEs and BNNs. This trick enables backpropagation and gradient-based optimizers, which excel in deep learning. We will include this information in the final manuscript. We also share the code (and will release it on GitHub) so that the model can be understood in depth and easily reproducible.

Anomaly decision (R1, R2). As exposed in the last paragraph of section 2, we estimate the likelihood of a new test sample with Monte Carlo estimation. This likelihood is defined by eq. 2 and is used as the anomaly score. Notice that this likelihood can be understood as the reconstruction error or probability. We further clarify this in the manuscript.

Evaluation (R5). It is pointed out that we only evaluate our method on a single experiment. Notice that skin cancer is one of the most prevalent cancers worldwide, and CSC is particularly challenging to detect. Thus, we address CSC neoplasm malignancy detection, which is of special interest. We consider that the proposed model suits this clinical problem well. To show the efficacy of DAUD, we leveraged real-world datasets from two different hospitals. We also conducted an additional assessment of the capability for OoD detection on an external dataset of a different skin neoplasm. We also report figures (R2, R5) to better justify the model performance. We can see in the t-sne plots that while AE distributes test points uniformly, DAUD mixes the spaces. This fact means that our model disentangles the information between the domain features from the semantic ones. We acknowledge that studying the model in other CPATH scenarios would be interesting. We are grateful for the suggestion and will work on the model generalization to other tumors, adapting the latent variables for future work (R1).

Other clarifications (R5). Spitzoid tumors and CSC neoplasms are characterized by spindle-shaped cells. Although they come from different origins, both manifest in the skin and are typically diagnosed by dermatopathologists through skin biopsies, such as those used in this study. We believe it is difficult to determine the magnitude of this semantic shift. Average pooling is employed among the features because of the great capability of PLIP for feature extraction; however, we consider using other attention methods further. To report 95% CI, we used a typical standard statistical procedure calculating mean and std, extensively used when several replicates are done.

We will thoroughly proofread the text to cover the writing. Thank you again for your valuable input and consideration, which have strengthened our manuscript and inspired us to conduct future extensions."
https://papers.miccai.org/miccai-2024/242-Paper2284.html,"R1
1 Why the process does not stop at Step 1?
[15] does not improve segmentation accuracy, but focuses on segmentation consistency. Training with RL4Seg on the target data substantially improves the Dice and anatomical validity at each iteration.

2 Anatomical Metrics (AM) give an unfair advantage over baselines.
To our knowledge, our approach is unique in its utilization of AM with RL (only during training). The baselines we compared to thus do not include such concepts.

3 Statistical shift between datasets.
US images from different manufacturers have very different visual signatures, causing important domain shifts. This was confirmed by Frechet inception distance between the source and target domains being 10x higher vs intra domain.

4 More details on Reward Dataset [â¦] the output validity defined without AM.
The output validity is computed with the AM (Sec.2.3-Dr), which determines the choice of left/right flow. In the correction branch (left), the initial segmentation is invalid and the corrected version is considered valid. In the perturbation branch (right), the initial segmentation is valid, and the perturbed segmentations are always considered invalid. Regardless of the branch taken, tuples added to Dr are formed of input images, invalid segmentations and the pixel-wise error between the valid/invalid pairs.

5 Paper shouldnât rely on external sources [15] + overview of the method.
Due to space limits, we briefly outline methods like [15] (Sec.2.3-Dr) since RL4Segâs methodological content does not depend on the implementation of [15], but on the general concept of spatio-temporal post-processing correction.

R3
1 RL, reward, action, and AM not clear enough.
Cf. answer to R1Q4.

2 RL4Segâs distinction compared to SOTA?
As opposed to other works, RL4Seg is a new self-supervised segmentation RL formalism (Sec.2.2) for domain adaptation without target domain ground truth. PMID 33789178 requires ground truth data to train trajectories of refining predictions on the same image.

3 How perturbations are designed?
Like data augmentation, we apply small perturbations to remain close to the actual data distribution while increasing variety. Perturbations (1-6% noise on model weights, 5-10% Gaussian noise on images, image contrast reduction by 20-30%) are enumerated in the ablation study in supp mat.

4 Why PPO?
RL4Seg is inspired by ChatGPT which uses PPO. Advantages are the KL term (prevents the policy from drifting away) and the high sample efficiency. Other policy-based methods may work, but because segmentation RL has trajectories of length 1 and the very large action space, value-based or model-based methods are less suitable.

5 How to evaluate anatomical plausibility?
With the 10 anatomical metrics (cf. supp. mat)

6 How does uncertainty information help?
Uncertainty estimation is a by-product of our framework and is not used to improve segmentation results in the RL loop. We are currently exploring its potential for an upcoming journal paper.

R4
1 Poor initial predictions.
A good initial policy generating decent predictions is key for the success of RL4Seg. However, poor predictions are filtered out by the anatomical metrics mitigating the risk for biases induced by the initial model (cf. R1Q4). Despite a domain shift between the source and target data (cf. R1Q3), RL4Segâs performance is excellent, underlining its generalizability.

2 Computational costs.
Processing 10K images requires 5 hours on an NVIDIA 3090 GPU, which is reasonable for such database size, and a significant reduction in time compared to manual annotation by an expert, which is the grand objective of our work.

3 How AM influences learning/generalizability to other modalities.
RL4Seg enables the policy to learn AM implicitly through the reward network, preventing deviation from valid results during training. [16] demonstrates that such metrics are effective across US and MR cardiac images, indicating that our method can suit other modalities."
https://papers.miccai.org/miccai-2024/243-Paper4215.html,"R1,R3 and R4:
Contribution and Baseline Experiments:Our primary contribution is introducing a novel multitask learning methodology with a unique dynamic, Domain and mutual information (MI) based task loss weighing metric.This learning methodology resulted a more robust model with better classwise accuracies than other sota/baseline multitask learning methodologies detailed in Sec 3 and Tab 2-3 and is thus broadly applicable to other medical domains with similar complexities between tasks.Weâll improve clarity on this.

R1:
Assumption of Anthropometric Features:BMI, height, weight, and all others utilized in the paper are established indicators of malnourishment by WHO and CDC.However, the main essence of the methodology is to not depend on a single indicator but to utilise the predefined (domain based) and learned (MI at epoch) dependencies between multiple tasks to improve model performance. For instance, using logistic regression, Individually, Height (AUC=0.58), Weight (AUC=0.63), BMI (AUC=0.70), and Age (AUC=0.54) show weak to moderate predictive power. However, when these features are combined, they achieve a higher AUC of 0.87.

R1,R3:
Bias, Ethical Considerations and Dataset Curation:The dataset includes diverse demographics (age, gender) and diversity in terms of data collection (illumination, pose, environmental setting) and includes 2162 individuals with an average age of 10.5 years and gender distribution of 1355 males and 786 females to depicting inherent bias.Model testing on a web-scraped dataset of 3050 subjects (annotated by expert medical professionals) from three ethnicities (White, Black, and Brown) showed an accuracy of 80.58% for binary malnourishment classification showing generalisability.We shall detail the curation process, data statistics and this case study in the supplementary material. We have obtained ethics committee approvals and guardian permissions for the use of this data for research, and will include relevant ethical discussions in the manuscript.

R3:
Title and Domain Terminology:By the term âDomainAdaptâ We intended to emphasize integrating domain knowledge within our dynamic task-weighing method and how it is adapting the training and loss function. While we believe the current title captures the essence of our approach, we are open to considering alternative titles such as âDomainKnowâ to reflect the core contributions better and avoid confusion with domain adaptation literature.

Multiple Label Prediction Methods and Pose-wise Feature Fusion: We provided information on multiple label prediction in Sec 2.1 and 3 - Rep. of Tasks.We appreciate the comments and will improve the clarity and details, including the range of alpha and beta, in the updated manuscript.The ablation study in Tab 4 shows that the lateral pose has a higher RMSE than other poses, making it not the best choice.The all-pose model performs best or on par for regression, ensuring robustness in real-world settings with multiple poses.For classification, accuracy has better classwise accuracy balance as demonstrated in Tab 2-3.The posewise feature fusion is done by concatenating the features after passing through their respective feature extractors as depicted in the Fig1 and Section2.We shall improve clarity in manuscript and figures.Multiple poses are used keeping in mind a real-world setting where we may come across varied poses and this learning methodology incorporating multiple pose fusion exhibited increased robustness and generalizability.

R3,R4:
Evaluation Metrics and Architecture Figure: We appreciate the reviewersâ inputs.Although additional metrics for all experiments were not included due to space constraints, we will add more detailed tables in the supplementary material.For example, the Map for T1, T2 and T3 is 0.94, 0.92 and 0.75 respectively and F1 is 0.70, 0.85 and 0.69 respectively.We will include standard deviation in Tab1-4 and elaborate on figure captions.Fig1 will be improved to reflect suggestions"
https://papers.miccai.org/miccai-2024/244-Paper3214.html,"We thank the reviewers for their valuable comments. In the following, we respond to their comments into a few major categories:

[Q1] The random selection of pseudo-bags impacts reliability,  reproducibility, and weakens the interpretability.
R1: In our experiments, we set the random seed to 42 during both training and testing phases to ensure consistent results under the same experimental setup. The use of pseudo-bags is intended for the pseudo-bag guided learning phase, providing diversified inputs to the model. During the testing phase, we tried to reduce the number of pseudo-bags to one, which almost does not affect the modelâs performance. To assess the modelâs robustness against randomness, we also tested its performance with the random seed set to 2024. The results showed that the AUCs for ER, PR, and HER2 were 0.9201, 0.8653, and 0.8956, respectively, similar to the results with the seed set to 42. Overall, our evaluations showed that the random selection of pseudo-bags helps in training our multiple instance learning (MIL) model without adversely affecting its reliability, reproducibility, and interpretability during testing.

[Q2] Mean and standard deviation of 5-fold cross validation should be provided.
R2: We will provide them in the final version.

[Q3] Make the open source codes publicly available.
R3: To maintain the peer-review processâs integrity, we will release our codes on GitHub after publication.

[Q4] Clarify missing values in Table 4.
R4: The studies mentioned in the upper half of Table 4 were not reproduced by us; instead, we directly cited their results from the papers.

[Q5] Why C=D=2 for the MSA and L-MSA blocks.
R5: Due to overfitting issues in training slide-level MIL models, a deeper model with more MSA and L-MSA blocks does not necessarily yield better results. We chose this number based on the previous papers (e.g., Shao et al. NIPS 2021) and also our preliminary experiments.

[Q6] Include simultaneous prediction of the marker Ki-67.
R6: Within our datasets, Ki-67 expression rates manifest as continuous values, typically presented as percentages. The process of categorizing Ki-67 expression into positive or negative relies on applying a predetermined threshold. However, determining what constitutes positive or negative expression for Ki-67 warrants thorough deliberation. Some papers consider Ki-67 expression greater than 10% as indicative of high expression, whereas in the molecular typing of breast cancer, the threshold is set at 14%. Therefore, there is a need to explore multiple thresholds, such as 10% and 15%, for categorizing patients as Ki-67 positive or negative. This is a key focus of our ongoing multi-label classification study.

[Q7] Insufficient analysis of underlying factors contributing to the performance of the model.
R7: The superior performance of our model can be attributed to several factors. First, our pseudo-bag guided learning enhances the diversity and quantity of bags, which effectively trains the MIL framework and thereby improves prediction performance. Second, by stacking standard MSA and linear MSA blocks, our model can better learn global interactions among instances, resulting in improved instance aggregation. Third, our multi-label learning model exploits the correlation among biomarkers to enhance accuracy and efficiency in prediction.

[Q8] Ablation studies and statistical comparisons are not enough.
R8: In our experiments, we conducted ablation studies on both internal and external datasets, yielding similar results. However, we report only the ablation studies on internal dataset, as we determine the hyperparameters of our model based on these studies and aim to adhere to the paperâs length limitations. For statistical comparisons, we computed both the mean and standard deviations from 5-fold cross-validation. These results will be incorporated into the final version.

[Q9] Minor language and inconsistency issues
R9. We wil address them in the final version."
https://papers.miccai.org/miccai-2024/245-Paper2829.html,"We sincerely thank the reviewers for their valuable feedback. We are pleased to see that the reviewers agree on the novelty and efficacy of the proposed model for aneurysm image segmentation. Some major questions are addressed below.
(1)Visualisation: R4. For the segmentation results in Fig. 3, we will crop the images and adjust the masks to take up a larger proportion of the images, thus enhancing the visualisation of the differences between the comparative modelsâ predictions. 
(2)Ablation experiments: R1, R4. The core innovation of this paper is the proposed SCM-MLP and ARC-MLP, they are the fundamental reason for the excellent performance of the DPMNet in this paper.  So as can be seen from Table 3, even global branching containing only three layers of the encoder-decoder structure achieves good results. In addition,  the dual-path strategy using global and local branches allows the model to capture both the long-range spatial dependencies between image patches and the high-level semantic details of the image. The adoption of the dual-path strategy logically solves the problem of FC layersâ lack of localisation; as a result, it improves certain segmentation performance. However, it does bring about a growth in the number of model parameters, and we will consider optimising the network to reduce the number of model parameters in future work to achieve a more lightweight model with better segmentation performance. 
(3)Colour in Fig. 2: R1. The colours used in Fig. 2 are intended to provide a more intuitive and aesthetically pleasing representation of the process of shifting in width and height and mixing the different channels. 
(4)Multiclassification: R5. Data labelling of medical images is a very time-consuming and complex task. The aneurysm dataset used in the paper was manually labelled with aneurysms by skilled medical professionals as a ground truth. Since the labels obtained are binary, only binary segmentation of the aneurysm images is performed in this paper. 
(5)Comparison experiments: R1, R5. All hyperparameters of the other models compared used the default values from the original paper.
Once again, we sincerely thank the reviewers for their valuable feedback, and we will take all of their comments into consideration for the camera-ready version of our paper."
https://papers.miccai.org/miccai-2024/246-Paper1276.html,"We thank all reviewers for their insightful comments and for underlining that we propose an interesting (R1-4), novel (R1-4) and well-organized (R1-3-4) approach to solve an important clinical problem (R3). Below, we address concerns raised by reviewers.

Regarding the evaluation process (R1), the authors acknowledge that the formulation of the methodology may have been misleading and will be clarified in the final version. Indeed, we implemented the method suggested by R1, using cross-validation within the training set to select hyperparameters. Then, we evaluate each of the five trained models on the hold-out test set, aggregating the results as meanÂ±std.
As for the patient-wise stratification (R1), each TCGA patient is linked to a single, hand-selected slide with minimal artifacts (pen marks, etc.). 
Coverage rates and percentages of modality combinations for the dataset (R3) are given in the experiments section and Table 2.

Novelty and related work (R1-3): We propose the first method to learn disentangled representations through mutual information and combine them with a two-stage attention-based fusion on incomplete radiology, pathology and genomics data. Our method can be extended to any task and any new modality. All the mentioned references (R1-3) use single stage fusion and do not use dedicated disentangling criteria. R1.2 (reference 2 of R1) is designed for bimodal interaction without easy modality extension and do not rely on mutual information as advocated by (Bengio et al. 2019, ICLR). R3.3 is designed for 3 temporally aligned modalities and handles missing modalities only at inference. R3.3 and R1.2 aim to reconstruct the missing latent space, a non-trivial task; our approach differs by using only available information. Both R3.2 and R3.4 use only imaging modalities, applying attention to the learned tokens before they are input into specific decoders: the closest comparison possible may be the one with the vanilla MAF in Table 1.

Regarding component influence analysis (R3), we detailed an ablation study on the disentanglement term in the appendix.

Comparisons clarification (R1): We retrained all comparative methods. As our study is the first to tackle these modalities with incomplete data, no pre-trained models are directly available. Also, some methods do not provide their data splits, or use cross-validation without a hold-out test set, which does not meet our evaluation criteria. We agree with R1 on the use of different backbone encoders. However, since contributions in these methods arise from fusion techniques and auxiliary losses (with simple CNN/MLP backbones), we chose to fix modality encoders for all our experiments. This ensures consistency in parameters and training processes across comparisons, focusing on where the novelty lies.

Marginal gain over max fusion (R1): although our method yields a slightly better C-index, the gap is much larger when looking at the model calibration (IBS, INBLL), especially with varied modality combinations (Table 2). Previous methods may deteriorate with added modalities, ours consistently improves. Paired T-tests will be added in the manuscript if allowed.

Results in Table 2 (R3) indeed come from a single unified model handling any combination of modalities as variable-sized sequences, which is faster than using zero-filled tensors.

DRIM-U (R3): indeed, it highlights our methodâs ability to derive unsupervised embeddings achieving competitive results by quickly fine-tuning top layers.

We agree with including results for patients of various grades (R4) for greater clinical relevance.

We thank the reviewers for considering the page limit while providing suggestions for future work which align perfectly with the aim of the paper, with new datasets (R4), methods (R3-4) and the further exploration of the latent space to better discern patient phenotypes (R1). This will be accelerated by the full availability of dataset preprocessing, code and models."
https://papers.miccai.org/miccai-2024/247-Paper1741.html,"We thank all reviewers for their highly positive appreciation.
1.Excellent novelty(R1-ânovel methodological componentsâ, R4-âinnovativeâ, R5-ânovel approachâ, R6-âvery innovativeâ) 
2.Boost performance(R1-âboost performanceâ, R4-âimproved performanceâ, R5-âSOTA comparisonâ, R6-âsignificant improvementsâ) 
3.Clinical significance(R5-âclinical relevance demonstratedâ, R6-âclinical applicability and benefitâ)

Furthermore, we deeply appreciate R5 for the strong acceptance and R1, R6 for their acceptance.

Q1: About random partition.(R4)
Our data partition is broadly recognized, validating the superior performance of our method.
-All experiments are conducted under the same data partitioning criteria, and our method obtains the best result.
-We provide thorough evaluations including qualitative, quantitative, and ablation studies, which are widely recognized by R1, R5, and R6.
-The similar partitioning as [1-3].
[1] Wu C, ICCV, 2023. [2] Bontempo G, MICCAI, 2023. [3] Graham MS, MICCAI, 2023.

Q2: Availability of genetic data.(R4)
Genetic data is clinically accessible. 
-Genetic mutations areÂ crucial biomarkersÂ in MPN classification, and genetic tests are typically recommended in clinical practice [4].
-The genes we use (JAK2, MPL, CALR) are common MPN driver mutations, usually performed on peripheral-blood DNA, making them widely available [5].
-Our method pioneers multimodal MPN diagnosis by fusing clinical data and WSI, aligning with clinical practice and enhancing diagnostic reliability [6].
[4] Thiele J, Am JÂ Hematol, 2023. [5] Cross NC, BJHaem, 2021. [6] Rumi E, blood, 2017.

Q3: Performance using only clinical data.(R4&R6)
Our comparisons have included the model using only clinical data, which does not attain optimal performance.
-In Table 1, the fifth row has shown a 13.82% lower AUC with only clinical data and the fourth row has shown a 10.91% lower AUC with only WSI, compared to our multimodal method. 
-This confirms that our multimodal method outperforms single-modality methods, effectively integrating clinical data and WSI to obtain optimal performance.

Q4: Claim of CF module.(R4)
Our CF module advances agent attention by designing a clinical-enhanced query as additional guidance, aiding in exploring diagnostic-relevant representations across modalities, thus enhancing the modelâs representation.

Q5: About the task.(R1)
Our work focuses on the subtype classification task of MPN, namely PV, ET, PrePMF, and PMF. The confusion matrices in Fig.4 have provided the individual values per class.

Q6: Potential misunderstanding of Residual.(R1)
In Eq. 2, the first part x_i denotes the residual, while the second part denotes the patch features designed by us. Removing the residual has minimal impact on performance.

Q7: Other details.(R1)
-We will release the code upon acceptance to reveal more details.
-We used the pre-trained ResNet-50 with weights from [17].
-âw/o DS & w/o CFâ means removing both the DS and CF modules.
(1) Details in DS module
-The absolute position encoding refers to encoding the spatial position in Transformer, which is set to a fixed-length sequence via zero padding. 
-Our dynamic random encoding maps the sequence of patches to a grid composed of random numbers, dynamically adapting to changes in patch quantity.
-The random grid is fed into FC layers, adjusting its weights to prioritize important features, thereby enhancing the subsequent selection of patch features.
(2) Details in clinical query block
-Clinical indicators refer to the demographic characteristics, blood test parameters, and genetic mutation status. 
-The enhanced clinical query C is derived from the concatenation of clinical and image features, followed by fully connected layers with ReLU activation and scaled pooling.

Q8:Future work.(R6)
-Thanks to R6 for the future work insights. Due to space constraints, we expect to explore model adaptation to other diseases, explainable AI, and long-term data collection for prognosis in the future."
https://papers.miccai.org/miccai-2024/248-Paper1357.html,"We appreciate your positive feedback on our technical novelties (e.g., ânovel networkâ-R1, âinnovative use of temporal informationâ-R3, and âexploit DSA-inherent propertiesâ-R4), and the effectiveness (e.g., âsuperior performanceâ-R3, and âsolid evaluationâ-R4). We kindly request that you reconsider our work in light of the following points:

1.Dataset and dataset size(R1,R3,R4)
Our dataset includes multi-device, multi-disease DSA sequences with both sagittal and coronal views at multi-resolutions to ensure data distribution complexity. It contains 70 sequences: 20 for an independent test set and 50 for 5-fold cross-validation. Neurosurgeons meticulously annotated the dataset, a challenging and time-consuming task that often requires 2-3 clinical days per sequence. Existing studies rely on smaller private DSA datasets, such as [3] with 20 images and [4] with 30 images, and unfortunately, there are currently no publicly available DSA datasets. Thus, we intend to release a larger DSA sequence dataset, including more than the 70 sequences used in this study.

2.Comparative experiment(R1,R3,R4)
DSA sequences dynamically display vessel flows, capturing partial contrast agents in each frame. Existing methods use only single-frame inputs due to their network designs, which fails to cover entire cerebrovasculature and pathological features and are not open-sourced for comparison.
To ensure a fair comparison with other SOTA methods, it is important to use the same input as our method. TransUnet achieves a Dice of 85.00 on DSA sequences, lower than with single-frame input. nnUnet shows subtle changes with a Dice of 87.64. This suggests these methods struggle to extract complete sequential features, while our method fully leverages spatiotemporal information through the delicately designed DFW and SFTA. We also benchmark against a recent sequence-based method, CAVE, which achieves a Dice of 84.39, much lower than the 89.34 achieved by our method.

3.Computational efficiency(R1)
Not only does structural design increase parameters, but channel size plays a role. For example, UNet has a maximum of 1024 channels, while our method has 512 channels and fewer parameters than UNet. We computed parameters/FLOPs for models using sequence inputs: U-Net (31.0M/219.4G), TransUnet (105.9M/128.6G), nnUNet (7.4M/54.8G), CAVE (83.2M/4513.2G) and our DSNet (17.05M/236.3G). Despite a slight increase, our method performs the best, demonstrating its effectiveness and superiority.

4.Function and input of the TEB and SEB(R3) 
The TEB and SEB have the same structure but differ in inputs. The TEB extracts temporal flow information from a sequence, while the SEB extracts spatial contextual information from a MIP image.

5.Influence of two samples in the batch(R4)
In TEB, sequences with dimensions (BTCHW) are input, merging B and T into N for convolution operations. This technique used in many studies like CAVE and STCN, maintains the independence of different samples.

6.Data resample and vessel deformation(R3,R4)
We resample sequences using the nearest neighbor algorithm to preserve continuity. During DSA sequence imaging, slight head movements can cause vessel displacement among frames. To address this, we use non-rigid registration to match vascular topology before resampling, as reported in Sec.3.

7.Backbone, other application and redundancy(R4)
Our backbone is a straightforward dual-branch encoder-decoder. We believe this dual-branch structure can be applied to TOF-MRA vessel segmentation by using previous slices to guide the current slice. Moreover, âredundancyâ refers to repeated vessels in neighboring frames, causing redundant segmentation and annotation, particularly in video segmentation.

8.Importance of different frames(R4)
DSA sequences depict blood flow from the bottom to the top and center outward. We believe that initial frames are more significant for large vessels at the bottom, while later frames are more relevant for fine vessels at the end."
https://papers.miccai.org/miccai-2024/249-Paper1793.html,"We sincerely thank the reviewers for their time and effort in reviewing our manuscript. We appreciate their valuable comments, constructive suggestions, and recognition of our work. We will carefully revise the manuscript to incorporate all the recommendations from the review panel.

The specific revisions will include the following: enhancing the description of the dataset, improving the presentation of figures and tables, providing a detailed introduction of the unimodal models, including comparisons with more recent multimodal fusion methods where possible, and improving the title of the paper.

All of the aforementioned revisions will be incorporated into our final submission.

Thank you again for your time and effort to improve the quality of our manuscript."
https://papers.miccai.org/miccai-2024/250-Paper2837.html,"We sincerely appreciate the constructive suggestions from reviewers to help further improve our paper, and will explain their concerns point by point below.
1) This framework seems to be a combination of the recent SOTA methods and lacks novelty. (R4)
Authors: We acknowledge that the submitted manuscript did not accurately convey the innovations of the proposed method. Here, we restate the innovation of the study: to better combine clinical prior knowledge: enhancement direction features in CEUS TI-RADS, this study proposed for the first time to introduce watershed analysis from remote sensing, and combined dual-modality data (US images and CEUS videos) to construct a new 2D+3D network, thereby improving the classification performance.
2) This paper should compare the proposed model with other state-of-the-art methods for thyroid nodule diagnosis. (R1, R3)
Authors: Existing work on the diagnosis of thyroid nodules is mainly divided into two types. Methods of the first type rely on single modality (US or CEUS) data, whose classification performance are close to that of single modality methods in Table 1, and are significantly worse than DWFN. Methods of the second type are combined with dual-modality data, whose classification performance are close to MUS+CEUS, yet worse than DWFN. However, most of these methods only utilize dual-modality images or videos, ignoring the distinct features of dual-modality data (morphological features and dynamic blood flow features) in clinical diagnosis. Therefore, this study aims to show the comparison between different modalities and whether watershed analysis is employed, and designs ablation experiments to verify the effectiveness of DWFN.
3) Statistical analysis reveals that results obtained from M_(CEUS+ED) are similar to those of the proposed strategy. (R3)
Authors: This phenomenon may be attributed to the fact that radiologists focus more on morphological features in US for clinical diagnosis, where InceptionResnetV2 is employed in DWFN to extract deep learning features, which are essentially abstract semantic features. However, without prior clinical knowledge as guidance, it is difficult for DWFN to focus on morphological features. Therefore, DWFN is not significantly improved after adding the semantic features extracted by US modality, which also verifies the effectiveness of DWFN due to the introduction of watershed analysis. This phenomenon is a weakness of this study, and further improvement work will be conducted to combine the clinical US features.
4) The pre-processing step seems complex and raises some doubts. (R3)
Authors: In the previously submitted manuscript, there were ambiguities in the description of pre-processing stage. Since clinically obtained CEUS data contains patient and device information, to reduce their impact on the model, this study involved a radiologist to crop US mask (irrelevant information), which does not rely on a high-level expertise, but simply crop the text around the image. Reviewers might have misinterpreted this mask as ânodule maskâ referred to in most studies, and we will revise the term âcrop maskâ to âcrop irrelevant informationâ to avoid confusion. 
5) The results fail to provide insight into the types of errors made by the network. (R3)
Authors: Due to space limitations, we did not present images of network misclassification, but we utilized the confusion matrix to calculate evaluation indicators. Images depicting misclassification by DWFN may result from the extraction of three categories of ED features: centripetal, centrifugal and scattered, yet centripetal and centrifugal are merged into non-scattered in clinic, which affects network misclassification. 
6) The inconsistency in referencing with Figure 2 and Table 1. (R3)
Authors: Due to typographical limitations, the order of appearance and corresponding text references of Figure 2 and Table 1 did not match, and we will modify their order of appearance in the revision."
https://papers.miccai.org/miccai-2024/251-Paper1197.html,"We have read the review comments carefully. After summary and combination, we got 8 major comments. Most of them, i.e., Comments 1~6, focus on the experiments or related details, where the main issues are caused by limited space. Comments 7~8 are for our method. The repetition level (RL) of the input data is the main factor of our method, but was ignored by reviewers. Our response is as follows."
https://papers.miccai.org/miccai-2024/252-Paper0950.html,"We thank reviewers (R1, R3, & R4) for their valuable feedback, describing our approach as ânovelâ and âvery well motivatedâ (R1), â robustnessâ (R3), âwell-organizedâ and âfirst investigationâ (R4). We sincerely appreciate their insightful comments and address their concerns as below.

Q1: Image shuffling influence and visualization. (R1)
A1: Thanks. The shuffling process employed in our method does not compromise the structural information of the image. This is ensured by the subsequent reorder operation, which can accurately reconstruct the original positions. To provide a more intuitive visualization, we have utilized Grad-CAM [1] to visualize the intermediate feature results on DPBNet. Despite the random cropping that generates separate targets, DPBNet can effectively focus on those targets by shuffle-and-reorder attention mechanism. We will incorporate the visual results and discussions in the final version.

Q2: Limitations of the Rand-crop methods and more clarifications. (R1)
A2: Thanks. The limitation of Rand-Crop methods in preserving integrous targets within medical images [2] is well-known, as the lack of predetermined target regions results in disruptions regardless of the random cropping method employed. We will provide detailed explanations of the Rand-Crop in the final version. to enhance readability.

Q3: Presentation. (R3)
A3: Thanks. We have made significant improvements to enhance the presentation of this paper in the following ways.
1) We have carefully double-checked the manuscript, including grammatical and typographical errors. and improving clarity. 
2) The symbols C, H, W, and D denote the number of channels, height, width, and depth within a feature map, respectively. To avoid confusion for readers, we will add a notation table in the final version, elucidating the frequently used symbols.
3) Inspired by your comments, we have carefully revised the notations, annotations, and sub-figures in Fig. 2. This includes providing detailed depictions of the convolution block, sigmoid function, shuffle ratios, and a more distinct illustration of the DFB Loss. Due to display limitations, the revised Fig. 2 will be included in the final paper.

Q4: Comparison with Transformer structure. (R1 & R4)
A4: Thanks. Firstly, in our previous manuscript, we have compared our method with an advanced transformer-based method termed CANet in Table 1, demonstrating the superiority of our method. Following your suggestion, we have conducted comparisons between SwinUNETR and DPBNet, empirically demonstrating the superiority of our method on the LA dataset. These findings highlight the inherent challenges faced by Transformer-based networks in LA segmentation, which is characterized by small-scale data and significant variability in atrial structures. Due to the rebuttal policy restrictions, these results and discussions will be included in the final version.

Q5: Formula (5). (R4)
A5: Thanks. Formula 5 is designed to assign weights based on the contrasting values surrounding the central point. We have revised this formula to make it clearer:
w_i={k^3 - f^3d(g_i) + 1, i=1; f^3d(g_i) + 1, i=0}. 
where k^3 represents the maximum possible count of foreground points within the k-nearest neighbors. Moreover, we employ a 3D-CNN to accurately determine the actual number of foreground points (with a value of 1) within the k-nearest neighbors of a central point. For instance, when the central point is classified as a foreground point, we determine the count of surrounding background points by subtracting the output of the 3D-CNN from k^3.

Q6: Reproducibility and code sharing (R1 & R3).
A6: Thanks. The complete source codes and pre-trained models will be released for reproducibility if the paper is accepted.

[1] Selvaraju, R. R., et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. ICCV, 2017.
[2] Shorten, C., et al. A survey on Image Data Augmentation for Deep Learning. Journal of big data, 2019."
https://papers.miccai.org/miccai-2024/253-Paper0814.html,"Reply to All Reviewersï¼
Thanks for your kind and valuable suggestions. 
(1) We will add a link to the source code in the camera-ready version to ensure reproducibility.

(2)For the comparison with fully supervised methods, although fully supervised methods achieve superior results, their performance significantly declines in cross-domain testing, which requires pixel-level annotations for each dataset separately, leading to a huge workload. However, in some clinical scenarios, the accuracy for nuclei segmentation may not be very high, only needing to roughly confirm the position and size of the nucleiÂ to aid in diagnosis. In such cases, using point label can train a segmentation model in a short time and outperforms the pre-trained fully-supervised models, which can also be put into use faster compared to generating full nuclei ground truth labels.
Since the current point supervision results are still not camparable with fully supervised approaches, we have not shown the corresponding comparison.

Thanks again for your kind review and your great effort to make the paper better!

Reply to Reviewer #1ï¼
Thank you very much for your recommendation and encouragement of our work. Regarding the issues you raised, please refer to the response to all reviewers. I hope our response has addressed your concerns.

Reply to ReviewerÂ #3ï¼
Thanks for your constructive comments. Hereâs the responses to address these issues:
(1)Â In terms of comparative learning, our innovation mainly lies in using CAM to sample features, thereby improving the representation of features and making CAM more accurate. In terms of mechanism, the original comparative learning mechanism is simple but effective, so we did not make too many changes to ensure reproducibility.
(2)Â After being cropped into 256*256 patches with 128 overlapping, each dataset contains over 250 images. In addition, if space permits, we will add comparative experiments on a larger dataset in the camera-ready version. 
(3)Â In Section 3.3, we have shown the analysis results of some challenging samples. Due to space limitations, we did not present the results of quantitative experiments, which we will add in future work.
(4)Â Please see the reply to all reviewers, I hope the response has solved the problem.
(5)Â We conducted in-depth research on the mentioned work and other articles in the domain, and we will provide a more detailed and comprehensive introduction and citation of existing works in the camera-ready version.

Thanks again for your feedback on our methods and experimental details!

Reply to ReviewerÂ #4ï¼
Thank you for your reply and for pointing out the shortcomings of our method.
(1)In fact, alpha sometimes surely cannot fully reflect the accuracy of pseudo-labels. Therefore, we have adopted a rather high threshold (0.8) when using CAM as pseudo-labels to reduce noise. We will improve the method for measuring the accuracy of CAM in future work to obtain more accurate pseudo-labels.
(2)Â We will add a clearer description of the CCL module in Section 2.3 in the camera-ready versionï¼
As described in Section 2.1, the initial pseudo label M hardly contains noise, so anchor features can be regarded as the ground truth for nuclei and background features. We pair the foreground and background feature sets selected from the optimized pseudo label P with anchor features by contrastive loss. This can make the features of the foreground and background regions in P closer to the corresponding anchor features, enhancing the feature representation of these regions. At the same time, this module can promote the regions with high attention in CAM closer to the foreground and the regions with low attention closer to the background, so that CAM focuses more on the nuclei rather than the background tissue, making the generated pseudo label P more accurate.

We have carefully read your suggestions and made detailed revisions, thanks again for listing the shortcomings of this article!"
https://papers.miccai.org/miccai-2024/254-Paper1903.html,"Reviewer 1:

Reviewer 3 and 4:

Reviewer 4:

parameters a and T of the synthetic motion are not documented and it is not easy assess the significance of the motion : 
  Thank you. For these experiments, they were fixed to the values of a = 0.2 and T = 1000 ms and we considered an acquisition of 2000 ms to match the conditions of [11]. The missing values will be added to the camera ready paper and influence of these parameters will be studied in an extended version of the paper.

robustness to motion errors makes âan assumption of a static scene and I believe a more compelling and realistic result would be use a motion estimation approach and investigate the errorâ:
The motion field used in the reconstruction method has been already estimated with the motion estimation approach proposed in [18] in this work (last paragraph of the Results section). There is no assumption of a static scene.

There are some typos in the references, e.g., pet -> PET, ct -> CT, mri -> MRI.
  Thank you, this will be corrected."
https://papers.miccai.org/miccai-2024/255-Paper2686.html,"Reviewer #1: We thank the reviewer for the feedback. We will add more details pertaining to the Weakly-Supervised Learning (WSL) task in the revised manuscript. We would also like to note that our baseline is a re-implementation of the first reference mentioned in the initial comment by the reviewer.

In response to the second comment, we would like to clarify that the baseline is not solely the WSL, but a combination of a view classifier with a CNN-based classifier following it. This is already an improved version of the baseline implemented in: Zhang, Jeffrey, et al. âFully automated echocardiogram interpretation in clinical practice: feasibility and diagnostic accuracy.â Circulation 138.16 (2018): 1623-1635.

Regarding the remaining comments, we will rectify the spelling mistakes, add references to the tables, and include figure explanations in the revised manuscript.

Reviewer #2: We appreciate the comments by the reviewer. We will address the first point by adding details about the WSL implementation in the revised manuscript.

We will also add details about the different cross-validation schemes used in various experiments in the paper. We ensured that there was no data leakage at any point in the experiment. The foundational model used is self-supervised and does not cause double-dipping as no labels were used in training it. Furthermore, the validation and test sets were isolated from the downstream training. For the remaining comments, we will ensure that the inconsistencies and figure details are addressed in the final version.

Reviewer #4: We thank the reviewer for the comments. Only the baseline experiment was run on top of an A4c classifier, wherein the detected A4c views were used as input to the downstream classifier. All the remaining experiments use all available views. We will add architectural/training details in the revised manuscript. We confirm that the weights for the foundation model encoder were frozen after self-supervised pre-training."
https://papers.miccai.org/miccai-2024/256-Paper1328.html,"We thank all reviewers for their valuable comments and positive feedback, such as the thorough experiments (R5&R6), and promising results (R6).

Q. Poor SOTA Comparison (R3)
We compare EchoMEN with leading EF regression methods, including EchoGNN [11] (MICCAI 2022) and EchoCoTr [12] (MICCAI 2022), which are recognized as SOTA on the EchoNet-Dynamic dataset according to latest 2024 review [21]. We also extend SOTA imbalanced regression algorithms, such as RankSim [2] (ICML 2022), to EF regression, demonstrating that RankSim outperforms most existing EF regression methods in the challenging HFrEF and HFmrEF categories. (Table 1)
[21] Sanjeevi, G., et al. âDeep learning supported echocardiogram analysis: A comprehensive review.â Artificial Intelligence in Medicine (2024): 102866.

Q. Low Performance Difference (R3)
Considering the scale of the EchoNet-Dynamic dataset (10,030 videos, over 1.5 million clips in the beat-to-beat pipeline), the absolute improvements achieved by EchoMEN are substantial. EchoMEN outperforms the baseline and all competing methods in overall MAE, with a 2.96% relative improvement compared to 2.22% for the best existing method EchoCoTr [12]. On the overall GM metric, which better reflects algorithmic fairness, EchoMEN reduces prediction error by 3%, while the best imbalanced regression method RankSim [2] only achieves a 0.07% reduction. Furthermore, existing EF regression methods underperform the baseline on GM, indicating insufficient fairness. By following the experimental setup in [11-13], we ensure a rigorous comparison and demonstrate the superiority of EchoMEN. (Table 1)

Q. Novelty and Significance (R3)
As stated in the Introduction and shown in Fig.1 (c), data imbalance in EF regression undermines health equity. Unlike previous works that focus on overall accuracy [11-13], EchoMEN enhances performance across abnormal EF ranges without compromising the accuracy for majority of normal samples, ensuring fairness for all populations.

Q. Low Mean Values (R3)
MAE and GM are both error metrics [2,20], where lower values indicate better performance. Detailed formulas are available in Supplementary Materials.

Q. Positive Sample Selection (R5)
EchoMEN defines positive samples as clips from the same category (P(i) and Q(i) in Section 2.2). It covers both choices you mentioned (inter-patient and intra-patient similarities). For samples in a batch, we apply data augmentation, which is common in CL, generating positive samples from the same patient. Additionally, if a batch contains clips from different patients but with the same EF value, they will also be considered as positive samples. We appreciate your suggestion and plan to further investigate the impact of selection strategies in future work.

Q. Additional Datasets (R3&R5)
The EchoNet-Dynamic dataset (10030 videos) is substantially larger than CAMUS (500 videos) and has been the sole benchmark for recent EF regression methods [5,11,12,15] (MICCAI). Therefore, we focus on EchoNet-Dynamic for comparative experiments and plan to conduct experiments on CAMUS in future work.

Q: Additional Evaluation Metrics (R3&R5)
As our focus is tackling data imbalance, we adopt MAE and GM which are the most used metrics in imbalanced regression literature [2,20]. These two metrics provide a comprehensive evaluation of our methodâs effectiveness in improving both accuracy and fairness. Moreover, we visualize the RMSE distribution in Fig.1 (c) to illustrate the superiority of EchoMEN.

Q: Effect of Regression Loss (R5)
While using only L_LDW-SupCon in the first stage is possible, we found that incorporating the regression loss accelerates convergence. The regression loss provides strong guidance for learning meaningful features, while L_LDW-SupCon encourages feature separability. Combining both losses enables more effective representation learning.

Q: Language (R3&R6)
We will carefully revise any typos and notation errors to enhance readability."
https://papers.miccai.org/miccai-2024/257-Paper1027.html,"We thank the reviewers for their positive feedback and valuable insights. In our response, we clustered the main points to address specific comments and suggestions made by the reviewers.

Extending explanations to multiple cycles: The reviewers suggested extending the explanations beyond single cycles to cover entire heart scans. Our proposed method requires ED and ES detectors to identify ED and ES. Previous work has shown that this can be automated. We agree that a more comprehensive approach is necessary for videos that cover multiple cycles. Potential strategies include adopting temporally denser graphs and integrating ED/ES detection heads directly into the model.

Design choices: Thanks for the recognition from the reviewers that one of the main innovations lies in the combination of echocardiography features coming from a vision model that processes the video and a LLM that converts these features - potentially less meaningful to a human reader - into human explanatory text. Our decision to use a GCN to extract features and LLaMA as LLM was based on experimental considerations, following the evaluation of several alternatives. For reproducibility all details are documented in the repository and supplement, and we plan to make code and data openly available. The 6 features used in our model are based on what the clinician could be observed from the video and limited to the left ventricle. In the future, we plan to extend the features towards the observation of left atrium and the right side of the heart. We anticipate that future research will also explore the use of automated feature selection, employing techniques like reinforcement learning.

Evaluation: Thanks for the opportunity to clarify and discuss our conclusions from the experimental results. As we can see, it boils down to these three points, which we will also emphasize in the camera-ready version. (1) Evaluating free text is a complex task itself, and our experiments showed that many traditional methods fall short in the task since it involves prior knowledge that is field-specific and sensitive. Consequently, we developed a Mistral-based metric that aligns more closely with structured data. We acknowledge that the paper lacks independent experiments on this metric due to space constraints. (2) Our model appears to outperform general-purpose models, such as GPT-4 Vision, as well as models specifically trained with medical data, such as LLaVA-Med. (3) The self-instruction component enhances our modelâs performance by which a more diverse dataset is generated and improves accuracy & robustness of the explanations conversely.

Clinical Use & Motivation: The reviewers commented on clinical value and general motivation. The proposed model not only produces LV contours and EF, but also more geometrical features along with a text explanation. The proposed method allows the user to decide to trust the visual prediction or to verify it by assessing more information. This will be a first step towards a holistic AI-assisted diagnosis & reporting and may also facilitate better education of inexperienced clinicians. The reviewers commented on the size and diversity of our annotated data. We note that a lot of data is still implicitly entering our model by the pre-training of the GCN (on the full EchoNet train set) and the LLM. However, we agree that we will need to extend and diverse the data to complement evaluation. Especially for LLM, a thorough clinical evaluation will be necessary.

In conclusion, we are grateful for the high scores and constructive comments from the reviewers. We believe that the enhancements discussed, including the extension of explanations to full scans, more rigorous evaluation, and emphasizing clinical utility, will significantly strengthen our work. We look forward to integrating these improvements and continue to advance the field of AI in echocardiography."
https://papers.miccai.org/miccai-2024/258-Paper0158.html,"We thank all the reviewers for their constructive feedback, and appreciation of our work.

Response to R1âs comments:

Response to R3âs comments:

Response to R4âs comments:

Reference:"
https://papers.miccai.org/miccai-2024/259-Paper1980.html,N/A
https://papers.miccai.org/miccai-2024/260-Paper3783.html,"We thank the reviewers (R1, R3, R4) for their constructive critiques.  R1 highlights the significance of the problem tackled âthe potential to accelerate a time-consuming pre-processing step in diffusion MRIâ. R4 recognises the novelty âthe first deep-learning-based method for correcting eddy current distortions in DWIâ, the thoughtful approach âcarefully designed and meaningfully constrained [â¦] that requires a deep understanding of the problem domainâ, and performance âshow a high-performing modelâ. All reviewers highlight the clarity of the submission, especially R1 âvery clearly writtenâ.

The only major concern was raised by R4 but this was a result of a misunderstanding. The âsynb0-discoâ and âDrDiscoâ methods R4 mentioned, as R3 pointed out, solve a different problem (correction of susceptibility-induced distortion). As R3 pointed out, this submission is the first deep-learning-based approach to correct eddy-current-induced distortion - âa problem sparsely studiedâ.

Minor points:

1) R1 suggested the inclusion of computation times. For the dataset of size M (n=32 for training and n=8 for validation), using a GPU NVIDIA GeForce RTX 4090 with a batch size of 4, it took about 4 hours to train the translator and 16 hours to train the registration model. Inference takes a split second.

2) R1 and R3 inquired about generalizability:
In the present form, Eddeep is robust to some extent to brain shapes and orientations and to some unseen b-values thanks to the spatial and contrast augmentations preformed at training. It cannot however be considered capable of zero-shot generalization to any DW acquisitions. Future work may consider additional training data augmentation employing realistic simulated DW data to improve generalizability.

3) R1 suggested that Fig. 2 might have included images with b-values different from the values reported. After double-check, we can confirm that the indicated b-values are correct. For each displayed volume the intensities have been scaled by their maximal value. At b=650, the brightest voxels are now located in the cortex, making the CSF voxels appear dark.

4) Regarding R1 suggestion about the work from Nilsson et al. (2015): This interesting work is somewhat in the same vein as what is done in Tortoise, except high b-value volumes are predicted using CHARM model. Such synthetic data generation may indeed be promising future avenues to improve Eddeep generalizability."
https://papers.miccai.org/miccai-2024/261-Paper2831.html,"First and foremost, we would like to extend our sincere gratitude for the time and effort you have dedicated to reviewing our manuscript. We will explain your concerns point by point.
In response to reviewersâ common concerns, the code will be made publicly available upon acceptance. 
To R1:
3.1: We appreciate your comment and need to clarify that most technical parts, such as the directed scan module, dynamic graph layer, and gender adaptive strategy, are orthogonal to the content in literature [10]. 
3.2: Based on literature [13], interconnected regions of interest in the BAA task are related to bone age. We understand your concern, but we propose that BAA with X-ray bone images is indeed adapted to casual reasons. Thus, Mamba is appropriate for the BAA task.
3.3: The gender-specific graphs M and F in GGVMamba offer partial interpretability. Our future research will prioritize improving interpretability. 
7.1: Thank you for your suggestion. The h(t) signifies the intermediate latent state in the transition from x(t) to y(t).7.2: The reason for the importance of inter-gender disparity is that, based on clinical experience [9], the local information of hand X-rays for different genders varies. While existing studies use gender as an explicit input [2,5,6,8,24], they lead to model degradation. Making inter-gender disparity a learnable factor improves robustness, as validated by our experiments.
To R3:
3: (1) The fluctuations are due to domain biases among the three datasets. Nevertheless, GGVMamba outperforms the SOTA models. (2) We appreciate your reminder. PEAR-Net [16] is the baseline. Table 1 shows GGVMambaâs performance gains. (3) We believe that the results for âroi segmentedâ models [2,5,8] are listed in Table 1.
6 and 7: (1) We have stated the public availability of the datasets at the end of the introduction, âwe gathered data from three public datasetsâ¦â. (2) The race bias and details describing these datasets are documented in their respective references [5,7,12]. (3) We recommend zero-shot learning because GGVMamba demonstrates competitive generalization performance. In future work, GGVMamba could be applied to other unseen radiology tasks, and we will invite medical experts to conduct visual evaluations.
9: This is a valid assessment, but we claim that we have discussed the evaluation metrics in sections 3.2 and 3.3 and detailed our contributions in the introduction. 
To R4:
3.1: The concept of âdirectedâ is narrowly defined, indicating the order of semantic relevance information among patches. It is pertinent to mention that natural language possesses an inherent semantic sequence order, whereas visual data lacks this order [17]. Consequently, from this standpoint, an X-ray image after patch embedding is nondirected. 
3.2: Our design includes four directed sequences that retain complementary spatial information. Unlike the Cross Scan [17], we guarantee spatial adjacency for each patch. Unlike the BiDirectional Scan [25], the directed scan module utilizes four complementary traversal paths to address the limited contextual awareness of S6 in linear complexity. The ablation experiments confirmed that this design enhances performance.
3.3: (1)(What is causal information?) We characterize semantic relevance information as causal information, which relies on the assumption that sequential dependencies exist within the patch sequence. According to [13], X-ray patches exhibit causal information due to the interrelated structure of bone elements. (2)(Why is this statement correct?) The directed scan module represents a linear mapping of bone elements in different sequential orders, with four complementary sequence arrangements containing four types of causal information. Unfolding image patches into sequences along four specific traversal paths enables the vision Mamba block to effectively integrate causal information from different directions, helping to establish a global receptive field [17] for each bone element."
https://papers.miccai.org/miccai-2024/262-Paper2548.html,"We thank all reviewers for their constructive comments and appreciate their praise for our paperâs contribution. R1: the paper makes âcontribution to advancing cortical surface parcellation techniquesâ. R3: the paper presents âa novel application of an established method in the graphics community to parcellate the brain in the native spaceâ. R4: âThe paper presents a significant advancement in the fieldâ. We addressed the major concerns below:

*[R1]-Performance with advanced morphological features
Weâd like to emphasize that the goal of our study is to develop a lightweight and stable method for cortical parcellation with as-simple-as-possible precomputations, due to this reason only vertex coordinates were used as the input. Notably, if morphological features are involved, our method could further improve the performance. In contrast, Spherical U-Net and SPHARM-Net using morphological features performed worse than ours using solely coordinates.

*[R1&R4]-Computational demands & complexity
The computation cost is related to the number of eigenvectors (i.e. k) and the resolution of cortical surface. Specifically, it takes 44 seconds to decompose a cortical surface with 130,921 vertices when k=200 (the default setting in our experiments), which is acceptable. 
More importantly, we should indicate that our method is also very efficient on standard computational resources. It takes only ~4.5 seconds to parcellate a high-resolution surface with 141,802 vertices on a standard PC with an Intel Core i7-11700 CPU.

*[R1]-Effectiveness under extreme conditions
Our experiment was conducted on publicly released datasets, under the assumption of no topological errors. Considering that our method processes directly original surfaces, it by nature works also under topological errors, which is an advantage compared with spherical networks. We pursue to check the effectiveness under such conditions in the future.

*[R1&R4]-Empirical justification for nonlinear mapping of high-frequency information & efficiency of full-band spectral accelerated spatial diffusion
At the end of section 3.2, we conducted an ablation experiment, in which we compared the performance of two kinds of spatial diffusion mechanism across different situation, demonstrating our designâs superiority.

*[R1]-Comparison with existing methods
We merely aimed at proposing a backbone that can be directly performed on cortical surface represented by mesh, foregoing the need for spherical mapping. Therefore, we mainly compared with spherical mapping-based methods, i.e. Spherical U-Net and SPHARM-Net, where the latter is a more advanced spherical convolution network than Parvathaneni et al., and mesh deep learning method, i.e. SubdivNet. Although supervision manner is not what we are interested in, we can combine our model with unsupervised learning mechanism in future work. And we will consider adding a comparison with Spectral GCN. Directly comparing with point cloud methods is unfair because they typically search for nearest neighbors in Euclidean space for convolution and are therefore not suitable for highly convoluted cortical surfaces, while we can utilize the predefined connectivity of mesh.

*[R3]-Using Dice as the evaluation metric
We calculate the Dice score according to its standard formula in vertex-level space rather than volumetric space. Notably, Dice is a commonly used metric by almost all methods in this field.

*[R3]-More detailed description to support our claim
Following the suggestion, weâll update the manuscript to describe our claims more clearly.

*[R4]-Surface reconstruction of dHCP
We directly used the surface files released by dHCP project, and weâll update relevant description in the manuscript.

*[R1, R3, R4]-Reproducibility
Weâll release our code, and it is currently not available due to anonymity. The datasets we used are all publicly available."
https://papers.miccai.org/miccai-2024/263-Paper3444.html,"Regarding the DICE score: 
It was used as both our scoring function and performance metric, for the convenience of showing the score of supervised methods as an upper bound for direct comparison. We have also tested the mIOU as scoring function and observed similar patterns.

Regarding the 100 query size:
Given different data availability, our framework could potentially adapt to different query set sizes. Here, we used 100 as a simplified demonstration of real-life limited data scenarios. We searched query sizes of 25, 50, 75, 100, and 150 (using K-means centroids over varying Ks) and found that although having more (>100) will help, 100 seems to be a reasonable amount to represent the target distribution. However, using a query size <50 may hurt the prompt selection quality. We acknowledge the potential limitations and tradeoffs, thus in the discussion section, we have included one paragraph discussing the limitations and drawbacks of decreasing the query size.

Regarding clarity and bibliography:
We have reorganized Fig. 3 and the caption and will update it in the final version: âMeta-driven Visual Prompt Selection (MVPS) Framework. Given a training set (e.g., HAM10k) and a testing set (e.g., ISIC), the MVPS framework constructs tasks for meta-training and meta-testing. Each task consists of a pool of 1000 unlabeled images and a query set of 100 labeled image-mask pairs. The goal of meta-training is to learn a prompt retriever that selects the best prompts from the unlabeled pool, and labels them as prompt pairs to enhance segmentation performance on the query/evaluation set. The retriever takes in embeddings extracted from frozen LVMs to reduce the dimensionality of the data and reduce the computational costs associated with the retriever training. The retriever is optimized by scoring function (e.g. DICE) taking in segmentation masks of the LVM prediction against the ground truth masks in the query set. For meta-testing, the retriever selects prompts from a simulated prompt pool of 1000 images in the test dataset. These prompts are used by the LVM to perform segmentation on new query images. The evaluation is done by comparing the segmented output against the ground truth, assessing the modelâs performance in a real-world scenario.â
Additionally, we will include a more comprehensive comparison with all the related works mentioned by the reviewers and revisit the text to improve clarity.

Regarding the comparison with methods like SupPR, we compared with SupPR but didnât include due to:
Efficiency: SupPR involves contrastive pretraining of a vision encoder with around 315 million parameters, which contradicts our goal of efficient LVM adaptation. Our method, MVPS, uses a frozen vision transformer and a lightweight prompt retriever that is only 7% the size of SupPR. This focus on efficiency is why we chose to compare with LoRA, emphasizing our methodâs adaptability to medical imaging without extensive computational resources.
Technical Contribution: MVPS differentiates itself by focusing on a data-centric approach rather than modifying the LVM architecture or prompt inputs. MVPS actively retrieves the best set of prompts given an unlabeled prompt pool. Our method provides a case study on prompt effectiveness, offers a lightweight prompt retriever for selecting and analyzing effective prompts, and can be combined with other approaches without changing the underlying architecture.
Domain Generalization: While SupPR offers valuable insights in the natural image domain, its primary focus is not on optimizing prompt selection for medical imaging. For example, on dermatology datasets, MVPS achieves a 3.47% average gain over TopK, compared to SupPRâs 3.28% which has ten times our training parameters. The significant domain shift in medical images poses challenges for SupPR, which relies on cosine-similarity matching. In contrast, MVPS uses task augmentation during meta-training, resulting in better generalization across domains"
https://papers.miccai.org/miccai-2024/264-Paper0627.html,"We thank all the reviewers, R1, R3, and R4 for their insightful and constructive comments.
Dataset details (R1 W1,C1, R3 C2):
The type of surgery for each procedure and the surgeonâs years of experience will be provided as meta-information of the dataset. The analysis of meta-information and the details of dataset collection will be included in a future extension version of the paper.
Comparison with other non-uniform masking methods (R3 W1,C7):
We compare our model with SurgMAE, which adopts non-uniform masking to sample tokens from high spatio-temporal regions, achieving state-of-the-art phase recognition performance on the Cataract-101 dataset. As the papers mentioned by Reviewer 3 refer to are  image-based MAE methods, these cannot be directly applied to video-based phase recognition. To avoid misunderstandings, these papers are added to the citations as examples of non-uniform masking approaches.
Gaze heatmap (R3 C3):
The gaze heatmap is generated from the gaze data recorded by Tobii. This gaze data will also be released as part of EgoSurgery.
Dataset size (R3 C6)
As mentioned in the conclusion and future work section, we intend to enrich this dataset by augmenting the video content and incorporating footage captured from various perspectives to advance the automated analysis of open surgery videos.
More experimental settings and results (R4 W1)
The experimental results on the cross-validation setting and the performance of our model for each phase will be conducted in the future extension version of the paper.
Runtime (R4 WC1)
The GFLOPs of our model are 16.2.
Data augmentation (R4 W1)
We employ spatial data augmentation techniques, such as rotation, horizontal flip, and vertical flip. In addition, to handle the class imbalance, we adopt re-sampling strategies. Specifically, we use the ImbalancedDatasetSampler, which rebalances the class distributions when sampling from the imbalanced dataset."
https://papers.miccai.org/miccai-2024/265-Paper0065.html,"Req. 1: AI must handle incomplete annotations. Annotating medical images at a voxel level is expensive, so public datasets are often tailored to specific classes, thus partially labeled.

Req. 2: AI must handle a continual data stream. The conventional training strategies that heavily rely on âepochââa complete pass through the entire datasetâare impractical.

Req. 3: AI must handle new classes/tasks. Most anatomical structures and abnormalities are currently unannotated in medical datasets; but we anticipate that they will be annotated soon, and the AI should not require repeated retraining for each new class/task.

Req. 4: AI must train using manageable resources to avoid excessive storage and computational demands in clinical settings, particularly from an online âstreamingâ learning perspective.

Currently, no single method satisfies all the aforementioned requirements. [1-6] violated Req. 1 as they required the inputs to be fully labeled. [2-4] violated Req. 2 as they required the AI to see the entire dataset multiple times. [1-2] violated Req. 3 as they discarded the possibility of new classes/tasks. [1,5] violated Req. 4 as they required additional extensive memory (2,000  vs. our 128 samples) for data selection.

Our contributions: the CLIP module can segment agnostic classes given corresponding embeddings (Reqs. 1,3), and can learn with partially labeled datasets (Tab. 2,3). Next, the use of a replay buffer under our experiment setting eliminates the concept of âepochâ (Req. 2), enabling single-pass training on data streams (Tab. 1,2). Last, our technical contribution of DP and SUP can efficiently select important data without the need for additional extensive parameters (Req. 4), while achieving similar performance to epoch-based training (Tab. 2).

Evaluating more continual learning methods (R3). Both suggested works [3-4] required the AI to see the entire dataset multiple times, which is impossible to implement in our data stream setting (Reqs. 1-4). Unlike [3-4], our method only has to pass through the entire data once, actively learns from important samples (Fig. 3), and effectively mitigates catastrophic forgetting (Fig. 2).

Poor performance & experimental setting (R4). The setting of data streaming aims to make AI learn from the vast and ever-expanding medical data where the AI agent cannot revisit previous samples or repeatedly pass through the entire dataset. Also, the conventional static dataset with a fixed number of structures has problems when extending to new classes/tasks. On the other hand, the results of single-pass training in Tab. 1 justify the efficacy of the streaming setting. The reason behind the performance drop in Tab. 2 is due to the shifted distribution and different annotation policies between sub-datasets instead of the streaming setting.

Confusing presentations. (R3) Our final results in Tab. 2 and Fig. 2 are obtained by the combination of DP and SUP. The baseline refers to Liu et al., ICCV 2023 compared in our experiments. (R4) Fig. 1 is an overview of the method. The data point is stored in the replay buffer once arrived and the iteration number is proportional to the sampling rate. We discard the sample with the âminimum similarity to its nearest neighborâ. \sum_c^{m} S_c is the numerator and S_c is the denominator in Eq. 1. The dataset size (N=2,100) is consistent with the GitHub of Liu et al., ICCV 2023. Performance restoration is due to the shared classes across these sub-datasets. (R5) Norm function is to normalize the results across classes and S_c is computed from gt mask. We will include standard deviation in our manuscript."
https://papers.miccai.org/miccai-2024/266-Paper1468.html,"We are glad that the reviewers found our submission to be well-written and novel, and we thank them for their constructive comments. We have reviewed the manuscript carefully and addressed many of the comments on wording and missing details as appropriate. We further address the major questions and comments below.

[R1, R4, R5] Paper title needs to be more detailed.
We appreciate that the title may have been a bit vague. We have updated the title to âEmbryo Graphs: Predicting Human Embryo Viability from 3D Morphologyâ.

[R1, R4] Why not benchmark against vision transformers / other ML models?
We chose to compare our results with ResNet as CNNs (such as itself) form the basis of most recent works in automated embryo selection. Using a ViT would make it harder to disentangle performance gains from the use of a ViT from those arising from our contributions. Moreover, Kromp et al. [22] demonstrated that vision transformers (Deit and Swin) arenât universally superior to standard CNN architectures across embryo tasks.

[R1, R5] Why canât we release the dataset? Are there any open datasets that we could have used?
The dataset was obtained from 3 clinics across 3 different countries. The terms of the contracts we have with the clinics unfortunately mean that we cannot release the data publicly. Though there are open embryo datasets (such as that of [22]), none of them, to our knowledge, come with the genetic data needed for our experiments.

[R1] The pipeline was developed using a specific kind of embryo incubator (Embryoscope). How widespread are they? Can the pipeline be used with other incubators with different numbers of focal planes?

The Embryoscope is among the most widely-used timelapse incubators in the field. As such, our and many other works have focused on data captured on Embryoscopes to improve translatability to the clinic. While variations in the number of focal planes can be up/down-sampled using the Super-Focus method (which, as noted in the manuscript, we use as a pre-processing step), we cannot say for certain that the pipeline would work with other incubators with different optical setups. However, it is encouraging that some other deep learning systems such as the commercially-available CHLOE have been able to be used out-of-the-box with other incubators such as the Geri.

[R1] How much influence does the user have on the generation of reconstructions when determining âuser-defined intervalsâ within Step 2.
The âuser-defined intervalsâ define the spacing between layers in the generated mesh. The smaller the interval, the more vertices in the mesh and the smoother it looks. In the software, the interval is simply a parameter with a default value which the user can but does not need to change. In hindsight, it may be better to refer to the interval as a âfixed intervalâ - we have updated this in the manuscript.

[R5] Why was the GNN only evaluated on the American dataset?
The American dataset was the only dataset that had genetic data which was necessary to determine which embryos could be included.

[R5] Why are there more Belgian embryos in the test set?
As all clinics used the same imaging hardware, the datasets were combined and split based on the number of cells in each embryo rather than the source clinic.

[R5] The GNN and total contacts have similar AUCs, why is the GNN preferable and does this mean that the number of contacts is the most important feature?
Though the GNN achieves a similar AUC to the total contacts method, it is better in terms of recall and F1. Moreover, the GNN allows additional features such as cytoplasmic texture to be integrated relatively straightforwardly. The results do seem to suggest the number of contacts to be the most important feature we looked at. Thereâs a biological basis for this - namely that more contact leads to improved intercellular communication.

[R5] How would the total contact method perform with the other NMSs?
Poorly - this is partly what motivated Stack NMS."
https://papers.miccai.org/miccai-2024/267-Paper1181.html,"First of all, I would like to thank the reviewers and chairs for their questions and suggestions on our work. We have noticed the questions involves different aspects, mainly the details of the  modules and the generalizability of the model, the origin and processing of the dataset, the readability, etc. And then I will respond to your questions.

To Reviewer#1:
Q:Why not compare the method with SwinUnet?
A:We are grateful for suggestions for further experimentation and we will consider swinUnet as a comparison in future work. But in this paper, we have chosen the CCNet model, which is an excellent segmentation model proposed recently. And we think it has better performance than swinUnet. Experiment results demonstrate the good performance of our model compared to CCNet.

Q:The DSPConv module is similar to shuffleNet.
A:In our DSPConv module, our ideas of âconvolution only on part of the channelâ and âmixing the convolved and unconvolved parts by channel shuffleâ are not found in shuffleNet. Moreover, we have compared with shuffleNet in the ablation studies, proved that DSPConv has better performance.

Q:The method is mainly based on SegFormer.
A:Firstly, the reason we imitate the Segformer is that it has a lightweight decoder which fits better with our lightweight intentions. And then, we designed the DSPConv, VAA, and S-MHA modules to replace the original modules of Segformer. As can be seen in the ablation studies, our EMFormer is completely different from the Segformer.

To Reviewer#3:
Q:The paper does not fully explore the generalizability of EMFormer across a broader range of datasets. 
A:Thank you for your question. In our experiments, we have carefully selected the dataset to try to include both large and small targets, expecting to maximize the generalizability of our models and achieve better performance on different shapes and target sizes.

Q:Real-time applications. 
A:Weâve done the delay calculation before, and the delay of EMFormer is lower than Unet, reaching 0.03s. In the future, we will open source our codes to verify our claim

Q:Further discussion on the clinical implications.
A:In the Conclusion section, we mentioned, âExperimental results demonstrateâ¦â This further demonstrates that for clinical applications, our method can achieve good segmentation results without demanding hardware resources. This will help to improve the efficiency.

To Reviewer#4:
Q:Naming Conflict.
A:Thank you for your question, we did not realize that the name âEMFormerâ has been used, and we will revise the title appropriately. After reading the âEMFormerâ you provided, we found that it is a method applied in the field of NLP and that it reduces computations by storing features in âmemoryâ. Therefore, it is still different from our proposed âEMFormerâ

Q:Data Transparency.
A:The ACDC dataset is from a previous work âDeep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: is the problem solved?â. The hippocampus dataset is from âThe medical segmentation decathlonâ. They are both publicly available datasets. In data processing, we remove frames with only the background and split the sequence into 2D slices. Additional details are mentioned in section 3.1 of the paper.

Q:Need a discussion on the clinical significance.
A:In the Conclusion section, we mentioned, âExperimental results demonstrateâ¦â This further demonstrates that for clinical applications, our method can achieve good segmentation results without demanding hardware resources. This will help to improve the efficiency.

Q:Accessibility and Clarity.
A:Weâll be revising the abstract to ensure readability, rewriting some sentences in the paper to avoid grammatical errors, and increasing the font size of the figs

Q:Source codes are not provided.
A:In the future, we will open source our codes."
https://papers.miccai.org/miccai-2024/268-Paper1923.html,We thank all the reviewers for their thoughtful reviews. We will modify the notation and figures of the draft as suggested.
https://papers.miccai.org/miccai-2024/269-Paper1632.html,"We thank the reviewers for their comments and constructive suggestions on our manuscript. We are committed to revising our manuscript according to the suggestions, which we believe will significantly improve the quality and clarity of our work. Below, we explain our responses in detail.

Output distillation and feature distillation [R1]
We appreciate your suggestion to clarify the pros and cons of output distillation and feature distillation. In this study, we proposed the first feature distillation-based unlearning method â Model-Contrast Unlearning (MCU). In our ablation study in section 3.2, we replaced our MCU with two output distillation-based methods [5][34]. Our results demonstrate that MCU can better preserve performance on the test set while achieving a similar forgotten error, as shown in Fig 2. In future studies, we will further explore and compare output distillation and feature distillation in unlearning to highlight the pros and cons of both strategies.

Clarification the way to switch between the MCU and FGMP [R1]
We apologize for causing misunderstanding and promise to revise it in the final version.
The revised description in section 2.3 is shown below for your convenience (revisions are indicated by [brackets]):Specifically, we conduct [FGMP] every T_FGMP iterations (e.g., T_FGMP is set to 10 in our experiment) while [MCU is continuously executed], resulting in an unlearned model M_un.

Difference with the existing federated unlearning studies [R3]
In this paper, we proposed Model-Contrastive Unlearning, which marks a pioneering step towards feature-level unlearning. To the best of our knowledge, no method has yet considered encouraging the student model to learn from the âBad Teacherâ at the feature level to guarantee a higher level forgetting, marking an opportunity for innovation in unlearning. This approach differs from other studies that focus on output distillation and shows better performance in our ablation study in section 3.2. Additionally, we introduced a novel Frequency-Guided Memory Preservation method that ensures smooth forgetting of local knowledge while maintaining the generalizability of the trained global model, thereby avoiding performance compromises and guaranteeing rapid post-training.In the second paragraph of section 1 of our original manuscript, we discussed the main weaknesses of existing federated unlearning (FU) studies, highlighting their compromise of performance or privacy. Our method outperforms other state-of-the-art FU frameworks in terms of model performance and unlearning speed without compromising privacy.

Presentation quality of Fig. 2. [R3]
We promise to improve it in our final version.

Further veriï¬cation of privacy performance [R4]
We appreciate the advice on further verification methods, such as Membership Inference Attack (MIA) or empirical analysis of privacy performance. We will explore more evaluations of privacy performance and conduct empirical analysis in our future work.

Number and categories of unlearn samples [R4]
In the FU review paper [25], the authors summarized three objectives in FU: Sample Unlearning, Class Unlearning, and Client Unlearning. Previous researchers usually focus on one of these objectives, as in the paper you mentioned. In our study, we focus on Client Unlearning, which is peculiar to federated settings where the client that requests the unlearning would like to erase the contribution of its entire local dataset.

Retrain outperform Origin in Table 2 [R4]
To clarify, Retrain only outperforms Origin on one metric, i.e., accuracy, in Table 2. In our study, we have simulated heterogeneous multi-source data (non-IID setting) as described in Datasets in section 3.1. We appreciate the reviewer for raising this point. We will explore how the distribution and quality of unlearned data affect model performance in federated learning in future research."
https://papers.miccai.org/miccai-2024/270-Paper0556.html,"We are grateful for the Reviewersâ (Rs) constructive comments and valuable insights. We appreciate the acknowledgment by all the Rs on our novelty, where we propose a text-free inference in image segmentation compared to current language-guided models that necessitate clinical reports with the images. We also thank the Rs for commenting on our technical contribution to our framework, which learns cross-modal knowledge from multi-modal information during training and realizes text-free inference by self-guidance.

We respond to questions from the Rs below:

Method Elaboration (R1, R4-Q3): We were concise in our method (including training details), given the page limit, and therefore, some sections were abbreviated. We will shorten the instruction of existing components (e.g., UNet) and provide a more detailed explanation of the proposed methods (e.g., self-guidance). If space permits after adjustment, we will move the tables and figures to the main text while keeping the additional explanatory sections in the supplementary materials.

Generalizability (R3, R4-Q4): We focused on automated X-ray image analysis in this study. To this aim, we used the QaTa dataset as it is the only X-ray segmentation dataset with reports. This dataset is widely used as a benchmark in the field for language-guided segmentation tasks (e.g., LanGuideSeg [15], LViT [14] in the main text). The QaTa dataset provides a substantial, diverse collection that addresses the typical concerns of overfitting associated with smaller datasets and enhances clinical applicability with its wide range of thoracic disease representations (It is not limited toÂ QaTa but covers 14 diseases). Future studies will extend our approach to other modalities, such as the MosMedData (CT-scan). Our model can be generalized to other datasets as the proposed attention mechanism and self-guidance framework (section 2.3) can autonomously identify and prioritize crucial information from reports (location information in the QaTa) regardless of the dataset. We will add a statement regarding this as a limitation and future work.

Method justification/clarification (R4-Q1,2): We used a U-shaped network, which has been demonstrated to be effective in medical image segmentation. Transformers were selected as they can be integrated with cross-attention modules, the most widely-used mechanism for multi-modal fusion. The alignment between image and text is not our primary focus. We employed the most effective multi-modal fusion technique proposed in LanGuideSeg. We will revise our text to emphasize our method justification.

Human Evaluation (R4-Q5): We agree that human evaluation offers different perspectives on the quality of generated reports. In this study, given that our reports are semi-structured, we adopted well-established metrics [1, 2] for assessing the quality of the generated text, which can greatly reflect the accuracy of the generated reports and provide a more reproducible and objective evaluation. 
[1] Vinyals, O. et al. âShow and tell: A neural image caption generatorâ, CVPR 2015
[2] Zhihong C. et al. âGenerating Radiology Reports via Memory-driven Transformerâ, EMNLP 2020

Computational Cost (R4-Q6): The network without self-guidance has 153M parameters, and the self-guidance network has 33M (17.74%). Despite this, Adding the text generation component does not increase inference time, as it processes parallel with the UNetâs encoding phase. This ensures the generation is completed before UNet needs the generated text for decoding guidance.

Comparison to LanGuideSeg without text (R4-Q7): The LanGuideSeg framework necessitates textual input during the inference phase. We already have experiments to replace the requisite text with noise representations or randomly selected textual data, but this resulted in a substantial performance degradation (~18%)."
https://papers.miccai.org/miccai-2024/271-Paper0223.html,"We thank all reviewers for their valuable feedback. We appreciate that the reviewers find our work to be âtechnically soundsâ(R3), and âhas novelty, clinical significanceâ(R4). Both (R3, R4) agree that our method is effective in challenging surgical scenarios and achieves SOTA performance. We address all concerns and unclear details as follows.

Details of Lightweight MLPs are missing. (R1)
The effectiveness of MLPs has been demonstrated in previous works (Cao et al., Hexplane, CVPR 2023). Similarly, we utilize Hexplane (without learnable weights) for temporal dynamics factorization and multiple MLPs for capturing deformation (Section 2.2). We have proved the effectiveness of the Hexplane + MLPs with SOTA results on dynamic scenes. We will add more details in the manuscript.

Details and inheritance of Depth-Anything. (R1, R3)
Adopting Depth-Anything is a small contribution where our novelty lies in 4D Gaussian Splatting for monocular endoscopes. For monocular surgical reconstruction, depth prior is vital in our method instead of Depth-Anything. We utilize Depth-Anything to generate depth prior instead of inheritance or innovation, other pre-trained models are also compatible with our pipeline. Additionally, other works also adopt pre-trained depth (Wang et al., SparseNeRF, ICCV 2023). Regarding the choice of model, we consider accuracy, memory usage, and computation efficiency, and choose DA-small as our depth foundation.

Shortcomings of section 2 were resolved by other papers. (R1)
Although there are previous works in different domains/tasks, we are the first to adopt monocular depth estimation in endoscopic reconstruction. Our method focuses on challenges like slow inference speed, prolonged training, and inconsistent depth estimation. Additionally, we propose novel techniques to combine depth prior with Gaussian Splatting and achieve SOTA performance.

How the overall loss function can be motivated? (R1, R3)
Our final loss is a linear combination of components from different optimization aspects. We define the confidence loss by regarding dynamic areas as inherent noise since its supervisions are inherently inconsistent (different time step has different geometries while the confidence is time-agnostic). The dynamic areas have lower confidence and we penalize them by the confidence loss for detail refinement. We also notice that geometry constraints are significant. (Cheng et al., GaussianPro, ICML 2024). Therefore, we propose depth and normal regularization with novel definitions. We will clarify these in the update.

Lambda parameters for surf and conn make them insignificant. (R1)
Depth, normal, and confidence are regularization losses instead of dominant l1 loss for RGB. Therefore their lambda can not be too large to affect the main optimization. The weight for regularization terms is also relatively small (0.0001) but effective in the other works (Wu et al., 4DGaussians. CVPR 2024). We have also shown the effectiveness of the regularization with small lambda in our ablation.

Reproducibility. (R1)
Details of code can be found in the anonymized link in the abstract. We will release full details to the public.

More extensive testing. (R1)
Since it is hard to access the in-vivo surgical data, we follow the previous works (Wang et al., EndoNeRF; Yang C et al., Lerplane. MICCAI) and test on 3 scenes with 38 different time stamps. There exists a large deformation of tissues between each evaluation time stamp, and our method performs robustly against the deformation. We will add a more extensive evaluation for all time stamps.

How does the design adapt to the endoscopic data? (R3)
While it is hard to retrieve depth from the monocular endoscope, our contributed method can achieve robust real-time rendering for surgical scene reconstruction.

We will also fix the minor problems for better clarity. All the suggestions (references) will surely be considered and added to the final manuscript. (R3, R4)"
https://papers.miccai.org/miccai-2024/272-Paper0225.html,"We thank the reviewers for their efforts and great comments on our paper. We will fix all the typo and grammar for the manuscript of camera-ready version. We would like to clarify some main misunderstandings from the reviewers:

Lack of comparison with recent foundation models developed specifically for endoscopy videos. (R1)
This is what we plan to do for our future works. Thanks for the advices.

There is a lack of explicit information regarding the criteria used for selecting the two example sequences for the comparative analysis of Pose and Intrinsics Estimation. (R1)
We select these two sequences followed many pervious works like[1-2].

Why was it necessary to adapt the original LoRA approach? (R3)
LoRA has been proven a  very effective method for fine-tuning in NLP and language based foundation models. Therefore we plan to use LoRA-liked method as a great way to adapt foundation model for medical scenes.

Justification for dataset split is missing (R5)
The split followed previous works[1-2].

Do the methods used for comparison present the best outcome after 40 epochs?(R5)
We apply the same training settings for different baselines. All the other method converges around 30 to 40 epochs and our method mainly converge before 20 epochs.

the ground truth images should also be included (R5)
The ground truth depth for SCARED is sparse and only have the relatively full map for the first frame in a video. All the other GTs are obtained by reprojecting the GT of first frame with known camera kinetics. Therefore, the GTs for some of the selected visualization only contain small valid parts and we choose to not show the GT also followed previous work[1-2]

[1] Shao S, Pei Z, Chen W, et al. Self-supervised monocular depth and ego-motion estimation in endoscopy: Appearance flow to the rescue[J]. Medical image analysis, 2022, 77: 102338.
[2] Yang Z, Pan J, Dai J, et al. Self-Supervised Lightweight Depth Estimation in Endoscopy Combining CNN and Transformer[J]. IEEE Transactions on Medical Imaging, 2024."
https://papers.miccai.org/miccai-2024/273-Paper1282.html,"We appreciate the acknowledgment from R1 and R4 regarding the technical soundness and clinical significance of our work. We will release the Polyp-Twin and Polyp-Path datasets, as well as the training codes and checkpoints of EndoFinder. Addressing concerns from R1 and R3, we aim to clarify several points.

Doubts on Classification Settings (R1): We chose a binary classification task to address the clinical need for making binary decisions (remove the polyp or leave it in situ) during colonoscopy examinations. The labels, based on histological results and revised Vienna criteria, differentiate benign from pre-malignant/malignant polyps. Vienna category 1 is considered benign, while categories 3, 4, and 5 represent pre-malignant/malignant. The Polyp-Path dataset reflects a balanced distribution (57% vs. 43%). We reported sensitivity, specificity, and F1-score to comprehensively evaluate classification performance (Table 2). While more fine-grained classification benefits precise diagnosis and is acknowledged for future work, our binary classification task validated the feasibility of our image retrieval-based framework for explainable polyp diagnosis.

Lack of Validation on Public Datasets (R1): We acknowledge R1âs suggestion but note that the Kvasir dataset lacks fine-grained histological labels, making it unsuitable for direct comparison. The PolypSegm-ASH dataset, first reported in October 2023[1], is used in the ongoing AI4PolypNET challenge[2] and was not available until 15 May 2024. Our survey of related work on page 2 indicates that few papers on polyp diagnosis release datasets, and no image datasets for polyp re-identification exist. Thus, we consider Polyp-Twin and Polyp-Path significant contributions. We would appreciate if R1 could provide more comments on the public datasets.

Questions on Online Application (R1): EndoFinder encodes images into semantic hash codes for real-time retrieval from a reference database (Figure 1). This reference database, composed of hash code and clinical semantics pairs, can be built within a single centre or multiple centres. It scales up with accumulated records, and the exclusion of original images ensures secure transfer compliant with regulations for multi-institutional exchange.

Response to R3: We believe there are significant misunderstandings regarding our work. The motivation for our image retrieval-based method is clearly described in the Introduction (page 2): âTo mitigate the limitations of existing classifiers, we present EndoFinder, an image retrieval framework enhancing diagnostic explainability for colorectal polyps.â EndoFinder is built on two hypotheses: (a) self-supervised image features can retrieve the same or visually similar polyps, and (b) visually similar polyps likely share the same malignancy label. These were tested in polyp re-identification and classification tasks. Our methodâs unique advantage is that classification results are accompanied by visually similar polyps in the reference database with clear histological outcomes, enhancing explainability for clinicians. Our goal was not to outperform black-box supervised classification models in accuracy but to enhance explainability. Furthermore, from a machine learning perspective, our approach is novel. While existing methods combine mask image modelling and contrastive learning, our context involves polyp images where polyps may not dominate the image. We propose a polyp-aware mask reconstruction task using polyp masks for sampling, building a polyp-aware representation that outperforms state-of-the-art CBIR methods (Table 2). To meet real-time clinical needs, we discretise output into hash codes during inference, improving retrieval speed without compromising accuracy. We appreciate the feedback on typos and will correct them in the final version. We hope our arguments are considered and the score is revised accordingly.

[1] Y Tudela et al. 2023.
[2] https://pages.cvc.uab.es/ai4polypnet/?p=250"
https://papers.miccai.org/miccai-2024/274-Paper1232.html,We appreciate the reviewersâ constructive feedback and efforts. Below is our response. All updates will be included in the final submission.
https://papers.miccai.org/miccai-2024/275-Paper0470.html,"First, we appreciate all the reviewers for their valuable suggestions. We are thankful for all reviewers in general appreciating that this work is novel, interesting, important & useful.

[R#1, Q1] Subjective Study  for Generated Videos?
We use standard video generation metrics, including FVD, FID & IS, for evaluation. Our method outperforms in quantitative results across three datasets and illustrates superiority through visual demonstrations and two downstream tasks: semi-supervised video classification and 3D Gaussian reconstruction. Besides, we appreciate your suggestion regarding the inclusion of a subjective experiment, which will undoubtedly enhance the comprehensiveness of our methodâs evaluation.

[R#3, Q1] Difference &Comparison to DiT ?
(I) We respectfully disagree with methodology similarity between DiT and our model as well as the reasonableness for a direct comparison with DiT, as DiT is only a 2D image generation backbone and the DiT paper does NOT involve ANY VIDEO-related model designs or experiments. Thank you for pointing out it & we will clarify it in revision.
(II) Although a direct comparison with image generation architecture DiT isnât feasible, weâve presented results of a naive adaptation of DiT to video formats as a baseline: As shown in 1st row of Table 3 in paper, our full model significantly outperforms the DiT-adapted video baseline (FVDâ: 611.9 vs. 460.7, FIDâ: 22.44 vs. 13.41).
(III) We respectfully argue that our model exhibits significant effort and methodological innovation compared to DiT. Adapting DiT backbone for video format is a complex process due to the need for temporal dimension interaction. As detailed in Sec 2.1, weâve applied strategies to harness diffusion models for video format. In Sec 2.2, we introduce a spatio-temporal interlaced transformer that effectively extracts features from both spatial and temporal dimensions.

[R#3, Q2] Unique Challenges of Endoscope Video?
Video generation in dynamic scenes, such as in endoscopic medical videos, presents unique challenges due to the need to model temporal consistency and fluidity/dynamics of endoscopic tissues. As pioneers in this field, our contribution is primarily on exploring generation in endoscopic medical videos, without an existing benchmark for comparison. This exploration and the insights gained warrant attention from the community.

[R#3, Q3] Comprehensiveness of Experiments?
In Tab 1, we have demonstrated a full comparison with GAN and diffusion methods (LVDM: latent video diffusion). In Tab 2, we present a promising prospect of using generated data for the training of downstream semi-supervised models, and show a significant improvement (+10.8 F1) compared with the current generation methods. We also present another potential downstream application of 3D reconstruction in Fig 3.

[R#4, Q1] Literature review.
We acknowledge suggestions to include extra works on endoscopic image generation for a thorough literature review. However, weâd like to respectfully argue that we have indeed reviewed a range of generative tasks in the medical domain, including GAN or Diffusion-based image generation, image reconstruction, and translation in Introduction. Compared with the literature, medical video generation, particularly in endoscopic scenarios, is an uncharted area. Our proposed model, distinct from previous studies on medical image generation, presents the first exploration in generating medical videos for endoscopic scenes.

[R#4, Q2] Details.(I) What is âCLIPâ: CLIP [14] denotes Contrastive Language-Image Pre-training. 
(II) Data size: The size of training data is 210 videos for Colonoscopic, 1000 videos for Kvasir-Capsule, 580 videos for CholecTriplet.
(III) Evaluation protocol: All the compared models use same endoscopic data for training as ours.
(IV) More results for Tab. 2 & Fig. 3 & Videos: Due to page limitation, we are unable to include all content in submission. We will include these details in revision."
https://papers.miccai.org/miccai-2024/276-Paper1656.html,We sincerely thank all the reviewers for their encouraging feedback and constructive comments. We will include the reviewersâ suggestions in our final version and carefully proofread our paper.
https://papers.miccai.org/miccai-2024/277-Paper0791.html,"First, we express our gratitude to all reviewers for their valuable feedback and the general consensus that the work is innovative & effective, reads extremely well (R#3). Now, we address the specific comments.

[R#1, Q1] Further Details.
[(I) In Fig 3(a), why Full model worse than Geo prior in Geometry quality?] 
This could be due to the added diffusion prior having a slight negative impact on geometric quality. Nevertheless, the complete model outperforms the +Geo-Prior variant in terms of SSIM and visual metrics (e.g., PSNR+1.11, SSIM+0.035), indicating a significant overall gain. 
[(II) Why warm-up?]  We adopt warm-up in line with common settings in [27,16]. Initially, we optimize only the static 3D representation and then we train the deformation of static Gaussian splatting across different time frames.
[(III) Training time consumption] The average training time is about 2.0 minutes.

[R#3, Q1] Suggestions for Writing.
We greatly appreciate your suggestions! In revision, we will carefully revise literature, including standardizing the format of literature. Also, we will thoroughly modify revision for the entire text.

[R#3, Q2] Limitation?
Although promising, our method needs precise camera poses. Future work could explore unconditioned scenarios with sparse, inaccurate, or unavailable camera poses, leveraging recent progress in unconditioned natural image reconstruction (Wang S, DUSt3R, Arxiv 2024).

[R#4, Q1] Is SDS applied to images rendered from virtual cameras, or unobserved time points?
For ENDONERF dataset with fixed viewpoint and different times, we apply SDS at unobserved time points due to missing frames. For SCARED dataset with static scenes from different viewpoints, we apply SDS at unobserved viewpoints due to missing viewpoints. We will clarify it in revision.

[R#4, Q2] How the model estimate tissue deformation given 3 training views? How appearance prior help it?
Please note that we have selected three training viewpoints at equal intervals covering all sequences. Given few training views, the rendering from the reconstructed tend to contain some distortions. Intuitively, the diffusion prior learned from large-scale datasets will correct them to be more plausible and consistent with the distribution of real images.

[R#4, Q3] Novelty compared to (EndoNerf, MICCAIâ22) & (Endo-4DGS, Arxivâ24)?
We respectfully argue that our work is novel in its use of geometric prior, as it differs from these works in the problems addressed and the priors used: (1) While both EndoNerf and Endo-4DGS utilize estimated depth maps to supervise NeRF training across all views, our approach refines the geometric quality of rendering from sparsely supervised views. We dynamically update the depth on newly rendered images, providing a feedback supervision signal. (II) Different Geometric Priors: Compared to the ad-hoc endoscope depth estimator used in EndoNerf, our geometric prior is learned from a large dataset using Visual Foundation Models. Itâs also noteworthy that Endo-4DGS is a concurrent work on arXiv, and âarXiv papers are not considered as prior workâ based on MICCAI review regulations. We will include more acknowledgement & discussion of Endo-4DGS in revision.

[R#4, Q4] Split of Training & Testing Images?
We follow the common train-test split setting in EndoSuRF that splits training and testing images in a 7:1 ratio, with the test images are evenly sampled. When the sparse-view setting is imposed, we evenly sample k views from the training views and discard the remaining.

[R#4, Q5] Is the model fitted using monocular or stereo images?
The input for training the model is oriented towards monocular images.

[R#4, Q6] Ablation studies from 3 frames to the entire sequence.
We have shown the result under different k-view setting in Fig 3(b). As expected, more views correlate with better visual and geometrical output quality."
https://papers.miccai.org/miccai-2024/278-Paper0221.html,"Thank the reviewers for their valuable feedback. We appreciate that the reviewers find our work to be âwell motivated & thoroughâ (R1) and âsuperiorâ (R3). We address the major concerns as follows:

Clarify the Prompt Block (R1, R3, R5):
We use additional learnable prompt parameters to encode key discriminative information about brightness degradation levels (over & underexposure). The API block is used to learn discriminative information, while GPS integrates the learned prompt information with the image feature, guiding the modelâs learning process. We randomly initialize P values.

We observe the feature clustering of over & underexposed images in the CEC test set with t-SNE. After passing through the 1st, 2nd, and 3rd prompt blocks, the feature clustering between over & underexposed images became more distinct and better clustered (Davies-Bouldin Index 1.68->1.55->0.53). After removing the prompt blocks, we find worse clustering results (2.24->1.94->1.87). This indicates that the prompt block can help optimize the feature clustering based on the discriminative illumination information.

How the Prompt Block Works on One Type of Degradation (R1, R5):
When there is only one type of degradation, the prompt block acts like an attention mechanism and increases the depth of the network, which will not affect the performance. Tab.3 results show that our model still works effectively when only having the LLIE task.

Key Contribution Clarification (R1, R5):
The main motivation is to tackle the brightness anomalies (under & overexposure) in WCE. The core contributions are: 1) designing a prompt block to guide the image enhancement; 2) developing a new dataset.

The Selective-Scan from VMamba [13] & DWConv are from existing papers. We develop our prompt block based on these methods. We will explain our motivation & contributions in an easy-to-understand way and clarify the relationship between our proposed block and existing works.

Overexposure Results (R5):
Tab.1 shows the overall scores for under & overexposed images. When tested separately on overexposed images, our method also achieves SOTA results (PSNR: Ours 29.84, PyDiff 26.92, PromptIR 27.32).

Discussion on Less Satisfactory Results (R5):
We are sorry we have not discussed the less satisfactory results in Tab.2. Our method primarily restores images at the pixel level, leading to top-1 results in PSNR & SSIM. However, it falls behind LANet in feature perception quality, while our method still ranks 2nd in LPIPS.

Confusing & Wordy Method Writing (R1, R5):
DFT: Diffusion Transformer
FFN: Feed-forward Network
In Page 5 Para 2, we construct kernels of different sizes to provide the model with different receptive fields, and fuse them with Avg & Max Pool. We will remove the abstract descriptions of âself-adaptive dynamic feature spaceâ & âdynamic selectionâ.
We will fix all inconsistent or missed symbols in the figure for better clarification.

Result Robustness (R1):
Below are the STDs for results in Tab.1 (PSNR: Ours 29.65Â±3.51, PyDiff 28.18Â±3.47, PromptIR 28.27Â±3.79). Our STDs are within an acceptable range.
We use the downstream lesion segmentation to validate that our model can benefit clinical uses (Tab.3 last row).

Real-time Deployment (R1):
Here are the FLOPs & speed (Ours 14.36M/3.29FPS, PyDiff 97.89M/8.65FPS, PromptIR 35.59M/4.23FPS). Our modelâs size is not big, but the inference speed needs improvement. This will be our focus for future work.

Dataset Generation Process (R1, R3):
We simulate the camera aperture settings with the Adobe Camera Raw SDK to adjust exposure values (EVs). Changing the exposure value is equivalent to changing the camera aperture size. We render the raw image with varying numerical EVs to change the exposure range of highlights and shadows, emulating real exposure mistakes. Raw images are randomly assigned EV from a defined range (underexpose [-4, -3.5], over [3, 3.5]).

We do not add any additional experiments, but only use new metrics."
https://papers.miccai.org/miccai-2024/279-Paper0765.html,"We sincerely thank all reviewers for their constructive comments. Due to strict space limitations, we respond to their main concerns below.

Reviewer#1:
Q1: More discussion about the expert models in general.
A1: In general, each expert model scores a desired attribute of the generated reports. These attributes can be medical diagnosis information, disease distribution, and pathological relationships, etc. We view the product of these medical expert models as a probabilistic energy model, allowing a flexible combination of various heterogeneous attributes and refining the report generation distribution during the inference phase. To ensure applicability, the pre-trained expert model in the proposed ECRG framework requires accurately reflecting the satisfaction degree of the relevant medical attributes through the score, which is used to calculate the energy function in Equation 2 and thus affects the quality of the generated report. Regarding the limitations of ECRG, first, the performance of the pre-trained model is a significant factor affecting the final report generation results, which also applies to other methods using pre-trained systems. Additionally, although the ECRG framework based on the energy model can flexibly combine various heterogeneous energy functions through linear combination, different energy functions may inhibit each other. As shown in Table 1, the ECRG(full) model may be slightly worse than other models assisted by only one expert model in some metrics. Addressing these problems warrants in-depth research and discussion in the future.

Reviewer#3:
Q2: What is the innovation of the proposed method compared to previous studies using pre-trained systems.
A2: In essence, our main contribution is the proposal of an Energy-based Controllable Radiology Report Generation (ECRG) framework. Compared with other report generation methods using existing models, the energy-based framework has the following characteristics and advantages: First, each pre-trained expert model only needs to calculate a scalar score as an energy value to reflect the satisfaction of specific attribute. Second, the product of these medical expert systems is viewed as a probabilistic energy model, allowing a flexible combination of various heterogeneous attributes without designing specialized structures. Finally, ECRG circumvents the training process and refines the report generation distribution during the inference stage, which does not require any task-specific fine-tuning and gradient optimization. In addition, we also propose an acceleration algorithm to improve the efficiency of sampling the complex multi-modal distribution of report generation. Regarding prior studies, Zhang et al. (2020) modeled medical knowledge using a graph convolutional neural network. Wang et al. (2022) introduced a medical concepts generation network to predict fine-grained semantic concepts. These methods require designing specific network structures according to the situation and necessitate retraining on large-scale medical datasets, resulting in poor flexibility. Thus, our approach represents a novel technical solution for controllable report generation and provides a new perspective on addressing the challenges of this topic.

Reviewer#4:
Q3: Further experimentation and comparison.
A3: We select one of the state-of-the-art methods RGRG as the baseline model and present two examples illustrating the use of pre-trained expert models and medical prior knowledge to design energy functions. Ablation experiment results demonstrate the effectiveness of the energy functions designed in these two cases on the baseline model. In fact, the baseline model RGRG achieves better performance than the existing methods listed on page 1. Due to strict space limitations, we are very willing to conduct comprehensive experimental evaluations in the supplementary materials or in future work, including comparisons with more existing methods and a broader range of datasets."
https://papers.miccai.org/miccai-2024/280-Paper1856.html,"Thanks for your careful and valuable comments. We will explain your concerns point by point.

Q1 (R1&R4): Data: no evaluation on public and challenging data.
A1: Currently, there are no available datasets on depth estimation of surgical robotic scene with GT depth. Therefore, we first extracted required clips from our in-house dataset and then performed the evaluation on top of it. Moreover, we utilize standard stereo matching method to calculate GT depth. Additionally, surgical scenes may suffer from smoke, blood, and poor lighting. It is meaningful to estimate the metric depth of these scenes. However, there is no public data for these scenarios, and it is difficult to obtain GT from such data. In this case, we have not evaluated the proposed method on the more challenging datasets.

Q2 (R4): Data: how does error in GT depth calculated by stereo matching affect evaluation?
A2: We evaluated the proposed scale-aware depth estimation on clinical surgical data. The only way for calculating GT depth for clinical data is stereo matching. The stereo matching has been evaluated on public SCARED with high accuracy (RMSE of 2.959mm). Therefore, we believe that the collected data is sufficient to evaluate the performance of scale-aware depth estimation.

Q3 (R4): Data: details about the collected data.
A3: We extracted data from the in-house daVinci robotic prostatectomy dataset. The instruments used in surgery are standard daVinci instruments with a cylindrical shaft with a constant radius 4.5mm.

Q4 (R1): Method: since the compared methods are not targeted at scale-aware depth estimation, the evaluation is biased towards the proposed method. Some adjustment for scale would have been a fairer comparison.
A4: Among the compared methods, âMonoDepth Stereoâ can estimate scale-aware depth from monocular images; nevertheless, our method outperforms it. For those methods not targeted at scale-aware depth estimation, we have manually performed scale adjustments using GT information.

Q5 (R1): Method: how does the algorithm handle when instruments are out of view?
A5: In most cases, for safety reasons, the surgeon should keep instruments in view while performing procedures. If instruments are withdrawn from the scene, instrument-tissue manipulation is considered to have ceased since the surgeon cannot see instruments, so we will continue to wait for the instrument to return to the field for further surgery.

Q6 (R4): Method: the authors should report the comparison of the proposed multi-resolution depth estimation with and without the scale-aware module.
A6: This problem means the scale-aware module could improve the depth quality. However, we utilized the module to recover the 3D pose of the instrument and calculate the scale. The merging strategy is used to enhance depth quality. In our evaluation, the depth estimates without real scale will be first scaled using GT and then the accuracy is calculated.

Q7 (R4): Method: how does low-res input affect network performance? How to choose image resolutions when performing depth merging?
A7: In our paper, we described that the depth estimated by network with low-res image loses many details. You can refer to supplementary materials for further information. Besides, we have done the ablation of choosing resolutions in our paper.

Q8 (R1): Method: the runtime of the pipeline.
A8: The runtime of the proposed pipeline is around 66ms per frame.

Q9 (R2): Method: The authors claim that âwe first propose â¦â but [1] have proposed a framework that merges depth.
A9: Sorry for the confusion. âFirstâ here means that merging depth estimations is the first step in our pipeline. Our approach focuses on the scale-aware depth estimation of monocular endoscope images, not just on improving depth quality. However, [1] aims at depth quality improvement.

Q10 (R4): Experiment: providing depth evaluation results using GT tool masks.
A10: Currently, the predicted masks are accurate enough for tool pose estimation."
https://papers.miccai.org/miccai-2024/281-Paper3447.html,"We would like to thank the reviewers for their positive and constructing comments on our manuscript.

We agree with the reviewers and acknowledge that our dataset currently is mono-centric. To the best of our knowledge the way this dataset is acquired is unique. We are conducting efforts to increase our data diversity by including 200 more patients from our centre, and note that the protocol used by our scanners is consistent with that others have used for this vendor. Additionally, we aim to enrich our quantitative validation methodology with more well-established metric such as the SSIM and deep feature representation. In this paper, we focused on showcasing the precision of the quantitative values inside the areas of disease which is crucial for treatment monitoring.

Ablation studies to reveal more deeply how each change over the DNIF architecture improved our results is something we are also very interested in. However, due to the marginal differences in the quantitative image measures, their effect can only be meaningfully assessed in the qualitative reader study we performed and it is part of our future plans. From our current experiments we having noticed that the inclusion of spatial context through neighbouring slices in the training instead of a single slice improved the perception of contrast during image transition. Incorporating ADC calculation in the loss function resulted in more accurate values in the resulting ADC maps indicating it acts as a regularizer.

Furthermore, we apologise for some âpackedâ presentation of the results and the figures, it was done due to the page limit.  We would like to provide further information, such as that our deep learning network underwent hyperameter optimization, the ADC map was calculated using a linear fit and we did not notice any artifacts from geometric averaging. The downsample and upsample blocks are independent and are only connected through a skip connection."
https://papers.miccai.org/miccai-2024/282-Paper0845.html,"We appreciate the suggestions from the reviewers on our work, and we will consider their suggestions to further improve the quality of this work in either the final version or future work.
Reviewer #1â¨
Q1: Clarify the distinctions between Auto-FedAvg and Auto-fedrl.
A: Both methods input and update the hyperparameter distributions online, and the results sampled from these distributions are used as hyperparameters. In contrast, FedGraphRL uses the client state vectors of each communication round as input to update the aggregation weights offline, thus improving the efficiency of exploration and exploitation.
Q2: Writing of the article.
A: Inference loss is measured on the validation set. We will make every effort to improve the paper and enhance its readability.
Q3: Compare computation time and communication frequency of each algorithm.
A: We plan to supplement our journal version with a comprehensive analysis of FedGraphRL, including its training time and communication frequency.
Q4: Detailed explanation of pairwise distances.
A: The visualization of the relation graph results from the normalization of the learnable adjacency matrix in Eq. 5, hence the diagonal values are not 1. We will provide a more detailed explanation of the pairwise distances in the final version.
Reviewer #3
Q1: Clarify benefits in terms of health equity.
A: This study ensures that clients do not lag behind in the federated system, thereby maintaining fairness in federated learning performance. Evaluation metrics like equalized odds for group fairness are not applicable for assessing our method.
Reviewer #4
Q1: Discuss the practical deployment limitations and its applicability to other domains.
A: We plan to supplement our journal version with a more comprehensive analysis of FedGraphRL and extend its application to more clients and datasets to further establish its utility."
https://papers.miccai.org/miccai-2024/283-Paper2283.html,"We thank the reviewers for their valuable comments. We are happy they appreciated our work: The paper is well written (Rev#3 & Rev#4), novel (Rev#3 & Rev#4) and original (Rev#4), showcase some innovative elements (Rev#1), and can be considered for future submission to a top-tier conferences or journals (Rev#1). We provide our rebuttals around the following main points:

Clarity of description

Concatenated usage of frozen CLIP text encoder (Rev#1).
We aim for the embeddings of gait parameters that preserve the numerical continuity. To achieve this, the frozen CLIP text encoder is used twice. First, it encodes token embeddings of gait parameter description into F_gp, so that the clustering based on textual similarity can be avoided for final F^num. The token embedding [IS] is separately treated to help preserve the relational information between F_gp and number embedding. Second, it generates a feature vector from F_gp, [IS], and the number embedding, ensuring that F^num aligns with the pretrained CLIP text-visual embedding space. We propose to redraw Fig.3 to show the tokenization of raw text, to better present the idea.

Experiments

Generalizability of the method (Rev#4)
Our work demonstrates how to enhance representation learning efficiently, even with a small dataset. It offers a new alternative to incorporate patient metadata, which often comes in the form of tabular data."
https://papers.miccai.org/miccai-2024/284-Paper2459.html,"We want to thank the reviewers for their insightful comments. Below, we respond to their concerns individually.

Reviewer 1:

Data pre-processing details: We follow the standard practice in the literature for data scaling, normalization, and gene selection reported in [11]. As we stated at the end of the introduction, all benchmark data and source code from our project will be publicly available upon acceptance.

Reviewer 3:

Relevance for MICCAI: We want to address explicitly the reviewerâs concern that our work may not align well with the conferenceâs focus. Our primary contribution is the most extensive standardized benchmark for gene expression prediction from histopathological images. This task, at the intersection of histology and transcriptomics, is poised to shape the future of next-generation histology applications, where AI bridges the gap between medical imaging and molecular data types. We believe that this new benchmark in an emerging modality will not only be highly valued by the MICCAI histology community but also, as the reviewer rightly pointed out, will have a profound and lasting impact on numerous computational pathology and spatial transcriptomics applications.

Comparison with single-cell RNAseq reference-based imputation: We would like to point out that those methods belong to a different family of gene completion algorithms that require a paired single-cell dataset to work correctly. This paired dataset can sometimes be challenging or impossible to find, limiting the usability and practicality of such techniques. In contrast, SpaCKLE is a reference-free completion method applicable to more general setups.

Median Completion Initialization: We used median completion for three main reasons: (1) it allowed the calculation of a complete reconstruction loss between input and output matrices, which ensured faster training convergence and guaranteed non-zero predictions; (2) having a rough estimate of missing data allowed us to supervise the model in regions where a large patch of data was missing, and (3) as the previous state-of-the-art [11], it was a natural baseline to build upon. Positional encodings did not provide any noticeable improvements to the results.

Reviewer 4:

We are delighted to learn that the reviewer recognizes the significant value of our benchmark for the histopathology community, particularly considering the current lack of reference points for gene expression prediction from histology images. We fully acknowledge the importance of evaluating statistical significance when comparing models and will include this analysis in the final version of our paper. The implementation details of our methods will be fully documented in our paperâs public code repository. We also agree that there are other experiments worth considering, such as filtering the genes based on their tissue relevance. The source code of our project is already designed to be flexible, allowing users to select any specific set of genes of interest."
https://papers.miccai.org/miccai-2024/285-Paper3261.html,"We thank the reviewers for their time and suggestions. We will start our response with common feedback from reviewers.

Questionable effectiveness of the gaze data
This work aims to close a specific gap in the literature: how/whether human expertsâ eye gaze data could facilitate chest X-ray (CXR) analysis with LLM-based VLMs. From our results, integrating human expertise improved model performances for some downstream tasks, e.g., differential diagnosis (DDx), while not as much in others. Such observations imply that eye gaze data does not always improve VLMs, which is a valuable finding not reported in the literature. Both positive and negative observations from our results will stimulate future work in improving the utilisation and understanding of eye gaze data with VLMs.

Open source
For the camera-ready version, we will provide a link to the code and the dataset.

Reviewer #3
Randomly inserted errors for the Error Detection Task
It is worth clarifying that all errors introduced are clinically significant. The randomness is only used in picking which errors to use. Specifically, with the help of radiologists, we identified the top 33 most important phenotypes that must be reported in reports if present in corresponding CXR. So, when more than one of such phenotypes exists, we randomly choose a phenotype and introduce an error based on it. We will clarify this selection approach in the camera-ready version.

Vague experimental details 
About the integration of eye gaze, we rendered the gaze data directly onto the original CXR using red dots. âWithout the raw CXR imagesâ is misleading and incorrect, we made a typo. It should be a model trained âwith the raw CXR images.â We will correct this for the camera-ready version.

Reviewer #4
Limited novelty
The novelty of our work is that it is the first comprehensive study on LLM-based VLM in handling eye-gaze information for CXR analysis. By using a LLM, we extended VLMâs application to 4 real-world clinical applications. To the best of our knowledge, this is the first work in the literature on this particular topic.

No VLMs trained on medical data (e.g. LLaVA-Med) tested
This is an inaccurate assessment of our work as we have already included LLaVA-Med as well as CXR-LLaVA which was trained with MIMIC-CXR data. The result is shown in Table 3.

Reviewer #5
Limited VLM tested
We tested 10 models, two of which are our own trained models. Although they are LLaVA-based models or use LLaVAâs approach for training, each model has a diverse selection of the backbone LLM and vision encoder or usage of medically relevant data in the training phase. This ablation study of LLaVA variant models conducted in our paper allowed us to have a clearer understanding of medical domain training or even CXR-specific training effects, model size effect, and fine-tuning impact with eye gaze information.

Model response samples
We will add samples of the response in the supplementary information section of the camera-ready version.

Related work
Due to the page limit, the related work section is limited to two previous works. We will include some of the suggested works, but still, the models do not leverage text modality restricting their usage in clinical applications.

Reviewer #6
Larger models do not perform well due to their higher model parameters is a simplistic assertion
In our manuscript, we did not assert that the larger model did not perform well because of the larger model parameters. We only said the higher model parameters do not always guarantee performance improvement (âThis result suggests that adding more parameters does not directly translate to performance improvementsâ). We hope this response clarifies the misinterpretation of our findings about the larger model. In general, we agree with the reviewer that in-depth analysis and studies focusing on the larger modelsâ poor performances would be a very interesting future research direction.

Thank you for your time and consideration."
https://papers.miccai.org/miccai-2024/286-Paper0703.html,N/A
https://papers.miccai.org/miccai-2024/287-Paper2889.html,"We greatly appreciate the thorough feedback provided by the reviewers and their acknowledgement of DataDIVAâs efficacy in enhancing model generalisability. We have responded to the questions and further enhanced the paperâs clarity in light of helpful suggestions.

We have verified the efficacy of this approach through internal and external tests, with two network backbones, on different clinically meaningful tasks. Nevertheless, we acknowledge that more insights can be brought by 1) investigating the performance of DataDIVA on long-tailed classifications with a complex mixture of multiple diseases; 2) studying the impact of sample size M (we currently use a small sample size M considering such a label workload is manageable for clinical experts in real-world scenarios); and 3) comparing the performance between DataDIVA model with that trained on full data. We value the reviewerâs suggestions regarding these experiments and intend to incorporate them in the future extension, considering the substantial involvement of additional benchmark datasets, performance comparisons, and space for results and discussions.

2) We will add more details to Figure 1, including the involvement of foundation models and clarification on various components. All suggestions relevant to the presentation will be incorporated into the camera-ready submission.

3) Within each cluster, we calculate the distance between data points and the cluster centroid. To remove those points that are extremely far away from the centroid (defined as outliers in this paper), we set a threshold of 95% and removed the data points that were distributed farther than 95% of the distance distribution. This clarification has been included in the paper.

4) The number of quantile subgroups for each cluster should impact the model performance. We allocated more space to investigate the influence of diverse and balanced sampling (Table 1), cluster numbers (Supplementary Figure 1), the basic models for feature extraction (Supplementary Figure 2), and various network backbones (Table 2, 3). The further study of hyperparameters will be included in the future extension.

5) We adhere to default data splitting for EyePACS on Kaggles. The AIROGS, without default splitting, was randomly split into 70%:30%, where 70% works as the unlabelled pool and 30% for internal evaluation. Only the images sampled from EyePACS (600 images, 1.7% of the EyePACS train set) and AIROGS (1200 images, 1.7% of the AIROGS train set) were used for model training. All external datasets (APTOS-2019, IDRiD, REFUGE, and ORIGA) remain isolated for external testing. We have revised the description to avoid any confusion.

6) Although certain datasets are dedicated to the same tasks, e.g. EyePACS, APTOS, and IDRiD for diabetic retinopathy, the model performance on each may vary due to dataset characteristics, including data size, heterogeneity of imaging devices and quality, and degree of label noise. This dataset discrepancy makes the âchallengeâ of datasets varied, so that sometimes internal test performance does not necessarily outperform that of external tests. This can be observed in Figure 2 of RETFound [31], where the model trained on APTOS-2019 performs similarly on external tests with IDRiD, compared to the model trained and tested internally on IDRiD."
https://papers.miccai.org/miccai-2024/288-Paper0920.html,"We thank all the reviewers for their constructive feedback, we have grouped concerns raised by the reviewers and provided answers to each below:
Innovation of the proposed methods (R1, R3): While R4 commented positively about the novelty of our work, R1 and R3 raised concerns. Masking part of the input and asking the model to reconstruct the masked parts exists as a pretext task [23,3] targeting single 2d or 3d image classification or segmentation as the downstream task. The loss function is usually a simple l1 loss. The novelty of our method is in formulating this pretext task for finding the changes between the baseline and follow-up MRIs.  Here, instead of a single masked input that the model had to reconstruct the masked parts, we had two inputs. For one, we masked random parts (eq1) and randomly added Gaussian noise, blur and Gibbs noise. Then, the task of the model was to highlight these masked regions, which indicate the regions that changed (op1 and op2 in eq2). This is done with a novel combination of loss functions (eq3-6) and training procedure (sec 2.1). To our knowledge, this has not been studied before . Our downstream task then utilized  two timepoint MRI scans available in the MSSEG-2 train set to train and evaluate our model to detect new lesions (sec 2.3). 
There are several works on synthetic lesion augmentation in MRI scans. In LesionMix [10] and CarveMix [4], they used already available lesions in the dataset to add synthetic lesions, and in [5], they utilized GAN to generate synthetic lesions, which is not feasible to do during the training loop. However, our method did not depend on the datasetâs already available lesions. It could generate random lesion shapes and textures on its own, placing them randomly on white matter. Since we utilized basic image processing operations (sec 2.2), we could use this method during the training loop. Perhaps the most similar paper to our method is âLabel-Free Liver Tumor Segmentationâ Hu et al. proposed for synthetic tumor generation in CT scans, in which we modified their method to generate synthetic white matter lesions.
Fairness in comparison with other methods (R3 and R4): Since we only had access to the MSSEG-2 training dataset, we evaluated our method using 5-fold cross-validation on this dataset. In experiments, we compared our method with three different research studies, all of which have published their code. However, only PreUnet has published their 5-fold cross-validation results on the training set of MSSEG-2.  Coact reported their result on a single fold of train set of MSSEG-2, and SNAC utilized the test set of MSSEG-2, which we did not have access to. Hence, we decided to use the same folding split as PreUnet, which was done with the MONAI CrossValidation function with a seed=42 to avoid retraining their method.  We utilized this folding split to train our model, Coact and SNAC.    Hence, we assure R3 and R4 that all the comparisons are based on an equal folding split. We will also add these details in the final version and thank R3 and R4 for pointing out this matter.
Potential overlap between MSSEG-2 and MSSEG dataset (R3): The reviewerâs concern is valid since having an overlap between the pretraining and training datasets could invalidate the results. We have considered this observation in our experiments and emailed organizers of the MSSEG-2 challenge (Emails available on the challenge website), and they confirmed that there was no overlap between the MSSEG-2 and MSSEG dataset. We thank R3 for pointing out this matter and will include this detail in the final version.
Clarity: While R4 praised the clarity of the writing, R3 and R1 expressed concerns. We thank R1, R3, and R4 and assure all reviewers and area chairs that we will make detailed revisions, focusing on the issues raised by R1 and R3, to simplify the methods section and diagram in the final version, enhancing both clarity and quality."
https://papers.miccai.org/miccai-2024/289-Paper0511.html,"We greatly appreciate the positive feedback and insightful comments provided by all three reviewers. We are confident that these will improve the quality of the manuscript. Below are our replies to each point raised:

Reviewer 1

Q1: âIdeally, one of the three datasets used in the study â¦ should have been held out. However, based on the description in the manuscript, this is not the case.â

Thank you for highlighting this; we agree with your observations. We believe that evaluating performance on both internal and external datasets is crucial to compare how results from internal settings generalize to external ones. However, due to space limitations, we were unable to include this analysis in the current paper; it will be addressed in our forthcoming journal extension.

Reviewer 3

Q1: ââ¦ quantifying the uncertainty in the model projections would be desirable and would presumably make this closer to clinically usable.â

Thank you for your intriguing suggestion. In line with our proposed Latent Average Stabilization, we believe that estimating the deviation from the theoretical mean is an effective starting point for quantifying the uncertainty of the predictions. Further investigations will be detailed in the journal extension of this paper.

Q2: âCiting a few specific clinical applications where such longitudinal generative modeling is being used or has strong potential for use would be helpful.â

Thank you for the suggestion. We are pleased to include potential clinical applications in the introduction section. Disease progression models, such as BrLP, are primarily used for patient stratification in clinical trials and treatment deployment [1]. These models help identify the stage of the disease at which a treatment is most likely to be effective. By focusing on patients within this optimal window, fewer participants are needed in clinical trials, thereby reducing costs. Similarly, BrLP can be used to develop a generative Digital Twin (DT) [2] for patients with neurological disorders. Preliminary studies are investigating the capability of generative DTs to digitally replicate the disease progression of control subjects in clinical trials [3,4]. Developing this capability would enable the analysis of treatment effects at the individual level and potentially reduce the need for control participants who receive no therapeutic benefit.

[1]  Young, Alexandra L., et al. âData-driven modelling of neurodegenerative disease progression: thinking outside the black box.âNature Reviews Neuroscience(2024): 1-20.

[2] Bordukova, Maria, et al. âGenerative artificial intelligence empowers digital twins in drug discovery and clinical trials.â Expert Opinion on Drug Discovery 19.1 (2024): 33-42.

[3] Fisher, Charles K., Aaron M. Smith, and Jonathan R. Walsh. âMachine learning for comprehensive forecasting of Alzheimerâs Disease progression.â Scientific reports 9.1 (2019): 13622.

[4] Walsh, Jonathan R., et al. âEvaluating digital twins for alzheimerâs disease using data from a completed Phase 2 clinical trial.â Alzheimerâs & Dementia 18 (2022): e065386.

Q3: â2.1 seems out of place with the rest of the section. This is probably better served as itâs own âbackgroundâ section.â

Thank you for the suggestion. We will move the âBackgroundâ subsection out of the âMethodsâ section.

Reviewer 4

Q4: âWhat is the average interval time between the base scan and follow-up scans in the used dataset?â

The average time interval between baseline and follow-up scans is 4.3 years (SD = 3.1), with a maximum span of 16 years. We will add this information in the revised manuscript.

To Reviewers 1, 3 and 4: Thank you for your corrections on typos and figures. We will address these in the camera-ready version of the paper."
https://papers.miccai.org/miccai-2024/290-Paper1644.html,"We sincerely appreciate the reviewersâ insightful comments and constructive suggestions. Their unanimous recognition of the innovation, importance, and practical application value of our method is truly encouraging. Below, we address concerns raised by the reviewers:

Following the constructive suggestions, we will report five-fold cross-validation results in the final version. We will also make our code publicly available along with the cross-validation results to promote reproducibility. We will also improve the manuscript following the reviewersâ specific suggestions.

R3: 1) In the present study we evaluated our method on binary classification problems. However, it is straightforward to apply our method to multi-class classification problems. It merits further investigation to extend the method for survival analysis. The method does not directly work in a completely unsupervised setting, but it can be used to finetune models built using self-supervised contrastive learning or integrated with self-supervised contrastive learning in a semi-supervised learning setting. 2) Despite the promising classification performance, the classifier-driven methods are not equipped to learn compact image representations. We will improve the abstract following the reviewerâs constructive comments to provide accurate descriptions of existing methods.

R4: 1) The studies of Yang et al and Basak et al both learn features guided by information at an image patch level, while ours use contrastive learning to learn features guided by information at the WSI level. We will include these references and discuss the similarity and differences between them and our method. 2) We have compared our method with those built upon self-supervised contrastive learning, Transformers, and graph-based techniques since they are widely used. More comparison results with cross-validation will be provided in the final version, particular those built upon contrastive learning. 3) Our method achieved remarkable accuracy, even though its AUC value on the lung dataset wasnât the best. 4) Semantic information refers to the visual patterns of the WSIs (e.g., shape, color, common patterns). WSI class information means specific disease related information (e.g., disease and normal regions).

R5: 1) We will update the results with cross-validation and provide statistical significance test results. 2) For a fair comparison, we trained all models using code released by the respective authors with consistent training and testing splitting settings, i.e., all the methods under comparison were based on the same training and testing datasets. 3 ) We will improve the manuscript and correct all typos."
https://papers.miccai.org/miccai-2024/291-Paper2280.html,"We thank the reviewers for their thoughtful and constructive feedback. We are glad that our work was received positively and appreciate the reviewersâ detailed comments and suggestions, which would be invaluable in improving our paper. Below, we try to address some of the reviewersâ comments

Segmentation Results Validation (R1):
To the best of our knowledge, we did not find datasets on tissue segmentation and TILs segmentation for other cancer types, making it difficult to perform a quantitative evaluation. Therefore, we assessed the model qualitatively to ensure satisfactory performance. At this stage, our priority was establishing the core concepts before investing more time in making the segmentation model more generalizable and better validated. In the future, we plan to focus on annotating slides from other cancer types for better generalizability and validation.

Experiments Implementation Details (R3)
Our experimental designs follow other established survival models in the literature, such as PatchGCN, HVTSurv, etc. Our model draws significant inspiration from PatchGCN, including adopting the GNN architecture design with a few modifications. These modifications include incorporating edge attention using GATv2, more layer normalizations, and the addition of Nystrom attention for improved pooling. Consequently, we decided to use a similar set of training hyperparameters to PatchGCN, such as learning rate (lr), epochs, optimizer, hidden dimensions, etc.
Regarding model baselines, we followed the same hyperparameters used in the PatchGCN code repository for PatchGCN, AttentionMIL, and Deep Attention MISL models. We adjusted the number of layers to two for all the graph models (PatchGCN, DGNN, GTN). This configuration led to better training performance for 10x patches compared to the 20x patches originally used in the PatchGCN paper. For GTN, HVTSurv, and TransMIL, we used the hyperparameter settings from their respective original code repositories.

Our code will be made available online, which will further clarify the settings used for both the baselines and our model.  We will incorporate the feedback regarding more clarity(R1, R3, and R5) to the best of our abilities."
https://papers.miccai.org/miccai-2024/292-Paper1928.html,"Dear Reviewers,

We appreciate the time and effort you invested in reviewing our paper and providing valuable feedback. Below, we address the concerns raised, particularly clarifying the novelty of our approach.

The innovation of ECDR hinges on two novel components, namely a disentangled anomaly generation process and an ensembled inference procedure, explained in more detail below. Our experiments demonstrate that these two innovations are key to overcoming the limitations of previous methods, which fail to detect subtle anomalies.

Disentangled Anomaly Generation (DAG): [20] introduced Foreign Patch Interpolation (FPI) to generate synthetic anomalies using only square shapes. [16] uses an enhanced FPI approach, with random shapes and smoothed interpolation edges to avoid sharp edges in the synthetic anomalies. Differently from [16], we first introduce a normalization step so foreign patches have the same intensity ranges as the training images. Furthermore, we propose to generate anomalies by separately manipulating shape, texture, and a novel intensity bias component. The intensity bias component randomly shifts specific tissue types at the synthetic anomaly location. DAG allows for more realistic and varied synthetic anomalies compared to existing methods [16, 20], achieving a better coverage of the anomalous distribution which in turn improves the robustness and generalizability of our method.

Anomaly Localization with Ensembled Restorations: Unlike previous works that used a multi-step restoration for inference, our method proposes an ensemble of single-step restorations, each based on a different assumption on the severity of the anomalies present in test images. By considering different assumptions of severity, our method is more robust than [16,20] and generalizes better to real brain MRI anomalies, specifically those that appear as subtle contrast changes from the expected normal anatomies.

The relevance of these novel contributions is strongly supported by a consistent performance improvement upon [16], by 5% and 13.8% in the BraTS-T2 and ATLAS datasets respectively, and most importantly, by 41% in BraTS-T1. Regarding R3âs comment, âthe authors claimed that the approach can generate subtle abnormalitiesâ, we want to clarify that we do not claim our method can generate more subtle anomalies. Our claim, supported by the quantitative results on BraTS-T1, is that our method is more robust to subtle, real medical anomalies at test time, thanks to the interactions of DAG and the Ensembled Restorations inference procedure.

We appreciate the Reviewersâ suggestion to evaluate our method on additional modalities, like Chest X-Rays. We now have preliminary results showing strong performance in Chest X-Ray that will be included in future work. Consistently, [10] shows that methods performing strongly across the three Brain MRI datasets were also often competitive in CCR and CheXpert datasets.

We are also evaluating Wolleb et al.2022 in our future work, although this approach is weakly supervised, requiring both healthy and anomalous training samples and image-level annotations, making comparisons not like-for-like.

R4 noted that the results reported in [16] do not match the results in Table 1. We clarify that [16]âs arXiv submission was updated on 05/03/2024, and we included this latest version.

We also thank R3 for suggesting to quantitatively evaluate restoration image quality. Although not critical for Unsupervised Anomaly Detection, counterfactual restorations address important interpretability aspects, and R3â suggested experiment is an interesting idea to expand this work.

As suggested by R4, we will include the multi-step anomaly score in the qualitative comparisons. We will also include details regarding patch-wise pipeline and its justification as suggested by R1. We use patch-based training to make the pipeline more flexible and applicable seamlessly to other image modalities with higher resolution."
https://papers.miccai.org/miccai-2024/293-Paper0300.html,"We sincerely appreciate all valuable comments from reviewers (R1&R3&R8) and AC. Overall, they found our method well motivated (R8) and novel (R3) with extensive tests (R3&R8), addressing a clinical challenge (R3), and achieving real-time speed and superior performance (R1&R3&R8). Below, we address specific comments."
https://papers.miccai.org/miccai-2024/294-Paper1378.html,"We thank reviewers for their comments. Replies are itemized as follows:

Q1(R1&R6):Clarify the wavelet convolution layers in multi-scale spectral analysis.
In Eq2, the 1-D signal of length N from each channel in x_CE is taken as input, resulting in x_p after periodic padding. Wavelet decomposition occurs simultaneously across all channels C. We will add subscripts to x_CE for clarity. When R=2, the starting index is N, and the end index is negative. Periodic padding ensures indices exceeding the signal length wrap around to the start.

Q2(R1&R3&R6):Explanation of ablation studies.
(1)Multi-scale division is based on the 6 SEEG rhythms. Since single-scale canât fully capture brain activity, features across all rhythms are necessary. Spectral scales align with temporal and spatial domains. Analyzing scales 1 to 6, we found multi-scale increased temporal, spatial, and spectral accuracies by 15.4%, 13.9%, and 12.9% compared to single-scale, confirming the need for multi-scale feature extraction.
(2)Many studies show Db4 excels in processing SEEGs, so we chose it to extract rhythms. We selected wavelet types from db_n, sym_n, and coif_n(nâ{2, 3, 4, 5}), at levels from 1 to 6. Accuracy increases with more levels, with db outperforming sym and coif. At 6 levels, Db4 achieved the best accuracy of 99.89%, confirming our conclusion.

Q3(R1):Discussion of model generalization.
We used regularization to avoid overfitting. To evaluate generalization, we conducted cross-patient study with 4 patients for training and 1 patient for testing, performing 5-fold cross-validation. The accuracies for each patient are 1, 0.991, 1, 1, 0.998, averaging of 0.994, confirming the modelâs good generalization.

Q4(R1):Discussion of other methods for extracting multi-domain or multi-scale features. Discussion of the results.
(1)As stated in the introduction, previous studies overlooked seizure propagation and its multi-domain, multi-scale variations, with few related methods. Existing multi-scale RBF and TripleGAN improve temporal-spectral extraction but ignore spatial propagation and have unstable training. Our method deeply mines seizure features across domains and scales, effectively avoiding these issues. (2)We discussed results immediately after each experiment to clearly demonstrate our modelâs performance. We can move them to the discussion section and extend them.

Q5(R6):Clarify the CE module.
The part from âinput SEEGsâ to âdynamic subband SEEGsâ is the CE. The embedding of CE consists of a series of âConvâ layers(see in orange in Fig1), which compress and expand  channels. No âLinearâ layers are used. In Fig1, â134â and â138â denote dilation rates of 4 and 8, while âXâ in â13Xâ denotes the number of SEEG contacts for different patients.

Q6(R1&R3&R5&R6):Publicly available data and code.
We will public the code on GitHub and anonymize the LTSZ dataset. If needed, please contact the authors. We will provide
necessary data support.

Q7(R5):Analyze the differences between datasets.
As stated in the paper, our LTSZ dataset differs from the Bonn and CHB-MIT datasets in neural activity patterns. LTSZ comprises SEEG, CHB-MIT comprises EEG, and Bonn includes subsets C&D with deep electrodes, E with strip electrodes.

Q8(R5):Comparison experiment.
When using the time series prediction model Informer for epilepsy detection, the training speed is slow, and the final accuracy is 0.948, lower than our methodâs performance.

Q9(R5):Differences with Transformer results. Discussion of model complexity.
Considering seizure propagation, the multi-scale, multi-domain captures SEEG patterns better. While TSS-Transformer is slightly more complex than Transformer, it reached 100% detection for all patients except ID2. Our method also extends to epilepsy prediction and localization studies.

Q10(R5):Differences in formulas in TSS-Transformer and Transformer.
The TSS-Transformer is formally identical to the Transformer, with the name clarifying module connectivity."
https://papers.miccai.org/miccai-2024/295-Paper1131.html,"We thank reviewers for their valuable feedback. We are glad that they 1) found our task âimportantâ (R4), 2) realized our method âsound and novelâ (R3, R1) addressing âuntrivial harmonization challengesâ, which makes it âvaluable in real worldâ (R1), 3) described our evaluation and baselines âthrough and relevantâ (R1, R4), and 4) found our algorithm description âdetailedâ and our articulation âreally niceâ (R4). (The main text reference list is used)

a) R4 asked for clarity on supervision in our method. The âsupervisionâ is the scanner effects that appear as dissimilarity in matched images, which are images of a subject taken on the scanners with short gap. Such images show same brain with scanner effects as voxel-wise differences. Matched data made of matched images for a population and is ideal for evaluating harmonization but is inherently small. Ours is one of the largest [3, 4]. We used our matched data as âtarget dataâ and used its matched aspect (supervision) for evaluation and neither augmentation nor model training. We did cross-validation and evaluated our methods on the âcombination of all tests sets across foldsâ. For the tissue-type augmentation, we hypothesized that scanner effects appear as tissue distribution differences, which is not the golden truth (scanner effects). We clarify this point in paper.

b) R1 was concerned that MISPEL surpassed our method in SSIMs and that lack of ground truth does not validate the post-harmonization increase of SVD grouping as the accurate biomedical difference. We state that scanner effects appear as image modification and disturbance of biological signals in data. These should be evaluated âalongsideâ, leading us studying SSIM, bias for biomarkers of AD, and SVD as biological signal. We surpass MISPEL in the last two as more important criteria. Showing âsolelyâ improvement in possibly disturbed biological signal was commonly used for harmonization evaluation in literature. Spec. increase in effect size was conducted in [3, 13]. Thus, golden truth for SVD grouping is not necessary.

c) R4 was confused with SSIM interpretation in our harmonization evaluation and ablation study. We clarify that SSIM was used for âevaluating harmonizationâ and not showing the anatomy preservation. We expect âminorâ increases in SSIMs as the result of increased similarity at the image level. For anatomy, we rely on our evaluation using anatomy-based biomarkers of AD. Please see response a) for more details. For the ablation study, we state that MAE and JD were reported for augmentation removal on âaugmented imagesâ and SSIMs were for harmonization evaluation on âmatched dataâ. We thus can conclude that the trained models for ablation removed augmentation but did not harmonize as the result of unchanged SSIMs. We clarify this point in paper.

d) R3 was concerned with our technical contribution. We remind R3 that our first augmentation method was taken from another task and used mainly as a baseline. Our second augmentation is pure novel and beats all methods. Also, we used MISPEL in our pipeline to be able to validate that harmonization is due to augmentation when compared to MISPEL as SOTA.

e) We apologize for ambiguity of our paper to R3. We assure R3 that their concerns will be resolved by brief clarifications in paper, e.g., our objective for âvalidation on augmentationâ. For R3âs concern with âconfusing references for matched aspect of target dataâ, we refer them to response a). We also remind R3 that we defined âmatched dataâ, target dataâs demographics and scanner information in Introduction, Section 3, and Supp Table1, respectively. For more clarity, we refer R3 to responses a) and b).

f) Minor concerns
We refer R1 and R3 for their concern with size of data for evaluation to response a). 
We inform R1 that we trained CALAMITI in a supervised setting. We clarify this point.
We inform R4 that we will add their comment for limitation of our method to target scanners to the Conclusion."
https://papers.miccai.org/miccai-2024/296-Paper2292.html,"We thank the reviewers for taking the time to review our submission and for giving a thorough feedback. We are glad that the reviewers find our paper âwell-written and easy to followâ (reviewer 3) and that the âcontribution [â¦] is clearâ (reviewer 1, 3, and 4). In the following, we address the concerns of the reviewers in separate sections.

Reviewer 1:

INR vs other methods:
Linear interpolation on the per-voxel ODF estimates is reasonable for moderate to high SNRs, but fails at high resolutions due to the low SNRs, resulting in poor estimates with extremely high variance (see SHLR-Raw results in [5]). Our deep neural network learns continuous spatial basis functions, promoting spatial regularity and smoothing in the ODF field, thereby improving estimates for noisy high-resolution data.

Using the Gaussian process prior in Eq. (3):
This GP is in fact used for prediction. The mean and covariance of the Gaussian in Eq. (3) are both a function of the deep network parameters, which learns the spatial correlation structure of the ODF field. Therefore, inference using the resulting GP model will result in flexible spatially regularized inference + interpolation that is robust to noise + sparse angular samples.

Models beyond the FRT:
Our methodology is adaptable beyond the Funk-Radon Transform (FRT). It works with any fixed, linear, and rotationally invariant forward model, affecting only the G matrix while maintaining the rest of the approach.

Reviewer 3:

FSIM as the only quantitative measure:
We chose the FSIM measure due to its sensitivity to image âvisual qualityâ, fine-grained details and texture, which are of the utmost importance for high-res imaging.
The reviewer rightly points out the importance of analyzing bias/variance in downstream diffusion quantities. We will include this analysis in the supplementary material.

HashEnc on standard spatial resolution:
HashEnc is suitable for lower resolution dMRI scans such as the ABCD dataset [1], but our focus is on high-resolution data where traditional methods fail to perform. For the performance of SIREN on the ABCD dataset, see the supplementary section of NODF [5].

Related references & tractography:
We appreciate the feedback and will include the references in the related work, in addition to a tractography figure.

Reviewer 4:

How HashEnc enhances training speed:
For inference at a single voxel, HashEnc requires only 13k parameters, speeding up both forward and backward passes compared to SIRENâs 10 million parameters. Increasing HashEncâs embedding vectors from 2 to 4 doubles its capacity but adds only 1792 parameters to the forward/backward pass (size of input feature vector increases from 28 to 56). In contrast, doubling SIRENâs parameters means that all 20 million parameters are required for any voxel inferences, greatly increasing computation time.
We will add this additional explanation to the method section.

Comparison with other methods:
We focus solely on SIREN to benchmark against similar implicit neural network methods. Other approaches arenât resolution agnostic and use different inputs, such as raw signal data. At submission, to the best of our knowledge, the only published INR method for dMRI data with publicly available code was NODF [5]. We will add the 2 additional suggested references from reviewer 3 to the related work.

Counter intuitive results of SIREN:
We thank the reviewer for the feedback and recognize that the results for SIREN M=70 are unintuitive compared to M=40 and M=20. As we were rerunning the experiment to validate the numbers, we realize a mistake in the selection of the regularization strength. We will fix the FSIM scores for SIREN M=70 and update the table accordingly.

References:
[1]: Casey et al. âThe adolescent brain cognitive development (ABCD) study: imaging acquisition across 21 sites.â
[5]: Consagra et al. âNeural Orientation Distribution Fiel"
https://papers.miccai.org/miccai-2024/297-Paper3515.html,"We thank the reviewers (R) for their insightful feedback. This discussion aims to clarify confusion and fill in missing information from the paper. We kindly request reviewers to refer to our supplementary material for a comprehensive comparative analysis of DSC and uncertainty metrics variability across slice propagation methods, datasets, and UQ methods. It also contains GIFs demonstrating uncertainty progression in volumes for different UQ methods (R4). Editorial comments will be addressed in the final version.

NOVELTY and CONCLUSION (R1, R4, R6): Self-supervised slice propagation methods promise to reduce the burden associated with automatic segmentation. However, as our work demonstrates, such approaches are prone to failure and thus cannot be trusted in sensitive clinical scenarios without UQ. Our work exposes and addresses this significant gap. While we do not propose a novel method of epistemic UQ estimation, we provide open-source, non-trivial extensions of existing methods to this new task and benchmark their performance. The resulting contribution is two-fold: we both enhance the safety and usability of slice propagation methods and provide an analysis of the effectiveness of epistemic UQ estimation methods in a new context, which is significantly underexplored. We highlight the strengths and limitations of current approaches, paving the way for future innovations required for slice propagation methods. The guidance on scalable reliability and trustworthiness via UQ is also valuable to the MICCAI community, where safety and real-world applicability are crucial.

CHOICE OF UQ METHODS(R1,R4): The selection of 5 diverse SOTA scalable [2] epistemic UQ methods is informed by their varied approaches (covering frequentist and bayesian perspectives) to addressing model uncertainty effectively. Each method was chosen for its unique strengths: Deep and Batch Ensembles provide robustness through model averaging; MC Dropout and Concrete Dropout facilitate practical uncertainty estimation during training and inference, critical for deployment in clinical environments; and SWAG captures variability in model parameters through its approximation of the posterior distribution. This diverse toolkit allows us to comprehensively evaluate and enhance the predictive reliability and interpretability of self-supervised slice propagation methods. We choose to use only four ensemble members based on empirical findings, balancing computational(limited GPU memory) requirement and performance gain.

EPISTEMIC vs. ALEATORIC UNCERTAINTY(R1): Epistemic UQ is a more difficult task with multiple proposed methods all of which rely on assumptions/have limitations to make them scalable - necessitating the need for benchmarking. Aleatoric uncertainty (UQ) is typically learned as a function of input data by making the output probabilistic. This method is well-established, so itâs less of an open research question. Since this paper focuses on weak/sparse supervision(via single slice annotation), quantifying the modelâs knowledge base trained using the available supervision via Epistemic UQ is more important clinically and for future design of new algorithms. We acknowledge the significance of both uncertainty types in comprehensive diagnostics. Our future aim is to integrate advanced modeling techniques, such as probabilistic deep learning, to effectively address and mitigate aleatoric uncertainty.

DATASETS(R1,R4): We selected datasets aligned with those used in the original validations of Sli2Vol and Vol2Flow, ensuring the relevance and comparability of our findings. Sli2Vol and Vol2Flow methods are domain-agnostic due to the underlying self-supervised registration method (using edge profiles similar to MIND features). Our experiments demonstrated minimal differences in model performance due to dataset shifts(organ or domain, ct vs mri). However, these methods are sensitive to post processing techniques, which we will share on our GitHub Repo."
https://papers.miccai.org/miccai-2024/298-Paper3085.html,"We thank the reviewers for their positive feedback. This work examines the fairness of Neural Collapse (NC) when training with biased datasets, which the reviewers find an interesting topic (R1, R3). The reviewers find the paper clear and easy to follow (R1, R3). The results derived from 3 public datasets on the correlation between fairness and NC are well presented (R3). The main novelty of this work is the evaluation of model training dynamics under group-bias through the phenomenon of NC. Our results can help researchers better understand model bias and build NC-inspired fair methods.
Concerns and our reply:
1) Prior research on learning under label noise limits the paperâs novelty (R4):
We want to emphasize that our focus is not on label noise in general, but on label bias (noise affecting a single group). We are the first to examine the influence of label bias on deep classification models through NC in the context of group fairness, building on the literature on label noise and NC [1-2].
2) The paper should explore setups where labels correlate with sensitive attributes (R4):
If we understand the reviewerâs comment correctly, we did exactly what is suggested. By injecting underdiagnosis into one group, we intentionally correlated labels with sensitive attributes. Example: flipping 25% of âpositiveâ labels in the âfemaleâ group increases the correlation of positive labels with the male attribute.
3) The paper provides limited practical indications on model training and performance-fairness trade-off (R1, R3, R4):
Firm conclusions on performance-fairness trade-offs in general cannot be drawn due to the lack of good fairness metrics [3]. Our work aims to examine the dynamics of biased training. The main novel indication is that NC reduces the sensitive information encoded in the modelâs features (section 4.3). Sections 3 and 4.4 show that different NC configurations between groups indicate a biased model. We will clarify this in the paper.
4) The paper does not consider prior work on learning from noisy labels (R1, R3, R4):
This work aims to study the impact of biased datasets on standard training methods, rather than developing a novel learning method for noisy labels. Prior work on learning from noisy labels is therefore outside the scope of this work and is a future effort.
5) NC is not achieved in practice and train NC does not guarantee test NC (R4):
We studied models as they âapproachâ NC and not necessarily after reaching it. This will be underlined in section 2. We donât make any assumptions on test collapse. We show that while both clean and biased models approach train NC (section 4.1), the clean model achieves better test NC (section 4.4), confirming that test collapse depends on the quality of the train set as shown in the literature.
6) Random choice of biased samples is not realistic (R1):
We agree that bias occurs in hard samples, but datasets lack âeasy vs hardâ labels. It is not clear how to inject the bias to hard samples before training since obtaining the embeddings requires prior training. Note that we followed the random bias injection process of [4].
7) The paper only includes one level of label noise (R4):
The study is designed assuming label bias in general is sparce, with 25% as an extreme case. Thus, methods designed based on our results are expected to work for real-world bias levels. Exploring various noise levels requires training 60 models per level, a task currently being executed for extending this work.
8) Choice of NC1 (R3):
Unlike NC2-4 which involve shared model weights and class means, NC1 relates to individual sample embeddings, allowing better group comparisons.
[1] Liu2020: Early-learning regularization prevents memorization
[2] Nguyen2022: Memorization-Dilation: Memorization-Dilation: Modeling neural collapse under noise
[3] Mbakwe2023: Fairness metrics for health AI: we have a long way to go
[4] Jones2023: The Role of Subgroup Separability in Group-Fair Medical Image Classification"
https://papers.miccai.org/miccai-2024/299-Paper0689.html,"We thank all reviewers (R) for their constructive feedback and for recognizing the scientific value, clarity, and organization of our work.

R1: The method would not be applicative to pathology/atypicality data, e.g. tumor or infant MRI data
A: The reviewer is correct that we primarily targeted neurocognitive MRI studies focusing on identifying subtle differences, which we will acknowledge in the manuscript. One could extend our framework to the above applications by confining infant MRIs to a narrow age range or tumors only appearing in certain brain regions.

R1: focusing solely on individual regions might overlook the overall brain structure
A: This concern is unlikely to materialize as our evaluation complements cortex-level evaluation (important for neurocognitive studies) with global image metrics (like SSIM and FID) and an overall quality score of the segmentation. Furthermore, extending the framework to include global gray and white matter scores is straightforward.

R1: Relatively low number of methods evaluated
A: We evaluated all (i.e., 6) state-of-the-art MRI synthesis methods for which implementations were available to us.We even reached out to authors for code for MedGen3D (without getting a response) and implemented a recent model designed for CT (Medsyn).

R1: The proposed metrics are rather straightforward, of limited novelty
A: The novelty of our paper lies in the development of a comprehensive framework (unified pipeline) for comparing brain synthesis, which has the potential to significantly enhance neurocognitive studies. Additionally, the proposed metrics provide cortex-level measurements to address a core aspect of neurocognitive studies. Unlike previous metrics, it can accurately measure anatomical plausibility, offering a more detailed and relevant evaluation of synthesized brain MRIs.

R3: Recommend comparing our work with standardized pipelines like HCP and AFNI. 
A: As we will clarify in the manuscript, we are essentially using a scaled-down version of HCP that only requires the necessary steps to robustly perform a skull strip and align an MRI to a template.

R3:  The word fairness in the title is misleading
A: We will revise the title to  âTowards a Consistent and Anatomically Plausible Evaluation of Brain MRI Generators.â

R3: Do you trust the quantitative or qualitative assessment, why?
A: The motivation behind our proposed framework was that many methods performed equally well with respect to quantitative metrics (i.e.,  global semantic-level similarity), while they greatly differed with respect to their qualitatively. To ensure that qualitative differences were properly captured, we added regional brain measurements that measure anatomical plausibility. At this point, we trust those qualitative assessments more as these are also the metrics used by neuroscience studies.

R3: summary explanation of the utilized metrics
A: MS-SSIM: Evaluates perceptual quality by comparing structural similarity across multiple scales. MMD :Measures the distance between distributions of generated and real data in a high-dimensional space. FID : Compares the distribution of features from a pre-trained network for both real and generated MRIs. Lower FID scores indicating higher similarity to real MRIs.
For all of them, we provide a statistical measure of the overall similarity between synthetic and real MRIs.

R4: needs more background lacking discussion on other generative models.
A: GAN and diffusion methods are the state-of-the-arts. There are also deformation field-based methods, but they fail to increase the data diversity as they are based on an existing MRI. We will add to the introduction as space permits.

R4: compare with other anatomical-level evaluation strategies. 
ï»¿A: To the best of our knowledge, all previous generative methods using perception-based metrics did not involve cortex-level evaluation.

R4: needs downstream tasks
A: We agree and will include this in the discussion."
https://papers.miccai.org/miccai-2024/300-Paper2142.html,"Dear Reviewers and Area Chairs, 
   We would like to express our gratitude to three reviewers (R1, R3, R4) for their careful reviews and constructive feedback. We are pleased that the reviewers acknowledged the paperâs novelty (R4), writing (R3, R4), and effectiveness (R1). Below, we have summarized the main concerns raised and provided our corresponding responses:

Regarding the novelty and contribution:
 Q: R1 stated that the paper heavily relies on existing works, resulting in limited contribution.
 R: We would like to emphasize that our paper focuses on rectifying the concept misalignments originated from VLMs, which have been ignored in previous works [1, 2, 3]. While it is true that training CBMs with VLMs has been explored, the reliability of the explanations is hindered by the concept misalignments due to the training paradigm of VLMs [4, 5]. We propose to utilize evidential learning to model concept uncertainty, which helps rectify concept misalignments for label-efficient training. Furthermore, we introduce uncertainty-aware intervention, which could enhance the diagnosis performance with less interventions. In summary, our method improves the reliability of concept explanations, whether trained with concept supervision or VLMs. This contribution is particularly significant in the medical domain.

Regarding the experiments:
 Q: R1 pointed out that the paper compares evi-CEM with other CBM variants that are not pretrained with VLMs, resulting in an unfair comparison.
 R: We clarify that in Table 1, evi-CEM does not involve VLMs and the comparison is fair, which seems to be misunderstood by R1. The results presented in Table 1 compare the models under complete concept supervision, indicating that all the models are trained solely with concept labels and do not incorporate VLMs. To prevent any further misunderstandings, we will explicitly emphasize this point in the revised version.
 Q: R3 suggested evaluating the modelâs performance in more diverse clinical contexts.
 R: We appreciate the constructive suggestion provided by R3. Due to the space limitations and the rebuttal policy, we are unable to include additional results in this version of the paper. However, we acknowledge the importance of assessing our model in diverse clinical contexts, and we will consider incorporating more datasets and contexts in our future work.

Regarding the writing:
 Q: R1 mentioned that the paper is challenging to follow and grab the key points.
 R: We appreciate R1âs comment, and we will address this concern by reorganizing the introduction section of our paper. Additionally, we will enhance clarity by emphasizing the contributions with bullet points.

References:
[1] Yang et al., Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classiï¬cation, CVPR 2023.
[2] Oikarinen et al., Label-Free Concept Bottleneck Models, ICLR 2023.
[3] Yan et al., Learning Concise and Descriptive Attributes for Visual Recognition, ICCV 2023.
[4] Yuksekgonul et al., Post-hoc Concept Bottleneck Models, ICLR 2023.
[5] Yun et al., Do Vision-Language Pretrained Models Learn Composable Primitive Concepts, TMLR 2023."
https://papers.miccai.org/miccai-2024/301-Paper1871.html,"We thank the reviewers for their invaluable help and input to improve the manuscript.

Contributions and novelty: As observed by reviewers #1 and #4, the key contributions of the paper are not the bounding box and keypoint detection methods themselves, which has been further clarified in our paper. Our contribution is their combination with uncertainty estimation and explainable (tractable) classification in a particularly noisy setting.

Neither the original residual log-likelihood paper or any following paper have used the uncertainty estimates in a substantial way beyond regularizing the keypoint detection. Novel to our model and as shown, these can be propagated to model classification uncertainty. Moreover, we show that this uncertainty is well calibrated to human experts.

The fuzzy classifier contributes over a hard condition decision tree (direct computation) as shown in our ablations (rows 1,5), by yielding probability estimates permitting loss backpropagation, which informs the keypoint regressor on the classification criteria. We show that this increases the F1-score for the first classification case, and overall improves the second. From an explainability  perspective, it enables us to separate the probability contributions of the fuzzy classifier and image-feature classifier.

Interpretability vs. explainability: We thank reviewers #1 and #4 for their comments on explainability methods.  We will attempt to address some misunderstandings and the difference between interpretability and explainability, where we have avoided using the former term in our paper.

According to e.g. the ISO/IEC TR 29119-11:2020 standard, interpretability is the level of understanding âhow the underlying model worksâ, while explainability reflects âhow the model came up with a given resultâ.  Most work addresses post-hoc methods for interpretability, like Grad-CAM, which are not applicable to the explainability desired in this paper. We emphasize that these interpretability methods only rank feature contributions to predictions without providing tractable reasoning on their use in the model. Our model integrates the neural network feature extraction with decision tree transparency, aligning with clinical decision-making like GSQ.

Saliency maps: Visual feature importance methods, such as saliency maps (proposed by reviewer #1) generally aim to provide visual interpretable insights to the model but cannot be translated into an explained mode of reasoning. Although such methods can help understand model behaviour, they do not provide the tools required to comply with decision support systems, and we respectfully disagree that they could be used to accomplish the paper objective. However, we agree with reviewer #1 that the discussion should be updated to emphasize this, and that saliency maps would be an interesting addition.

Random forest comparison: While a single decision tree is explainable by construction, a random forest is an ensemble and requires feature importance which suffering from the same lack of model reasoning as saliency maps. We have clarified this in the paper.

Clinical translation: Concerning the clinical relevance of the interpretability, we thank reviewer #4 for encouraging suggestions for future work and agree that this is an important direction to pursue, but we believe it is out of scope here and best suited for a paper on pure clinical translation.
Readability and clarity: We thank all reviewers for their comments on readability and clarity of the figures and methods, which we have made sure to address.

Thanks to reviewer #4 we have also added that there were 2 clinical annotators with excellent inter-reader agreement and added details on the hardware used. Moreover, we thank reviewer #4 for their important suggestions to improve the statistical analysis, and agree that while we provide errors, a significance test would have further strengthened the paper and will be added if allowed by the guidelines."
https://papers.miccai.org/miccai-2024/302-Paper2315.html,"We thank the reviewers for the invaluable comments. We have carefully revised our paper as per the reviews and reply below:

Q1 (R1): Concern about the usage of raw kspace data.
A1: The public datasets used do not contain raw kspace data. We converted the image data into pseudo-kspace data using Fourier transform. All degradation simulations (down-sampling, motion simulation and noise addition) were performed in the pseudo-kspace (supplementary material). 
A1.1: Only a few databases offer the access to raw kspace data, the majority of current DL methods employ pseudo-kspace data for degradation simulation. 
A1.2: The model trained on pseudo-kspace simulated data was evaluated quantitatively and qualitatively on real data, and the results (Table 2 and supplementary material Fig. 3) demonstrated the superior performance of our method when faced with real clinical scenarios.

Q2 (R1): Explainability. 
A2: In the supplementary material, some points with higher explanation values overlap with pixels with more artefacts. They do not fully cover the band artifacts in the cortices. This may be caused by two factors: 1) in addition to the band artifacts, other pixels in the image are affected by motion, which presents a challenge in fully localizing the band artifacts. The classical explainable method (Grad-CAM) devotes a comparable degree of attention to the band artifacts as to the whole brain [1, 2]. 2) the samples are also degraded by low-resolution/noise, which has resulted in a focus on motion evidence being less prominent in multi-label classification.

Q3 (R1): Backbone.
A3: The quality of the images restored by Restormer alone was comparable to those restored by Resvit, with the elimination of noise and artifacts. However, the grey-white matter contrast and the maintenance of image details were noticeably inferior to those achieved with the addition of the proposed components (Cyclic and CAG).

Q4 (R1): Compare with methods designed for a single source of degradation.
A4: Considering that methods designed specifically for a single source of degradation suffer from significant performance decline when confronted with other sources of degradation or mixed degradations, we used general baselines for a fair comparison. In future work, we will compare our approach to other SOTA methods on each specific task separately.

Q5 (R3, R4): Lack of comparison with PromptIR. 
A5: We did not include PromptIR as a competing method based on two main considerations: 
(1) PromptIR was only tested out on images corrupted by a single source of degradation (rain/haze/noise) and did not report any restoration performance against mixed degradation (e.g., joint rain and haze removal). In the context of this paper, MRI images may be affected by multiple degradations simultaneously, such as motion with low resolution, and therefore PromptIR is not suitable for comparison. 
(2) Resvit is a unified image-to-image translation framework designed for MRI images. It achieved SOTA performance in a variety of tasks. Given the considerable heterogeneity between natural and MRI images, we thought a better competing counterpart was  Resvit. 
As recommended by the reviewers, we compared our method to PromptIR and obtained that the scores on the two datasets were: Cam-Can (PSNR: 30.52, SSIM: 0.905 and RMSE: 0.063), UCLA (PSNR: 29.90, SSIM: 0.882 and RMSE: 0.067); which was worse than our proposed method and had no advantage over Resvit.

Due to space limit, we could not report additional experiments in current conference version. We will add these results in the short future version as per the reviewersâ suggestions. We thank the reviewers again for the constructive suggestion.

[1] Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence. 
[2] Classifying MRI motion severity using a stacked ensemble approach."
https://papers.miccai.org/miccai-2024/303-Paper1324.html,"Q1: The framework lacks novelty (R1, R3).
A1: The core idea of our method is to explore the possible latent classes (i.e., existing but unlabeled ROIs) to guide the partially-labeled segmentation. Compared with previous attempts and semi-supervised learning, the main differences include:
1) We are aware of the latent classes in partially labeled segmentation and propose to utilize them to improve segmentation performance, under the guidance of an independent ROI classifier. Such a design could ensure that different ROIs could contribute differently to the network and improves the effectiveness of the framework.
2) We propose to regularize ROIs with enough certainty by CE loss with sharpening operations. These ROIs are regarded as latent classes and should be utilized to improve the segmentation quality by reducing the prediction entropy in them.
3) We utilize consistency loss on ambiguous ROIs. Compared with latent classes, as these ROIs may contain informative clues that could benefit the training, and the model should yield close predictions under perturbations.

Q2: The classifier is trained by using a large-scale dataset, whereas the compared methods are not (R1).
A2: The dataset size of TotalSegmentator is not significantly larger than the segmentation dataset (1153 in TotalSegmentator v.s. 1155 in segmentation dataset). MOTS benchmark does not contain organ labels for lungs and colon and thus we utilize TotalSegmentator to train the ROI classifier.

Q3: The quantitative analysis lacks ablation study, and the comparison does not include a set of SOTA methods (R3).
A3: Table 1 in the manuscript serves as the ablation study as well. The main contribution of our method is latent class mining, which involves the segmentation loss on latent ROIs and the consistency loss on ambiguous ROIs. If both losses are removed, the proposed framework degenerates to mean-teacher (2nd row in Table 1). Also, we investigate the consistency losses on ambiguous ROIs in the 3rd row in Table 1. DoDNet, 4th row in Table 1, serves as a previous state-of-the-art in partially labeled segmentation. We would include a more thorough comparison with SOTA methods in partially labeled segmentation in future works.

Q4: The Student-Teacher framework is introduced without descriptions on exact motivation and reasoning. The introduction of the ResNet-18 for latent class generation looks arbitrary (R3).
A4: We adopt mean-teacher as latent classes mining requires pseudo labels, whereas mean-teacher serves as a simple yet effective method to generate them. We adopt ResNet-18 as the classifier as the identification of ROIs is a relatively easy task, while the lightweight nature of ResNet-18 is ideal to guarantee latent class mining performance with little extra cost.

Q5: The training setup requires careful optimization of multiple hyperparameters (e.g., thresholds for the ROI-guidance), which affects the extendability to other tasks (R4).
A5: The performance of our framework is not sensitive to thresholds, as a well-trained classifier would always output very confident logits to all ROIs, no matter which ROIs the model is identifying.

Q6: The discussion on the classifier performance should be included (R1).
A6: Our pilot studies reveal that the overall accuracy of ROI identification exceeds 96%, which would not set restrictions on the proposed framework. We would add relative discussions in future works.

Q7: Authors use only Dice score as a segmentation metric (R4). 
A7: We do not include HD as the images are anisotropy in spacing, where HD might lead to a biased report of performance in the axis with a large spacing.

Q8: Task-specific networks and challenge winners have not been compared with the proposed method (R4).
A8: We do not compare with challenge winners as our experimental settings are different from challenges with fewer training tricks. Plus, the comparison of task-specific networks has been thoroughly discussed in previous works like DoDNet."
https://papers.miccai.org/miccai-2024/304-Paper3332.html,"Thank you for all the valuable feedback sincerely. I will respond to each reviewerâs comments individually below.
Reviewer #1: We will evaluate the impact of generated images on the accuracy of classification tasks in our future work. All the encoders are the same encoder in Fig.1. We will revise the text to eliminate any confusion.
Reviewer #3: We speculate that the suboptimal performance of ASP may be attributed to the fact that the similarities between the generated and real IHC images cannot represent the degrees of alignment between consecutive slices accurately. Because the positive/negative expressions of the generated images largely impact their similarities. Therefore, we utilize H&E and real IHC images to calculate the degrees of alignment.
Reviewer #4: Thank you for your careful review. I will check all the mistakes and typos. The adaptive weights are employed linearly in the training process. Owing to the page limit, we only present the results of HER2 in BCI and ER in MIST in the ablation experiment. âThe all but the last layerâ means that we obtain embedding vectors from the features of the discriminatorâs second-to-last layer. Loss_Adv represents the conventional adversarial loss of GAN."
https://papers.miccai.org/miccai-2024/305-Paper1874.html,"We thank reviewers(R1,R3,R4) for their positive comments: quite robust(R1,R4), well organized(R1,R3,R4), and interested by the related neuroscience communities(R1,R3). We clarify the main points:"
https://papers.miccai.org/miccai-2024/306-Paper0615.html,"We thank the reviewers for their comments.

R4(Q2,Q4)&5(Q2): Clarify the Table 1 results by multimodal training, T1w testing, and why the results of testing with fMRI are shown.
Sorry for the confusion caused by Table 1. We have reorganized it to align results with consistent training/testing modalities.
1) fMRI for Prediction: Previous studies used fMRI and achieved satisfactory results (rows 1,3,5,7). For comparison, we show our fMRI-only results (row 9) and obtain the best performance, highlighting the effectiveness of our proposed phenotypes activation maps.
2) T1w for Prediction: Due to the lower cost of Tw, it is a more practical setting to use T1w for prediction, which still shows significant challenges in existing works (rows 2,4,6,8).
3) Knowledge Transfer: Hence, we transfer knowledge from high-cost fMRI to low-cost T1w (training: fMRI+T1w) and achieve the best performance using T1w for testing (row 13) (outperforming existing works only using T1w (rows 2,4,6,8)), fulfilling our original intention.
4) We aim to address missing high-cost fMRI rather than multimodal fusion, so we didnât present fusion results.
5) Phenotype prediction is a recognized challenging task [3,15,20]. Previous fMRI-based predictions typically have PCC<0.2 and T1w-based predictions are even lower. Thus, our results are good enough.

R4(Q5)&5(Q3): Discussion about the implication of the ability to transfer knowledge from fMRI to T1w.
Brain structure determines function, and function determines phenotype. The unclear nonlinear relation between structure and function is a key focus in neuroscience. Through the downstream task (phenotype prediction), we transferred the knowledge from fMRI to T1w, not only exploring this relation but also establishing the mapping from structure to phenotype for precise prediction. Our method can be applied to other tasks, e.g., brain mental disorders classification.

R1&4&5(Q1): Clarify the Fig.2.
We will label the PgKT and MpP modules with boxes in Fig. 2. The PgKT includes cross-modality alignment, phenotypes activation maps, and modality knowledge transfer. The MpP involves modality-aware phenotype codes decomposition, multi-phenotype prediction, and modality-aware phenotype.

R1: Writing errors, letters explanation.
We will fix all the writing errors. Fsâ and Ftâ are equal twice in Fig.2. F/T represent image/phenotype features. The letter superscript {â} indicates features after phenotypes activation maps bootstrapping and {s/t} denote fMRI/T1w features.

R1: Clarify the Linear Projection and Sinkhorn layer.
The Linear Projection is an MLP layer to project image features into the transformer space. The Sinkhorn layer, commonly used in graph matching [23], facilitates intermodal node matching.

R1: Phenotypesâ selection.
Based on [15], we randomly selected 12 phenotypes predictable through fMRI. To justify that our method is robust to the number of phenotypes, we eliminated 2 phenotypes and predicted 4 at the same time. Phenotypes #7-#12 are the remaining 6 phenotypes.

R1: Ablation studies.
Compared to our full method (PCC=0.24), removing phenotype activation maps and the modality knowledge transfer loss (Lmc) reduces performance (PCC=0.22,0.08), verifying each componentâs effectiveness.

R4(Q1): Explain why simple modality transfer was not conducted and why our method is not overcomplicated.
Predicting phenotypes is not a simple task [3,15,20]. Simple modality transfer (PCC=0.09) performs much lower than our method, proving it canât achieve precise results. Our ablation studies show that each module is useful, validating that our method is not overcomplicated and phenotype prediction is indeed a complex task.

R4(Q3): Cost-to-efficiency.
Inference times are as follows {[17,7], our}: {0.47s, 0.53s, 0.32s}. Our method achieves shorter inference times, higher accuracy, and lower data acquisition costs, enhancing its clinical value.

R4(Q6): Code reproducibility.
Our code will be made public upon acceptance."
https://papers.miccai.org/miccai-2024/307-Paper1577.html,"We thank reviewers for their careful review and constructive feedback.

R1.1) Novelty of method.

Although past studies have applied federated learning (FL) in medical imaging, to our knowledge, we are the first to explore FL on VLMs like CLIP in this context. Existing FL solutions, which naively transfer all model parameters to the global server are not applicable to VLMs due to their size (e.g., 10^8 parameters for CLIP). Inspired by FedCLIP, our method only shares the parameters of a local attention module (AM) learned independently for each client. However, our work improves FedCLIP in two important ways. 1) While it uses a shallow network (2 layers) for its AM, we propose a more complex architecture better capturing the data variability across clients. 2) Unlike FedCLIP, we address domain shifts directly by incorporating a domain adaptation (DA) loss in our model. As shown in results, our method significantly outperforms FedCLIP in all test cases.

R1.2) Generalization performance.

Our formulation follows Eq. (1) of [12]. However, we agree that the modelâs generalization performance should be evaluated on clients that did not participate in training. We will clarify this in the updated version.

R1.3) Need for auxiliary dataset.

This indeed adds a constraint, yet we point out that this auxiliary dataset does not require labeled samples. As there are many publicly available datasets in medical imaging (e.g., UK Biobank, HCP for brain images), we believe that this constraint is less important than the advantages brought by our methodâs DA capabilities.

R1.4) Impact of heterogeneity.

When aggregating parameters in Eq. (7), we give a larger weight to clients with more training samples. In the non-iid setting, this enables the global model to learn from the most knowledgeable clients, preventing performance degradation. In the iid setting, the lower performance of other methods could be due to the need of fine-tuning the entire image encoder or to the shallower AM.

R1.5) Error running the code.

We fixed the mentioned bug (missing space) in the code and provided a detailed tutorial on how to run it.

R3.1) Implementation details.

We will add the suggested information in the final version.

R4.1) Readability of results.

âAverageâ is the mean for both non-iid and iid conditions. The red line (ours-w/o DA) matches the orange line in the plot. We will clarify these in the updated version.

R4.2) AM design.

FACMIC employs a different architecture for its AM (more layers, different activation, etc.), which boosts its performance in the iid setting. Since CLIP was mainly trained on natural images, this stronger module is needed to adapt the image features to medical imaging tasks.

R4.3) Improvements of FACMIC under Real condition.

For the Real dataset, each clientâs data is from a difference source (SC, HAM10000, or ISIC2019) whereas, for BT and SC, all clients have samples from the same source but the number of samples for each class differs. As our DA strategy is applied on image features it is more useful for the domain shifts encountered in the Real dataset.

R4.4) Performance for different alpha.

As mentioned above, the domain shift for BT and SC affect the distribution of labels on which our DA technique has less impact. Echoing our answer to R1.4, our aggregation strategy giving a larger weight to clients with more samples already addresses this type of shift.

R4.5) Difference between âOurs-w/o DAâ and FedCLIP.

Our model employs a different architecture for its AM, which is more suitable for adapting CLIP features to medical imaging (see our answer to R1.1).

R4.6) Adapting the last layer of image encoder.

This MLP layer has many more parameters than our AM (10^7 params vs. only 5x10^5) hence adapting/sharing it incurs greater computation/communication costs. We also tried updating the last layers of the image encoder for FedAVG in BT (alpha=0.9) and obtained  a lower accuracy (72.34% vs 81.73%)."
https://papers.miccai.org/miccai-2024/308-Paper2609.html,"Thank you for your thoughtful review of our manuscript. We appreciate your valuable insights and suggestions, which will help us improve the clarity and comprehensiveness of our work.

We have addressed the reviewerâs comments regarding the following aspects:
Novelty(R3): Our work introduces two key novelties: Masked GOT (MGOT) and incorporating Eudermic skin labels. We agree GOT is foundational. However, MGOT utilises a learnable mask to prioritise disease-relevant patches during alignment, leading to better accuracy than GOT. Additionally, including the Eudermic skin label allows MGOT to distinguish healthy from diseased regions, further improving alignment. In MGOT, the mask weights are learnt using a neural network. These innovations address the specific challenges of clinical skin image alignment.

Text Encoder to Generate Encoding(R4): We evaluated different text encoders for clinical label encoding, including BERT, Medical BERT, and OpenAIâs text-embedding-3-large. The latter achieved the best performance and was also the most explainable (evaluated based on the t-SNE plot shown in Fig. 1 of the paper). As it a pre-trained model, it requires a one-time computational cost for encoding and has minimal impact on overall model complexity (FLOPs).

Loss Function(R1): The target branch uses the standard cross-entropy loss for classification. The sensitive attribute branch employs cross-entropy and confusion losses, similar to FairDisCo. Finally, the alignment branch leverages the GOT loss for aligning image and text representations. We will briefly explain each loss function in the camera-ready version.

Baseline(R4): We employed several strong baseline models to ensure a comprehensive evaluation. We began with a simple ResNet-18 architecture as a foundation. We then incorporated the results from FairDisCo, a well-established approach, as a baseline for comparison. Finally, to demonstrate the effectiveness of our multi-tasking strategy, we included the performance of a multi-tasking model as an additional baseline. Also, as suggested, we will try to add more baseline models to the camera-ready version.

MultiTasking(R1): Multi-tasking necessitates a modified loss function because it concurrently trains the model for both skin condition prediction and meta-label prediction. This requires cross-entropy for classification for the meta-label prediction task. Multi-tasking implicitly encourages alignment through shared representations across both tasks. In contrast, PatchAlign explicitly addresses alignment through the GOT loss.

Ablation(R1 and R3): Our initial ablation study included BaseVIT with GOT, the original FairDisco model, and two variants of PatchAlign (with and without the Eudermic skin label). To enhance the comprehensiveness of the ablation analysis in the camera-ready version, we will try to include Base VIT as a baseline and FairDisco with VIT for a more direct comparison.

More Skin Tone Fairnes Studies(R4): Unlike prior fairness-focused methods that often require extensive data normalisation[2] or additional human intervention[3], PatchAlign leverages a simpler and potentially more effective approach: cross-domain alignment with MGOT. This strategy promotes generalizability across diverse skin tones without significant data manipulation or human oversight. We will try to include a more detailed comparison of these techniques in the camera-ready version to elucidate the advantages of PatchAlign further.

Lastly, We will thoroughly revise the manuscript to eliminate grammatical errors before submitting the camera-ready version."
https://papers.miccai.org/miccai-2024/309-Paper3105.html,"We thank all reviewers for their insightful feedback. We appreciate positive comments such as the importance of fairness in clinical datasets (@R3,@R4) , the novelty of our method (@R3,@R5), the superiority of our experimental results (@R3,@R5).

To help the community better reproduce and build upon FairDiff, here is an anonymous codespace link: anonymous[dot]4open[dot]science/r/FairDiff-FB40. We will release the code and data after the paper is accepted.

Although OCT provides detailed visualization of retinal microstructures, the high cost of OCT machines makes it less prevalent in primary care. Meanwhile, SLO offers high-resolution images (5-10 Î¼m) that are more detailed compared to color fundus images (15-20 Î¼m).

Additionally, as far as we know, SLO technology is not in the status ofnot very relevant any more. In contrast, it is continuously advancing, with innovations such as adaptive optics SLO[1],  rapid and extensive imaging[2], and ultra-wide-field SLO images[3]. Also, SLO products are still available from vendors like OPTOS and YESight.

[1] Alignment, calibration, and validation of an adaptive optics scanning laser ophthalmoscope for high-resolution human foveal imaging. Applied Optics (2024).

[2] Scanning Laser Ophthalmoscopy Demonstrates Pediatric Optic Disc and Peripapillary Strain During Horizontal Eye Rotation. Current Eye Research (2024).

[3] Algorithm of automatic identification of diabetic retinopathy foci based on ultra-widefield scanning laser ophthalmoscopy. International Journal of Ophthalmology (2024).

The existing public OCT and fundus datasets lack the demographic information needed for fairness study. Therefore, we chose to conduct our experiments with the Harvard-FairSeg, which is the first fairness-focused dataset for medical segmentation.

According to studies [4], fairness-improved CNNs on radiology images can address the issues of underdiagnosis and misdiagnosis in underserved groups, ensuring more equitable diagnostic accuracy across diverse patient populations. Similarly, enhancing fairness in our SLO fundus segmentation models can lead to improved clinical performance.

[4] CheXclusion: Fairness gaps in deep chest X-ray classifiers. (PSB 2021)

We have controlled the total number of training samples to be the same across different settings to reduce the effect of extra data. Take Table 2 as an example, the baseline where the TransUnet model is trained on the full real dataset. The total number of training samples is 8000 (with 752 Asian, 1161 Black, and 6087 White). Our training samples are as follows: Asian 2667 (184 real  + 2483 syn), Black 2667 (295 real + 2372 syn), White 2666 (1521 real  + 1145 syn).

Also, analyzing the metrics, our method may not significantly improve segmentation performance (mIou), but it does enhance the fairness of segmentation (ES-mIoU).

Thank you for your suggestion. We provide comparison with ControlNet (one-stage label-to-image synthesis) in the following table, compared with our two-stage pipeline where we first sample labels and then synthesis images. The results are as follows:

Our Point-Mask method shows effectiveness in generating diverse images, as reflected by the highest COV (Coverage) score among the methods evaluated.

In fact, we have conducted experiments on another segmentation backbone, SAMed. Due to page constraints, we put it in the supplementary (Please refer to Tab.1-4)."
https://papers.miccai.org/miccai-2024/310-Paper3697.html,"The main concerns are 1) minor writing issues; and 2) novelty of our work. For 1), we replied respectively; for 2), we pointed out that our method is a major improvement with enough novelty.
Examples for briefs: R3 is Reviewer #3. Q6.1 is the 1st inquiry in Question 6.

R3

Q6.1 Score definition different in Eq. 2 and Algorithm 1.
It is a typo in Alg 1. We will correct it.

Q6.{2-4} Tasks for metrics unclear; is EOpp1 the difference between true positive rates (TPR) of two groups of the fairness attribute? Is EOdd mean of EOpp0 and EOpp1?
Precision, recall, F1-score are accuracy metrics; EOpp0, EOpp1, EOdd are fairness metrics.
No, EOpp1 is obtained by computing the TPR difference for each target class and taking the average.
No, EOdd is obtained by getting the differences of TPRs and FPRs between two groups for each class and taking the average.

R4

Q6 FairQuantize vs. ME-FairPrune on Fitzpatrick-17k. Accuracy difference should also demonstrate fairness.
Fitzpatrick-17k is unbalanced and accuracy does not reflect performance well. It is why we report precision, recall, F1 instead.

Q10.1 Extend to sensitive attributes with multiple values?
Future work will address multi-value sensitive attributes.

Q10.2 How were hyper-parameters defined?
The hyper-parameter Î² represents weighted sums. E.g., a score may be 3 * importance^u - 2 * importance^p. By normalizing, we get 2/3 for the second term as a Î² value.

R5

Q10.{1,4-6,8-9} âPrivileged and unprivilegedâ was explained in Sec 2.1 but first used in Introduction. Optimize notation and clarify the model description in Sec 2.1. Add round operation for log2 in Sec 2.2. Missing definition for âscoring setsâ. Clarify computations and flags in Algorithm 1.
H^u and H^p are calculated on each pair in S^u and S^p. It is why line 10 takes an average. This is not a mistake.
Two subsets of the training set are randomly chosen with the same number of data points for two groups of the sensitive attribute and are used for calculating Hessian matrices, known as scoring sets. We will add the corresponding definition.

Q10.2 âHowever, such suppression of informationâ¦ greatly degrading the prediction performance.â conflicts with [16].
Section 5.1.1 and Table 2 in [16] show that the proposed methods do not outperform âweighted lossâ on the exclusive test split, and have a small performance drop compared to âstandardâ on the co-occurring split. We will tune down our statement as suggested.

Q10.3 Why âtherebyâ in âFairQuantize addresses this by adjusting the computation precision of these pivotal weights through quantization, thereby balancing accuracy and fairnessâ¦â
We will change it to âFairQuantize addresses this by adjusting weight precision instead of directly getting rid of them, so as to provide better balanceâ¦â

Q10.11 Explain improvement difference between datasets.
Fitzpatrick-17k is a 114-class skin disease dataset while ISIC 2019 is an 8-class dataset, making Fitzpatrick-17k more complex and difficult to improve.

Q10.12 How Î² is selected.
We did a grid search from 1.0 with a step of -2/9, with approximations for fractions.

Q{10.7;6,12} Any special difference in Eq. 2-4 from FairPrune? Novelty of the work.
In Eq. 2-4, we use the quantization error instead of weight magnitude (in FairPrune) in computing the importance score, because quantization only changes weights to quantized values, while pruning changes the weights to zero.
Our work shares the same motivation as FairPrune, but it improves upon FairPrune by offering a finer-grained optimization through quantization. Specifically, quantization adjusts weights by allowing a weight to be quantized to different levels while pruning either leaves a weight unchanged or sets it to zero. Besides, quantization can be used alone or after pruning, and studies show that though strongly backbone-dependent, it generally outperforms pruning in maintaining accuracy. In conclusion, our method is theoretically and empirically superior to FairPrune."
https://papers.miccai.org/miccai-2024/311-Paper2447.html,"We sincerely thank all reviewers for their kind reviews and constructive comments.

â The reviewers had a concern about the novelty of our work (FALFormer and FALSA) and asked to clarify the difference between our work and other related works:

The main difference between FALSA and NystrÃ¶m Attention (NA) is how to find landmarks. NA groups the tokens in sequence by a fixed order (left-to-right) into segments and computes the mean for each, which ignores the spatial relationships among patches. Alternatively, we propose to obtain landmarks by using K-means clustering, which groups similar patches (in feature space) together. In this manner, FALSA becomes aware of spatial relationships among patches/tissues, better approximates self-attention, and improves classification performance.

In NystrÃ¶mformer (Y. Xiong, et al.), Segment-means was preferable to K-means, which is contradictory to FALFormer. We believe that this is mainly due to the intrinsic difference in the data. NystrÃ¶mformer was originally developed and tested for NLP data. FALFormer is specifically designed for pathology images where the spatial relationship among patches/tissues is known to play a critical role.

There exist similar approaches with FALSA. These works differ from FALSA as follows. In (G. Ziyu, et al.), patches were grouped and pooled and then processed by a variant of self-attention. In (D. Saisal, et al.), self-attention was adopted with a reduced number of patches. In (T. Jin, et al.), NA was employed for patch aggregation. However, FALSA utilizes the entire patches with spatially-aware landmarks. (G. Bontempo, et al.) used a multi-scale strategy, which differs from above-mentioned studies. Moreover, TransMIL utilized NA to reduce the computational complexity. FALSA can be applied to TransMIL, but the relationship between FALSA and PPEG needs to be explored. Since FALSA only requires a small number of landmarks, it will fit in the memory.

To highlight the novelty of our work and the difference with others, we will update Fig. 1 and the manuscript. For credibility, we will also reference TransMIL in the introduction.

â The reviewers pointed out that the experiments and evaluation on FALSA need to be extended: Due to the policy of MICCAI, we cannot provide additional experiments and results on FALSA and other related works. However, we adopted two popular WSI datasets (CAMELYON16 and TCGA-BRCA) and two popular models (CLAM and TransMIL) that have shown to be applicable to a wide range of datasets/problems. The results reported in our work suggests that FALSA can achieve superior performance on other pathology datasets in comparison to other works.

â The reviewers asked to clarify the patch sampling and clustering procedures:

We refer âpatch samplingâ to the process of tiling a WSI into non-overlapping patches. Given a WSI, all tissue regions are considered to obtain patches, while background inside/outside tissue regions are ignored. Those cropped patches are used for training without further sampling.

We set the maximum number of segments N_s to 256. Since the number of all patches N is greatly larger N_s, N_s will stay the same. In fact, N_s does not affect the architecture of FALFormer and FALSA.

â The reviewers asked to provide a detailed explanation of methodology and experiments:

We will provide detailed description of our method and experiments and improve the mathematical notation in particular for NA. For instance, d_model is to 768 as in Vision Transformers. The number of layers (L) is set to 2 same as TransMIL.

For CAMELYON16 and TCGA-BRCA, we employed the data split that is popularly used. We will provide the details in the final manuscript and make the split publicly available for reproduction.

FALFormer and other models were evaluated under identical conditions, using the same random seed and environment. The best model was chosen based on a validation set. The experiment was conducted only once for FALFormer and other models."
https://papers.miccai.org/miccai-2024/312-Paper2456.html,"Effectiveness of the layer-wise progressive distillation (All): 
In our ablation study, we compared our layer-wise progressive distillation scheme with (i) layer-wise distillation without progression (i.e. âFITNETS: Hints for Thin Deep Nets.â suggested by R1); and (ii) logits-level distillation (as in MobileSAM [ref.32] suggested by R3). For (i) and comparing to our layer-wise progressive distillation, it converges ~3 times slower and underperforms with a significant 3% decline in Dice score (p<0.05); For (ii), MobileSAM-like logits-level distillation failed to converge in all our experiments. This might be because of the more challenging nature of training on 3D volumes, resulting in quite aggressive distillation factors, combined with the limited data available for distillation. We have modified the manuscript to clarify those points.

Ablation on the layers of the teacher & student models (R3&4): 
Rather than only using the first 6 of the 12 teacher layers, we use all 12 layers as the teacher model. We eventually configure the student model to 6 layers, because our progressive distillation scheme enables distilling multiple separate layers (m=1,2,3) in the teacher to a single layer in the student. This results in the student model having 12/m layers. When increasing m from 1 to 2, there is no significant performance decline, but the model complexity reduces by ~50%; when m=3, performance declines by more than 70%. Hence we set m=2, resulting in 6 layers in the student model. We have modified the manuscript accordingly to include these details.

Details on 3D sparse flash attention (R3&4): 
Due to space limitations, we provided detailed descriptions and configurations for our 3D sparse flash attention in the code provided via an anonymized link. We use all three configurations depicted in Fig.1 (lower right), each configuration is assigned to two attention heads. Other details including the hyperparameters, and model checkpoints are also available in the readme file available via the anonymized link.

Low performance (R4): 
3D volumetric medical image segmentation is considerably more challenging than 2D segmentation, especially for the segment anything models. This observation, along with a general insight that SAM-like models do not yet perform at the same level in instance segmentation tasks, whether 2D or 3D, is well supported by previous works, such as [ref.27] and [1]. While we agree that both our method and SAMs, in general, will benefit from future research on enhancing performance, the primary contribution of this work is the design of an efficient and lightweight SAM for volumetric segmentation that does not compromise performance. Tab.2 provides quantitative evidence in support of this contribution. 
[1] âSegment anything model for medical image analysis: an experimental study.â Medical Image Analysis 89 (2023): 102918.

Details about data preparation and preprocessing (R4): 
We follow the data split and preprocessing pipeline in SAM-Med3D [ref.27] for a fair comparison."
https://papers.miccai.org/miccai-2024/313-Paper0234.html,"Dear Area Chairs and Reviewers,
We would like to thank you all for your positive feedback, and the opportunity to clarify the points raised about our vision-language open-set detector for bone fenestration and dehiscence, FD-SOS, solving âa clinically significant issue in dentistryâ (R3).  FD-SOS is a âpioneering methodâ (R1), âan interested frameworkâ, âincorporating innovative componentsâ â with potential to be used in practical dental settingsâ (R3). The âproposed model outperforms baseline modelsâ (R4) and the âevaluation of FD-SOS includes enough experiments and comparisonsâ (R3). Yet there are reservations:
i)  Limited dataset size, biases, and statistical significance (R1, R3, R4): 
Considering the dataset size, unlike image-based classification, our dataset contains dense bounding boxes for 1,800 teeth. Teeth-level annotations are costly since dentists need to adjust each CBCT scan to diagnose each individual tooth. In the dataset, the inter-rater agreement over 40 CBCT volumes (480 teeth) is reliable (Îº > 0.9) for the gold standard. The held-out test set, used to evaluate the 14 models listed in Table-2, as well as dentists, includes 480 annotated objects (e.g., teeth), providing reliable detection metrics that are statistically significant when compared to the baseline (p<0.0004), with attention-maps on gingiva.
ii) Dataset availability, and missing details (R4): 
The dataset with details will be made public along the code. Two dentists annotated the data, with disagreements resolved through consensus. The performance comparison with dentists involves different human evaluators (All with over 5Y of clinical experience in orthodontics).
iii) Use of obvious baselines & technical contribution (R4): 
We address R4âs potential misunderstanding about DINO (Caron et al.) & DINOv2 (Oquab et al.) with DINO (Zhang et al.) & G-DINO (Liu et al.):  âDiâstillation ânoâ labels (DINOv1&2) are self-supervised models for classification and are not obvious baselines. DETR with Improved Denoising (DINO), and its SOTA, G-DINO, are baselines with eight other SOTAs (Table 2). We agree both are confusingly âDINOâ, but they differ significantly, with DINOv2 proposing self-supervised (SSL) models. Object detection is standardly initialized with supervised object detection, as itâs more competitive than SSL. We initially tried simple teeth-level classifier approaches (e.g, on top of DINOv2). Yet, OSOD offered a more competitive baseline. Though simple yet novel, Conditional contrastive denoising (CCDN) effectively avoids VLM catastrophic forgetting by leveraging the separability of the shared dental semantics. The âconditionalâ probability does not necessarily sum to 1, especially where conditions represent a subset of events; The first two lines should sum to 1 as it encompasses the entire label space, ð¦.  Yet, the third line considers only the anterior teeth, half of the overall label space of ð¦. We follow G-DINO with p=0.5.
iv) Why OSOD? (R1, R3): 
We experienced lightweight object detectors, sparse detectors, and multi-label baselines, all exhibited limited performance, even warmed-up, mainly due to the task complexity (Table 2). Conversely, OSOD pre-trained on vast amounts of multimodal data performed better even without warmup, due to their inherent generalizable pre-trained knowledge. As R1&R3 highlighted, these OSODs suffer catastrophic forgetting. To solve this, we integrated publicly available datasets but the shared dental semantics restricted optimization. Thus, we propose conditional contrastive denoising (CCDN), which allows the OSOD to benefit from the shared separability of the dental semantics between the datasets, significantly boosting performance. FD-SOS shares the same complexity as the baseline G-DINO (64.02M), higher than DINO (48.04M), and significantly lower than GLIP (122.8M). We will follow your advice to expand the table with complexity. 
We hope our clarifications will help readjust the ratings to accept FD-SOS."
https://papers.miccai.org/miccai-2024/314-Paper2251.html,"Regarding correlation statistics, the Pearson correlation coefficients considered (1) the relationships between raw FDs computed with different extractors and  (2) the relationships between raw FDs and the average differences across mean Likert ratings for all 16 models.

Our public GitHub repository provides further methodological details."
https://papers.miccai.org/miccai-2024/315-Paper1767.html,"Appreciate to the reviewers for their meticulous examination and expert guidance on our manuscript. We acknowledge a widely discussed issue, namely the open-sourcing of code. Although we did not provide the code constrained by time, however, we commit to organizing and releasing it on GitHub immediately upon the paperâs acceptance.

Regarding the detailed method of dataset split: We employed the classic and basic 70-30 split (70% training, 30% testing). In fact, we tried use 10-fold cross-validation to evaluate the proposed method, and still achieving best results comparing with other methods which are also evaluated by cross-validation. However, considering the DEAP datasetâs modest size, cross-validation might lead to training subsets too small to represent the overall dataâs statistical characteristics, hence this experimental result was not mentioned in the paper. So, we finally adapt the 99.49% accuracy obtained by 70-30 split.

Regarding the specific details of the experiments: We utilized the complete DEAP dataset, the hardware details of which are available on the official website. The dataset includes EEG recordings from 32 subjects, each recorded while watching 1-minute videos, resulting in a total of 40 sessions per subject. Thus, the tasks involve cross-individual and cross-session analyses. In the training phase, the hyperparameters were set as follows: batch size: 32, learning rate: 0.0002, weight decay: 0.0001, and epochs: 12. Additionally, as mentioned in Section 2.5 Classification, the structure of the 3D-CNN was customized to accommodate the shape of the data output from the Mutual-Cross-Attention (MCA).

Regarding the details of MCA: We have not overlooked the detailed description of MCA. Actually, it is a simple yet elegant structure, as mentioned in Section 2.4 Feature Fusion. In brief, MCA initially applies Q=K=PSD, V=DE to the basic attention mechanism formula, equation 4, to derive intermediate feature A, then uses Q=K=DE, V=PSD with the same formula to obtain intermediate feature B. The fused feature is A+B. The entire MCA process is independent of the 3D-CNN and is a mathematical method that does not require learning. The advantage of this method is presented by its ability to allow information to flow in two directions, providing more comprehensive feature interaction and enhanced feature expression, thereby helping to improve the modelâs generalizability and reduce bias.

Regarding other datasets: Given the generality already considered with DEAP, our method can be applied to other datasets with some slight adjustments. Moreover, due to the page limitations of conference papers, we could not provide further comparisons with other datasets. Although we cannot add new experiments in this rebuttal according to official guidelines, we have planned for future work. Once we confirm the applicability of MCA on DEAP, we will begin validating its generalizability on other datasets. We appreciate the reviewersâ suggestions!"
https://papers.miccai.org/miccai-2024/316-Paper0410.html,"We thank all the reviewers for their constructive comments.

Rev#5, #4, #6: Thank you for your positive feedback and recommendation for acceptance. We ensure reproducibility by releasing our Python/PyTorch code, including baseline and FSG-models (CNNs and ViT), and dataset toolbox with public splits. We will add the GitHub link in the abstract where we mentioned the code release.

Rev#4, #5, #6: We assure reviewers that Misawa Ref. [9] is a public database related to the SUN dataset that is cited as public in the MICCAI 2023 paper âWang Z. et al, Foundation model for endoscopy.â We cited them per author guidelines (âCitationâ guidelines at the download page in Ref. [13]).

Rev#5: We did not use CBAM in our paper because it combines CNN and attention mechanisms (i.e., hybrid method). We opted for pure CNN methods and pure transformer architectures to avoid biases in interpreting the results. The methods used in comparison already include regularization techniques like Dropout and Batch Normalization. We will add this discussion on CBAM and its potential as a future comparison to further contextualize our improvements.

Rev#5: Regarding âcomputational costs, deployment in clinical settings,â we reported the number of parameters for each model in Table 2. Experiments were conducted using an NVIDIA V100 GPU with 32GB of memory, with full training settings detailed in the supplementary materials. We used ResNet-18 and ViT Tiny, the latter having half the parameters of ResNet-18. All models can operate in real-time with a suitable NVIDIA GPU, requiring approximately 16GB of memory for training and 5GB for inference on 512x512x3 input images. The FSG adds few parameters and results in an imperceptible increase in FLOPS compared to the standard model without FSG. We will add the FLOPS column to Table 2 for reference and clarify this further.

Rev#6,#4: We will enhance Table 1 by naming the datasets. We will expand RGB/LOC/DPT discussions to show benefits like improved generalization from FSG and GR. FSG promotes sparse connectivity. GR optimizes FSG with dual forward passes, focusing on key features and removing redundancies when main model parameters are frozen.

Rev#4: Depth maps are computed using MiDas, a pretrained model from Ranftl et al. [29], used in its original form. The pretrained model is included in the toolbox released with the paper.

Rev#3: While âsparse connectivity/dual forwardâ arenât new for overcoming overfitting in natural imaging, their use in medical image analysis is novel. Our GRâs dual-phase optimization differs significantly from Bengio et al.âs greedy layer-wise approach in [25] by separately optimizing FSG and main model parameters, using trainable FSG weights with sigmoid activation. Reviewers (#4, #5, #6) have recognized this method for its novelty and effectiveness. Our unique integration of these components to address overfitting in medical image analysis is a key innovation.

Rev#3: Concerning the comment on polyp sizing being an âold task..Overfitting is very hardâ, we agree that polyp sizing is an established task. This is the first benchmark that includes public Python code, standardizes datasets with public splits, and introduces baseline models. Our novel dual-phase optimization and FSG modules address the hard challenge of overfitting.

Rev#3: Addressing the suggestion to explore âTasks other than polyp size assessmentâ we conducted experiments on both polyp sizing (regression in medical imaging) and CIFAR-100 (classification in natural imaging) to assess generalizability. Our validation included over 50 experiments, confirming the robustness of the approach. We consider tasks such as polyp characterization and bowel preparation score prediction in future work. These tasks are expected to provide similar insights to those observed in our CIFAR classification and polyp-sizing regression experiments.

We appreciate the recommendations and will include them in the final version."
https://papers.miccai.org/miccai-2024/317-Paper1841.html,"We sincerely appreciate the careful reviews and insightful comments from all reviewers and AC, which are invaluable for revising and enhancing our manuscript.

Common questions:
Robustness of Reference Selection and Domain Independent: The reference images selected for the experiment were chosen randomly, and we acknowledge concerns about their potential impact. To assess robustness with the reference, we randomly selected reference images 10 times. The resulting average performance is actually 0.8727, with a standard deviation of 0.0381. The low standard deviation suggests relatively stable performance regardless of reference selection. Our method is training-free, and both prompt generation and segmentation are class-agnostic. Consequently, our methodâs reliance on the chosen reference is relatively minimal and domain independent. 
Excessive Prompting Impact: Weâve noticed that over-prompting a specific region can lead SAM to neglect other areas. However, a dense distribution of negative prompts in regions with heterogeneous background features may degrade SAM segmentation performance. Conversely, in targets with homogeneous features, a dense positive prompt distribution doesnât degrade SAM segmentation performance with ViT-L and helps counteract the impact of some erroneous negative prompts.

To R3:
Insufficient Experiments: The limited availability of high-quality annotated medical images poses a challenge, leading us to focus on comparing one-shot and few-shot approaches instead of fully supervised methods. Additionally, Table 2 demonstrates that our method outperforms ref-17 in this task.
Exploration of Few-Shot: We value the professional suggestion. While our proposed method has shown superiority over recently proposed few-shot methods, our next focus will be on exploring the potential of few-shot data in prompt generation.
ViT-H vs. ViT-L Performance: We observed that ViT-H is more sensitive to prompts compared to ViT-L. In cases of GBM, ViT-H may overlook distant parts when many closely spaced prompts are present, while ViT-L is less affected by this.
Hard Samples Analysis: We observed that negative prompts in background resembling the target can improve segmentation. We use hard samples to simulate this, aiming to enhance segmentation performance.

To R4:
Patch Matching: For both forward and backward matching, we match the patches of the source image and the target image successively. Once a patch is matched, it is not considered in subsequent matches, thus preventing multiple points mapping to one point or one point to multiple points.
Hard Sampling: The hard sampling only focuses on the negative prompts in our work, due to the heterogeneous of the background relative to foreground.
Patch Center offset: It actually has relation to patch size, the large size may cause patch center offset, while relatively small sizes (In ours) reduce the probability of offset, allowing the patch feature to be roughly represented by its center point.

To R5:
Novelty Concern: Although our approach doesnât introduce a completely new model, like medical image-based foundational models, which often demand substantial data and computational resources for training. 
Instead, we focus on autonomously generating prompts to assist SAM in medical image segmentation, especially in low-resource scenarios like one-shot learning. This tackles specific challenges in medical imaging, reducing reliance on extensive resources while offering practical solutions for clinical applications.
Moreover, our automated prompting framework effectively bridges the gap between SAM in medical and natural images, improving segmentation effectiveness and facilitating the integration of medical images with the foundational models of other domains in a training-free manner.
Hyperparameter: Due to page limitation, hyperparameter selection is not included. Optimal results were actually achieved when setting Dsp of negative to 70, Dsp of positive to 0, and Dex to 140."
https://papers.miccai.org/miccai-2024/318-Paper0368.html,"Thanks to R1,3,4 for their feedback. Weâre glad they all value the significance of our work, our methodâs elegance and significant improvement over strong baselines (R1), our well-rounded experimental coverage (R3), and our algorithmâs effectiveness and clarity (R4).

Local DA is costly (R1) requiring training for each target (T), but this applies to 4/5 compared methods accessing T. Only UniverSeg incorporates T data directly at inference, but lacks adaptation mechanisms for domain gaps, and requires centralizing huge datasets and costly GPUs for training.

Privacy (R1): Sharing both S_s and F may raise privacy concerns, e.g. due to membership inference. Future work involves integrating differential privacy and encryption.

T labels (R1,3) include âthree midpoint slices, extracted from three random volumes from D_tâ (Sec 4.2). While not in the text, we explored higher values. Fine-tuned methods (nnUNet,SAM,MedSAM) peak in accuracy with a few full labeled volumes. Ours and UniverSeg reach high accuracy quicker, showing good results already with 3 slices, a notable strength.

Nodes > 2 (R1): Our FL step has 3 nodes. In local DA, 1 node acts as source (S), 1 as T, totaling 2 domains like conventional DA. An extension to S nodes > 1 may be an ensemble method adapting T data to each S and averaging softmax segmentations.

Reproducibility & private data (R1,3,4): We missed to state code will be released if accepted. 5/6 datasets are public, largely favoring reproducibility.

DSL (R3): 1) shares modalities across centers and aligned to a common space; 2) doesnât use DatasetGAN but pix2pix for generation conditioned on segmentation masks; 3) needs all modalities paired with common labels for training; 4) is trained only for generation, then used for downstream segmentation. Instead, we combine unpaired image translation and segmentation in one model, requiring labels for only 1 S modality. We consider DSL similar to FedMed-GAN, so itâs not in our benchmark. Weâll add it in Sec 2.

We omit implementation details (R3) for nnUNet(+DAug) (supervised with S data), and FedMed-GAN (unsupervised with S, T data) as they use standard setups. Weâll add them.

Shared data (R3): Client t uses F to âgenerate syntheticâ¦ samples x^s_j that resembleâ¦ their native domain D_sâ (Sec 3.2). Embeddings donât need to be shared and t doesnât access real S images, but uses fake x^s_j to compute cycle consistency.

The claim about domain-invariant /specific features (R3) in W marks our use of a single latent space, contrasting with most disentangled representations in unsupervised translation that encode style and content separately. Weâll remove it for clarity.

Unclear terms (R4): While complimenting our methodâs effectiveness and clarity, R4 flags unclear terms as major concerns, expressing hope they are checked and corrected. R4 points to âmultimodalâ and âasynchronousâ. Multimodal refers to multiple imaging modalities as in the literature: we explore methodsâ ability to segment PET, CT, T2w using labeled PDw (Sec 4.1). Asynchronous describes how DA operates independently at each node, opposed to FL that involves all nodes.

The segmentation branch (R4) isnât our methodâs core. Itâs used only for local training. Its architecture is from DatasetGAN [28]. The core is F, involved in conditional training via FL, disentangling W, and local DA when integrated with encoders E_s and E_t for image reconstruction and translation and branches S_s and S_t for segmentation. S_s and S_t are trained with segmentation losses (Sec 3.2).

FedDG (R4) is a DG technique so doesnât access T, but uses a multi-S setup: itâs the only method âtrained using all datasets except the targetâ (Sec 4.2). We use FedMed-GAN with nnUNet as alternative DA. While we evaluate a wide range of methods, we highlight thereâs a lack of reproducible methods merging DA and FL in medical segmentation literature, underscoring the novelty of our work.

Typo G=F and better Fig 1 will be in a new version."
https://papers.miccai.org/miccai-2024/319-Paper2717.html,"We sincerely thank all reviewers and ACs for their recognition of the novelty, performance, and presentation of this work. Here are responses to their invaluable suggestions and remaining concerns.

R1Q1: Comparison with SimAgg
As suggested, we will compare FedEvi with SimAgg on the three federated segmentation tasks and summarize the comparison results in the camera-ready version.

R1Q2: Performance on IID and Non-IID data
The datasets utilized in our study belong to Non-IID data. We will further partition the Kvasir dataset (client1 from the polyp dataset) into 4 equal-sized subsets to simulate IID data from 4 centers. The comparison results will be provided in the journal version.

R2Q1: Experimental details
FedDG and FedCE are designed for federated segmentation tasks, while others are task-agnostic. The implemental details are as follows.
FedProx: It constrains the distance between local and global model parameters in local training.
FedProto: We followed FedSeg (CVPR23) to calculate local categorical prototypes, thereby aligning global and local prototypes. Besides, clients and the server also communicate model parameters alongside prototypes for fair comparison.
FedSAM: It adds a small perturbation to local models in local training to enhance the generalization capability of the global model.
FedBR: We introduced the projection layer following the encoder of 2D-UNet for both global and local models, enabling feature projection and contrastive learning.
FedLAW: It constructs a proxy dataset for optimizing the aggregation weights on it.
FedGA: We used the change in Dice loss of the validation set to measure the generalization gap to adjust aggregation weights.
L-DAWA: It adjusts layer-wise aggregation weights based on the parameter divergence between the global and local models.
FedUAA: We first calculated pixel-wise uncertainty and Youden index and then determined aggregation weights based on the averaged Youden index of each client.

R2Q2: Revision of Fig.1
Thanks for your constructive suggestion! We have revised Fig.1 in the camera-ready version to clarify the distinction between the surrogate global model and the global model, along with a clear depiction of local training.

R3Q1: Error in Eq.10
The Dirichlet parameter is linked to evidence by Î± = e + 1 (EDL, NIPS18). By reducing the evidence of incorrect predictions, the corresponding Dirichlet parameter decreases to its minimum of 1. Therefore, it should be KL[Dir(Ï|Î±)||Dir(Ï|1)] rather than KL[Dir(Ï|Î±)||Dir(Ï|0)].

R3Q2: #Client in fundus dataset
Thanks for pointing out the writing error! To ensure clarity and avoid potential confusion, we have revised the number of clients to 6 and modified the data source names of client3 and client4 to REFUGE(Zeiss) and REFUGE(Canon) in the camera-ready version.

R3Q3: Measurement of global generalization gap
As recommended, we will analyze the impact of the global performance on the global generalization gap in the journal version. Specifically, we will conduct an additional experiment by multiplying the global epistemic uncertainty with the Dice loss on the validation set to evaluate the global generalization gap."
https://papers.miccai.org/miccai-2024/320-Paper0366.html,"We thank all reviewers for their thoughtful comments.
R3: The federated SAM is limited to similar organs at the same modality.
A: If each client has different organs and modalities, the training in federated learning will be difficult to converge, resulting in poor performance. So current federated learning methods are mainly applied in the same type of organs and modality.
R3: Do commenting properly in code.
A: We improved the comments in our code.
R4: Different clients should use different sources of data.
A: In our experiments, the datasets from each client actually originate from different institutes. E.g., in Fundus, the datasets of clients A, B, C, and D are sourced from REFUGE, ORIGA, G1020, and Drishti-GS1, respectively. Weâll replace A, B, â¦ with the names of the institutes.
R4: Add more background introduction of federated learning.
A: Following your suggestions, weâll add more introduction: Federated learning is widely applied in medical images since it allows collaborative training across healthcare institutions without sharing sensitive data, preserving privacy and complying with regulations. It leverages diverse, high-quality datasets from different sources, enhancing the modelâs performance and generalizability while ensuring data security.
R4: No statistical tests.
A: Based on the existing results, we calculated the p-values between FedSAM and SAM, as well as between FedMSA and MSA, which are both greater than 0.5. This indicates that the differences are not significant, i.e., the results of federated training and centralized training are similar. 
R4&R5: Limited novelty.
A: We aim to investigate if SAM performs as well in federated learning as in centralized training, so we utilize the fedAvg framework. Other federated learning algorithms are also applicable, such as FedProx and FedNova. Our innovations include: 1. collecting various real medical segmentation data from multiple institutions, whereas other federated learning studies typically use only two datasets, 2. implementing federated learning for foundation models and exploring its effectiveness, 3. using MSA to achieve a more efficient method.
R4&R5: Include other SOTA methods for comparison, such as nnU-Net.
A: According to our attempts, due to the ineffectiveness of nnU-Netâs preprocessing under federated learning and its instability when training on real multi-institution (Non-IID) datasets, the average dice of FednnU-net are lower than those of FedSAM by 27.96% for Prostate, 9.86% for Brain Tumour, 9.73% for Fundus, and 2.75% for Nuclei. FedSAM shows better generalizability and stability.
R5: The paper is built on a non-peer-reviewed paper published on arXiv.
A: FedAvgâs paper was published on arXiv and PMLR. We corrected the reference to: Communication-efficient learning of deep networks from decentralized data. In: Artificial intelligence and statistics. PMLR (2017).
R5: The proposed framework did not demonstrate the beauty of SAM.
A: Following your suggestions, weâll add demonstrations of SAMâs adaptability and effectiveness under federated learning.
R5: Was the NVIDIA A800 for fine-tuning or aggregation? What does the model efficiency analysis in Table 2 include? List the average predicting time per 2D image.
A: In our framework, clients are used for fine-tuning, server is used for aggregation. They both run on the NVIDIA A800. In Table 2, GPU Memory Usage, Average Training Time, FLOPs and Learnable parameters are for client learning (i.e. fine tuning), Learnable Parameter is also used to calculate the parameters the aggregation needs to communicate and sum. Calculated from existing results, FedSAM and FedMSA require 0.118 s and 0.127 s respectively when predicting per image.
R5: The paper does not provide sufficient information for reproducibility.
A: The submission has provided an anonymized link to the source code, dataset and environment setting.
R5: Spell out abbreviations.
A: MAE is Masked Autoencoders. FLOP is Floating Point Operations."
https://papers.miccai.org/miccai-2024/321-Paper1182.html,"We sincerely thank all reviewers for their insightful feedback. For writing mistakes, we have revised the manuscript according to their suggestions. Due to space limit, some main concerns are addressed as follows:

Reviewer #3
Q1: Can the method be applied to annotations with certain slices or subregions? 
A1: Our method is tailored for incomplete annotations concerning subregions, but not specifically for slices. Recognizing the relevance of this issue in medical practice, we plan to explore it further in our future research.
Q2: Randomly removing lesions may not be realistic. 
A2: In this paper, we make the first effort to address incomplete annotations in the scope of FL, even under specific assumptions of unlabeled lesions. In the future, we will delve into more realistic settings.
Q3: How to choose T. 
A3: In the experiments, we show that our method is somewhat robust and does not depend heavily on the tuning of T. However, your suggestion is well-taken; an approach with adaptive T is better, and our method needs to be improved accordingly.
Q4: Only dice is presented. 
A4: Qualitative results (Fig. 3) demonstrate that FedIA recalls all lesions with fewer false positives, leading to the best segmentation performance, whereas other methods suffer from extensive false negatives or fail to segment any lesions. Following your suggestion, we will consider adding recall as a new metric.
Q5: The use of number of regions.
A5: The annotation may include a large lesion but miss many small ones, leading to a low completeness rate. 
Q6: All slices?
A6: Only slices with lesions are considered.
Q7: The effect of two elements in reweighting function.
A7: We consider adding an ablation study in future work.
Q8: Confidence and threshold.
A8: The confidence means the predicted probability; the feature map goes through the linear layer and then softmax to obtain the probability. We conducted an ablation study in the supplementary material about threshold.
Q9: The impact of lambda.
A9: We havenât conducted experiments to evaluate the impact of lambda and we chose its value by observing changes in IoU in the early phase.
Q10: The equation for completeness rate.
A10: The equation is an estimation designed by us. It varies across datasets due to different annotation levels. The computed results of the equation are not exactly equal to the actual annotation completeness but indicate which clientâs annotation is more complete.
Q11: The impact of changing T for MS
A11:Â MS is more sensitive to the changing T, but it can also maintain stability to a certain extent.

Reviewer #4
Q1: Loss is only assessed empirically. 
A1: The assessment is not solely empirical. Our setting is IID; the low completeness means a high noise level. In many early studies, loss was used to measure noise level, confirming the relationship between the two. 
Q2: First-order polynomial function. 
A2: We derived the fitting with inspiration from the CVPR paper ADELE, which uses an exponential function. Considering that the IoU increases more slowly in FL compared to CL and that the first-order function is the simplest, we chose a first-order polynomial function, which performs better than an exponential function and fits the early values well in practice.
Q3: Output confidence. 
A3: Please see Q8 in Reviewer #3.
Q4: Only consider the false negative lesions. 
A4: A critical problem of multi-lesion annotation is the labor-intensive process. We aim to develop a well-performing model with incomplete data and focus on handling the incompleteness issue. In our setting, only false negative lesions exist without false positive lesions, highlighting the incompleteness problem.

Reviewer #5
Q1: FedA3I. 
A1: In our initial consideration, FedA3I focuses on the noise near the boundary, where the annotation is complete. Thus, we did not include it in our experiment. We will consider including FedA3I in our experiments.
Q2: The choice of a linear function. 
A2: Please see Q2 in Reviewer #4."
https://papers.miccai.org/miccai-2024/322-Paper2266.html,"We thank the reviewers for their constructive comments and suggestions. References follow our submitted paper.

Lack of Numerical Results (R1)We disagree with R1âs comment. It seems that R1 overlooked our detailed numerical results in Figures 2, 3, and 4, covering over 50 experiments across six medical imaging datasets of various sizes. Notably, conducting a single experiment on our splits of the CheXpert dataset requires a full day of computation on a high-end V100 GPU. In total, our experiments consumed 546 GPU hours.

Evaluation Metrics and Low Performance (R1, R3)We clarify a misunderstanding of the âLTRâ metric by R1. In our paper (section 3.3 - Reported Metrics), âLTRâ stands for âLong-Tailed Recognition,â not âLong-Term Retention,â which is not mentioned in the paper. Regarding R1âs concern about adaptability to new distributions, our adapted LTR measures performance across all seen tasks, and we presented continual learning curves in Figure 2, which demonstrate the modelâs adaptability to new tasks. We used LTR to highlight the impact of imbalanced datasets, as shown in Figure 2, but we accept R3âs suggestion to include AUC for a broader perspective on performance. We will add AUC for the tested methods, which can be derived from existing data without additional experiments. For instance, the AUC scores for the FedAvg method are 73.4 on PAPILA and 66.9 on OL3I, comparable to those reported in MEDFAIR Table A8 [34]. These AUC results emphasize the importance of dealing with imbalanced datasets. Specifically, a model trained on a class-imbalanced dataset tends to over-predict the majority class, resulting in a high AUC but a low LTR. Examining the AUC vs LTR values across datasets resolves R3âs concerns about the seemingly low performance on Non-COVID, PAPILA, and OL3I datasets. In response to R3, we used a ResNet-18 backbone for all experiments, following MEDFAIR [34]. We will include this discussion in our final version.

Demographic Factors and Simulated Splits (R3, R4)We acknowledge R3âs suggestion to use multicenter splits. However, public multicenter datasets are limited in scale (usually 2-5 clients), which does not fully represent the complexity of large healthcare networks (e.g., all hospitals in a country like the UK, involving more than 2000 clients). Our methodology enables the simulation of such large-scale scenarios using any publicly available dataset that includes attribute metadata, such as CheXpert. This scalable approach is useful for researching various medical tasks. Previous works, such as MEDFAIR, have demonstrated significant disparities in performance when datasets are split by age. This is why we used age as the primary attribute in most datasets. Note that we used other attributes such as skin type in Fitzpatrick17k. Nonetheless, our benchmark is easily adaptable and can incorporate other single (e.g., device manufacturer) or multiple attributes (e.g., sex and age) with minimal additions to the code. We appreciate the reviewersâ suggestions to perform a comprehensive analysis of these demographic factors, and we aim to explore this in future work.

Why focus on images and not include NLP tasks? (R4)Recent studies showed that the most common type of FDA-regulated ML systems involved image processing [11], making the safe development of such systems an urgent matter. Due to the limited space in conference submissions, we focus on images in this work. Yet, our FedMedICL framework can work with any modality, and extending it to NLP tasks is a promising future work suggestion.

Why T=4 in the localized split? (R4)Our choice of four tasks is motivated by seasonal variations in disease patterns and hospital admission rates, which affect demographic variations in hospitals. For example, the prevalence of seasonal flu can alter the demographic composition of hospital patients. Our choice captures such temporal and demographic shifts to enhance the realism of our simulations."
https://papers.miccai.org/miccai-2024/323-Paper1176.html,"We sincerely thank all reviewers for their insightful feedback. Due to space limit, some main concerns/misunderstandings are addressed as follows:

Reviewer#3
Q1&2: Fig. 2 is confusing.
A: In our paper, missing classes and non-missing classes mean negative classes and active classes, respectively. The ambiguity might arise due to ânegativeâ can also represent the negative samples of a class, which are indicated by 0 in the diagram. The negative class and active class in the legend are denoted only by color and are not related to the number of boxes. To avoid misunderstanding, we will correct in the camera-ready version.
Q3&7: Text description issue.
A: We will correct these errors. Thank you.
Q4: Data leakage problem.
A: Previous studies [1] have shown that multiple averaging of features can better protect privacy. In our method, the features of each class are first averaged locally, and then the class prototypes are averaged again after being uploaded to the server. This process makes it nearly impossible to reconstruct the original samples.
[1]Rethinking federated learning with domain shift: A prototype view
Q5&6: Regarding the issue of experimental details.
A: The paper focuses on FL scenarios under task heterogeneity. To simplify the problem, we make an assumption on data distribution, and explored performance improvements under different task missing rates.

Reviewer#6
Q1: Related work on label distribution skew is not covered extensively.
A: We will consider to cite related works. However, we should note that this paper explores task heterogeneity but not general data heterogeneity.
Q2: Logit adjustment techniques have been applied to FL.
A: Although logit adjustment techniques have been previously utilized, we employ this technique as it effectively addresses the new challenge we present.
Q3: In the MLD module, there are too many (four) hyper-parameters. How did authors select these hyper-parameters?
A: In our experiments, we find that fine-tuning the parameters has little impact on overall performance due to the adaptation to class difficulty. However, due to page limitations, we opt for a more moderate value and do not present ablation experiments on the hyper-parameters.

Reviewer#7
Q1: The experimental settings are not very clear.
A: To simplify the problem, the dataset is randomly partitioned into IID subsets. The missing rate of classes for each client is consistent, and the results are demonstrated on an unbiased test set.
Q2: It is not clear how well the techniques will work on real-world federated datasets. There are real-world federated datasets available now where the name of the hospitals where each medical image originated is known. For example: https://github.com/owkin/FLamby
A: This is a problem worth exploring, and we will investigate it in future work.
Q3: The idea of âhotâ and âcoolâ is intuitive, but not clear how it is modeled in the experiments.
A: We indeed propose âhotâ and âcoolâ categories based on intuition, broadly referring to categories that are more and less recognizable by clients. Details can be found in Fig. 3 in the supplementary material. The second and fifth categories have the lowest annotation rates and can be considered as âcoolâ categories, while the first and third categories have the highest annotation rates and can be considered as âhotâ categories.
Q4: I think that the test set at each hospital should also be biased towards the classes they consider important and have labeled.
A: Actually, the global test set represents to the aggregation of all local test set; thus both settings are equivalent. 
Q5: It is not clear why the authors call it federated multi-label learning and not federated multi-task learning?
A: In deed, these two concepts are different. In multi-task learning, a loss function has multiple terms (i.e., objectives), which is different to our focus."
https://papers.miccai.org/miccai-2024/324-Paper2368.html,"We thank the reviewers for their positive and constructive suggestions, highlighting our novel and effective method for medical image classification. Here are our responses:

Not related to MIC or CAI (R1, R4): We appreciate the acknowledgment of our paperâs strengths and the suggested venues such as ICLR, ICML, and NeurIPS. We will certainly consider these for future work. However, we emphasize that the primary objective of our work is medical image classification using federated learning (FL). Our work addresses the crucial challenge of data heterogeneity in FL-based medical image classification tasks. We utilized popular datasets for skin cancer and diabetic retinopathy, demonstrating that our approach mitigates challenges associated with FL-based computer-assisted diagnosis. For comparative analysis, we employed well-known popular baselines. Notably, in MICCAI 2022 and 2023, there were a total 20 accepted FL-based papers using similar baselines, underscoring the relevance of our approach. We believe our proposed approach is a valuable contribution to the healthcare domain and aligns with MICCAIâs objectives. We humbly request reviewer R1 to reconsider the score and acknowledge our workâs significance.

More details on the data preprocessing, model parameter settings, and validation procedures would enhance reproducibility (R3): Due to page limitations, we could not provide extensive details in the manuscript. Some of these are mentioned in Section 4.2. We will include the remaining details in the camera-ready paper and share a GitHub link to facilitate reproducibility.

The manuscript is generally well-written but occasionally suffers from jargon (R3):  We will review the entire manuscript and revise it to reduce jargon, ensuring better accessibility for all readers.

Potential scalability issues with increasing numbers of clients and the high computational overhead (R3) : Our model is designed for hospitals, clinics, and research institutions, enabling cross-silo FL with typically a few hundred clients. While the paper focuses on the efficacy of FedMRL in addressing data heterogeneity, future research will optimize computational aspects and explore deployment strategies for large-scale FL systems. We will explore distributed computing and resource-sharing models to alleviate the server-side burden. Additionally, we will investigate more computationally efficient MARL algorithms.

Comparisons with other RL-based or advanced FL strategies (R3): We have compared our work with four popular state-of-the-art baselines addressing data heterogeneity. Most existing RL-based works use a single-agent approach, differing from our multi-agent approach. In future work, we will explore these RL-based methods for comparative analysis.

Ablation study (R3,R4): Due to page limitations, we were unable to include ablation studies. We will add a small section discussing the outcomes in the camera-ready paper to provide readers with an understanding of the different parts of the architecture.

The normalization or the weight control in the loss function and carefully consider the design of Eq 3 (R4): The loss function in FedMRL balances local loss, proximal regularization, and fairness across clients without explicit weight control. These terms collectively address data heterogeneity and ensure fair convergence across hospitals. The proximal term (Î¼) is dynamically adapted through our multi-agent RL approach, providing implicit weight control based on client state and performance feedback. This adaptive mechanism adjusts the regularization termâs importance according to each clientâs data distribution and model performance. Exploring optimal weight settings and their impact on performance is a promising direction for future work, potentially leading to extensions of the current work.

Github link for reproducibility (R1, R4): We will provide the Github link in the camera-ready version of the manuscript."
https://papers.miccai.org/miccai-2024/325-Paper3884.html,N/A
https://papers.miccai.org/miccai-2024/326-Paper0102.html,"We are encouraged that reviewers find our work âwell-written and clear organizationâ (R3,R4), âtechnically soundâ (R4), âpractical settingâ (R3), âgood motivation and well-supported design decisionsâ (R1,R3) and âpromising performanceâ (R3,R4). Thanks for the constructive comments. Our responses to major concerns are as follows.

Q1: (R4) It would be better if the authors could discuss or compare [1,2,3,4] in the revision, making a more thorough study.
A1: Thanks for suggesting these references. While our focus was more on methods specific to the medical field, we acknowledge the importance of discussing broader techniques. We will include and discuss these studies, noting how [1] utilizes labeled prototypes to rectify pseudo labels, [2] introduces feature perturbation, and [3,4] innovate with adaptive Gaussian mixtures for a new type of consistency.

Q2: (R1,R4) It would be better if the authors could add more datasets in the experiments; (R4) and compare recent SOTA 3D medical image analysis methods [5,6,7].
A2: (1) We agree that including more datasets can enhance the study. Due to MICCAIâs rules, we may not directly add dataset to the current version, we plan to include it in our extension. We further validated our method on a left atrium benchmark, following the data split in [8]. With only 24 cross-labeled scans (72 labeled slices) and 56 unlabeled scans, the Dice of Sup(cross)/CPS/AC-MT/CAML/UPCoL/DeSCO are 0.772/0.789/0.800/0.720/0.809/0.798. Our MF-ConS achieves 0.829 (+AL: 0.842), while the upper bound is 0.916. 
(2) [5,6] focus on self-supervised learning. While we will discuss them in the final version, including them into our current experimental design is challenging as they are beyond our scope. As for [7], while it is an excellent study, we currently ensure fair comparison by using the same backbone across all models.

Q3 (R3): (1) The masking probability transits from 0.25 to 0.5, why choose this design? (2) Why only label three orthogonal slices per scan? (3) For the prototype-driven consistency, what are the differences between [23] and this paper? (4) Why choose the entropy-based sampling as active selection?
A3: (1) As the modelâs perceptual capabilities become stronger during training, we gradually increase the difficulty on this NIOP consistency regularization. 
(2) As mentioned in Sec.2.1, while our study uses the baseline that labels one slice per plane, the strategy can be adapted to label additional slices per plane for more challenging tasks or when fewer training samples are available.
(3) As mentioned in Sec.1, considering the scarcity of reliable labels in both sparsely-labeled data and unlabeled data for prototype generation, our strategy involves the fusion of prototypes derived from both data types to comprehensively represent the distribution of the feature space instead of separate usage in [23] with relatively adequate ground truth.
(4) This active function has proven robust and effective to achieve informative volume identification. While our paper does not focus on new active functions, more advanced functions could potentially yield further benefits.

Other minor concerns will be carefully revised.

Ref:
[1] Querying Labeled for Unlabeled: Cross-Image Semantic Consistency Guided Semi-Supervised Semantic Segmentation. TPAMIâ23
[2] Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation. CVPRâ23
[3] Sparsely Annotated Semantic Segmentation With Adaptive Gaussian Mixtures. CVPRâ23
[4] Modeling the Label Distributions for Weakly-Supervised Semantic Segmentation. Arxivâ24
[5] VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D Medical Image Analysis. CVPRâ24
[6] Self-supervised pre-training of swin transformers for 3d medical image analysis. CVPRâ23
[7] nnU-Net. Nature Methodâ21
[8] Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation. MICCAIâ19"
https://papers.miccai.org/miccai-2024/327-Paper3732.html,"We are grateful to the reviewers (@R1,R3,R4,R5) for the constructive feedback, as well as the recognition of the strengths of our work such as the satisfying novelty (@R3,R4,R5), extensive experiment evaluations(@R1,R3,R4,R5), and state-of-the-art performance on few-shot segmentation tasks(@R1,R3,R5).

Concerns and Questions:

â Supplementary material, implementation Details (@R1,R3,R4,R5). We apologize for the confusion. We accidentally uploaded the wrong file in our initial submission and will add implementation details back in the final version of our main paper.

â Statistically significant difference (@R1). Thank you for your suggestion. We will add statistic analysis of our experiments in the final version.

â Typos, formats and figure presentation (@R1,R3,R5). We apologize for the confusion. We will revise accordingly in the final version.

â VAT as 3D few-shot segmentation baseline (@R3). 1. In Table 1, we are evaluating against conventional 1-/5-shot (2D) segmentation methods on Abdomen-CT and CHAOS-MRI datasets and our method is not based on VAT. On the other hand, VAT does not directly provide evaluation results on these two benchmarks. Thus we omitted reproducing VAT on conventional 2D few-shot segmentation tasks. 2. While most of existing few-shot segmentation methods do not have open-source codebase available, we choose to adapt VAT (code available) to 3D few-shot segmentation case as our baseline for performance evaluation on few-shot 3D segmentation tasks.

â Visualization of tubular data segmentation (@R3). The visualization of few-shot 3D segmentation on tubular data is presented in the videos included in the uploaded supplementary zip file.

â One-shot case (@R4). We apologize for omitting the details in our main paper. For 1-shot case, we randomly apply data augmentation techniques, e.g. horizontal flipping, affine transformations or color jittering and input ânâ number of augmented support images (as pseudo n-shot case) to explore comprehensive morphology information from the single support.

â Selection mechanism of the supports (@R4). For conventional 2D FSS task (Tab. 1), we randomly select n support slices within same data volume for each query image (intra-patient), for fair comparison with existing FSS methods. For few-shot 3D segmentation on our proprietary data (Tab. 2), intra-volume and inter-volume refers to selecting support images/sequences across same and different patient volumes respectively.

â Support in 2D/3D (@R5). We apologize for the confusion. Our proposed MSFSeg is compatible with support in both 2D and 3D format. Though conventional FSS methods typically take 2D support (n-shot refers to n number of slices), we design our MSFSeg starting from conventional 2D FSS baseline network and employ multi-head attention followed by multi-surrogate fusion to distill information across different support sequences. Specifically, our MSFSeg takes n support sequences, with each support sequence containing d_i (c.f. Section 2.1A, d_i >= 1) slices, where the task downgrades to conventional 2D few-shot segmentation when d_i=1. The multi-head attention module in MSFSeg allows flexible length of input for each support sequence (d_i >= 1), which aggregates intra-sequence information by attention computation. We then apply multi-surrogate fusion to explore inter-patient morphology information across different support sequences for improved 3D few-shot segmentation."
https://papers.miccai.org/miccai-2024/328-Paper2320.html,N/A
https://papers.miccai.org/miccai-2024/329-Paper4161.html,"We greatly appreciate the reviewersâ insightful comments and recognition of our workâs strengths including the soundness and clarity of our approach (R1), the suitability of the CBCP method for microscopy (R3), and the novelty of addressing extreme class imbalance in medical imaging (R4). Below are our answers:

RWBC: Raabin-WBC

1- Data (R1) and code sharing (R3, R4):
We will release the code upon acceptance, the data is already publicly available.

2- Effectiveness of each component of I2DA (R3, R4):
     | Ldis | Lsim | Lcls | Ldis+Lsim | Ldis+Lcls | Lsim+Lcls | Lsim+Ldis+Lcls
M5    | 38.0 | 39.7 | 41.9 |   42.2    |    43.4   |    43.8   |   44.2
This validates the effectiveness of each component of the I2DA loss. Lsim+Ldis+Lcls results in improved results.

3- Base architecture of other competitive methods (R4):
For a fair comparison, we have used YOLOv5, as used by AcroFOD and AsyFOD.

4- Few-shot results with only CBCP (R4, R3):
Employing only CBCP (w/o I2DA), our results are still better than the source-only model: M5:39.7 vs 19.9 (Tab.1), RWBC:66.5 vs 27.2 (Tab.2). CBCP effectively handles class imbalance by strategically augmenting source and target cells based on target-domain visuals, leading to these improved results.

5- Performance of CBCP in extreme domain shifts (R1):
Thank you for an interesting question, the shifts highlighted by R1 are quite challenging. However, input adaptation methods could partially reduce such domain shifts. Our algorithm could be run alongside such algorithms to overcome the limitations (especially class imbalance in src. & target) of input adaptation methods.

6- Effectiveness of our alignment (I2DA) over existing work (R3):
AcroFOD alignment results w/o augmentation (M5:37.8 | RWBC:58.7)  are much less than our alignment-only I2DA results(M5:44.2 | RWBC:66.5). AcroFOD alignment prioritizes target-similar examples, but we believe that small few-shot sets canât fully represent the whole target population. Our approach extracts moderate knowledge from the target set, ensuring generalizability to larger test sets while optimizing performance across majority few-shot scenarios.
7- Effectiveness of CBCP over augmentation techniques and combination of AcroFOD and CBCP (R3):
AcroFOD{align} and CBCP results in lower mAP (M5:45.8  and Raabin-WBC: 69.8) as compared to our method. Similarly, using our I2DA{align} along with AcroFOD aug. also results in decreased mAP (M5:43.0  and Raabin-WBC: 67.9).
NOTE: our method (CBCP+I2DA) results are M5:48.9(Tab.1), Raabin-WBC:70.7(Tab.2)

8- I2DA results on RWBC (Table:2) (R3):
As shown above (6&7), I2DA and CBCP complement each other. Please note even using alone I2DA, gives better results than SOTA on 3 & 4 shots on RWBC and all shots on other experiments.

9- How quickly the algorithm adopts (R1):
We train for 100 epochs and at around 80-85 epochs it starts generalizing to a larger test set.

10- 2% of batch size (R4):
Thank you for pointing this out. We will make it more clear in the final manuscript. We used a batch size of 4, in our experiment,  due to limited GPU. However, training on larger batch sizes, it is suggested that the real target samples from the few-shot set should comprise 2% of the total batch for better generalization.

11- Why so low thresholds (R4):
We chose low Lambda 1,2,3 weights to scale the complete I2DA loss with the YOLOv5 detection loss.

12- Appendix and typos (R3, R4):
We will update it in the final version."
https://papers.miccai.org/miccai-2024/330-Paper2187.html,"Thank you for your valuable feedback. Overall, this method is considered inspiring, directive (R3), innovative, and interpretable (R4), which simultaneously achieves few-shot (R5) with highly promising results (R4 R5). The main concerns raised relate to reproducibility, comparison, and the ablation study. Below, we will clarify the important points and resolve any possible misunderstandings.

Q1. Code (R3,R4,R5)
We will release the code upon acceptance to facilitate verification. We had previously uploaded the code to an anonymous GitHub repository, but providing the link is not allowed in rebuttal.

Q2. Compare with SoTAs. (R3, R5) 
We have compared our method against many STOAs, such as TOP (NeurIPS 2024), SimpleShot (Nat. Med. 2024), and the end-to-end method Trans.+FT (CVPR 2023), as shown in Tables 1 and 3. Ourfew-shot methodachieves an AUC of 98.7%, which is more effective than othermany-shotend-to-end results like DTFD-MIL+FT and CLAM+FT, which are around 96%.

[To R5] Adding more experiments to Fig 1 would result in a crowded visualization. However, the requested experiments have been included in Tables 1 and 3, where CLAM is shown to outperform other traditional methods in a few-shot setting (e.g., 58% for CLAM vs. 54% for TransMIL).

Q3. Our novelty (R5) 
Existing few-shot methods for WSI are mainly borrowed from computer vision and use fixed average features (e.g., Prototype), which show much lower results than many-shot approaches. In comparison, we propose a novel non-parametric classifier, which can prevent the model from overfitting on a few whole slide images (WSIs). At the same time, our approach utilizes informative local features to leverage the potential of a small number of mask labels. This novel design can achieve high performance, even greatly surpassing the performance of doctors (AUC 72.4% mentioned in CAMELYON16 vs. our 98.7%). As appreciated by R3 and R4, this approach is innovative.

Q4. Data and ablation analysis (R5) 
Firstly, the results are absolutely credible (see later code and result records). The 29.07% and 40% AUC increase in Tab. 1 & 4 can be attributed to our effective utilization of crucial local data (patch) and task-specific designs (refer to above innovation part), which suit WSI much better than existing methods borrowed from general CV. Besides, visual evidence is shown in Fig. 3, where existing methods predict many wrong tumors in normal WSIs and INC solves these errors fine. All the results suggest that our task-specific designs are very reasonable and necessary.

Q5.Description and expression (R3, R5) 
In Fig. 1b, the notion of a parametric classifier can be regarded as an FC layer with fixed weights, while a non-parametric classifier (e.g., KNN) uses the globally averaged feature. In comparison, our INC method keeps all local features and matches them dynamically according to individual test examples, making it a dynamic classifier.

[To R5] Regarding the overall architecture, it should be easy to follow. The symbols in Fig 2 are matched to the corresponding equations, which are then linked to the later code. Moreover, we will polish the paper to make the explanations even clearer.

Q6 Ablations on gallery samples. (R4) 
Tumor size of the gallery is another influencing factor. We selected WSIs with middle-range tumor sizes (1000-3000 tumor patches) to avoid the lack of tumor patches in tiny tumors and the excessive normal cells (noise) within large tumors. This approach increased the AUC by 3.35% compared to using all tumor size varieties. The results suggest that sample variety is influential, while our method is robust and can perform well without too much variation in the tumor sizes.

Q7. 5-fold (R4) 
The results are evaluated on the official test set, instead of using 5-fold cross-validation, which follows the setting in prior work. We simply take varied gallery samples (5 repeats) to ensure robust results, while maintaining a fixed test set."
https://papers.miccai.org/miccai-2024/331-Paper2611.html,"We thank all the reviewers (R) for reviewing and recognizing our work. Motivation and details of our proposed method are clarified thoroughly, and the writing will be improved.

Q: FiMA (R1, R3)
A: FiMA stands for Fine-grained Context and Multi-modal Alignment.

Q: Fine-grained and coarse-grained (R3)
A: Fine-grained features represent shallow, high-resolution features. Coarse-grained features represent deep, low-resolution features.

FiMA simultaneously capture spatio-temporal information on multi-scale features, rather than capturing fine-grained spatial information and coarse-grained temporal information separately.

Q: Motivation (R3)
A: FiMAâs motivation is to improve the reconstruction performance through multi-scale spatio-temporal learning and temporal information mining of multiple IMUs. We propose ReMamba, Adaptive Fusion Strategy, and Online Alignment Strategy to address the challenges in them.

1) ReMamba addresses the challenge of long-range dependency from the large number of patches during spatio-temporal information mining. It addresses this challenge with the long-range dependency management capability of the state space model (SSM).

2) Adaptive Fusion Strategy addresses the problem of IMU noise impact in mining temporal information from multiple IMUs. It fuses multi-IMUs with adaptive weights to extract appropriate information through mining the correlations between images and IMUs.

3) Online Alignment Strategy further mines the information of IMUs for unseen data, it uses IMU information as pseudo-labels to capture appropriate temporal features and improves the reconstruction performance on unseen data.

Q: Details of forward / backward direction (Section 2.1) (R1, R3)
A: Forward direction represents the direction along the length of the input features with shape (batch, length, channel). Backward direction represents the reverse direction along the sequence length.

Q: Adaptive Fusion Strategy (Section 2.2) (R3)
Q1: Effectiveness
A1: Adaptive Fusion Strategy significantly improves on all metrics compared to direct fusion (Table 1).

Q2: Details
A2: Fusion module does not directly correct the acceleration/angle, it projects the acceleration/angle of multi-IMUs into high-dimensional features, and explores their correlations with the image features, thereby mining additional temporal information to improve reconstruction performance. (Section 2.2 and Figure 4).

Q: Loss Function (R1, R3)
A: We refer to previous work [Ref1] which did not use balancing coefficient. The loss based on translation and rotation reported better results than Transformed Points Distance loss [Ref1].

Q: Dataset (R3, R4)
Q1: Details of the dataset
A1: We use 4 IMUs as shown in Figure 1. The acquisition frequency is 30 fps, and for each frame, we acquire ultrasound image, measurements of 4 IMUs simultaneously.

Q2: Public data
A2: The reason we did not use public data is that, to the best of our knowledge, no public data currently available contains information of additional IMUs.

Q: Statistical analysis (R3)
A: FiMA significantly outperforms the compared methods in almost all metrics for both arm and carotid datasets (t-test, p<0.05). The standard deviations have been provided in Table 1.

Q: Clinical significance (R4)
A: Freehand 3D ultrasound allows acquiring 3D volumes of vessels, plaques and other anatomical structures where 3D probe is restricted. Our method requires only ultrasound images and affordable IMU for high-precision reconstruction, and in the future, IMU can even be integrated inside the probe to minimize the impact to the sonographer.

Q: Segmentation mask of vessels (R4)
A: We manually annotated the segmentation masks of vessels for the qualitative results.

[Ref1] Luo, M., Yang, X., et al. âRecon: Online learning for sensorless freehand 3d ultrasound reconstruction.â Medical Image Analysis (2023)"
https://papers.miccai.org/miccai-2024/332-Paper2750.html,"R1&R3: Specific designs targeting medical image; Explore applicability to other domains.
Thank you for the insightful comments. While FPT can be potentially adapted to various domains, it specifically targets medical imaging; its merits are particularly highlighted when applied to medical images. We shall analyze its applicability to other domains in future work.
The specific designs of FPT targeting medical imaging includes:

R1: More behavior analysis on components.
Thank you for the suggestion. Several design choices are involved in the proposed components. We have analyzed the ratio of token selection in Table 3. Due to space limitations, more analyses will be included in our journal version.

R3: Fusion module may introduce complexity.
Thank you for the comment. Simplifying FPT will be a main future research direction.

R3: FPT is contingent on the quality of LPMs.
We agree that FPT depends on the quality of LPMs, which nevertheless is common to transfer learning methods. Reducing such dependence is a very good suggestion, and we will explore it in the future.

R4: Can FPT be called tuning?
Thank you for pointing out this matter. We will modify the description of FPT to clarify it as parameter-efficient transfer learning.

R4: Fine-tuning the entire FPT.
Fine-tuning the entire FPT performs similarly as full fine-tuning baselines. Due to rebuttal constraints, results will be provided in our journal version.

R4: [1] is proposed for giga-pixel images.
Thank you for the comment. We will add this related work in the revised manuscript. [1] makes training on giga-pixel images feasible by splitting the whole image into low-resolution patches, while FPT focuses on enhancing efficiency for high-resolution inputs. For clarification, we will specify our claim in the context of high-resolution inputs in the revised manuscript.

R4: Data augmentation for baselines.
We apologize for any confusion. To ensure a fair comparison, all methods were trained employing the same data augmentations as those for low-resolution inputs in FPT.

R4: Is the 512x512 the best resolution?
Higher resolution improves performance, but it gradually saturates. The 512x512 resolution is a balanced choice. More analyses will be provided in our journal version.

R4: Preloading for linear probing.
Important token selection with feature preloading reduces memory usage. While preloading can be used for linear probing, it requires no augmentation, which would degrade the linear probing performance. For fairness, we used data augmentation for all methods, and thus did not employ preloading.

R4: How fast is FPT?
FPT is faster than other PEFT methods and only slightly slower than linear probing.

R4: Is the number of tokens increase?
We apologize for the confusion. The prompts in the intermediate sequence of the side network are removed after the forward layer, and thus the number of tokens of each layer is the same.

R4: LoRA details.
We added LoRA layers to q and v.

R4: Can FPT work well on SSL models?
FPT can adapt to different pre-trained LPMs, including SSL models. Due to rebuttal constraints, we shall provide more analyses on LPMâs adaptability in our journal version."
https://papers.miccai.org/miccai-2024/333-Paper3154.html,"We thank reviewers (R1,R4,R6) for their valuable insights, highlighting technical novelty, clarity & thoroughness of our work. Below, we address the comments to enhance our paper.

[R1, R4] LR selection: We thank R1 for the suggestion & will explain the reason for choosing LR in the main paper instead of Supp as it is important & might have been missed by R4. Reasoning behind LR choice for optimizing Grid Search(GS) is in Supp Fig1, showing linear mode connectivity (LMC) (Î¸=Î»Â·Î¸A+(1âÎ»)Â·Î¸B) between 2 models Î¸A & Î¸B generated during GS. By varying Î» & calculating performance at Î¸ for each model pair, we analyze the impact of hyperparameters(HP). Ideally, LMC should be an inverted parabola or a straight curve if models are in the same basin. We observe solely changing seed & augmentation(aug)(while other HP are fixed) yield smooth curves(models with huge difference cause drop in F1), while LR variations(subfigs c&f) result in erratic patterns & significant F1 drops, indicating models lie in different basins. This implies LR plays a crucial role in guiding models to specific basins, while other aid in converging to global optima. [33,24] also support LR as a critical HP.

[R1] Augmentations: While we only employed heavy aug in our FGG we see similar trend of better results in most cases compared to greedy soup of FGG-models(Ablation study Supp). For consistency across several datasets, we didnât opt for dataset specific medical aug but will be explored in future work.

[R1, R4] Fission: starts from base models(8),cyclically varying LR for 17 epochs generating multiple models(5 each). Will explain in detail in paper. Fission helps models escape several local minima in rough loss surface & generate better generalizable models, facilitating easier model averaging (Fusion using HS).

[R1,R4,R6] Computation Cost: Training: Model soups GS requires 2400 epochs to generate 48 models(50 epochs each), whereas our FGG needs only 536 epochs(850 + 817), 4x less than GS. Time taken per epoch in both settings is same.
Inference: Unlike ensembles, we hierarchically average model weights to get 1 model without incurring additional inference or memory costs.As we need only 1 model for inference, it is imp. in hospital settings where compute resources are often limited like portable ultrasound devices.  Details will be added to the paper.

[R1] Comparison to natural datasets: We focus on addressing the challenge of rough loss surfaces, which impede traditional model soups designed generally for smoother surfaces. Greedy soup in medical datasets, often fails to merge best model with others, potentially compromising generalizability. Our method targets medical datasets while demonstrating superior performance on natural domain datasets.

[R6] Results: Performance variations across datasets range from minimal to significant improvements. Notably, our approach (GoG) achieves a 6% improvement in datasets like HAM(ResNet50) & CheXpert(DeiT-S) compared to SOTA greedy soup. FGG is part of our method & we donât compare with it.

[R6] Statistical Significance: Starting from fixed initialization, given that GS involves training multiple models with different hyperparameter settings, mean & std. is generally not computed in model merging works[33,8,34,3] as merged model is average of weights from multiple runs.

[R6] Metrics: Tab1 uses different classification metrics for different datasets, as they are the significant metric used for those datasets[arxiv-2203.01825, doi-10.1007/978-3-031-45673-2_44].

[R6] GoU: GoU is FGG+HS. In Tab1 of Supp, we conduct ablations for various combinations. GS+HS(GoU) employs GoU(a HS technique) on GS models.

[R4]Future work: To show the effectiveness of our approach for several datasets(7), we utilized commonly used Imagenet pretrained ResNet50 & DeiT-S arch. Future work will explore bigger model arch, training techniques(SSL), other tasks like segmentation & medical pretrained models."
https://papers.miccai.org/miccai-2024/334-Paper0050.html,"We are glad that reviewers find our work âwell-writtenâ (R1), âclear motivationâ (R1), ânovel and promising methodâ (All), âextensive and convincing evaluation with sensitivity analysisâ (R3) and âpromising performanceâ (All). The only negative recommendation came from R4 (weak reject). Thanks for the constructive comments. Our responses to major concerns are as follows.

Q1 (R1): (1) Add more SOTA semi-supervised methods like SASSNet, MC-Net and Co-BioNet. (2) Comparison with SAM and variants w/o active learning components. (3) The presented supervised baselines are trained with 100% labeled data? and lack of other supervised models? 
A1: (1) We plan to add these baselines in future extension. Notably, CAML (MICCAIâ23) [7], as shown in Table 1, is an improved version of MC-Net, and ACMT (MIAâ23) [22] also set recent SSL SOTA on our used datasets. 
(2) Direct comparison with SAM may be confusing as it requires manual prompting, whereas our goal is to develop a fully automated specialist. We have included several SAM variants to analyze our frameworkâs sensitivity as shown in Fig.3 (c).
(3) Training with 100% labeled data is the upper bound. The baselines are supervised models trained with limited cross-labeled data. We keep consistent backbone and training protocols with previous works to ensure fairness.

Q2 (R1): (1) Pseudo mask from 3 views should be distinct, yet similar in Fig.2. (2) Why 2D generalist models rather than 3D generalist models? (3) Applicability to multi-class segmentation?
A2: (1) Sorry for any confusion. Fig.2 shows different labels of the same slice for clear comparison, and we intended to say âpseudo masks generated via prompting from the three viewsâ. We will revise the caption of Fig.2. 
(2) We chose 2D generalists for their robustness and prevalence in the AI industry, coupled with the flexibility in model choices. Besides, automatically generating precise 3D prompts with on-the-fly coarse masks is more challenging. 
(3) Yes. For now, we follow recent SSL work [22] that segments WT only. We consider multi-class tasks for future work, where our methods, including SAM, can accommodate prompts with predefined class numbers.

Q3 (R3): (1) The relationship between the two data-centric schemes, i.e., independent or interactive?
A3: They interact cooperatively. The expert-model collaboration enriches the model with more expert supervision within the specialist-generalist collaboration, providing better object-specific prompts for higher-quality generalist-based labels. An improved model, in turn, delivers more precise informativeness signals for active selection.

Q4 (R4): Should compare with SAM-based method such as [r1] and [r2].
A4: Thanks for the recommendation. [r1] uses SAM to select reliable pseudo labels, while [r2] enforces consistency between point-prompted SAM outputs and student outputs. We reimplemented them and found that [r1, r2] only achieve {78.35%, 77.49%} Dice in our LA setting (60 labeled slices), which may be due to lack of high-quality label generation and calibration designs. We consider adding them in our future extension.

Q5 (R4): The paper employs the original mean-teacher (MT) method without any novelty.
A5: As highlighted in the paper, our novelty lies in pioneering a shift towards more data-centric designs in future SSL, driven by the generalist foundation models. This shift reduces the emphasis on model-centric advancements (e.g., more advanced methods than MT). Excitingly, incorporating the simple MT model, we have already achieved promising results, better showing the efficacy of our two proposed data-centric designs.

Other minor concerns will be carefully revised. Code will also be released to help follow the details.

Ref:
[r1] Segment anything model for semi-supervised medical image segmentation via selecting reliable pseudo-labels. ICONIPâ23
[r2] SemiSAM: Exploring SAM for enhancing semi-supervised medical image segmentation with extremely limited annotations. arXivâ23"
https://papers.miccai.org/miccai-2024/335-Paper0320.html,"We thank the AC and reviewers for their time and valuable feedback. We are delighted and encouraged that reviewers find our work is âthe first foundation model-enabled one-shot landmark detection frameworkâ (R1, R3), ânotably requires zero unlabeled dataâ (R3, R4), âmeticulously compares FM-OSDâs performance with current relevant baselinesâ (R3, R4), and has âclear coherence and readabilityâ (R3, R4). We would like to further clarify several questions raised by the reviewers.

Label the symbols in Fig. 1 (R1).
Thanks for the suggestion. We will include the symbols of different components in Fig. 1 in our final version for better clarity.

Details of tackling the domain gap between natural images and medical images (R1).
As stated in our abstract and the last paragraph of the introduction section, we use learnable decoders supervised by a distance-aware similarity learning loss to narrow the domain gap between natural images and medical images and integrate domain knowledge from a single labeled template image to some extent.

Distinctive aspects of the proposed FM-OSD compared with previous deep learning methods in the introduction section (R3).
The deep learning methods mentioned in the first paragraph of the introduction section typically require a large number of high-quality labeled data to train their models and achieve accurate detection results. However, it is extremely time-consuming and difficult to obtain such high-quality labeled data from domain experts. By contrast, our proposed method utilizes solely a single labeled template image and does not require any additional unlabeled data.

Some literature reviews include performance evaluations on datasets like the Chest dataset for landmark detection are absent (R3).
We will include some references that discuss performance evaluations on different datasets in the introduction section of our final version.

Disparities between reported results in this paper and results in the original paper (R3).
We appreciate the reviewer for checking the consistency of the reported results of SOTA baselines with those documented in their respective papers.
(1) For some of the SDR results for SCP, upon careful re-examination, we find that we accidentally referenced the data from the row above the target values for SCP in Table 2 of the original paper. We apologize for this and will update the correct values in our final version. Notably, with the correct values, our proposed method can still outperform the SCP method by a substantial margin, that is 77.92 (ours) vs 53.05 (SCP), 84.59 (ours) vs 64.12 (SCP), and 91.92 (ours) vs 79.05 (SCP) in terms of SDR under 2.5 mm, 3 mm, and 4 mm, respectively, demonstrating the effectiveness of our proposed method. We have thoroughly double-checked all the other numbers to ensure the correctness of the results in our final version.
(2) Regarding EGTNLR, we find that the template image used on the Head dataset by EGTNLR differs from those used by other methods after careful examination. Additionally, the information on the template image used on the Hand dataset was not provided by EGTNLR. Thus, as mentioned in the first paragraph of âComparisons with State-of-the-artsâ on Page 7 in our manuscript, we re-implemented EGTNLR using their released code and reported the results under the same training and testing settings as other comparison methods as well as ours on both datasets for a fair comparison."
https://papers.miccai.org/miccai-2024/336-Paper1614.html,N/A
https://papers.miccai.org/miccai-2024/337-Paper1306.html,"We thank the reviewers for their insightful comments and are grateful for the opportunity to improve our manuscript based on their feedback. We observe an inference time of 500ms for a pair of mammogram images (MLO and CC) on a V100 GPU, requiring less than 8GB of GPU memory. We will release our code to ensure easy reproducibility of our results. Our methodology, currently designed for one breast, can be extended to bilateral studies, making it more applicable to clinical settings. Additionally, we will update and maintain color consistency in Figure 3, correct all typos, eliminate duplication in references, and improve mathematical notation in the camera-ready version of the manuscript. Thank you once again for your valuable feedback."
https://papers.miccai.org/miccai-2024/338-Paper3161.html,"We sincerely thank the reviewers for their insightful comments and valuable suggestions.
Q1: Novelty and comparison with existing work (R1) R: Thank you for the comment and bringing the reference for our attention. The term ânot novelâ is inaccurate. Though AutoDVT (Tanno et al, MICCAI 2018) uses compressibility to diagnose DVT, it is not a segmentation but a classification task. We are still the first work using vessel compressibility to segment ultrasound images. The ThinkSono App claims to be able to segment groin vessels, but the method and performance remain unclear. Meanwhile, we are the first to propose the idea of utilizing force data to assist in segmentation, which introduces a new problem setting with benchmark dataset. This is also a key contribution of the article. Our proposed force guided approach can more effectively capture and utilize compressibility. Especially when force changes irregularly, directly selecting multiple frames will result in higher computational costs.
Q2: Ability of artery-vein differentiation (R1) R: Thanks for the comment. In the fourth row of the quantitative results (Fig.3), both Swin-UNet and Transunet misclassify part of a vein as an artery, while force guided networks does not encounter this error. In the third row, UNet also incorrectly identifies part of veins, which is mitigated by FG-UNet. We will conduct further experiments to quantify the modelsâ ability to distinguish between arteries and veins. It is notable that the proposed approach aims more than artery-vein differentiation. Precise vein segmentation is equally crucial. Comprehensive experiments on three baselines, along with visual results, confirm the effectiveness of the force guided approach.
Q3: Performance boost compared to added complicacy (R3) R: Thanks for the comment. The main contribution of this article lies not in performance boost, but in introducing the problem setting of utilizing force data to assist in segmenting ultrasound videos. We put forward a force-guided segmentation approach along with corresponding dataset, for further exploration by other researchers. This task can provide inspiration to the field of medical segmentation. Additionally, half of the self-constructed ultrasound dataset consists of carotid vessels which are extremely difficult to identify, making the task highly challenging. Even current SOTA methods perform poorly on this dataset, yet we are still able to achieve some performance improvements. Further tuning and modifications (such as applying nnUNet, suggested by R5) may lead to better performance. Moreover, although our method increases the complicacy during training, we can achieve nearly the same inference time as baseline models during testing by computing key frame embeddings only once per video. This feature will be available upon code open-sourcing.
Q4: Data acquisition and training details (R3) R: Thank you for pointing that out. Due to length limitations, these contents are omitted in the article. All of the mentioned details will be further illustrated when the dataset and code are released.
Q5: Key frame selection (R5) R: Thanks for the insightful comment. During data collection, we consider one palpation (one compression and one dilation) as a video clip. Given the short length of ultrasound videos, we employ this key frame selection method. For longer videos, approaches like selecting within a certain time window can be utilized without altering the overall structure of the network.
Q6: Possible usage of nnUNet (R5) R: Thank you for this valuable suggestion. In this article, we valid the force-guided approach can improve the performance of multiple U-shaped networks. We also notice that nnUNet may achieve better performance due to its self-adapting ability and will do more experiments in the future. 
Q7: Enhancement of the writing (R3&R5) R:  Thanks. We will carefully revise typos, notations and add quantitative results to the abstract to enhance readability."
https://papers.miccai.org/miccai-2024/339-Paper3334.html,"We thank all the reviewers for their valuable comments and feedback. The reviewers acknowledged several strengths of our paper:

While the reviewers recognized the novelty and utility of our work, concerns were raised related to the reproducibility and comparison with additional state-of-the-art:
(i) Releasing Code (R1): While external links are not permitted during the rebuttal, we commit to releasing our PyTorch code and will provide a GitHub link in the camera-ready version.

(ii) Releasing Dataset (R1): Our datasets were collected from multiple hospitals through multi-institutional collaboration. Unfortunately, we do not have permission to share the dataset publicly.

(iii) Evaluation on additional public datasets (R3): Our method has been extensively evaluated on 2 private datasets collected from multiple sites using different OCT scanners with large inter-scanner variations. Existing public retinal OCT datasets focus on predicting the current disease stage or segmentation tasks. To the best of our knowledge, there are no public longitudinal retinal OCT datasets with future conversion time-point labels to evaluate our method. We will discuss this as a limitation of the current work in the Conclusion section in the camera-ready version, if accepted.

(iv) Additional SOTA comparisons (R1, R4): We evaluated our method against several popular survival analysis methods, including an ODE-based continuous-time model (SODEN [18]), a semi-parametric model (DeepSurv [4]), and two multi-class classification methods modeling the CDF (Censored Cross-Entropy loss [20]) and hazard function (logistic hazard loss [12]). While more comparisons could strengthen our paper, MICCAI guidelines prohibit additional experimental results at this stage. âLMT: Longitudinal Mixing Trainingâ suggested by R4 is an ODE model  similar to SODEN [18] which we compared against but additionally incorporates a temporal mixup augmentation for longitudinal data that can also be employed while training our method. If accepted, we will discuss LMT in the related work section and future research directions in the conclusion. We also aim to provide additional comparison results with LMT in our GitHub repository.

(v) Clarification on risk calibration (âweakness Sec.â, R4): We predict a risk score r for each OCT scan, inversely related to its time-to-conversion. We calibrate these unbounded risk scores to a [0,1] scale for better interpretability. R4 suggested elaborating on this calibration step and raised concerns about potential bias if the validation set isnât representative of the target population.
Details: After training, r is obtained for all scans in the validation set. We learn a bicubic interpolation which maps k-th percentile of r values to k/100 in increments of 10 percentiles (0,10,20 â¦. 90,100 percentile values are mapped to 0, 0.1,0.2 â¦ 0.9, 1.0). Calibration is part of training, so the test set isnât used. Instead, validation set quantiles are used to learn interpolation parameters, similar to any hyper-parameter tuning in Machine Learning. This interpolation is then applied during inference to normalize the risk scores.
We will expand the calibration step description in the implementation details and discuss the potential for bias if the validation set distribution isnât representative of the target population (as noted by R4) in the conclusion section."
https://papers.miccai.org/miccai-2024/340-Paper0245.html,"Q1: Grammatical errors, Fig. 1, Fig. 2 and Fig. 3 (R1)
Weâve done a thorough grammar check and revise the first paragraph in 2.3.  We will change the color of Fig. 1 in the revision. In Fig. 2ï¼we select images with small lesions (first row) and low contrast (second row) as representative examples to show the effectiveness of our method. In fig 3, the two images are skin (the former) and polyp images (the latter), images 3,4,7,8 are the feature maps of the input images to show the feature learning ability of our proposed frequency domain consistency.

Q2: Are the transformers for MRSC pretrained? Please clarify that in the text (R1)
As shown in Fig 1, MRSC does not contain any trainable parameters, it only includes the feature transformation (down sampling, flatten operation and region similarity computation). We mentioned them in 2.3 Multi-granularity Region Similarity Consistency.

Q3: No cited evidence, evaluation metrics, improve the result and discussion sections, enhance clarity in Section 2.2 (R3)
MT [19] and URPC [15] are based on pixel-level consistency and they cannot model the relationship between local regions well and we will cite them in the revision. For segmentation evaluation metrics, we follow URPC [15] and SASSNet [10] and adopt these metrics to evaluate our method. We will enhance clarity in Section 2.2. in the revision.

Q4: I am curious about why the author chose the DCT over the Fast Fourier Transform (FFT) or Wavelet Transform (R5)
DCT generates real number coefficients, which simplifies subsequent processing and is more intuitive than the complex number coefficients of FFT. Wavelet Transform is very effective in removing image noise. Therefore, we empirically employ the DCT for the frequency-based consistency.  In order to be more rigorous, we will study the differences among these frequency transformations in our future work."
https://papers.miccai.org/miccai-2024/341-Paper1818.html,"We thank all the reviewers (R1,R3,R4) for their constructive comments. We address the major concerns here and will carefully incorporate all suggestions into the revised manuscript.
Q1(R1,R3,R4) Reproducibility: We will release our code and video segments upon acceptance.
Q2(R1,R3) Limited validation dataset: Due to page limit, we only choose SCARED Dataset to evaluate our method for two reasons. 1)It consists of nine different surgical scenes with challenging conditions, e.g. textureless surface, illumination changes, large camera translation, motion blur. 2)It contains GT camera trajectory for pose evaluation. Its results demonstrate that our method could handle these challenges and generalize well on different surgical scenes. We will validate on more surgical datasets in future work.
Q3(R1) Tissue deformation: We agree that tissue deformation is an important problem in surgical reconstruction. Our proposed consistency check can locate and remove the outliers to maintain rigid and reliable points in the optical flow for pose estimation (P5, Sec 2.3), which is promising for handling dynamic environments but may fail in severe deformation. In this paper, we mainly focus on SfM-free GS for static/semi-static scene. We will discuss the limitations in final version and extend to deal with tissue deformation in future work.
Q4(R3) Sensitivity to depth error: Before submission, we have conducted experiments to validate different depth models (DPT, Depth Anything, MiDaS) and choose Depth Anything which behaves the best perfomance for initialization. While there may be depth errors due to domain gap, the initialization stage will not affect the subsequent optimization too much for two reasons. 1)The adaptive density control helps prune outlier 3D Gaussians. 2)The scale-invariant depth loss helps avoid inconsistent depth during 3DGS optimization.
Q5(R3,R4) Detailed optical flow calculation: We use the pretrained RAFT model to compute the optical flow between frames, which is used as pseudo-GT to constrain the projection flow from 3DGS. Specifically, O_t-1->t is obtained by computing the optical flow between frames I_t-1 and I_t (R3). To deal with the transient objects and photometric inconsistencies, we employ the consistency check to obtain the flow mask. This could help identify and preserve correspondences that are rigid and reliable for accurate matching. After filtering out outliers in optical flow, we will perform flow-based pose estimation by minimizing the flow loss (P6, Sec 2.3). We will add more details of optical flow in final version.
Q6(R4) Real-time clarification: Yes, only the inference is real-time while the reconstruction process is offline. We will carefully update all related statements.
Q7(R4) Evaluation of depth reconstruction: We agree that depth evaluation is important for geometry-aware reconstruction. Before submission, we have conducted experiments to validate the performance of the depth estimation. However, the results show that the depth performance only achieves slight improvement compared to Nope-NeRF. 3DGS is popular mainly due to its efficiency and rendering quality, but is not very good at modeling the underlying geometry yet due to its multi-view inconsistent nature. In this work, we mainly focus on how to remove the reliance of SfM in 3DGS, while still achieving robust and accurate pose estimation and visual reconstruction. In future work, we will enhance our method to achieve geometrically accurate surface reconstruction.
Q8(R4) Iterations of 3DGS optimization step: During progressive growing (Fig. 2), we set 30 iterations for both pose estimation and 3DGS optimization. After estimating the poses for all the frames (fixed thereafter), we randomly sample frames to optimize the 3DGS following the original implementation. We will clarify this in final version.
Q9(R4) Meaning of Con: âCon.â refers to the consistency check to maintain rigid and reliable points. We will add the description in Table. II."
https://papers.miccai.org/miccai-2024/342-Paper1596.html,N/A
https://papers.miccai.org/miccai-2024/343-Paper0931.html,"We appreciate the unanimous recognition of novelty, highlighting the potentially significant impact on the community.

R1: Which part in MUE is more critical?
In MUE, video generation contributes more than unbiased sampling, as justified by our ablation study. Compared with the full model (73.5% MAP, 83.7% AUC), replacing videos with augmented images gives large drops (70.2% MAP, 80.7% AUC), while removing unbiased sampling mainly influences MAP (71.6% MAP and 83.2% AUC). This indicates that video semantics are vital for trustworthy diagnosis.

R1: Motion controllability and quality of generative videos.
a) Quality: Our empirical validation shows that generated videos greatly enhance diagnostic accuracy, proving the quality of precision medicine.  We also assess the generation quality with Inception Score (IS). The real data achieves IS=2.09, while generated videos give IS =2.01. The close IS between them justifies the high quality of generated videos.
b) Controllability: The motion intensity is controlled by the hyperparameter gamma, while motion types are determined by how video foundation models semantically interpret the given image. Note that our focus is to explore how generative videos improve diagnosis. Our future extension will add text guidance to further improve controllability.

R1: Whether the information gain from generated videos inherently improves performance.
Yes. Compared with the baseline (68.1% MAP, 80.4% AUC), introducing videos (w/o. unbiased sampling) gives large gains (71.6% MAP and 83.2% AUC), justifying the crucial factor of video-based information gain.

R2: The efficacy of consistency loss.
We have validated the losses in Table 2 with ablative settings of image-to-image (I2I) consistency and video-to-image (V2I) distillation losses. Consistency loss greatly improves MAR, making the model more robust by reducing missing errors.

R2: Why generate 25-frame videos.
Generating more frames needs significantly larger GPU resources, which is also challenging in natural imaging. It is a common practice to generate 25 frames, including 1 given image and 4 seconds of 6-FPS video. We will explore the generation of more frames in future.

R2: Computational efficiency.
In the test, we only use the image classification model, achieving real-time inference with 129 FPS on NVIDIA 4090. This performance level is sufficient for all clinical practices.

R3: More proper references.
We will update suggested references for more focus.

R3: The claim is too strong: Video generation does not change the semantics of given images.
We will relax it to that generated videos can maintain semantic consistency with reference images to a certain degree, since the disease area may move out of the frame in some cases.

R3: Clarify unbiased sampling.
Unbiased sampling enhances rare classes by collecting more frame images, promoting a balanced distribution for diagnostic fairness. Compared with the full model (73.5% MAP), removing unbiased sampling causes a drop (71.6% MAP), justifying its vital role.

R3: Clarify SVD usage and why dermatology videos are generated.
The used SVD is pre-trained on large-scale online videos with superior generalization capacity, spreading natural and medical domains. For dermatology images, generated videos simulate rational camera movements, e.g., translation and zoom, which are crucial for performance gains. Since CholecSeg8k is segmentation data, we will explore this fine-grained setting in the future.

R3: Minors.
We will improve the manuscripts accordingly: make Fig.2 more readable, correct confusing symbols/typos, and clarify the following details. The used SVD is pre-trained on large-scale online data. In SVD, images act as the condition to guide generation, enabling the videos to have similar semantic content. MUE generates videos and then samples images for balanced distribution. We use 5% images to generate videos as hold-out experiments, which can be further improved with larger ratios."
https://papers.miccai.org/miccai-2024/344-Paper2709.html,N/A
https://papers.miccai.org/miccai-2024/345-Paper1396.html,"Thank the reviewers (R) for their critical assessment and insightful suggestions. We want to clarify some major critiques as follows:
(R1,R3,R4) Missing comparison to existing methods:
We previously experimented with Marginal Loss, one key component of ConDistFL. However, we observed poor performances, which we did not report so far:  Marginal Loss led the network to only learn structures present at many clients (liver & spleen), highlighting a drawback of earlier partially annotated FL works, which involved datasets with a single easy-to-segment label. In contrast, we have many labels only available at certain clients. 
(R1) Highlighting Contribution:
We show that for more diverse label distributions the problem can be elegantly mitigated by using Bayesian techniques. Properly utilizing uncertainty improves performances, especially for under-represented structures with the same computational and communication load as FedAvg. The introduction is adjusted accordingly.
(R1,R3,R4) Missing related Work:
We provide a more in-depth explanation of previous work on uncertainty-aware FL (Linser et al., 2021; Boughorbel et al., 2019) and uncertainty quantification with a multi-head UNet in the centralized setting (Fuchs et al., 2022) and added standard works on uncertainty estimation in medical image segmentation (Kohl et al., 2019; Kendall et al., 2017).
(R3) Why are untrained structures visible in uncertainty?
We rewrote our paragraph in the discussion to explain why we think that the different segmentation heads learn distinct structures in their uncertainty. In the federated case, two heads with non-overlapping labels might use the same feature maps, while in the central case, the heads tend to use different ones. 
(R3) Intuition behind uncertainty reweighting unclear:
We apologize for any confusion about subtracting the uncertainty, which likely stemmed from Fig. 2a, which we have adjusted for clarity. We use the multiplication $(1-u)p_bg$ , where $u$ is the uncertainty and $p_bg$ the background logits. The idea is that $u$ represents the probability of âsomethingâ being present, making $1-u$ the probability of ânothingâ. The background channel encodes the probability of nothing and is reweighted by $1-u$, which acts as a form of quality measure. 
(R3) Evaluation Strategy:
We performed a 80-20% train-test split at each client. For having a baseline, we trained on the single clients (no FL) and in the intra-client scenario, we evaluate on the same client. In the inter-client scenario, we use the same model and test it for a specific label on the test split of all other clients where this target label is available. We adjusted the text to improve understanding. 
(R3) Calibration Metrics:
Although not explicitly stated, we did use the ESCE, which we will clarify. We calculated a per-class-averaged ESCE to address unbalanced size of labels (e.g. many background pixels) and will add this to the results:
CenAvg 22.0Â±26.1
CUNAvg 16.5Â±22.1
FedAvg 21.3Â±22.0
FUNAvg 12.4Â±10.4
(R3) MC Dropout might suffer from grid-aligned artifacts:
We use the sum of the uncertainties from all clients, which resembles an ensemble-like uncertainty prediction, improving the uncertainty estimates. 
(R1,R3) Uncertainty Formula:
We added a brief description to Eq. 1. T is the number of MC sampling steps, $\otimes$ denotes the outer product, $f$ denotes the network with parameters $\hat{\theta}_t$ in step $t$ for input $x^*$.
(R1,R3,R4) Clarity of presentation:
We added a table in the appendix for the performance of each segmentation head on each structure across all datasets and give examples in the text (R4).
We separated the results and discussion section (R1).
We removed intersecting arrows and reorganized the text for better readability in Fig. 1.
In Fig. 4b & 5, we unified the color coding so that each organ is now represented by a distinct color.
We added information in the bibliography (R3).
We rewrote ambiguous phrases and repositioned the figures (R3,R4)."
https://papers.miccai.org/miccai-2024/346-Paper1347.html,"The authors would like to thank all reviewers (R1, R4, R5) for their valuable and constructive comments. In this rebuttal, we address the key concerns raised by the reviewers.

(R1) Human assessment. Regarding our own model, the final score is 2.12 (+/-1.07), and we will include the standard deviation in the final version. However, we observed that the visualization results of the other two methods, Seg2Vid and Med-ddpm, did not perform as well, as they were not tailored for this specific task. Therefore, they may not provide a meaningful comparison for human evaluation.

(R4) Why not diffusion models. We have compared our model with a recently proposed diffusion model, Med-ddpm, and the results demonstrate the superiority of our approach as shown in Table 1. We will discuss diffusion models related to our studied problem further in the introduction of the revised paper.

(R4) Noise impact in knowledge mask. We conducted comparative experiments with different thresholds for knowledge masks. Due to space constraints, these results were not included in the paper. Currently, we have selected hyperparameters that ensure the best generation results. Our future work will further analyze the impact of noise in the mask. While we acknowledge the presence of noise, our simple method for obtaining masks has effectively covered various scenarios that contain diverse types of noise in our experiments, and the results demonstrated that introducing the proposed knowledge mask can enhance performance, consistently.

(R5) Application value and data release. We would like to emphasize that our main contribution lies in the application studies including work focusing on translation. Specifically, we introduced a novel problem of generating dynamic FFA videos from static fundus images, which has clinical significance and represents a key innovation. As there were no existing datasets or methods for this problem, we collected our own data and designed clinical knowledge masks and the entire network architecture. The datasetâs creation was not the focus of our task, and due to privacy concerns, it cannot be made public, but we promise to release our codes in our final version.
Regarding generalization ability, our modelâs adaptable structure not only addresses the novel task of FFA video generation, but also has the potential to extend to video generation for other similar modalities. We plan to explore these extensions in future work, building on the demonstrated feasibility of using GANs for image translation across different modalities, such as from fundus images to ICGA images.

(R5) Additional experiments. Regarding the suggestion to include experiments with additional hyperparameters, we have done experiments with different thresholds for the knowledge mask and different lambdas. Due to space limitations, we only presented the results of the best settings. As for comparing with more novel models, we have chosen Seg2Vid (2019) based on optical flow and Med-ddpm (2023) based on the diffusion model, both of which we consider sufficiently representative in the video generation field. We will include more comparison methods in the final version.

(R1&R4&R5) Expression issues & typos. We will correct and improve unclear or incorrect expressions in the final paper.
R1: Regarding the validation split, we used its results during training to select the best epoch, but the fair evaluation is on the test split, hence we did not present results from the validation split. For Table 1, we will add a column indicating whether ground truth was used and symbols of different loss functions. For Figures, we will either add more legends or move unnecessary title descriptions to the main text to shorten captions and make figures self-contained.
R4: For the concern on the words âextended to other imagesâ in the introduction, considering our current experiments did not cover this, we will rectify it.
R5: We will thoroughly proofread the paper and correct any typos."
https://papers.miccai.org/miccai-2024/347-Paper2376.html,"We appreciate all feedbacks. Our work was evaluated as novel (R3, R5), good experimental (R3, R5) and reproducible (R5) method. We have proposed a robust segmentation method FABR with two novelties: 1. designed transformer-like fuzzy-attention network to handle feature representationsâ uncertainty; 2. presented a novel GLCF module that decoupled organ regions as cube-trees, focused only on recycle-sampled border vulnerable points, solved discontinuous, false-negative/positive issues. Comprehensive experiment studies on four lung organ datasets proved our methodâs efficacy.

To R3

To R4

To R5"
https://papers.miccai.org/miccai-2024/348-Paper1763.html,
https://papers.miccai.org/miccai-2024/349-Paper2298.html,"We thank the reviewers for their valuable feedback.

Contributions & Analysis of Challenges (R1, R4):

Our key contribution is the geometric and depth regularizations for improving and adapting Gaussian Splatting (GS) for monocular endoscopic images (Fig.1a-b; pg. 2, para. 2). These regularizations mitigate common issues (floating artifacts, surface irregularities) in GS-based endoscopic reconstructions (sec2.3). The proposed regularizations align Gaussians with colon surface topology, enhancing geometric and anatomical accuracy (suppl. video 3:45-5:30min). Additionally, we proposed SLAM integration with GS by utilizing RNNSLAM[16] which has previously shown to be robust for monocular endoscopic pose/depth estimations. We emphasize that our method is designed to complement any robust depth estimation system[4].

Comparison with Other Methods (R3, R4):

Detailed comparison was performed with 3 SOTA methods closest to our method, at the time there were no comparable GS methods. EndoGSSLAM was unpublished at this submission. It is worth mentioning that EndoGSLAM relies on depth maps, so RNNSLAM or similar would still be required. Moreover, EndoGSSLAM doesnât perform normal regularization, so our proposed Pancaking could improve its performance.

RNNSLAM Dependence & Geometry-Pose Joint Optimization (R3,R4):

We chose RNNSLAM for its rapid and reliable depth/pose estimations, effective in handling textureless challenges compared to traditional SfM methods, which often failed to generate sparse point clouds(see Fig.1C). Details of RNNSLAMâs capabilities are discussed in [16] and referenced in our paper. Depth/pose estimation in such environments remains an open research area.

We acknowledge R3 suggestion of end-to-end GS and camera pose using photometric loss. However, given the challenges in 3D reconstruction from monocular endoscopic images, this would require making significant design choices, parameter tuning and ablation studies, shifting the focus of our method. Hence, it is not in the scope of this paper.

Normal Loss Evaluation (R3):

We used cos angle difference loss due to the effectiveness in aligning Gaussians with surface topology, yielding more realistic geometric reconstructions than isotropic gaussians. We refer to 2D GS (SIGGRAPH 2024), published after this submission, that corroborates that using a normal loss for surface alignment is an effective regularization.

Direct Comparison with RNNSLAM[16] (R4):

We verified through qualitative comparison that GS textures are more photorealistic than those generated by [16]. This outcome aligns with expectations, as [16] is not optimized for rendering (images to be added in camera-ready).

Computational Efficiency & Reproducibility (R1, R3):

Our method reduces training to ~2 min, rendering >100x faster, compared to NeRF(noted by R1). Regarding R3âs concerns, RNNSLAM produces depths/poses at 10 FPS(sec.3.2), faster than traditional SfM methods used in NeRF, which often requires hrs. The compared methods and most GS methods equally require SfM pre-processing, so typically, these preparatory steps are not reflected in performance evaluations. For fairness, we havenât included the minute additional time required by RNNSLAM. We will release our source code, including the CUDA kernel for normal vector calculation.

Evaluation Details (R3,R4):

Table 2 metrics may not fully capture artifact reduction & geometric accuracy. While metrics for GS vs GS+Pancaking seem comparable (SSIM: 0.8346 vs. 0.8340, LPIPS: 0.2047 vs. 0.2115), true improvements are evident in the suppl. video 4:43-5:40min and Table A.1. Addressing R4âs concern regarding the slight drop in PSNR, such variations are expected when geometric losses are added, shifting focus towards geometric accuracy. We ensured that eval. metrics like SSIM were used for direct comparisons where possible (code was unavailable for COLONNerf).

Minor (R3):

One dataset was real colonoscopy videos (suppl. video 4:24-4:43min)."
https://papers.miccai.org/miccai-2024/350-Paper0974.html,"Q1: AC mentioned lack of essential Implementation information.
Here are some implementation details:
1.Gaze preprocessing details: the size of the Gaussian kernel adopted to generate gaze heatmap is 119*119; After normalization, we covert the heatmap into binary mask via a threshold of 0.1; To reduce potential noise of gaze data derived from the expertâs distraction, bboxes generated from the binary gaze mask with both height and width less than 128 pixels are removed. 
2.The Magnification of the crops made in the WSI: 200
3.Eye gaze tracker details: We collect the eye-tracking data with the Tobii 4C eye-tracker that records binocular gaze data at 90 Hz.
To enhance the replication of our study, we have released source code on github as it has been accepted.

Q2:R3 asked work flow.
The data was collected by a pathologist with three years of experience, who had been briefed on our idea prior to collection. The gaze data was collected during the annotation. 
In the training phase, Gaze-DETR takes both of the image, corresponding candida bbox(es) and gaze-only bbox(es) as the input. While in the inference stage, Gaze-DETR takes only the image as the input. The information of gaze-only bboxes faced during training have been encoded into gaze queries, which can be considered a kind of learned prior knowledge in the inference stage.

Q3: R3 mentioned lack of comparisons with other detection works that incorporate gazes.
To the best of our knowledge, our work is the first to incorporate gaze information into the field of object detection. Indeed, it is possible to extend gaze-based methods from classification and segmentation tasks to object detection task as well. Nevertheless, our gaze-only box can still serve as a complementary component that can be used in conjunction with these methods.

Q4: R4 concerned source of gaze data error. 
The error due to the eye-tracking device: According to the manufacturerâs website, the Tobii 4C eye-tracker has an accuracy of 97%. However, based on practical experience, the accuracy tends to decrease further during the data collection process due to the calibration deterioration caused by the pathologistâs movements.
The error due to the gaze processing method: This primarily arises from the thresholding step during the processing of the heatmap. This can be attributed to the non-uniform distribution of the pathologistâs eye gaze points across an object. Consequently, the resulting âgaze onlyâ box obtained after thresholding may not be accurately aligned.

Q5: R4 wondered does the scanpath trajectory impact the model performance.
In our model, as all the âgaze onlyâ boxes are inputted and processed in parallel, the scanpath trajectory does not impact the model performance. Although the scanpath trajectory presents an intriguing aspect for exploration, it is not within the scope of our work.

Q6: R5 concerned the rationale behind the choice of 4 times in gaze-guided rectification.
If the multiple times is too low, the constructed âgaze onlyâ queries may only learn low-quality representations compared to other input queries, resulting in limited enhancement. While if multiple times is too high, the âgaze onlyâ queries will dominate the query set, which can adversely affect the modelâs ability to learn background classes beyond the eye gaze focus and those around ground truth, which may suffer the model performance.
During the experimentation process, we observed that selecting values of 4, 5 and 6 yielded similar optimal performance. 
Additionally, choosing 4 as the multiple number aligns with the 4-scale setting used in the denoising detection model DN-DETR and DINO, which serves as a partial motivation.

Q7: Other weaknesses 
We acknowledge the valid concerns regarding the missing model description in figure1 caption, lack of failed cases, and insufficient references. We will thoroughly revise the manuscript based on your comments before the submission of camera-ready version."
https://papers.miccai.org/miccai-2024/351-Paper1797.html,"We thank all the reviewers for their efforts and insightful comments. We will address their concerns.

Q1: Novelty and Uniqueness of Our Method (R5)
Our GD-ViG simulates the diagnostic process of doctors by constructing effective graph representations using generated gaze maps. The method spatially correlates different lesion areas, prompting the network to focus more on these disease-related regions and mitigating shortcut learning, as shown in Fig.3. In contrast, GazeGNN and EG-ViT can only utilize the ground truth of gaze map as simple auxiliary information (e.g., additional channels or feature masks), and therefore cannot intuitively reveal the modelâs decision-making process. This characteristic significantly enhances the interpretability and reliability of our method in clinical applications. We will highlight the novelty of our method in the final version.

Q2: Comparative Analysis with Gaze Data-Utilizing Methods (R3 & R4) 
TSEN[1] and M-SEN[2] utilize GAN or biCLSTM for gaze generation and detection. However, they are constrained by the locality of CNNs, making it challenging to simultaneously consider lesion areas at different spatial positions. Our method overcomes this limitation by effectively generating gaze maps (Fig.S2) and using these maps to aggregate lesion areas that are beneficial for diagnosis (Fig.3). This method aligns with the diagnostic workflow of doctors, thereby enhancing interpretability. Experimental results show that our method outperforms M-SEN on the SIIM-ACR and EGD-CXR datasets (Acc 87.20 vs. 84.80; 85.05 vs. 78.50) and the saliency model EML-Net[3] (Acc 87.20 vs. 85.20; 85.05 vs. 77.57). We have publicly released the data and code and will provide a detailed analysis of the experimental results in the final version.

Q3: Explanation for Using Graph Model Instead of ViT (R4) 
While both the graph model and ViT involve dividing images into patches for feature extraction, the graph model has the advantage of explicitly aggregating distinct spatial lesion areas that represent various disease states. ViT relies on implicit learning of these spatial relationships, potentially limiting performance enhancement and network interpretability.

Q4: Clarification of Experimental Parameters (R3 & R5) 
The graph convolutional layer aggregates information from a node and its k-nearest neighbors, to improve feature representation. In alignment with the ViG [4], we selected the hyperparameter k as 9. The balance coefficient is chosen as 1 because we consider the optimization of the generator and classifier is equally important, and the network performance is better at this time. Moreover, the method and baseline methods have p-values less than 0.05 in paired t-tests on Acc, AUC, and F1 metrics.

Q5: Further Explanation of the Figure (R3, R4 & R5) 
Fig.3 shows two graph construction methods: (a) feature distance-based and (b) gaze distance-based. Feature distance quantifies the disparity between two regions in the feature space, while gaze distance reflects the variance in doctorsâ attention to regions. In (a) and (b), red patches represent nodes unrelated to diagnosis, linked to the central node but removed in the revised graph structure after merging distances. Conversely, blue patches denote nodes related to diagnosis, connected to the central node in (a) or (b), and correctly preserved after merging. The final paper will furnish a more comprehensive explanation of Fig.3 with quantitative distance metrics. Fig.1 and Fig.2 will also be revised according to the comments.

Q6: Definition of Shortcut Learning (R3) 
Shortcut learning [5] refers to the model prioritizing learning simple but task-irrelevant content from the data, affecting generalizability and dependability. Overfitting is when a model excessively fits the training dataset, failing to generalize to new, unseen data.

[1] j.media.2020.101762
[2] 978-3-030-00928-1_98
[3] j.imavis.2020.103887
[4] 3600270.3600873
[5] s42256-020-00257-z"
https://papers.miccai.org/miccai-2024/352-Paper2778.html,"We thank the AC and Reviewers for the constructive comments. We are encouraged by positive comments: âsignificant academic value and practical potentialâ (R#1), âsignificant advance in performance/outstanding evaluation/comprehensive ablation studiesâ (R#1/4/6), and âvery good clarity and organization/strong presentation/clear logic/well-structured/innovative/novelâ (R#1/4/5/6).
We clarify all concerns as follows.

Q (R#1/4/5/6): Reproducibility.
A: We will make the code available for scientific reproducibility.

Q (R#1/4/6): Dataset usage.
A: We chose the ABIDE dataset for three main reasons: (i) it is publicly accessible, ensuring reproducibility; (ii) it has widespread usage in prior Autism diagnosis studies, ensuring data consistency; (iii) its data is from multiple diverse sites, ensuring data heterogeneity. Thus, ABIDE is suitable for evaluating the effectiveness of the proposed method on Autism diagnosis. Nevertheless, we agree with the reviewersâ suggestion, and we have explored extensive datasets in our future work, e.g., our method has a 4.14% ACC improvement over the best comparison on the HCP dataset.

Q (R#1): Model complexity and its application.
A: Our method is more feasible for practical applications, with 0.17G FLOPs and 5.42M parameters, compared to Com-BrainTF with 0.72G FLOPs and 6.15M parameters.

Q (R#1): Extensions.
A: Our future work will further analyze the modelâs generalizability, interpretability, parameter setting, and visualization. Thanks again for the valuable suggestions.

Q (R#4): Comparison with prior works.
A: Our work differs from the two works given in two key aspects:
(i) Topic: We focus on intra-class compactness and inter-class diversity representation learning to explore the intrinsic characteristics of brain data, unlike the prior work on affinity matrix self-expressiveness learning;
(ii) Methodology: We use rank constraints to enhance the networkâs discriminative embedding power and satisfy the natural geometric properties of brain data for Autism diagnosis. In contrast, the prior work conducted feature extraction based on an independent subspace assumption, which may face scalability challenges.

Q (R#4): Novelty compared with FBNETGEN.
A: FBNETGEN uses group losses to extract GNN features but ignores global constraints. Our main innovation is utilizing global rank-aware constraints, making a 10.5% ACC improvement. In addition, we are the first to impose intra-class and inter-class rank constraints into the brain-aware transformer, effectively exploring the natural geometric properties of brain data.

Q (R#4): Ablation study of rank-constraints.
A: Table 2 in the submission shows the low-rank and high-rank constraints ablation study, where our strategy of simultaneously considering the low-rank and high-rank constraints can make a 2.3% ACC improvement.

Q (R#5): Mechanism explanation of our method.
A: Our geometry-oriented supervised loss can capture individuals within the same disorder group by differentiating the inherent geometric properties of brain data, achieving discriminative representation learning.

Q (R#5): Paper organization.
A: We will shorten the introduction in Sec. 2.2 and show the experimental analyses in a paragraph manner.

Q (R#5): SVD improves sensitivity.
A: SVD improvesÂ sensitivity by focusing on important features and regularizing the model.

Q (R#6): Overfitting.
A: To avoid overfitting, we used the rank k approximation with SVD (in Sec. 2.2), the dropout regularization technique with randomly dropping out neurons, and the pooling strategy with aggregating local information. We will add the details and revise Fig. 2 in our revision.

Q (R#6): t-test statistic.
A: Our method achieved statistically significant results compared to Com-BrainTF (P value=0.009944 on specificity) and BrainNetTF (P value=0.034862 on ACC). Moreover, our method (76.9Â±3.8%) had a higher average and a lower std on the specificity metric than the best comparison (65.7Â±6.4%)."
https://papers.miccai.org/miccai-2024/353-Paper2633.html,"Thank you for the feedback. We will address the questions about the counterfactual explanation architecture, Atlas-aware, comparison methods, and clarity (Figure 2 and results) problems in the following explanations. âRiQjâ means the answer to the i-th reviewerâs j-th question.

counterfactual explanation

R5Q2 and R6Q1, Q2: In existing counterfactual learning works in MRI, such as reference [7-8], they generate MRI directly, overlooking the high structural characteristics of FC, which necessitate specific design considerations. In counterfactual learning of FC, the challenge lies in generating the target label FC while maintaining strong intra-network correlations and weaker inter-network correlations. To address this challenge, we propose GCAN and AABT, which encode and reconstruct FC at atlas-network level while existing FC reconstruction works often ignore network-level generation, leading a poor performance.

R3Q1 and Q5: The model generates the target label FC, which is fundamental to the counterfactual explanation study. In this paper, the negative and positive classes can be seen as source and target labels in Figure 4. The corresponding counterfactual attention maps are generated by subtraction operation between FCs. We believe the results in Figure 4 and Table 2 can support our counterfactual study.

R3Q2: The reason why we use the average map is to supplement the disease information at the group analysis level rather than the individual level. We argue this strategy can provide more general information and can be adapted for large data analyses.

R4Q2: The pre-trained classifier provides additional information in predicting regions but will not lead to information leakage in diagnosing. Our model is trained on 1194 FC matrices when predicting AD-related regions, which is substantial given access limitations in fMRI. Inspired by pretrain strategy, we predict the regions in large datasets and transfer them to specific datasets for diagnosis.

Atlas-ware

R3Q3 and R4Q1: The reason we term it âatlas-awareâ is that the hyperparameters of patch embedding and inverse embedding are determined by the atlas network. This approach effectively tackles challenges in FC counterfactual learning. When applied to a new atlas, the hyperparameters need adjustment. This stands in stark contrast to multi-atlas representation learning and community-aware methods.

Comparison

R3Q4 and R5Q1: Selection of comparison methods is guided by the prevailing SOTA approaches. Due to limitations in accessing the source code, we adopt the primary architectures of these methods and evaluate them on our dataset. Existing SOTA methods for FC classification can generally be categorized into Convolution-based (doi: 10.3389/fnins.2020.00881; 10.1002/hbm.25529) and Transformer-based methods (doi: 10.1002/hbm.26542; 10.1109/JBHI.2024.3355020). Among the Convolution family, ResNet demonstrates superior performance owing to its residual architecture. Thus, we opt for ResNet as comparison method. Transformer-based methods typically leverage direct self-attention architectures. Hence, we choose transformers with varying numbers of heads for further comparison.

Clarity (Figure 2 and results)
R4Q3: In the BOLD response, MCI and SCD donât have a hierarchical relationship. So, itâs not surprising that HC vs. MCI performance is weaker compared to HC vs. SCD. While cognitive scores may show hierarchical relationship, BOLD response amplitudes differ in regions, rather than changes in the same regions. Similar result has been reported (doi: 10.1016/j.media.2021.102248)
R4Q4 and Q5: The output feature map aligns in size with the FC matrix, enabling direct addition into FC. The details of generating feature map can be seen in Figure 3. The L_d^c is the loss of image discriminator loss of C_g^s. In the latest version, it will be added nearby image discriminator in Figure 2."
https://papers.miccai.org/miccai-2024/354-Paper1951.html,"We appreciate valuable comments from reviewers and will consider them in the final manuscript.

R5Q1: Clarity of method and motivation
A1: Our GEM network aims to understand radiologistsâ attention allocation and visual search behavior patterns during image interpretation, providing interpretability and insights into diagnosis. Since eye gaze can reveal the relation between reports and CXR images, we introduce a context-aware module to generate a correlation map that highlights the most related regions of CXR image about report. To simulate radiologistsâ visual search behaviors and decision-making processes during image interpretation, we devise a VBGC to capture the eye gaze patterns as graphs, and a VBMatch to preserve real eye gaze patterns via graph matching. We will make the method and motivations clear in revised manuscript.

R5Q2Q3,R6Q1,R7Q2: t-test for Table 1,2
A2: We previously computed p-values for the results in Tables 1 and 2. The p-values across all metrics are less than 0.05, suggesting the statistical robustness of our GEM over other methods. These results will be included in Tables 1 and 2 in our revised manuscript.

R5Q4: Ablation study only w/ VBMatch
A3: GEM w/o context-aware module but w/ VBMatch improved baseline by 5.2 PCK@0.3 score, indicating its effectiveness.

R6Q2: Experiments on CXR datasets
A4: Since OpenI, MS-CXR and AIforCovid datasets do not include eye gaze data, we only perform quantitative experiments on the MIMIC-Eye dataset that includes eye gaze data. To qualitatively evaluate other datasets, we selected five CXR images from each and asked a radiologist to provide gaze points per image. Quantitative experiments were not performed on these datasets due to limited labeled data. We will include this experimental setting in our revised manuscript.

R6Q3: Weight of MSE and CE loss
A5: The hyperparameters for alpha and beta were determined through grid search, with the results indicating that optimal values are 1 and 0.1.

R7Q1: Compared methods for gaze estimation
A6: Our previous literature review found that no existing gaze estimation frameworks could predict gaze points on images with given texts. Existing methods either predict gaze solely from input images or use both images and head pose data, but do not incorporate text information. Our previous experiments showed that our method outperforms these approaches across all metrics, indicating its superiority in gaze estimation. We will update these results in Table 1 in revised manuscript.

R7Q3: Literature review of gaze estimation in medical imaging
A7: In medical imaging, gaze estimation is divided into two groups: 1) using the medical image as input, and 2) using eye region images for gaze prediction. Gaze is used to guide different medical image diagnosis tasks. We will include this literature review in the revised manuscript.

R7Q4: About graph construction
A8: 1) Yes 2) The edges in a graph can effectively represent high-order relationships between nodes. The arrangement of visual search patterns in gaze points is indicative of the relationships between these points, making graphs particularly suitable for this task compared to other feature alignment methods that may be inadequate substitutes.

R7Q5: Capability of modeling pathology and other region pattern.
A9: Our model can capture this pattern. We will add a new figure in the revised manuscript to illustrate the sparsity of eye gaze for pathology and other regions.

R7Q6: Advantage of gaze points
A10: Gaze points provide two main advantages. 1)Its distribution and density can reveal the radiologistâs attention patterns, areas of interest, and the relative importance of different regions within the image. 2)The sequence of gaze points over time provides insights into the radiologistâs scan path and the order in which different regions were analyzed. As these advantages cannot be obtained from other sources, gaze points are valuable for analyzing radiologistsâ diagnostic processes."
https://papers.miccai.org/miccai-2024/355-Paper1834.html,"We sincerely appreciate all comments from the three reviewers: [#1], [#3] and [#4]. While we acknowledge their constructive feedback, we would like to clarify the following points:

To [#1]: 
One of our main contributions is the parameter-efficiency framework RED for robust VF estimation from fundus photos. We introduce these equations for the optimization of RED, which provides a theoretical guarantee to optimize RED in an unsupervised manner.

Besides, we include the detailed decomposition of e in Eq.(11) to depict the composition of e for better understanding.  And we will omit it in Eq.(11) and keep the description of its decomposition below Eq.(11).

As for the noise modeling in our problem, to the best of our knowledge, existing domain adaptation methods usually model the domain shift/gap based on the Gaussian model, such as the referenced paper [15], where they model the encoded features by the Gaussian model with potential uncertainties, i.e., additive white Gaussian noise  (AWGN), which is illustrated in Eq.(5) and Eq.(6) in their paper. Inspired by them, we model the domain gap between the natural image and the fundus photo domain following it; here, the domain gap exists on the extracted features from the fundus photo using natural image pre-trained models. And we assume the extracted features contain AWGN. And we will clarify the above modeling more accurately. It is worth mentioning that there is no perfect model to handle the domain gap. Our experimental results in Table 2 and Table 3, as well as the Ablation Study, demonstrate this modeling and, with the proposed RED, can appropriately address the above domain gap, as illustrated by the improved performance on both internal and external validation data.

Besides, in the Ablation Study of âEffectiveness of REDâ, we aim to examine the effectiveness of the proposed RED; therefore, we compare it with alternative denoising methods,  and due to the lack of ground truth, we choose the mean and median kernels/filters as the baselines. The experimental results are reported in Table 3. And we will clarify them and include an additional description for better readability.

To [#3]:
This study aims to estimate vision loss from fundus photographs, driven by the hypothesis of âstructure-functionâ relationship, where we believe that structural changes captured by fundus photographs are associated with the visual loss detected by VF test. The ICL has become an important technique used for myopia correction. Some studies [1] have reported that ICL implantation for highly myopic eyes led to significant changes in retinal structure, such as retinal thickness and vessel density, which could be captured by fundus photography. Therefore, our proposed method could be applied to evaluate visual improvement in highly myopic eyes after ICL implantation.

[1] Xu Y, et al. Analysis of Microcirculation Changes in the Macular Area and Para-Optic Disk Region After Implantable Collamer Lens Implantation in Patients With High Myopia. Front Neurosci. 2022 May 19;16:867463. doi: 10.3389/fnins.2022.867463.

Besides, our method is parameter efficient, which introduces a shallow ReLU-based MLP with only one hidden layer. And we calculate the executing time (on a single NVIDIA V100) and floating-point operations per second (FLOPs), which is illustrated as follows:
Regression: 5.65x10^(-5) s / 26624.00 FLOPs
Regression + RED: 8.98x10^(-5) s / 288768.00 FLOPs
Therefore, the inference efficiency is only slightly reduced.

To [#4]:
We fully agree that validating our method on public datasets is important. However, to the best of our knowledge, no existing public dataset of high myopia with pairs of point-wise VF and fundus photographs is available. Therefore, we have collected real-world data from two clinical centers for external validation in this study. We will also publicize our code for better reproducibility."
https://papers.miccai.org/miccai-2024/356-Paper0781.html,"We appreciate the positive feedback and suggestions from both the reviewers on our work, and we will consider their suggestions to further improve the quality of this work in either the final version or future work."
https://papers.miccai.org/miccai-2024/357-Paper0411.html,"We appreciate the high-quality reviews from the reviewers (R1, R3, R4, R5) and their acknowledgment of our strengths in multi-center evaluation (R1, R3, R5), good results (R3, R4), method novelty (R3, R5), writing (R1) and task challenges (R4). We address their key concerns as follows.

R1, R4, R5: Statistical Analysis.
Due to the new rule in this yearâs Rebuttal Guide, we can only provide further details on the results already in the paper. All results in the paper were averaged over 3 experiments. Regarding standard deviation (R1, R5), the std for DSC in Tab 1 and 2 ranges from 0.003 to 0.011, and for CC, the std ranges from 0 to 0.14. For the t-test (R5), we calculated that the unified deformation is significantly better than the separate one (p=0.0127), and implicit registration is also significantly effective (p=0.0162).

R1, R3: Empirical Rules for Repairing Labels  / Reproducibility.
We will add details of these empirical rules in the final version, while we emphasize that they are used only to generate the WHS3D shape ground truth to ensure topological correctness. These rules are not used in other parts of the method. Given that WHS3D data will be open-sourced, we believe this will not affect the reproducibility.The rules include several morphological operations to ensure that the 8 classes each have only 1 connected component (CC). If not, we retain the largest CC after a closing operation. Additionally, we ensure connectivity between classes: ventricle vs atrium, aorta vs left ventricle, and right ventricle vs pulmonary artery. Since these labels come from real data, disconnected cases are rare (usually due to annotation noise), and we slightly dilate to ensure proper connection.

R3: Small Data Size.Our goal is to establish a latent variable model with rich cardiac structures, and there is very little fully annotated data publicly available. This is a limitation. To overcome it, we are extending our approach by using partial-label learning on more heterogeneous datasets.

R4: Patient Details.The source data for WHS3D comes from previous studies [12, 18, 21], primarily past MICCAI challenges. Unfortunately, these papers also do not disclose details about the patient cohorts, and we only know that there are no large anatomical variations among them. We will mention this as a limitation.

R4: Writing.
We will âshorten the introduction and preliminariesâ to allow more space for discussing âdatasets, future work, and limitationsâ. Additionally, we will add references for âcan be realized in various mannersâ [19, 25, 28] and âcan be learned through various methodsâ [15, 16].

R4: Why Scaling Was Excluded in Implicit Registration.The matrix R in A=(R,b) is a freely optimized 3x3 matrix, and not a rotation matrix as erroneously stated. Our apologies for the typo; it will be fixed. This means our method actually includes scaling and shearing.

R5: More Metrics. 
Due to the Rebuttal Guide, we cannot provide these metrics in the rebuttal. However, we will include CNR in Fig 3 to highlight the dataset differences, and also ASSD from [6] in Tab 1 and 2 to better differentiate topological assessments.

R5: Manual Annotations on OOD Samples.We think there is a misunderstanding. In the OOD in-house dataset, we do not use manual annotations for training; instead, it operates solely âby correcting the predicted onesâ.

R5: Cardiac Dynamics.
We completely agree that the deformed template is well-suited for dynamic cardiac studies. We are considering exploring this direction.

Other Issues:A. R1: Nerfies. We will discuss the relationship between Nerfies and Implicit Registration.
B. R4: Training Performance. Considering space limitations and the limited significance of training results, we cannot guarantee reporting training performance in the final version.
C. R5: References. We will order the references as they appear.
D. R1, R4, R5: Typos / Wording. We will fix these issues."
https://papers.miccai.org/miccai-2024/358-Paper2051.html,"We sincerely appreciate all reviewers for their dedicated comments. We are encouraged by the consensus on novelty (R1-5), paper quality (R1-5) and adequate evaluation (R1, R5). We are revising the manuscript accordingly and this rebuttal focuses on clarifying certain details.

To Reviewer #1
We deeply thank you for your recognition and constructive comments and will revise our manuscript accordingly.
â
To Reviewer #3
Thanks a lot for your recognition of our method and constructive comments on our manuscript.
R3.1 The constructive comment on clinical relevance is well taken. This paper aims to improve the downstream performance with effective sample generation, specifically, by generating the progressing samples to illustrate the transition progress of pathological stages. According to senior pathologists, taking ROSE for example, the positive samples are mutated from the negative ones, with positive features gradually appearing in the transition progress. Therefore, we take the information changes as measurements (0.5 for likely 50% of supports in positive identification) for cells that undergo changes in nucleus-cytoplasm ratio or morphology. We agree the importance on evaluation with actual progressing samples. Accordingly, we had put some ROSE samples which further confirmed the clinical relevance from the generated images matching the spatio-temporal evolutionary properties. We will put more details accordingly in the revision.
R3.2 We apologize for the confusing typo in âThe numerical performance in Table 2 confirms the effectiveness of HAS (ADD vs U-BDP)â which should be âThe numerical performance in Table 1 confirms the effectiveness of HAS (ADD vs ADD(no-HAS))â. The ADD with HAS performs better and is applied in all other experiments. We will revise the typo and put more details.
R3.3 We appreciate the comment on the dataset scale. Yes, larger dataset is helpful to evaluate the further value of this work. Still, the ADD is proposed to enhance DL methods in most pathological scenarios with sampling difficulty. Regarding the ablation suggestion, we need to point out that the suggested multiple sample size generation experiments had been put in the original supplementary material. We can improve this point and put in the body according to this valuable comment.
R3.4 We appreciate the comment on evaluating the method on semantic space with UMAP. We have referred to the provided article and performed the UMAP for comparison. In UMAP analysis, ADD samples keeps better consistence with real data in distribution space. We will improve the measuring illustration with more details. However, LPIPS is used in our manuscript as it is most generally applied in few-shot generation tasks [15,22,23]. Low-level and repetitive information such as texture, shape, and structure can be comprehensively captured in feature spaces to measure diversity.

To Reviewer #5
We deeply appreciate your recognition and are improving the method writing.
Our main innovation roots in the depth-control strategy and the intention to generate the progressive samples. Still, the proposed HAS is designed with improvement on the DDIB [18] backbone. Two attention-based U-nets are applied corresponding to the diffusion and denoising process, and we improve their attention modules with global and local priorities.
Specifically, in each block of [18], the features are processed by 2 CNN (C) and 1 attention block (A), following the order of CCA or ACC. The A module does not change the shape of feature X while its MLP and attention calculation prioritize the feature. In the diffusion process, we first perform MLP (with global view) to get the corresponding Q, K, V and then split them into multi heads for attention. On the contrary, in the denoising process, the features are split into multiple local heads and then the Q, K, V and attention are calculated with local MLP. We will revise and offer more details in the revision."
https://papers.miccai.org/miccai-2024/359-Paper1063.html,"Thank all the reviewers for your constructive comments.
R1:
Q1. Gene imputation methods
Thank you for pointing out these methods. However, our model is significantly different from the named methods. We would like to further clarify our novelty:
a)Task and model design: our model focus on TME prediction, a complex task with clinical significance, which heavily relies on gene guidance. In comparison, the named methods aim to predict genes from images, leading to distinct model designs by nature: our framework is end-to-end, while these methods have to impute genes before prediction.
b)Performance: the named methods perform well on several genes but are less effective overall, with mean correlations between predicted genes and ground truth mostly below 0.3(Gao et al., 2024, 101536 Table1). TME prediction needs hundreds of genes(we used 1810 knowledge genes). Inaccuracies in individual genes accumulate, resulting in poor overall prediction accuracy.
c)Training strategy: our model employs gene information at training stage to guide TME prediction, distilling the knowledge from genes to TME classification network trained with TME labels. The named methods use gene expression as labels directly. However, gene expression may contain biological and technical biases, especially in ST genes(ST-Net, BLEEP), limiting model training.
Q2. Feature extractor
We chose HIPT for its pyramid structure, which can extract the larger organizational features needed for TME. Our method is agnostic to SSL feature extractors. Here we use HIPT as an example. cTransPath and RetCCL can also work within our framework. We will make detailed comparison in the future.
Q3. Reproducibility
Tile size is 4096px with 20x magnification. Dynamic weight Î» is the same as lambda_p. We used 5-fold cross-validation on training set and calculated all metrics on test set. We will add a dataset partition flowchart in final version. We agree tests on more datasets can further bolster robustness. We will include this in future version.
R3:
Q1. Additional comparisons
Thank you for the suggestions. We can compare our method with gene imputation methods mentioned in R1Q1.
Q2. Large variance
We think Sample size(N) may cause variance. Correlations of N and std are below -0.6, confirming our assumption. Some samples not fitting neatly into TME classes may also be a reason. We will do further analysis.
Q3. Dataset partition
Yes, for each tumor type, there is an 85%-15% data split. We explained the data split in detail in R1Q3.
R4:
Q1. Gene and HIPT features baselines
Indeed, based on our previous tests, using gene alone predicted well(0.93 AUC). kNN on HIPT features directly achieved mean AUC of 0.60(pancancer) and 0.62(BRCA).The difference between the results in HIPT paper and ours is due to the different tasks. The former is a simpler binary classification task with clear histological features due to tumor origins, hence better results. 
Q2. Representation alignment & Gene encoders
Thank you for your constructive suggestions. Our contribution is mainly about proposing a framework that uses genes to guide the model. Multiple density alignment methods can also work within our framework. We chose cosine similarity as an example for its simplicity and speed. We are already exploring these methods and other gene encoders. We will add results in futher versions.
Q3. Stop gradient
In our experiments, we found not freezing the model made it hard to converge. For classification, learning a compact and discriminative latent space is a key. Therefore, aligning WSI image features to compact gene features helps improve classification efficiency. More variability leads to ambiguous decision boundaries.
Q4. ABMIL+Siamese
The structures of ABMIL+Siamese and â+Siameseâ are the same. But the former was trained separately on each tumor type, and the latter was trained on pancancer dataset once, which explains the difference in ROC scores."
https://papers.miccai.org/miccai-2024/360-Paper2943.html,"We would like to thank all the reviewers for their constructive comments and suggestions. We have grouped our responses by topic.

Novelty/Contribution: 
Response to R1
(a) The 3 suggested papers relevant to the background of this study will be added to the introduction.
(b) We will clarify in the paper that the individual models included in the ensemble DE are independent MVE models, which is further described in Section 2, Page 4. 
(c) As one of the objectives of our work is to be operational in resource-constrained settings, we focus on the parameter efficiency of QAERTS. We did find that adding QAERTS components to DE does slightly improve the performance, however not substantial (less than 3% improvement across any metric). We will add this set of experiments to the Supplementary file. With DE, the explicit ensembling already contributes to this, so the implicit ensembling through QAERTS is more beneficial for non-ensembled models. We have included this explanation in Section 4, Page 9, under the sub-heading Loss Landscape.
(d) The modifications to the backbone architecture/feature extractor presented in [26], activation function used and adaptations to the training process are now expanded in Figure 1 caption and where applicable in Section 2, specifically under the sub-headings: Parameterizing Rotational Representations, Uncertainty-Aware Learning and Modifying loss. Configurations regarding data preprocessing and augmentation to improve replicability have been added to Section 3, Page 5. We now emphasize the three main adaptations to the baseline proposed as: a) enabling the model to regress both a mean and standard deviation, b) expanding a single output head into multiple with each regressing means and standard deviations of different geometric transformations, and c) changing the loss function from mean squared error to a negative gaussian likelihood loss during training. The Supplementary file further reports ablations performed to validate the benefit of each adaption.

Clinical Applicability
Response to R4
(a) Sonographer use:
a. Our focus is on assisting novice sonographers and enabling scanning guidance with point-of-care probes in resource-limited settings. This is achieved by improving the 3D localization performance as it allows for better identification of standard planes and interpretation of anatomical landmarks in the fetal brain. 
(b) Parameter efficiency:
a. The proposed approach only requires ï¾41k more parameters than the baseline model to attain a considerable improvement in performance without a significant overhead in computational cost compared to DE, which requires ï¾100M more. Hence, in our specific scenario, we believe that large difference in parameters does affect both the latency and memory.
(c) Table 1 result:
a. The std is likely high due to the large range of potential values for both the 3D coordinates (reflects in ED, PA and MSE), and the non-normalized pixel values in the sampled 2D US frames (reflects in NCC and SSIM). In small structures such as the fetal brain, minor misalignments between ground truth and predicted values can cause exaggerated differences with some locations/frames. 
(d) Data augmentation:
a. Random scaling was performed on a sample-by-sample basis. For instance, the images were sampled from a 3D location first, then scaled, allowing for robustness to invariance caused during the randomness of US acquisition.

Improvements and Future Work
Response to R5
(a) Two potential avenues to improve our proposed work have been incorporated into the Discussion under the sub-heading Future Work.
(b) Yes, we are referring to the role of expertise level in locating standard planes, and this clarification has been added to the manuscript as well. 
(c) The manuscript has been proof-read and corrected for typos and grammatical errors thoroughly as suggested."
https://papers.miccai.org/miccai-2024/361-Paper3387.html,"First, we sincerely thank the reviewers for thorough reviews and for providing us with insightful and valuable feedback. The rebuttal focus on addressing the major concerns raised, and all requested changes will be included in the camera-ready version of the manuscript upon acceptance.

âLack of technical innovation might limit the perceived novelty of the research.â As stated by one of the reviewers, the use of U-Nets for segmentation is not novel for pre-operative glioblastoma segmentation, but there are still very few works employing these models on early post-operative segmentation. The segmentation of early post-operative images represent distinctive challenges such as small and fragmented tumors, and the aim of the study was to investigate specific sampling strategies and architectural modifications to improve the detection of these. However, none of the experiments lead to significant improvements in the scores. Therefore, the novelty lies in the effort to shed light on these challenges and the importance of the quality of the ground truth references, through a thorough comparison with human expert raters. As the focus of the research community is shifting from pre-operative to early post-operative segmentation, and a post-operative segmentation task is part of the BraTS challenge for the first time this year, the results of the study and in particular the comparison with a human baseline should be of interest to the research community.

âQuestionable quality of âground truthâ and consensus agreement annotations.â First, all annotations labelled as âground truthâ were annotated by single experts, independent of the inter-rater annotations. In the consensus agreement annotations, a voxel was only labelled as positive if annotated by at least half of the annotators, which in theory should make them more robust than the ground truth annotations. The motivation behind the inter-rater analysis was to contrast the model performance against human rater performance using a reference completely independent of both, which is why the ground truth annotations were used as a reference in Table 2 in favor of the expert consensus annotation.  Investigating  the quality of the ground truth annotations by comparison with the consensus agreement annotations was also an important motivation for this analysis, illustrating the difficulty of the task by the disagreement between human raters.

âSource code and data are not provided, limiting the reproducibility of the study.â The source code for the data processing, model implementation and validation pipeline as well as the weights for the best performing model are openly available on GitHub, and will be referenced in the camera-ready manuscript. The dataset cannot be shared openly due to patient privacy, but access can be granted through collaborative projects.

âLacking justification for selection of experiments, architectures, and loss functions.â The motivation for the experiments on sampling strategies for handling class imbalance, and different network depths and kernel sizes to avoid suppressing the thin and fragmented residual tumor lesions are thoroughly explained in the methods section 2.2 of the paper. Regarding the choice of architecture, it was shown in a previous study that the nnU-Net tends to produce more false positives, achieving a better voxel-wise segmentation performance on the cost of a poor patient-wise classification performance. The AGU-Net achieved similar segmentation performance with a reasonable classification performance. The attention component of the AGU-Net architecture was also deemed important to help the network locate the area of the residual tumor, which is why this was the architecture of choice for this study. Initial experiments with different loss functions were conducted but did not lead to any improvement, and further experiments were not prioritized due to  limited time but is part of planned future work."
https://papers.miccai.org/miccai-2024/362-Paper1178.html,"We thank the reviewers for their time and positive feedback on our paperâs technical novelty and effectiveness (e.g., âsignificant methodological contributionsâ by R1, âFirst steps towards a training loss based on GMMsâ by R3, ânovel in methodologyâ by R4). We appreciate the detailed comments and suggestions for improvement. Below, we address the concerns raised by the reviewers:

Q1: Clarify the objective is within-subject registration (R1, R3).
A1: Thanks for the constructive comments! This paperâs multimodal registration aims to align N modality images of the same subject to one modality (reference image T1) simultaneously. For each subject, N-1 spatial transformations need to be computed excluding T1. We will clarify our method as âwithin-subject multimodality registrationâ in the revised version.

Q2: Validation was mostly on simulated datasets (R3, R4).
A2: Simulated datasets are crucial for directly validating our method. In clinical datasets, obtaining ground truth labels for each imaging modality is very challenging, especially when specific structures (such as plaques) are not visible in some modalities. In simulated datasets, before applying nonrigid perturbations, all modalities were initially aligned and shared the same ground truth labels, enabling direct performance evaluation.
However, in real-world data, e.g., the carotid dataset, only T1 has corresponding manual labels. Thus, we must indirectly evaluate the registration performance by calculating the compositional segmentation DSC.

Q3: Why not done using just a rigid registration (R1, R3)?
A3: Rigid transformation cannot align the subtle deformations of vessel wall and plaques caused by blood pulsation. Therefore, we applied nonrigid deformation perturbations to the simulated dataset and employed nonrigid registration in both simulated and real-world datasets.

Q4: Ants and VoxelMorph (VM) exhibit poor performance in the visualization results (R1).
A4: In the clinical carotid dataset, Ants and VM with MI calculated similarity in both background and foreground regions, which may affect the registration performance in the foreground ROI regions. Our method proposed a GMM to model the intensity profile of the vessel wall and plaques, enabling to implicitly focus on the ROI regions and reduce the interference from the background without the need of ROI extraction.

Q5: The setting of the mixing proportions and the optimization of latent variables (R3).
A5: As pointed out by R3, when there are fewer than K anatomical structures present in the subject, we set the mixing proportion for the missing structures to zero, and the algorithm still operates effectively.
The GMM parameters were not fixed. As spatial transformations iteratively update, we recalculate the parameters in Equation (4) every 10 training epochs, ensuring the compositional intensity information represented by GMM keeps improving.

Q6: Label loss in Eq. (8) may not be differentiable to spatial transformations (R4).
A6: Label loss in Eq.(8) is designed to encourage smoothness in the predicted labels, ensuring that pixels in the neighborhood are more likely to share the same label. The predicted labels were derived from the maximum predicted probabilities of GMM, thus the gradients can be backpropagated through the Gaussian functions, allowing to update the spatial transformation parameters.

Q7: Lack of comparison with some conventional methods (R1, R4).
A7: Our paper focuses on within-subject groupwise registration and offers a thorough comparison with both traditional and deep learning groupwise registration methods. The two MICCAI papers noted by R4 addressed joint segmentation and registration tasks, making direct comparison less suitable. Nonetheless, weâll consider adding comparisons in the revised version if space allows.

Q8: Notations redundancy and typos (R1, R3).
A8: Thanks for pointing these out! In the revised version, we will fix typos and refine notations for clarity and brevity."
https://papers.miccai.org/miccai-2024/363-Paper1733.html,"Reviewer #1 
We would like to thank the reviewers for their favorable comments on our work.
Questions 3 
(1) The reviewer mentioned that we used natural language processing evaluation metrics. Using metrics to measure clinical efficacy would make our method more convincing. We will consider incorporating new evaluation metrics in our future work. 
(2) The reviewer pointed out that the methods mentioned in the paper are mostly from public databases or conference proceedings (even though they are top conferences), and papers in some journals can be considered. We have added references to journal papers, which has further enhanced the quality and credibility of our paper.
(3) About why the MTD module works: The MTD module is used to assist the GTC module in jointly assessing the severity. When conducting disease classification tasks, using one-hot labels can only represent the two extreme cases of the presence and absence of the disease corresponding to the radiographic. For example, two radiographsâone showing early-stage pneumothorax and the other showing late-stage pneumothorax may exhibit varying severities in the image. Using one-hot labels would classify both as simply having pneumothorax, thereby losing valuable information. Therefore, we use the MTD module to construct a non-one-hot probability distribution as pseudo label to capture and focus on these differences. The pseudo label is composed of a weighted sum of the true labels and the predictions from the MTD module (see Eqn. 6), The more severe the disease presented in the radiograph, the higher the confidence level of the MTD module for that disease. This is the effect achieved by model learning through pseudo-label constraints. Since the model cannot accurately determine the presence of a disease in the early stages of learning, using only the MTD moduleâs predictions as pseudo labels may lead the model to learn too much incorrect information, Therefore, we integrate the ground truth labels, enabling the model to learn the severity of each disease while ensuring accurate disease classification.

Reviewer #3
Thanks to the reviewers for their good recognition of our work.
Questions 3
(1)
(a) The reviewer pointed out that we did not repeat multiple experiments to obtain average values. We mainly did this to maintain consistency with the comparison papers;
(b) The original TopDown paper did not present results on the dataset I used. I reproduced the results from reference [13], which only provided TopDown results on the MIMIC-CXR dataset. Therefore, I only compared the paper on the MIMIC-CXR dataset;
(c) The reviewer mentioned that our method did not surpass the comparison model in some metrics on the IU-Xray data set. The main reason is: the IU-Xray dataset is relatively small and designed for specific tasks, differing in data distribution and task complexity compared to the larger, more comprehensive MIMIC-CXR dataset. We believe that achieving good results on the MIMIC dataset is more representative.
(2)(3) The automated construction of topic graphs and the addition of manual evaluation, as mentioned by the reviewer, are valuable and meaningful research points. We will strive to further investigate these aspects in our future work. 
(4) Fixed the writing problem.

Reviewer #4
We would like to thank the reviewers for their positive comments on our work.
Questions 3 
(1) The discussion on why the MTD module is effective can be found in Reviewer #1, point (3).
(2) Although there is only a slight improvement in the BLEU-1 metric, the improvement in the BLEU-4 metric is very significant. This is because BLEU-1 mainly focuses on the generation effectiveness of individual words, while BLEU-4 considers the matching of four-grams, since diagnostic reports are generally longer than natural text, the BLEU4 metric is more informative as it more comprehensively demonstrates the improvement in the generation capabilities of our proposed method."
https://papers.miccai.org/miccai-2024/364-Paper1889.html,"We would like to thank the reviewers for their comments and constructive feedback. All the reviewers have acknowledged the novelty of this work for ultrasound acquisition guidance. For brevity, we have grouped reviewersâ questions and provided a response. We will subsequently update the manuscript by including these clarifications.

Dataset and simulation:

We recognise that the lack of dynamic information such as cardiac motion is a limitation of this study, and the ultrasound image quality can be further improved by making it appear more realistic. We indeed plan to address these topics in our future work. In this work, we emphasized the simulatorâs correctness and consistency w.r.t anatomical structures and studied whether an agent can be trained to learn this context and navigate to desired targets. To facilitate this, all segmentations were obtained using an algorithm, which is referenced in the supplementary material.

We agree with the reviewers that this framework could be extended to work with 3D volumes as well as handle other ultrasound modalities such as TTE or ICE.

Evaluation:

When testing the model, the initial poses are randomly sampled, hence they are different for every method. For every view in each patient dataset, we sample 5 initial poses. While a high pose error indicates that the desired view is not reached, a lower error indeed does not imply clinical usability. Further analysis is required to correlate the error values to clinically acceptable ranges. We believe that our simulation pipeline can be used for this purpose.

Regarding the number of data augmentations K, weâve added results for K = 1 and K = 4 in the supplementary material. Our choice of K=2 yields better results, showing that the regularisation induced by the data augmentation loss is beneficial. At higher values of K, the performance gain saturates as the variability in the distribution caused by the additional random shifts is limited.

Applications of the proposed method: Several works [2, 3] have demonstrated the benefits of using AI to guide ultrasound image acquisition in reducing the variability among users and helping novices acquire diagnostic-quality images. In the manuscript, we used the term guidance to refer to this process, rather than interventional guidance, which is typically coupled with fluoroscopy. We will update the manuscript to clarify this."
https://papers.miccai.org/miccai-2024/365-Paper3737.html,"We thank all three reviewers for their constructive comments and appreciations for our strengths. Our point-by-point rebuttals are as followsâ 
Rebuttals to Reviewer #1:"
https://papers.miccai.org/miccai-2024/366-Paper2286.html,"We appreciate positive comments from reviewers: ânovel groupwise registration network with an implicit template for motion separation, a tensor-embedded module for contrast separation, and a refined loss for noisy frames (R3, R4, R5)â and ânovel physics-based and clinical metrics were proposed (R3, R5)â. To address other comments:

Not using DSC as evaluation metrics/Semi-supervised version fails(R3, R5)
We didnât use DSC or similar metrics because DTCMR is not a proper modality for anatomical structure delineation and unreliableground truthsegmentation could be produced due to low SNR and unclear boundaries. Moreover, manual delineations of 60+ frames per case were impractical. Although we used automatic segmentation (10.1002/mrm.28294) with denoising, inaccurate delineations were produced due to subtle deformations. Thus, segmentation-based metrics were not used, and we had also shown that incorporating DSC loss could not improve performance (see S4.b in ablation study).

Limitation/Lack of details of metrics(R3, R5) 
The proposed work was a proof-of-concept study on healthy cohorts. Due to page limits, detailed metric calculations were omitted, but the HAG limitation will be added in the final version.

Usefulness of proposed method(R3)
Despite small differences between Rigid and proposed method, deformable motion occurs during the cardiac cycle (10.1002/mrm.28294), and heart margins move (10.1148/radiology.209.2.9807578), making deformable registration essential.

Impact on SNR(R3)
We didnât measure SNR directly and further studies can quantify SNRâs impact on registration accuracy. In literatures (P2), structural MRI (e.g., T1 mapping in ref [7]) generally has higher SNR. Hence, we claimed that working with low SNR DTCMR is more challenging. Due to page limits, we conducted ablation studies on the original (SNR (mean(std) = 9.28(4.88)) and denoised images (SNR = 9.38(4.96)) with SNR defined as myocardium mean over background corner mean. Denoising improved registration performance, indicating higher SNR led to better accuracy. This underscored the importance of enhancing SNR in DTCMR image registration.

Data availability(R3)
We donât have ethical approval to share data openly for this retrospective study.

Lack of novelty(R4)
R3 and R5 acknowledged our innovations. R4 mentioned [1] for groupwise, but it applied to ex-vivo hearts only, without signal loss or deformable motion, focusing on atlas-based cardiac analysis, which differed from the motion correction task we faced. [1] collected high-resolution structural images (0.15mm) to support diffusion (0.6mm), while in our more challenging case with in-vivo low SNR diffusion images (2.8mm), we didnât use structural counterparts. 
R4 referenced [2] a contrast conservation step; however, disentangling anatomy and contrast remained questionable. Additionally, [2] chose the brightest frame as fixed, which was not optimal (ISMRM 2024, Abstract #2141). Our proposed method embedded the tensor information as a constrain for grouped frames with refined loss, unlike [2] used five different losses with extensive hyperparameter tuning just for disentangling.

Lack of comparison(R4)
R3 praised our comprehensive performance assessment against existing approaches. As mentioned, [1] need paired structural images and aimed at high-resolution cardiac analysis, which differed from our task. According per MICCAI policies, additional experimental results are not allowed in the rebuttal. We refer to the outputs in the SPIE Arxiv version of [2]: for R2 (mean(std)), RMSE, and NE% (median [p25, p75]), [2] reported 0.895(0.054), 6.865(3.481), and 1.7[0.6, 3.1], respectively. All metrics were worse than our proposed method.

Groundtruth (GT) in Fig 3/4(R5)
No GT labels were available for single-frame DTCMR, and segmentation labels were not evaluated. Similarly, no GT labels were available for our registration task.

Minor issues
We apologize and will correct the typos in the final version."
https://papers.miccai.org/miccai-2024/367-Paper3934.html,"We thank the reviewers for their valuable comments and their recognition of the strengths in our work:

Main questions:

More fMRI signals on gyri than on sulci. Discuss potential differences in signal-to-noise ratios (SNRs) between gyri and sulci and their impact on the results (R1).
We will add a discussion on this. The potential different SNRs do not influence our conclusion since we set the same threshold for gyri and sulci and we report the experimental results under different thresholds in the paper.

Use task fMRI instead of resting fMRI (R1).
For task fMRI, we have ground truth of activated brain areas to verify and support our conclusions. However, in resting fMRI, activated brain regions may vary across different subjects.

The hyperparameters of their networks. (R1/R4).
Due to MICCAI policy, only figures, tables, and proof of equations are allowed in the appendix. We will list the hyperparameters appropriately in the paper in accordance with the MICCAI submission guidance. We will also release/publish our code upon acceptance.

Lack of experiments (R4) / Test more thresholds (R5)
MICCAI rules prohibit new experiments.

More explanation of technical details (R4). 
Due to the rich content and MICCAIâs policy regarding appendices, we did not include all technical details. However, we will address this concern appropriately by publishing our code.

Visualizations of functional brain networks (FBNs) (R4).
Please see the appendix since we presented the visualizations in the appendix.

The advantage of the proposed Twin-Transformer over traditional methods (R4).
As recognized by Reviewers 1 and 5, our method can simultaneously extract temporal and spatial features from fMRI data, and the design of the Twin-Transformer can process signals from gyri and sulci concurrently. Additionally, the Transformer architecture, with its self-attention mechanism, has been well-proven in the machine learning community to have better feature extraction ability than traditional methods such as PCA, dictionary learning, and sparse coding.

Related works of Twin-Transformer (R5).
We will include a discussion of related works on spatial-temporal Transformers.

The color bar in Fig 3 (R5).
Thanks for your reminder. We will add the color bar next to the figures. Not all heat maps have the same scale."
https://papers.miccai.org/miccai-2024/368-Paper0500.html,"Thanks to the reviewers for their valuable suggestions, which inspires us to perfect our work!

Reviewer4: weekness1
Our work aims to design a more efficient multi-modal fusion strategy than conventional models. As shown in Table 1, conventional models such as UNet and ResUNet, get poor segmentation performance, and nnUNet, as their expansion, needs to add massive parameters (see its GitHub for details) and longer training time (due to more pre-and post-processing, it takes about a week to complete training) to achieve segmentation performance close to our model. Therefore, we believe that our proposed multimodal fusion strategy will have better clinical deployment prospects than conventional architectures.

Reviewer4: weekness2&comment1
We will open-source the code after our paper is officially published. Regarding the questions you may have about Win.Pool, I sincerely provide you with more details. In the article we use nn.AvgPool3d(tuple(window_size)), so each window can be pooled into a token.

Reviewer4: weekness3
As mentioned in the last paragraph of the introduction, the process of a radiologist segmenting a tumor can be divided into three steps: comparing PET/CT, locating the tumor, and outlining the tumor.
For the first step, we designed MCSA to promote information interaction between the two modalities, and the effectiveness of it is verified in Figure 4.
For the steps left, we developed TAMW based on the characteristics of PET/CT and our understanding of the U-Net. The deep layers, with their larger receptive fields, may focus on localizing the tumor, which is what PET is good at; while shallow layers, with larger image resolution, may focus on tumor contour, which is what CT is good at. Will adaptively weighting the contributions of PET/CT at different layers improve segmentation? The answer to this question is given in Table 2, and Table 3 supports our motivation.

Reviewer4: comment1
The training time of H2ASeg ranges from one day (CPU idle) to three days (CPU busy), with no significant difference from other models, except nnUNet. When the shape of input is (4, 2, 128, 128, 64), the GPU memory usage is 21000+Mib.

Reviewer5: weekness1&comment1
Firstly, in our experiments, we ensure that the loss functions of all models are bceloss + diceloss.
Secondly, in section 3.2, we added the modules to the baseline that also uses deep supervision to obtain the results in Table 2. The control variable method may answer your question.
Finally, we do not force all networks to adopt deep supervision but maintain the original configuration in their paper. For the models using deep supervision, we set the same loss weights. Because some models are imported from libaries, and some models have many outputs (A2FSeg has 16 outputs).

Reviewer5: comment2
We firstly deivded the test set and fixed it in subsequent experiments, then repeated the experiment on the remaining data.

Reviewer5: weekness3&comment3
We collected the weights of the TAMW in the fixed testing set.
For Table 3, as Reviewer4: weekness3 mentioned, the effect of TAMW mainly focuses on the awared targets, so we only analyzed the weights from the foreground emphasis. Secondly, the weights from the background emphasized often have large std, leading to weak interpretability.

Reviewer5: comment4
What a great question! We try to represent the focus of the model by using darker and lighter shades. In MCSA, after the inter-window attention, with PETâs positioning ability and feature interaction, both PET and CT features can detect the regions where tumors may appear, so we darken the color of the bar in these regions. Through intra-window attention, with CTâs structural information, model can determine whether these regions are tumors, so we set the bars in incorrect regions to a lighter color. The strategy in TAMW is the same.

Reviewer6: comment3
I sincerely provide you with the papers we refer to: ViT, nnFormer, UNETR, UNETR++, NestedFormer."
https://papers.miccai.org/miccai-2024/369-Paper3893.html,"We express our gratitude that all the reviewers acknowledge the novelties and contributions. Some details on the experiments and methodology details are clarified.

R3Q1: Other source domain? R: 1) We would like to clarify, in both experiments, using Domain A as source domain follows the prior evaluation protocol [11]. 2) Following your suggestion, we further select Domain F as source domain on prostate dataset. Our method yields 81.6% Dice, much better than [15] baseline with 77.5% Dice.
Q2: How values derived? R: Following the protocols of prior works, the results are derived from five-time repeated training. We will add these details in the revised version.
Q3: How results reported? R: The outcomes of [2], [6], [11], [12], [14], [17], [18] and [21] are directly cited from [11], while the outcomes of [15], [20] are reproduced by us. We will add these details in the revised version.

R4Q1: Loss typo. R: We will correct it as L_DRE (defined in Eq.3).
Q2: possible to lose feature? R: The proposed method consists of two branches (Fig.1), which learn the original and transformed features respectively. In other words, the original features are maintained in the first branch without potential loss.
Q3: possible loss conflict? R: We would like to kindly raise the reviewerâs attention that, L_DRE penalizes the channel-wise similarity which helps to enhance the representational capacity of features. L_HCD focuses on the semantic consistency before and after hallucination, while the features are projected to a low-dimension space. We believe that the enhanced representational capacity plays a facilitating role in learning domain-invariant features. Therefore, there is a synergistic effect between these two losses, which does not introduce conflicts during training. 
Q4: visual evidence. R: Many thanks for your valuable suggestion. As the rebuttal system could not upload any visual results, we briefly describe the visualization process and the outcomes as follows.
We extract the feature map from any block of the image encoder, and implement the global average pooling. Afterwards, we compute the self-correlation matrix and visualize. Compared the baseline and the proposed HSD, the channel-wise correlation on the off-diagonal regions is significantly alleviated, which reduces the feature redundancy. These visual results and outcomes will be included in the revised version.

R5Q1: More discussion on existing SDG methods. R: We will enrich the discussion of these methods in the revised version. In general, these prior works usually implement naÃ¯ve data augmentation before training the model to enrich the style diversity. Instead, our method is able to introduce random styles that are integrated in the learning pipeline and constrain their similarity, which is able to learn more generalized medical representation despite domain variation. 
Q2: Define style diversity. Why [11] canât? R: 1) According to the style hallucination reference (which we will add) in the machine learning community, the style can be quantified by the mean and standard deviation of per-domain image features (computed as Eq.1). The more varied distribution of them, the more style diversity. 2) [11] focuses on learning shape-invariant representation despite the domain shift, which does not assure enough style diversity for model learning and do not constrain the cross-domain content representation. 
Q3: Add style hallucination reference and clarify improvement. R: We will add the style hallucination reference accordingly. Existing references need to first extract and then inject the style in the target domain to the source domain. In contrast, our work can inject arbitrary and random styles for hallucination, which significantly enriches the style diversity. 
Q4: Equation definition. R: h and w refer to the height and width of the feature map. F_{i,n,j} refers to the feature map from the j^{th} channel in F_{i,n}. i refers to the i^{th} block of the image encoder."
https://papers.miccai.org/miccai-2024/370-Paper3513.html,N/A
https://papers.miccai.org/miccai-2024/371-Paper3535.html,"We appreciate the reviewersâ feedback and the opportunity to respond.
NOVELTY, PURPOSE(#R1) & AIM(#R3):The primary aim of the paper is to develop an automated method for assessing the diagnostic quality of LGE MRI scans of the left atrium (LA), focusing on fibrosis of the LA wall. Our novelty lies in using a hierarchical framework with two levels: the Sub-Bag Level and the Bag Level, inspired by a common expert assessment process. Experts often examine individual MRI slices to identify potential fibrosis (though methods may vary), similar to how our sub-bag module analyzes each slice. They then integrate these observations to assess the entire scan, which our bag module mimics by aggregating slice-level evaluations to provide an overall diagnostic quality score. We apologize for any confusion regarding the hierarchical and dual-module terminologies. We will clarify these and revise the terminology in the camera-ready version.
MODEL EFFICIENCY(#R3, #R4): We did an ablation experiment where we found our model is 700x and 89x more efficient in computation than fully supervised and two other models, respectively, since our model processes 2D image patches instead of full volume or 3D patches. MIL models are conceptually less complex as they aggregate information at the bag level rather than processing each instance individually. 
COMPARISON(#R4): As for the comparison with MIL, training ViTs with limited data is challenging due to their lack of inductive biases. ViTs often require pre-training on large-scale datasets like ImageNet, but the significant domain gap makes them suboptimal for fine-tuning on medical datasets with limited sample sizes (Ref: https://arxiv.org/pdf/2202.0670). Hence, we opted for the MIL approach. Also, the paper suggested by #R4 to compare focuses on the left ventricle (LV) by classifying LGE certainty to prompt real-time image optimization. In contrast, our paper focuses on the left atrium (LA), assessing the diagnostic quality of LGE MRI scans to evaluate fibrosis. For future work, we plan to incorporate self-supervised learning strategies like Masked Autoencoder.
SELECTIVE BIAS(#R4): We treated the number of sub-bags and instances as hyperparameters. To ensure fair comparison, we standardized parameters across all models. While these models might perform better with different parameters, using the same settings ensures that performance differences are due to model architectures rather than hyperparameter tuning. Ablation study, detailed in the supplementary, showed that 6 sub-bags and 60 instances provided the best performance. 
Some other comments weâd like to address:
#R1: The reason for using ResNet10 as a backbone network is its compactness and lower complexity, which reduces the risk of overfitting. Our main contribution focuses on the MIL part, making the choice of backbone network independent. 
#R1: The dataset is private; all 424 scans are from unique patients. No publicly available dataset exists for LGE MRI quality assessment of fibrosis in the LA wall. We plan to release the data with an MOU for access. We will add the data distribution regarding class imbalance in the camera-ready version.
#R1: The F1 score for DTFD-MIL is sensitive to hyperparameter settings, and using 0.5 as the default threshold might not be optimal. Hence, to provide a more comprehensive performance evaluation, we also reported the AUROC score, which shows that our model performed better overall. 
#R4: We will include examples of both diagnostic and non-diagnostic images in the camera-ready version.
#R4: We plan to conduct experiments using different sizes of training data for future work to address limited data scenarios.
#R3: We apologize for not clarifying Y_GT and for implying uniformity with âcognitive process.â We will address these issues in the camera-ready version.
#R3: For ABMIL, DTFD-MIL we took the official implementation and incorporated it with our dataset.
We will publish our code once it is accepted."
https://papers.miccai.org/miccai-2024/372-Paper3822.html,"We thank all reviewers for their constructive comments. We will improve the presentation according to suggestions. Below we address major concerns.

R1: Missing references [1-3] and comparison. 
A: We will include discussions regarding hard negative mining methods[1,2] and multiple instance ranking methods[3]. We reproduced the model in [1], achieving an ACC of 0.8605 and AUC of 0.8781 on Camelyon16, which is worse than our results.

R1: Novelty: hard negative mining + MIL has been explored extensively. 
A: Previous work (e.g., [3]) only mine negative samples for MIL using attention values. Our work is the first to mine hard negative samples for feature representation learning. We propose an elegant solution to leverage instance prediction to select hard negative samples, which are used for contrastive learning to improve feature quality. Extensive experiments show the power of our method in reducing feature representation learning time.

R1: Metric: AUPRC and P/R are missing. 
A: We follow previous works ([10,12,16,26] in the paper) and report ACC and AUC. We will add AUPRC and P/R as suggested.

R1: Efficiency gain is not shown. 
A: Testing on our machine (1 Nvidia A5000 GPU, 48GB VRAM), our method (top 5% negative samples) converges in 12 iterations in about 8 hours. In comparison, Its2CLR takes about 2 days.

R1: Visualization (Fig. 3): no difference with Its2CLR. 
A: They are supposed to be close. We are showing that with much fewer training samples, our method achieves the same performance as Its2CLR.

R1, R4: Implementation details. 
A: The code will be made publicly available upon acceptance to ensure reproducibility. The MIL aggregator was trained for 350 epochs. The fine-tuning was set to 15 epochs. For hyperparameters, we run experiments with different values and select them using the validation set. The value explored for w_r was in the range [0.05, 1], with step size = 0.05, K in {5, 10, 50}, r_n from {0.02,0.05,0.1,0.2,1}, w_b was set to 0.5 as in DSMIL, and r_p was set to 0.2 as in Its2CLR.

R1: Showing hard negatives compared with naive hard negative sampling methods. 
A: Will add hard negative samples during training. But as mentioned above, they are solving different problems. Previous methods only sample hard negative samples for MIL using attention values.

R4: Standard deviation. 
A: We provided std dev for TCGA in the supplementary, showing significant performance improvements over baselines using a t-test (95% confidence interval). We will add std dev for Camelyon16 as well.

R4: Low resolution in Fig.3. 
A: Fig. 3 shows patch-level ground truth and predictions. Each pixel represents a patch. Thus the low resolution. We will add magnified figures.

R4: Fig.3 Ground truth. 
A: We use the original ground truth from Camelyon16 and we did not manually annotate it.

R4: MIL aggregator selection and ablation study. 
A: We use DSMIL as our aggregator. In Tab. 2, âDSMIL + ranking lossâ shows results without iteration, and âOurs w/o rankingâ shows results without ranking loss. Our method improves both metrics, demonstrating the effectiveness of each component.

R5: Camelyon16: 130 testing images, acc numbers does not match the list. 
A: Although the official test set has 130 images, most existing works, including DSMIL and Its2CLR, excluded one test data due to quality issues. We use the same setting, with 129 test images, thus the acc numbers.

R5: Why not cross-validation for Camelyon16. 
A: We follow the official test set split to ensure fair comparisons with previous methods. The accuracy results are the average of three independent experiments.

R5: Baseline evaluation-DSMIL only used one resolution. 
A: Thank you for pointing out this oversight. We reevaluated DSMIL with two-scale features (optimal in the original paper), showing an ACC of 0.8837 and AUC of 0.9095. This is still worse than our results. We will update these numbers in the revised version."
https://papers.miccai.org/miccai-2024/373-Paper3604.html,N/A
https://papers.miccai.org/miccai-2024/374-Paper1451.html,"We appreciate all the reviewers for their insightful comments and have summarized them into three major and two minor concerns.

[Major concerns]
C1. Clarification of novelty and benefit of the HATs beyond PrPSeg (R1, R3, R5):
â We apologize for the lack of clarity and would like to highlight our contributions in this rebuttal:
(1) New dynamic EfficientSAM Backbone: We introduce a novel token-based dynamic EfficientSAM (d-EfficientSAM) backbone with enhanced multi-scale capability, achieving superior semantic segmentation for 15-class segmentation in both PrPSeg and HATs.
(2) Quantitative modeling of anatomical relationship beyond a binary matrix: The HATs model captures the relative relationships between multiple classes using a new hierarchical scale matrix (HSM). This model transitions PrPSegâs hierarchical taxonomy matrix (HTM) from a binary relationship to a fully quantitative relationship.
(3) More comprehensive setting in panoramic kidney pathological segmentation: We have doubled the complexity of panoramic multi-scale kidney pathological segmentation from 8 classes to 15 classes, allowing a more comprehensive pathological panoramic segmentation using a single model. 
(4) Open-sourcing the HATs model: We will open-source the HATs model to promote more reproducible research, unlike PrPSeg, which was not open-sourced.

C2. Performance gain from PrPSeg to HATs (R1, R3, R5): 
â The PrPSeg model implemented in Table 3 features our new d-EfficientSAM backbone. To avoid confusion, we will refer to this model as âPrPSeg (d-EfficientSAM)â in the manuscript. For clarity, we have also implemented the original PrPSeg with a CNN-based backbone, which has a performance of dice 65.94. Compared to the original PrPSeg (CNN), our HATs model outperforms it by 1.64%, demonstrating the added benefit of introducing d-EfficientSAM beyond HSM.
â The Wilcoxon signed-rank test shows that the improvements from HATs method with PrPSeg (CNN) and PrPSeg (d-EfficientSAM) are statistically significant (p-value < 0.001) in both Table 2 and 3.

C3. More comprehensive assessments on HATs (R1, R3, R5):
â As suggested by R1, we have repeated experiments with different random seeds with the settings from the ablation study in 8-class segmentation. According to the results (backbone only: dice 74.38, std 0.097, HTM: dice 75.12, std 0.085, HTM+HSM: dice 75.44, std 0.045), the proposed method consistently achieved better performance.
â Additionally, to address R3âs question, we will provide results from three ablation study settings in Table 3 using the PrPSeg (CNN) backbone (backbone only: dice 64.95, HTM: dice 65.94, and HTM+HSM: dice 66.65, p-value < 0.001 compared with HATs).

[Minor concerns]
C4. Motivation of the HATs for clinical application (R1, R5): 
â Recently, pathomics has introduced a fully quantitative approach to enhancing the current semi-quantitative clinical guidance, enabling the development of fully quantitative biomarkers. Detailed segmentation across multiple organs is essential to achieve these pathomics biomarkers. We hope that this model will facilitate comprehensive multi-scale, multi-object segmentation, leading to more thorough pathomics biomarkers for kidney pathology.

C5. Typos, detailed data information, discussion and PrPSeg illustration (R1, R3, R5): 
â We appreciate all of the suggestions and comments from reviewers. We will correct all of the typos and add detailed data introduction, PrPSeg illustration, and motivation discussion as suggested by the reviewers in the final version of the manuscript."
https://papers.miccai.org/miccai-2024/375-Paper2469.html,"We would like to thank the reviewers (R1, R3, R4) for raising relevant questions and providing invaluable feedback. Below, we address the comments from each reviewer:

[R1, R3]
We appreciate the reviewersâ emphasis on the importance of separate analyses for different organ groups and the suggestion to use a publicly available dataset for evaluating tubular structures. Our dataset comprises 30 organs, ranging from very small (lacrimal gland, cochlea, and optic nerves) to medium-sized (parotid and submandibular glands, brainstem) to large ones (mandible, spinal cord). Some organs are tubular (e.g., carotid arteries, spinal cord, brainstem), some are complex (e.g., mandible), and others exhibit unique properties. Although we omitted detailed group-specific analyses for brevity, we can confirm that our implementation yields higher errors on tubular organs compared to small or large ones. We acknowledge the importance of stratifying our analysis by organ group (as suggested by R3: noisy, small, complex, and tubular) and will address this in a follow-up publication.

[R1]
The reviewer raises a important question regarding the impact of reported observations on other distance-based metrics. We confirm that the identified differences in Hausdorff distance implementations affect other metrics such as MASD, ASSD, and NSD. We are preparing a separate publication that will thoroughly analyze these implementations and provide stratified analysis across different organ groups.

[R3]
We use a mesh-based representation of the segmentation masks due to its well-defined surface, allowing us to compute surface element areas and precisely calculate distances from query points to the surface implicitly. We use triangle meshes for simplicity, but we acknowledge that different mesh types could be employed. Our reference implementation subdivides surface elements into smaller triangles to mitigate the potential effects of mesh type and enhance calculation accuracy.

[R3]
We appreciate the suggestion to discuss the choice of structural elements. To maintain our focus on comparing implementations and referencing a baseline, we did not experiment with boundary extraction procedures for calculations in image space, but provide only description of what is used in the implementations.

[R3]
Please note that our dataset included up to 30 organs per case, with some cases missing segmentations due to limited field of view or other artifacts, which is the reason why the total number of comparison is not 1800.

[R3]
We acknowledge the importance of computation time and will address this in our follow-up publication with an extended analysis.

[R3, R4]
The reviewers correctly note that the results for MONAI and MetricsReloaded are identical. Given that these implementations do not share the same code and differ in some other metrics implementations, we chose not to report them in the same column.

[R3]
The reviewer highlights the importance of how different implementations handle cases where one segmentation is missing. We analyzed this but could not include it in the conference paper due to page limits. We will address this in our follow-up publication. Additionally, the reviewerâs point about the implementation of DICE is well taken. We confirm that all implementations yield identical values for binary segmentations (we did not test soft DICE).

[R4]
We agree with R4 and will change the term âshape-awarenessâ to âboundary-awareness.â

[R4]
To clarify, we first convert the voxel segmentation to a mesh solely for slight smoothing. We were also interested in the discretization effect produced by different voxel sizes, which we did not report in this paper for brevity."
https://papers.miccai.org/miccai-2024/376-Paper1453.html,We cordially thank the reviewers for their constructive comments. We will revise them in the camera-ready/journal version paper.
https://papers.miccai.org/miccai-2024/377-Paper1025.html,"We thank the reviewers for the early acceptance of our work, their feedback, and insightful comments to further improve our work. In the following, we address the main comments of the reviewers.

Phase-level texts and video-level abstract texts â R#1&R#4

In this work, we use the same pretraining videos from SVL dataset. In addition to the narration text from SVL, we curate the coarser-level phase and abstract texts using the metadata of the videos obtained from the e-learning platform. It should be noted that not all the videos include narration, phase and abstract texts at the same time. Out of 1326 videos from SVL dataset, we obtain 1007 videos with narration and phase texts, and 920 videos with narration, phase, and abstract texts. Our contribution is to curate these hierarchical texts and effectively use them for surgical video-language pretraining.

Hierarchical downstream tasks â R#1

Thank you for pointing out, the hierarchical video-language pretraining can improve downstream tasks of different hierarchical levels. For example, HecVL can significantly improve text-based video retrieval when the textual query for the longer videos, e.g., retrieving the whole video based on the abstract texts. Also, HecVL shows slightly better results on the fine-grained level tasks, e.g., action triplet recognition, because the clip-level pretraining is the same as SurgVLP. We report the surgical phase recognition because it is a phase-level task that HecVL can largely boost from the hierarchical pretraining.

TSNE â R#3

We have conducted the tsne visualization but cannot include it due to the page limit. We observe that the modality gap is narrowed during the hierarchical pretraining. Also, the gap is closer than the prior works. Modality gap, as a geometric phenomenon of the multi-modal models, hampers the transferability to the cross-modal tasks, e.g., image captioning. This visualization shows that HecVL can enable a better performance on vision-and-language downstream tasks.

Scene understanding from pretraining â R#4

Tab.1 and 2. show the zero-shot surgical phase recognition results without any fine-tuning on the downstream datasets. We achieve this by decomposing the phase labels into basic concepts and constructing textual prompts, as shown in the supplementary. It shows that the model understands the specific surgical operation more than high-level abstract phase definitions. It should be noted that the zero-shot performance of HecVL shows a degree of surgical scene understanding but there is a large performance gap compared to the fully supervised methods. Therefore, two avenues can be considered in the future, improving the pretraining process by incorporating external surgical knowledge and a data-efficient approach to adapt the pretrained model to specific surgical scene understanding tasks."
https://papers.miccai.org/miccai-2024/378-Paper1682.html,"We sincerely thank the area chairs and all reviewers for their valuable feedback, which has helped us greatly improve our manuscriptâs quality.
Reviewer#1
QA: The ASC module addresses multi-connected regions in breast cancer tumors, grouping different regions into prototypes for refined feature expression. The CFD module fully leverages support features to capture the coexistence characteristics between support and query data, optimizing boundary segmentation. The PkGRU module utilizes bidirectional GRU to fully extract the time series information of DCE-MRI, as the MRI image of the tumor region becomes more visible after passing the contrast agent. We will refine the description of how all the elements fit together in our final version.
QB: The subtype classification criteria are recorded in the NACT-Pilot clinical information workbook. In internal experiments, we use the NACT-Pilot dataset, training on three subtype categories and validating on the fourth subtype. For external testing, we train on three subtypes from the NACT-Pilot dataset and validate on the fourth subtype from the TCGA-BRCA dataset.
QC: We apologize for any reading difficulties caused by the figures. We will revise the figures and improve other details in our final version.
Reviewer#3
A support set consists of images with labels, while a query set contains images without labels. We randomly select the support and query data from the train set during training, ensuring they belong to the same subtype. The activation features and probability maps are connected through channel concatenation. We apologize for any reading difficulties caused by the figures. The figures will be improved in our final version. The high performance of our model over U-Net in Dice score is commendable because we need only one instance of labeled data to segment unlabeled data in the new subtype data. Most one-shot segmentation methods perform worse than U-Net.
Reviewer#5
Thanks for recognizing our work. Regarding preprocessing, we conduct zero-mean unit-variance intensity normalization for the whole volume. We use the DSC as the evaluation metric. Our final version will refine the data preprocessing stage and metrics calculation. We will improve statistical analysis, error analysis and technical details in future work.
Reviewer#6
Q1: Pharmacokinetic and hemodynamic are interrelated concepts, which are reflected in the principles of DCE-MRI: continuous MRI imaging after injecting the contrast agent allows for observing the distribution of the contrast agent at different time points, providing insight into the dynamic changes of blood flow in the tissue. The hemodynamic features are learned by PkGRU from MRI images at different moments and propagated in the ASC and CFD modules to enhance segmentation performance. Thatâs why we claim âthe model is driven by hemodynamicsâ.
Q2: Sorry for not mentioning that our experiments are one-shot settings. We will address this in our final version. The few-shot models in the comparative methods typically include both 1-shot and 5-shot settings. We only conduct experiments with the 1-shot setting. Our work focuses on achieving promising results with minimal labeled data (i.e., one-shot).
Q3-4: Regarding the data issues, we will provide more details in the dataset section. It is our fault for any confusion caused by the figures. In Figure 1, Fs and Fq enter the PkGRU module separately, as shown in Figure 2(a). Each block of 1-4 blocks comprises two convolutional layers and a pooling layer, while each block of 5-8 consists of a channel concatenation, two convolutional layers, and a sampling operation. We will revise our figures to make them clearer and provide a more detailed explanation of the figures.
Q5-8: The symbols in equations will be clarified. The 3D volume data is converted into a series of 2D slices serving as the model input. We will address the detailed issues in our final version and greatly appreciate the reviewersâ valuable feedback."
https://papers.miccai.org/miccai-2024/379-Paper1085.html,"We are delighted that the reviewers found our motivation ânovelâ (R1) and that our method âcontributes significantlyâ (R3). We address all reviewer comments below and will incorporate all feedback in the next revision. We will open-source the code upon acceptance at the earliest time.

R1:
Clarification on motivation.
Separating displacement and variance estimation is necessary because they cannot be optimized with a single objective. A joint objective from heteroscedastic variance estimation leads to undersampling higher-intensity regions as analyzed in Sec. 3 of the main text, degrading performance (Tables 1, 3; Fig. 2). Our method allows separate objectives for each estimation by alternating optimization during training, where performance improvements are validated in Table 1.

Intuitions on exponentiated SNR.
The exponentiated term gamma balances the trade-off between full SNR weighting (gamma=1, MSE as the data term) and constant weighting (gamma=0). We select gamma=0.5 from our experiments in Table 4.

Clarification on âthis is in contrast to â¦â
AdaReg and AdaFrame donât explicitly model noise variance, whereas ours learns the noise variance from the data.

Clarification on âImprovements â¦ during inferenceâ
One advantage of our method is that it maintains the same computational complexity as the baseline during testing, requiring only the forward pass of the displacement model. The improved performance results from our proposed collaborative learning strategy during training.

Clarification on Fig. 3
In column 3 of Fig. 3, labeled âEstimated Noise Variance,â red indicates higher uncertainty. Note that we assume the fixed image is a noisy observation of the reconstructed image (Eqn. 2). These red areas accurately correspond to locations with high discrepancies between the fixed and reconstructed frames (see columns 1, 2), validating our heteroscedastic noise formulation and variance estimator optimization.

R2
Poor comparison to literature
We disagree. The suggested virtual decimation paper has key differences from our work and thus cannot be regarded as a comparison; nonetheless, we are happy to cite it. The uncertainty estimates provided by their work focus on the epistemic uncertainty (i.e. model parameters uncertainty) rather than aleatoric uncertainty (i.e. uncertainty intrinsic in the data) characterized by our heteroscedastic noise assumption.

We have also conducted extensive experiments to compare our approach with existing literature. We detailed two lines of work in Sec. 2: (a) heteroscedastic uncertainty estimation and (b) adaptive weighting schemes. We implemented NLL, beta-NLL, AdaFrame, and AdaReg as baseline frameworks on two registration architectures (Voxelmorph, Transmorph) across three different datasets (ACDC, CAMUS, 3D Echo). Our proposed approach consistently outperforms them.

Implementation details
The network architecture and optimization hyperparameters were kept constant, across all experiments.

Details on the significance testing
We used scipy.stats.ttest_rel to compute the p-values between ours and the second-best method by conducting a paired t-test.

Deformation smoothness
To evaluate smoothness, we compute the percentage of pixels with a negative Jacobian determinant. In ACDC, we achieve 0.24% compared to 0.29% for the vanilla Voxelmorph, demonstrating smoother deformations.

Lack of estimated uncertainty evaluation
We disagree. In Sec. 5.2, we provided a detailed analysis including common sense indicators for both quantitative (sparsification error, Fig. 3 right) and qualitative (Fig. 3 left) evaluations of our uncertainty estimates.

Comparison to robust losses
We acknowledge that not all objectives assume uniform noise, which is why we state âoften relyâ in our abstract. Additionally, in Table 1, we compare our method with NCC and MI across two registration architectures and two datasets, and improve over both.

R3
Code availability
We will open-source the code after acceptance."
https://papers.miccai.org/miccai-2024/380-Paper1415.html,"Q1: Differences from [20] (R1)
Our proposed Dose Control Module addresses the clinical challenge of accommodating PET reconstruction across various dose levels. Based on PET imaging property, we utilize high and low pass filtering to explicitly extract frequency features, offering a more computationally efficient and explainable approach compared to the cross-domain feature fusion method in [20]. While [20] was initially designed for natural image super-resolution, we extend its application to 3D PET reconstruction through targeted modifications.
Q2: Reasons for using wavelet and Fourier transforms (R4)
We employ FFT to obtain frequency spectrum, which allows for explicit thresholding of high and low frequency components before feeding into the diffusion model. In contrast, due to noise interference, the boundary between high and low frequency components are implicit during the diffusion process. DWT is used as an adaptive segregation, which is more suitable in such context. Both FFT and DWT provide frequency-based guidance from different perspectives, enhancing the modelâs performance at various stages.
Q3: Loss function lacks innovation (R4)
Our main contribution lies in the idea of frequency separation and dose control, rather than new loss functions. Actually, FFT and DWT are classic digital signal processing algorithms, instead of learnable machine learning algorithms that often require optimization objectives. Details can be found in [A].
[A] Hayes, Monson H. Statistical digital signal processing and modeling. John Wiley & Sons, 1996.
Q4: Lacks mathematical analysis of high-frequency input (R4)
Mathematically, diffusion models are capable of handling images across the entire frequency spectrum. We have included a reference to [9] for comprehensive mathematical derivation of the diffusion process. Due to page limit, we did not conduct detailed mathematical analysis to prove effectiveness of the diffusion model in handling high-frequency information. However, this viewpoint has been mentioned and validated in numerous studies [5,7,13].
Q5: Limited samples (R1)
Collecting a large amount of PET images is particularly challenging and expensive in clinical settings. Our experiment included 197 subjects, each comprising 6 dose levels. For the 157 training subjects, there is a total of 30,144 training samples after splitting each image into patches as data augmentation. Most studies on PET reconstruction employed rather smaller datasets (i.e., 16 subjects for [B], 40 subjects for [C], and 36 subjects for [D]). In this context, our training dataset is substantial enough to meet the demand of generalizability.
[B] Zeng, P., et al., 3D CVT-GAN: A 3D Convolutional Vision Transformer-GAN for PET Reconstruction. MICCAI, 2022: 516-526.
[C] Rui, H., et al., DULDA: Dual-domain Unsupervised Learned Descent Algorithm for PET image reconstruction, MICCAI, 2023: 153-162 
[D] Jiaqi, C., et al., Image2Points: A 3D Point-based Context Clusters GAN for High-Quality PET Image Reconstruction. ICASSP, 2024: 99Q6: Artifacts in results (R2)
All comparison methods were implemented using a consistent data processing approach. The models were trained and tested on patches, and the artifacts occur due to stitching of patches into the whole image. Our method reduces these artifacts by addressing the low-frequency components separately through the pre-trained CNN, which enhances the clarity of structural information. Also, the Dose Control Module enables our method to effectively generate PET images across different dose levels.
Q7: MSE as an additional metric (R4)
MSE can be calculated from PSNR inherently. Considering page limit, we only provide results for SSIM and PSNR.
Q8: Comparative methods are insufficient (R2)
We have selected five typical competing methods, encompassing three representative categories: CNN-based, GAN-based, and diffusion-based models. We think that these may be sufficient and diverse for experiments in this conference paper."
https://papers.miccai.org/miccai-2024/381-Paper1207.html,"We thank the reviewers for their valuable feedback. R1, R3, and R4 appreciate our extensive experiments and ablation study. R1 and R4 find our approach logical and suitable for the medical field, while R1 and R3 commend our methodâs potential, writing, and new dataset. The major concerns are the metrics, details and some misunderstanding about method, dataset and experiments, which is clarified in the following.

â-R1,R4â-
(1) Metrics: 
Thanks for the valuable suggestion. Due to the page limit, we only select two metrics for evaluation, in the future journal version, we will add more metrics suggested by R1 and R4.

âR1â
(2) Detailed Statistical Support:
The performance drop in BLEU indicates that the model tends to overlook small lesions to some extent. The reason is that for description and disease tasks, the interest regions are normally small, and performance drop means indirectly showing they are not being adequately identified. We will add more metrics such as disease accuracy for better evaluation.

(3) More discussion for performance drop (-1.7;-3.2) in ablation study: 
For â-1.7â, IA module can capture more useful information, without IA would degrade the performance. For â-3.2â, without HR information, the model fails to detect small lesions, resulting in descriptions that overlook these lesions and consequently degrade performance. Will add discussions in the latter version.

âR3â
(4) Clinical value:
Our clinic value is to avoid cultural biases and errors caused by machine translation. Prior work, such as Huatuo-26M (Li, Jianquan, et al.) and Qilin-Med-VL [19], found that translating from English introduces biases and inaccuracies, compromising robustness. Hence, it is crucial to develop a native Chinese dataset, which is the focus of our paper. This will be discussed in the revised version.

(5) Evaluation on MIMIC-CXR: 
Our main goal is to design a robust MLLM for Chinese Medical QA. MIMIC-CXR is unsuitable for our evaluation for two reasons. First, MIMIC-CXR is an English dataset, and translating it to Chinese introduces cultural biases and errors, which we aim to avoid (see point (4) in our rebuttal). Second, MIMIC-CXR only contains reports, limiting it to report generation tasks, whereas our method focuses on Medical QA. Although we could create instruction-following data from the reports, this is complex and not our paperâs main objective. Finally, we compare our method with more SOTA MLLMs, such as RadFM and MedFlamingo, demonstrating superior performance.

(6) Novelty of our method (comparison to [7,a,b,c]): 
There are some misunderstandings. [b, c] use traditional methods limit in comparative analysis, while we use MLLMs for medical VQA, covering more tasks. [a] adds learnable bias and scale factors for instruction cues, but the visual context remains instruction-irrelevant.
The closest work is [7], designed for autonomous driving without considering challenges in medical image, e.g., comparative, instruction-related extraction. Differently, our method addresses all problems of [7,a,b,c], and we also conduct experiments to prove this and will add in the latter version.

(7) Relation between the method and the dataset: 
The dataset requires the ability of capturing high-resolution information, instruction-following and comparative analysis, which aligns well with our method. We also evaluate our method on Qilin-Med-VL and achieve better performance than SOTAs.

âR4â
(8) Evaluation on other datasets: 
We propose HiA for Chinese high-resolution, instruction-following medical data, which can not be fully evaluated by current MedQA datasets. Thus, we introduce Chili-Joint for comprehensive evaluation. Besides, we also conduct experiments on Qilin-Med-VL dataset and achieves consistent results with Chili-Joint. Detailed experiments will be added.

âR1,R3,R4â
(9) Typo errors: 
Will correct in the latter version."
https://papers.miccai.org/miccai-2024/382-Paper2707.html,"We sincerely thank all reviewers for their invaluable comments. The code will be published for reproducibility. 
R4(Q1): Discuss about adaptive modeling
A:We employed GNN for adaptive brain network modeling. The RMSE on PicSeq, Flanker, ProcSpeed, and ReadEng cognitive scores increased by 3.24, 4.19, 6.35, and 3.62. The learned brain network exhibited an average path length of 0.53 and a clustering coefficient of 0.67. The rewired brain network presented 0.42 and 0.76, suggesting strong small-world characteristics. This demonstrates that constructing a small-world network can enhance model performance and the ability of GNN to automatically learn the small-world characteristics is limited.
R4(Q2): Discuss about hyperparameter p
A:The choice of p indeed affects the modelâs performance, but the increase in RMSE remains around 2, indicating no unacceptable degradation. To address the issue of manually selecting p, we incorporate a grid search for p to accommodate different datasets or tasks.
R4(Q3): Provide the range of cognitive scores
A:The range for PicSeq is [76.42, 135.55], Flanker [84.9, 142.11], ProcSpeed [51.62, 154.69], and ReadEng [84.2, 150.71].
R4(Q4): More comparison
A: BrainNetTransformerâs performance on Picseq (RMSE:21.11, MAPE:18.06, CC:0.14), Flanker (RMSE:12.19, MAPE:10.26, CC:0.24), ProcSpeed (RMSE:15.39, MAPE:12.15, CC:0.32), and ReadEng scores (RMSE:14.92, MAPE:11.94, CC:0.33) will be listed in the final draft.
R4(Q5)&R1(Q3): Clarify small-world network
A: We constructed a small-world network due to its rapid information processing and transmission capabilities, mirroring the brainâs high-level cognitive functions. The rewiring method aligns with the small-world networkâs original definition as an intermediate state between regular and random network. 
R4(Q5)&R1(Q3): Other metrics
A: We implemented the modularity and heterogeneity metrics of the brain network, using clustering coefficients and degree centrality to guide the construction of brain networks. The RMSE of PicSeq, Flanker, ProcSpeed and ReadEng scores increased by (1.26, 1.34, 2.07, 0.92) and (0.52, 0.64, 1.64, 0.03) respectively, verifying the effectiveness of the proposed small-world metric based model.
R1(Q1): Clarify computational complexity
A: The computational complexity of PIE, SW-BNC and HGL module is O(NlogN), O(EF) and O(nH), where N is the number of vertices, E is the number of edges, F is the feature dimension, n is the number of concatenated features and H is the dimension of the features. 
R1(Q2)&R3(Q1): Novelty clarification
A: Our approach differs from previous methods in three ways: a) PIE identifies representative vertices at micro and macro scales; b) we simulate the information processing method of human brain and use the small-world property to guide the construction of the functional connectivity for the first time; c) we design a cognitive score prediction paradigm using the HGL module.
R3(Q2):Discuss on experimental results
A: Model performance is closely tied to its design. Models like RegGNN and Meta-reggnn, trained on important samples, lack stability and are highly influenced by data distribution. The BrainGB model might underperform due to edge weight consideration in self-attention, while BrainGNN, using extra regional information, yields subpar results and is difficult to train. Our model integrates multi-level brain information and uses a small-world network for faster GNN information processing.
R3(Q3&Q4): Discuss on dynamic brain data
A: We use rs-fMRI to predict cognitive scores and form a static brain network. Recognizing brain dynamics is vital for tasks like attention shifts, emotional changes, and disease progression. Our framework can adapt to dynamic feature extraction by segmenting continuous brain data into time windows and detecting dynamic community changes. We plan to further explore dynamic brain data processing."
https://papers.miccai.org/miccai-2024/383-Paper1863.html,"Thanks for your valuable comments. 
1.R#4-Q1&R#5-Q1: About public external validation.
A1:1)We noted a large public dataset,COPDGene,used in COPD-related studies,but it is currently in the application stage.We will report the corresponding results once the application is successful.
2)The code and the corresponding model weights will be disclosed after acceptance.
3)Due to involvement with partner institutions,we regret that the dataset we used cannot be publicly released at this time.

Q2:Comparison with some simple 2D methods.
A2:1)Gonzalez et al reports an AUC of 0.856 for the diagnosis of COPD,which is lower than the AUC of 0.896 achieved in our study. 
2) Although Gonzalez et al.âs approach is easy to implement,it captures less 2D information and is weak in information condensation.Our method conducts more extensive 2D sampling on a single CT scan and achieves more efficient information fusion through the use of the PLAF strategy, thus enhancing its effectiveness.

Q3:The low performance for COPD grading.
A3:1) There are limited existing methods for grading research,and the performance of grading is often hindered by issues such as small dataset size and significant class imbalance(directly related to the probability of disease occurrence or progression).
2)The method(Chaudhary et al.)utilizes paired respiratory data,which is challenging to obtain in practical scenarios and poses higher dose exposure risks.Besides,differences in cohort data(data size and quality)make direct performance comparisons unfair.
3)Inspired by the inter-class correlation in COPD grading tasks,we innovatively proposed the RSS, which enhances model predictions by accurately modeling the intrinsic association of classification labels.The ablation experiment results in Table2 further demonstrate the significant performance improvement.

Q4&Q5:The novelty of H-MIL & Comparsion with other hierarchical MIL.
A4&A5:We have reviewed the relevant paper you mentioned,and we believe it is a case of name conflict,as the methods differ significantly:
1)The PLAF strategy proposed by H-MIL in our paper focuses on key regions within key slices in key sub-bags through a hierarchical process of pixel-level,sub-bag-level, and bag-level feature fusion.This approach is distinct from the method proposed by Yan et al.
2)Yan et al.âs method involves dividing instances into K clusters, which require repeated sampling and combination into bags within each cluster.The labels of these combined bags inherit the labels of WSI.In contrast, our method does not require label inheritance or sampling.The PLAF strategy enables the step-by-step expression of bag-level features without introducing additional noise,making it more efficient.
3)We were inspired by [23],but for the characteristics of COPD,we introduced the PLAF strategy and RSS.These additions improved diagnostic performance by 4.96% and grading accuracy by 7.40%.

Q6:The vulnerability of COPD GOLD stages
A6:We strongly agree with this perspective.Our study also underscores the significance of CT in the early diagnosis and classification of COPD.Moving forward,we aim to integrate multiple modalities of information to further advance community development."
https://papers.miccai.org/miccai-2024/384-Paper3036.html,"We appreciate positive feedback from reviewers R1, R3, and R4, i.e., highlighting clear motivation (R3), high registration performance (R1, R4), significant clinical impact (R3), and flexible transformations (R4). Our code will be made publicly available. Below, we organize the main questions (Q) and corresponding answers (A).

Q1: Necessity of using neural networks to compute the inverse displacement field (R1, R3, R4).
A1: Scaling and squaring the negative velocity field is faster for calculating the inverse displacement field than the iterative method. However, registration accuracy with a potential velocity field is lower than without a velocity field [6, 11, 15, 21]. Therefore, we propose a velocity field-independent method to compute the inverse displacement field with neural networks. We rigorously assess the accuracy of the inverse deformation field through qualitative and quantitative experiments. Besides, our deformation-inverse network can be pre-trained.

Q2: Design of symmetric magnitude constraint loss (L_smc) (R1). 
A2: Proposed L_smc, a soft constraint, encourages alignment of anatomy to the middle of the deformation manifold for symmetry. Penalizing the sum of the absolute values or squares of displacement fields falls short of symmetry requirements. Another alternative constraint involves using a single displacement field with its negative counterpart. This hard constraint led to diminished performance, likely due to the limiting flexibility of transformation, as revealed by experimental results.

Q3: Difference with mentioned work (R3).
A3: Siebert proposed an Adam-based instance optimization registration without symmetry requirement. Iglesias and Greerâs approaches involve inverse consistency registration between image pairs. Our approach symmetrically and progressively registers image pairs to intermediate space. Namely, our registration network does not include inverse consistency. The independent deformation-inverse network handles inverse consistency and is used in the inference stage to obtain the final inverse-consistent displacement field. Therefore, we choose Ants (SyN) and SYMNet as the benchmark. We will discuss differences between our framework and the above-mentioned work in the final paper.

Q4: Geometric interpretation of composed transform (R4).
A4: Eq. 3 composes two mutually-inverse displacement fields defined in different coordinate spaces, resulting in an identity field. Geometrically, for the composition process of two displacement fields $u_1$ and $u_2$, firstly, according to the registration field of the first transformation, denoted as $\phi_1=Id+u_1$, we know that the displacement of the second transformation in the first coordinate system should be $u_2 \circ \phi_1$. Then, the composed displacement field is the summation of the two displacements, i.e., $u_1 + u_2 \circ \phi_1$. In Eq. 1, two fields, both defined in the same coordinate space, can be directly summed, aiming to facilitate symmetric registration.

Q5: Selection of hyper-parameters, network details, implementation, and reproducibility (R1, R3, R4).
A5: We train multiple networks with different weights of losses. We select the networks and weights to optimize Dice scores on the validation set and report the results on the test set. Additional details can be found in the open-source code.

Q6: Selection of datasets and registration pair (R1, R3).
A6: Our study incorporated results from both in-house datasets and OASIS, while excluding NLST due to page limit. These two datasets specifically address intra-subject registration for follow-up studies and inter-subject registration for multi-atlas labeling, thus comprehensively evaluating our method.

Q7: Missing discussions and citations (R3, R4).
A7: We will include those missing references and discussions in the final paper.

Q8: Issues of presentation, formatting, and grammar (R1, R3, R4).
A8: We appreciate all these suggestions, which will be included in the final paper."
https://papers.miccai.org/miccai-2024/385-Paper0460.html,"R1
Formulation of PPM: During the reshape operation,order of patch representations is kept consistent across hierarchical levels. z is simply reshaped differently at each layer to indicate the number of independent entities & positively paired patches. Motivation behind using a different projection layer per each level is to learn a separate secondary feature space, More baselines: We would like to acknowledge the previous works mentioned. Virchow and UNI are pretrained on large in-house data. Since, Virchow is not publicly available, we compare of our model to UNI. UNI achieves a lower linear accuracy on SRH compared to NCT-CRC dataset consisting of H&E images. Notably, many large vision models in histopathology are trained on H&E stained images, restricting their application on other image modalities like SRH. While we focused on building a model for SRH data, we also demonstrated how it can perform equally well on H&E stained image data using TCGA. The most notable prior work that explores hierarchy are HiDisc (Cheng et al), HIPT (Chen et al). Since we closely follow the visual hierarchical concept  introduced in HiDisc, we included it as our baseline., Confusion regarding the use of same patch representation (z) inL_HA: This is not a typo. The KL divergence between the most aligning text vector and the visual representation is calculated using the same z in all levels. Unlike our text hierarchy which is formed of separately curated granular descriptions per each level, the visual hierarchy is always formed on patch sized views. We traverse through visual hierarchy by altering the count of positively paired patches based on a common origin, Multiple reviewers mentioned the lack of diversity in downstream datasets, tasks and metrics: To address the lack of comparison to baselines, we compared HLSS with UNI. We also achieve significant results on kNN, linear and out-of-distribution classification on multiple downstream datasets,How visual and language hierarchy is formed: The granular text descriptions for each level are separately curated to form the language hierarchy by describing the dataset-specific visual characteristics at the given granularity. Input to the visual encoder is a patch sized image. Some prior work have explored visual hierarchy using a pyramid of visual encoders which is highly computationally expensive.We get inspiration from HiDisc and explore the visual hierarchy using a slightly different positive pairing mechanism per each level. This formulation integrates well with contrastive objectives. While we only utilise patch sized visual inputs, they are paired in a level-specific manner to create a visual contrastive objective unique for each level.
R3
We utilised multi-stage prompting: Provide a list of visual attributes present in Stimulated Raman Histology (SRH) images of brain tumor patients. These visual attributes should be categorised in relation to their granularity, based on whether they represent patch level features (cellular level), slide level features (tissue characteristics) or patient level features (hollistic view). Provide 3 separate lists for patch, slide and patient level features, each containing 128 non-overlapping visual attributes, for each of the above patch level features, provide four sentences, each describing a SRH brain tumor image with respect to the corresponding feature. These prompts are repeated for other levels. After obtaining visual attributes, they were manually inspected by an expert to remove hallucination. The main motivation behind the dataset-specific descriptions instead of sample-specific descriptions, is to avoid the noise in the language data. We first adopt the SIMCLR to our domain.
R4
beyond classification, we have provided interpretability tests in supplementary. It is conducted using unseen bio markers for tumor classes of SRH, obtained from a pathologist."
https://papers.miccai.org/miccai-2024/386-Paper0817.html,"We thank all reviewers for their time and effort in reviewing our paper and providing insightful suggestions, and we are grateful for reviewersâ recognition of our novel patch alignment-based translation method, the opensource for replicability, and the release of the first paired pathology image dataset with a high resolution of 2048Ã2048. We address major comments below:
1.Baseline methods for comparison[R1,R3]
We experimented on all baselines on our CD3 and PAX5 datasets and our model performed best among all methods. Results on CD3 are consistent with the trend on PAX5.
It is notable that Pyramid(CVPR2022) and ASP(MICCAI2023) have proven their effectiveness on their datasets against all other baselines we compared. Therefore, due to page limit, we only showed results from the best SOTA Pyramid and ASP on CD3 dataset, as comparison with the best SOTA conveys enough information to indicate the effectiveness of our model.

2.Ablation details
(1)[R1,R3] We conducted ablation studies on the directions and components in contrastive loss, and our design achieves the best performance. Loss between H&E and IHC is for detail preserving, while others are for staining style preserving. Due to page limit, we only show ablation results of major components for conciseness.
(2)[R3] We proposed FocalNCE loss for our method design as PatchNCE performed worse than FocalNCE in experiments. Due to page limit, we only included major results.

3.Clarification[R3]
(1)We made a clear statement in the manuscript: âwe apply the FocalNCE loss between the output and the ground truth images in a bidirectional mannerâ and it is also indicated in Eq. 3.
(2)Our patch loss is calculated patch-wise, and we conducted ablation studies on patch loss to indicate its effectiveness. Please see 7.

4.Model Pre-trained with Visual Image[R3] We observed performance increases when we applied pre-trained VGG with visual images in ablation studies. It is notable that visual and medical images share similar low-level features such as edges, textures, and shapes, and thus pre-trained VGG with visual images can also be useful in medical imaging. It was also adopted in Pyramid(CVPR2022). It is desirable but hardly achievable to obtain a pathology pre-trained VGG due to the scarcity of medical images.

5.Method evaluation
(1)[R3] We use the same FID to evaluate pathology images as ASP(MICCAI2023), and our model achieved best results in our datasets.
(2)[R3] We did the same as Pyramid(CVPR2022) using expert evaluation as additional and complementary results to the quantitative metrics to indicate the effectiveness of our method from the pathological perspective.
(3)[R3,R4] Current SOTAs all have low SSIM, which cannot fully reflect the image similarity, so we evaluated our model on 4 different metrics. Moreover, we noticed quantitative metrics may not reflect performance in clinical settings, so we conducted expert evaluation as complement.
(4)[R4] We used 1024*1024 images for scoring on five pathological factors, and our test set covered samples from various field-of-views to ensure a comprehensive evaluation.

6.Dataset details
(1)[R1] We have two rounds of registration(SIFT) in preprocessing with manual annotations of key points, and poor-quality pairs were removed by pathologists to ensure proper alignment. Figures in the manuscript indicate our datasets do not have the non-rigid deformation problem. Our datasets will be released for public use.
(2)[R4] We use the adjacent slide with two rounds of registration in preprocessing to ensure proper alignments in our datasets. We used 37 cases with 40 section slides for data collection.

7.Patch size choice[R4] 4*4 is the optimal size in metrics and visual appearance. We did not include results (window sizes ranging from 4~64) due to page limit, and we found larger sizes resulted in blur and artifacts.

8.Figures[R4] We compared all methods in Fig. 2. Due to page limit, we separated them into two sets in Fig.3 for better view."
https://papers.miccai.org/miccai-2024/387-Paper0796.html,"We thank all reviewers for their constructive comments. We will open-source our code and dataset upon acceptance to boost reproducible research.
To Reviewer #1:
R1.1: We designed the prototype learning module to select key tokens of WSI for cross-modal interaction, addressing redundancy and computational complexity of the patch sequence. We used uniform sampling to select key tokens and aggregate them back with a cross-attention layer. 
R1.2: You mentioned we did not compare the LGH module with MIL aggregators. Since our modules are specifically proposed for the task of report generation, we mainly compare our model (LGH + CMC) with report generation models (Table 1). And to verify that our model learned diagnosis-related information during this task, we compared the LGH with MIL aggregators in subtyping and survival analysis (Tables 3 and 4) by fine-tuning it with a learning rate of 1e-4, 8 attention heads, hidden dimension of 512, dropout of 0.1, and Adam optimizer.
R1.3: We conducted ablation studies on #region size (64, 96, 128, 256, 384, 512) and #prototypes (64, 128, 256, 512, 768). #region size 96 (fixed #prototypes 512) yields the best BLEU-4 score of 0.182. And #prototypes 256 (fixed #region size 96) yields the best BLEU-4 score of 0.185.
R1.4: Thank you for highlighting equation ambiguities. We omitted the first region encoder process in the equation but mentioned it in the text. We will revise this for clarity.
R1.5: We compared our DINOv2 ViT-L to several backbones on report generation under BLEU-4: Ours (0.184), Phikon (0.178), PLIP (0.053), UNI (0.151), CONCH (0.077). Ours outperformed others, with only Phikon (0.178) being comparable.
R1.6: We make some minor clarifications. The dataset used for pretraining is our proposed TCGA WSI-report dataset. The text decoder is a 3-layer transformer. The DINOv2 backbone dimension is 1024, reduced to 512 by the LGH modules, matching the CMC module input dimension (512 x 2048) for cross-attention and text decoder attention (512).

To Reviewer #3: 
R3.1: For report data preprocessing with GPT-4, this prompt is used: Help me check the formatting and spelling of the supplied pathology report, including incorrected use of punctuation like misusing of âxâ and âXâ, and capitalization as well as deletion of some words of unclear meaning. It checks formatting and spelling while preserving the reportâs meaning, ensuring clinical correctness in the final report.
R3.2: Regarding noisy WSI-report correspondence, we clarify that reports do contain irrelevant information like patient age/sex or lymph node metastasis. The former (irrelevant information) will be filtered more strictly in future work. For the latter (diagnosis information not shown in WSI), as deep learning has been used to predict lymph node metastasis, treatment response, or gene mutation directly from WSI, we anticipate our model to implicitly learn and reflect this in generated report.
R3.3: You mentioned our model surpass ABMIL only marginally. We claim that our DINOv2 backbone provided superior initial features so that a simple MIL model can already achieve good results. We verify this by using ImageNet-pretrained ResNet50 for feature extraction. ABMIL achieved 0.673 accuracy in UBC-OCEAN subtyping, while with our backbone, it reached 0.792.
R3.4: Please refer to R1.1 â 1.3 and R1.5 â 1.6 for more details and analysis of our work.

To Reviewer #4:
R4.1: We conducted t-tests comparing our method to SOTA methods, resulting in P-values of 0.0003 for report generation, 0.004 for cancer subtyping, and 0.005 for survival analysis, indicating significant performance improvements.
R4.2: We acknowledge the importance of manual evaluation. In future work, domain experts will be included to further clean the dataset and validate the quality of generated reports as well as the effectiveness of our method.
R4.3: Our algorithm takes a WSI as input and generates the corresponding diagnosis report, as shown in Supplementary Figure 1."
https://papers.miccai.org/miccai-2024/388-Paper0215.html,"Thanks to all reviewers for their valuable feedback. We are encouraged by all reviewers to find our idea and method novel. Below are responses to each point.
@Reviewer #1"
https://papers.miccai.org/miccai-2024/389-Paper3878.html,"We thank the reviewers for their comments. We appreciate that each agreed HoG-Net addresses a significant clinical problem with a ânovel, interpretable, and efficient architecture.â Below we address the key critiques.

We thank R1 for mentioning these - they will be included in the updated introduction. Please note that [1] is a 2021 arXiv preprint, does not provide code to reproduce, and studies a different outcome task. [2] is a conference abstract published after the MICCAI deadline. Neither was peer reviewed as a full paper. We have communicated with the authors of both studies and assert that HoG-Net possesses the following advantages:

a. Holistic modeling of a large set of commonly available OARs (see 5) rather than less common lymph nodes [1] or the supervoxeling method proposed in [2] which requires the number of supervoxels, clustering algorithm, sparsification extent, and radiomic feature selection parameters to be carefully tuned. 
b. Improved interpretability of specific OARs implicated in model predictions, and is extensible to any task with multiple pre-defined ROIs. [2] provides a coarse level of interpretability to non-specific anatomical regions and found unusually high activations in the brain, which should not be implicated in HNSCC.c. Validated on a held out cohort of 395 patients from datasets different from the training set compared to 0 [1] and 121 [2] patients, reducing overfitting concerns. On patients from the independent test set, HoG-Net slightly outperforms [2].

To the best of our knowledge, no other graph-based approaches have been proposed for radiotherapy imaging analysis.

The reviewer is correct that SuperGraph can be easily created to model inter-OAR attention. Doing so, we found that model performance was slightly diminished, and attention trends mirrored those presented in the original submission. Due to MICCAI guidelines, we cannot show these additional results but can provide them in the supplementary if the AC/reviewer requests. Because these OAR-OAR interactions do not have a well-defined pathological motivation, we leave further exploration of this interesting direction to future work.

As observed by R2 and corroborated by previous work [3,4], clinical features are significant for this task. Table 1 demonstrates that HoG-Net outperforms all baselines in the clinical+imaging setting, particularly F1 scores (0.651 vs 0.584, 0.582 vs 0.518, 0.658 vs 0.595) for ours vs. the top baseline across HN1, HNPET, and RADCURE.

Mateus et al. âImage basedâ¦â Scientific Reports 2023

To clarify, we propose OARenc to accommodate arbitrarily large differences in OAR dimensionality from patient to patient within a single batch. This would not be possible with a conventional CNN without cropping or resizing, potentially learning superfluous background features or losing OAR information.

OAR segmentations are acquired during routine radiation treatment planning allowing for easy inference in clinical settings. These structures are not commonly used in analysis of radiation-treated cancers; HoG-Netâs ability to leverage them is a strength and novelty of our work. HoG-Net generalized across data from 6 different institutions, suggesting robustness to OAR segmentation variability.

HoG-Net has <100k trainable parameters, comparable to graph methods in the hyperspectral imaging domain and ~80x less than CNN baselines including DenseNet-121 (~8M parameters). Training times on the same GPU are ~6 hours for HoG-Net and ~2 hours for DenseNet-121 for 100 epochs. Inference for HoG-Net is <2 seconds per case, in line with DenseNet-121."
https://papers.miccai.org/miccai-2024/390-Paper4020.html,"We sincerely appreciate the insightful reviews provided by the reviewers for our paper. We feel pleased for the reviewerâs acknowledgment of the contribution of our paper. We highly value the constructive comments regarding the writing quality and we will carefully address each suggestion to improve the quality of our paper.

For the writing. (#R1, #R3, #R4)
We acknowledge the need for further polishing to enhance clarity and readability. We will make the suggested corrections including refining incomplete sentences and correcting grammatical errors. (#R1, #R3)
We will carefully revise the captions and contents of some figures to improve understanding for readers.(#R3)
We also recognize the importance of presenting more detailed information within the main part of the paper. We will try to reorganize the introduction, method, and evaluation sections of our paper to provide a more comprehensive understanding of our approach.(#R4)

For the missing figure in page 5, section 2.2. (#R3, #R4)
We sincerely apologize for not thoroughly proofreading our manuscript before submission, which resulted in the occurrence of such mistake. As mentioned by #R4, the missing reference corresponds to Fig. S1 in the supplementary material. Due to space limitations, we moved the discussion on Î» from the main paper to the supplementary material, inadvertently forgetting the reference in the main paper. We will take the reviewerâs advice and move this part to the main paper. We will meanwhile restructure the main paper and make it more informative and understandable.

Fig. 3b illustrates the influence of the scale ratio (1-Î´,1+Î´). Does the sentence mean that the scale ratio is in the interval (1-Î´,1+Î´) ? If so, then why should the scale ratio be in that interval? (#R1)
Sure, the sentence means that the scale ratio is in the interval (1-Î´,1+Î´). The idea of our design is consistent with the random crop strategy during pipeline stage. We vary the crop size within a certain range to ensure that the cropped feature blocks can capture features of diverse scales. If Î´ is set too large, it will result in significant variability in the size of the cropped feature maps. Too small crops may fail to capture large lesion details, while too large crops would make it challenging  to learn features of small lesions. In other words, overly large Î´ will introduce more uncertainty. Meanwhile, overly small Î´ will inhibit the model from learning multi-scale features. So we carried out an ablation study as shown in Fig. 3b and finally set Î´=0.25 to achieve the best result."
https://papers.miccai.org/miccai-2024/391-Paper3209.html,"We would like to thank the reviewers for their positive impressions of our work and address the major points raised by them:

[R1,4] Justification/motivation for the proposed architecture over simpler methods: HuLP is built on the idea that âpredicting the future (the will-be) is more difficult than describing the present (the what-is).â In clinical contexts, clinicians are often trained and more confident in diagnosis (i.e. detecting diseases from EHR and medical images) than prognosis (i.e. predicting future survivals of patients). This is because the former is apparent, while the latter is uncertain and depends on ingesting abundant data to make accurate individual predictions. Conversely, a trained neural network may be limited by the number of samples per concept to make confident diagnoses, but can complement a clinicianâs workflow by being able to ingest all available information to predict individual patient survivals. This is the basis for which we allow clinicians to interfere with the modelâs prediction of EHR concepts and for the model to dynamically adjust its prognosis in response. In doing so, both clinicians and HuLP can benefit each other.

[R1,6] Clarification on test-time intervention: There appears to be some confusion regarding the role of p (and human input) during test time. To clarify, p is not random but learned during training. During training, p is randomly replaced with the ground truth label [0,1] with 25% probability per concept. During testing, a clinician may impart expert knowledge by replacing p with [0,1] to denote certainty in the absence/presence of a concept. For uncertain concepts, they may leave p as is. In Table 2, we emulate cliniciansâ intervention by replacing p with the ground truth labels. For missing concepts, we treat them as clinicians being uncertain of the absence/presence of the concepts and thus leave p as is.

[R1,4,6] Justification for the choice of baselines: DeepHit and DeepMTLR are chosen because they are both top-performing discrete survival methods in prognosis. We find these baselines appropriate as our HuLP implementation is also discrete. Fusion is chosen as a multimodal baseline and also because it won the HECKTOR competition, the same dataset used in this work. Compared to Fusion, HuLP distinguishes itself in its ability to perform implicit imputation from the images - in line with clinician workflows - rather than performing hard imputations. Additionally, rather than learning separate embeddings for EHR and images, HuLP differentiates itself in being able to jointly learn the information from both in such a way that the EHR guides the model of the imaging features to be extracted to inform prognosis.

[R1,4,6] Further optimization and experiments: The reviewers rightfully remark that there are multiple facets of possible investigations to optimize the method to improve the results. We did not investigate beyond the design choices mentioned in the paper, as we found the results to already be promising. Our work thus presents a methodological novelty with the potential to be optimized further. R6 mentions that analyzing the effect of incorrect human intervention would be a good ablation. We could not explore this in the context of this work but will keep the valuable recommendation in mind for future work.

[R4] Data exclusion: For fair comparison, we performed the same exact preprocessing, including data exclusion, for ALL baselines and HuLP experiments.

[R6] Code availability: To maintain anonymity, we put a placeholder link in the initial submission. The GitHub repo is updated in the camera-ready version."
https://papers.miccai.org/miccai-2024/392-Paper1531.html,"We would like to thank the reviewers for their time and for their helpful and constructive feedback.
R1, R3 and R4 highlight the advantages of synthetic data generation - inherent ground truth, scalability, and generalizability to real images. R1 and R3 acknowledge the quantity of synthetic frames generated. 
R3 notes our dataset accounts for mutual occlusions between hand and tool, and our generative model that enhances grasp realism. R4 acknowledges our novel sphere-based camera viewpoint generation that includes egocentric and non-egocentric perspectives, which was key to improving generalizability of our dataset.
We address the reviewersâ concerns as follows:
R1: We opted for Euler angles in our sphere-based camera view angle generation as the most straightforward method for camera orientation. The sphereâs division into latitude floors was chosen to control camera placement and minimize frame redundancy, rather than achieve perfect circle uniformity. The factor of 2 prevents overlap between circles, enhancing visual coverage. Uniform sphere sampling will be considered in subsequent enhancements of our model.
R1: Our hand model is based on a 3D skeleton estimation formed of 21 joints that was standardised in Erol et al. Obman, HO3D datasets and MediaPipe (https://tinyurl.com/handlandmarker) use similar annotations to measure the 3D joint error. Our task is primarily focused on 3D localising the hand and the probe as in the previous mentioned datasets (in reference to the camera location); however, the angle deviation can still be computed from the 8 keypoints as the error is measured from the 3D distances between predicted and ground-truth.
R3: Dataset Generalizability
We have actually tested on 2 models. One based on ResNet-50, similar to DeepPrior and another one based on HopeNet. We will further clarify in the final version. HopeNet has already shown great generalizability on other hand-object datasets like FHDB and HO3D.
R4: Clinical significance and practical applications
We outline the potential of our work for medical education, particularly in obstetric ultrasound, by utilizing mixed reality technologies for training and skill assessment, which is directly relevant to clinical practice. Improved hand and probe tracking may support the development of standardized training protocols, reduce the learning curve and aid automated assessment of clinical performance. We will revise this section to clarify potential clinical applications.
R4: Clarifications Table 1: The last column shows whether the dataset is clinical and lists the number of clinical tools used. Comparison to other datasets: ObMan is not clinical and features grasps on household objects. POV-Surgery and Hein et al focus solely on egocentric viewpoints. 
Our dataset includes realistic backgrounds, surgical gloves, and clinical tools. Spherical viewpoint sampling provides near-uniform distribution around the viewpoint.
Compared to ObMan and Hein et al we provide improved depth images by combining hand and object in relation to each other. We will amend the text to clarify these aspects. 
R4: Explanation of formulas 1 and 2: The terms r_{circ} and r_{sph} represent the radii of the surface circles and the sphere, respectively, which are illustrated in Fig. 2. We defined each parameter and give value ranges in the supplementary material, Table 1. 
Regarding reproducibility, we have provided a link to the HUP-3D dataset. Our manuscript outlines the grasp generation and rendering pipeline in Section 2, complemented by Fig. 1 and 2 for clarity and builds on well-documented prior works [7, 8] with available code. We will release complete source code with a detailed installation guide upon paper acceptance, ensuring full reproducible. Our intention is to facilitate easy reproduction and adaptability of our method, fostering further research in this domain. We are grateful for your insights and guidance, which will improve our work on this paper and beyond."
https://papers.miccai.org/miccai-2024/393-Paper1638.html,"We gratefully thank the reviewers for their remarks and suggestions. We appreciate the encouraging comments like âwell-organized paper, well-justified model design, clear and compelling results; addresses an important problem in MSK segmentation that has applicability even beyond BCRLâ of R1, âthe paper is well structured, and has high novelty in terms of the methods and rigidity of the experimentsâ of R2, and âthe proposal to solve complex layers and structure bias in MSKUS is novel; the improvement of the proposed method was solidâ of R3. For the questions and concerns, our responses are given as below:
(1) Q: âDefinitions of the polar transformation and relationships to the settings in the imaged structure geometryâ for R2
A: Exactly, considering the complex and hybrid-layer morphology, horizontal or irregularly curvilinear tissue layer structures in the MSKUS images, to promote the understanding of the hierarchical structures for segmenting, we convert the Cartesian coordinates of image pixels into polar coordinates, then calculate hierarchical-consistency positing embedding according to formulas (1) - (2), and combine structure-biased constraint for the final attention calculation. Empirical results prove that the proposed HSformer outperforms other segmentation models and has potential for wilder clinical application. The specific conversion process will be demonstrated in the publicly available code.
(2) Q: âExplanation of image/acquisition differences between the internal and public databasesâ for R2
A: The differences between internal and public datasets are as follows: Firstly, the collection equipment is different. The public dataset uses Alpinion E-Cube 12 system (Bothell, WA, USA) with L3-12H high-density linear probe for MSKUS imaging, while the internal dataset uses a SonoScape E2 machine; Secondly, as shown in Figure 5 and Figure A3 in the appendix, the anatomical structures of the MSKUS images from the two datasets are consistent, with a large amount of speckle noise and shadow artifacts that pose greater challenges to the segmentation task. Although there are significant differences in image quality and style between the two datasets, the proposed HSformer achieves a mean DSC of 0.90 on the public dataset, and 0.82 on the internal dataset without model training, which demonstrate the strong generalization of our model."
https://papers.miccai.org/miccai-2024/394-Paper2109.html,"We sincerely appreciate the reviews of all 3 reviewers and firmly believe that the core scientific contributions of our work are of interest to the MICCAI community and that they can be effectively communicated through minor edits.

RESULTS:

Regarding R4âs concern that âexperiments do not show a clear benefit over simply using a properly chosen fixed scale modelâ: We focus on controlling and reducing resource requirements dynamically at inference time without quality degradation. Using HS allows adapting the inference resolution to the hardware, time, or image requirements without deterioration in segmentation quality for a large resolution range and without having to a priori choose a fixed resolution for training.

As requested by R3, our claims regarding relative segmentation quality are now softened to that all 3 methods FS, AS, and HS yield similar performance on a large central section of the considered intervals. Yet, we note that HS is more robust than AS on each end of the resolution interval, particularly on the BRATS and MM-WHS datasets.

Regarding the performance drop at higher resolutions mentioned (R3, R4), we clarified that this (already stated) limitation can also be mitigated by resampling the data to a lower resolution, as it is done by FS. We thank R3 for their input, note that receptive fields can be extended via increased depth and/or kernel size, and added references [4,5] as future research avenues.

All reported inference time and GPU memory numbers cover the full pipeline, for the hyper-network including the forward pass and UNet instantiation for the HS evaluation. We believe this is the analysis mentioned by R4 that âcould have been an important contribution of this paper.â. We further clarified this at the end of section 4.2.

Training HS VS UNET: We thank R3 for the reference on HNâs slower convergence. Yet, in our experiment, all networks converged within the same order of magnitude of steps. We hypothesize that the structure shown by the CKA analysis allows a convergence at a similar rate. Also, the HN adds negligible memory or wall time overhead to the UNet training. We added this discussion at the end of section 4.3.

NOVELTY:

Regarding limited ââ¦novelty of the paperâ¦â (R4.6), we do not introduce a fundamentally new concept but design a novel, simple, and effective solution to a rather overlooked yet relevant topic to the MICCAI community. We believe that our paper furthermore contributes:

A method for distributing a non-discrete model generator that can be rapidly and smoothly adapted to the compute requirements and constraints.

The experiments performed on 3 diverse datasets were conducted without extensive hyper-parameter tuning and using standard training practices. This shows that the simplicity of the training of UNets translates to hypernetworks (that are sometimes very hard to train as mentioned by R3).

Adapting the network to the data at inference comes at a negligible cost, dwarfed by potential savings at coarser resolutions.

As pointed out as a strength of the paper by R4/5, the CKA analysis of HSâs output space provides interpretable insights allowing a deeper understanding of internal representations in UNets.

REFERENCES:

We thank R3 for providing several relevant additional citations. We added [3,4] in the related works section and [5] when mentioning future works.

REPRODUCIBILITY:

R3 expressed some concerns regarding the reproducibility of our results which we consider very seriously. We note that the training parameters (including optimizer, scheduler, number of steps) and UNet architectures are identical across experiments and are very standard. We added further details on the training procedure and architectures in section 4.1. We will provide full implementation details in the public repository, sufficiently documented for reproducing our findings."
https://papers.miccai.org/miccai-2024/395-Paper0897.html,"We appreciate the reviewersâ comments and insight suggestions, especially some interesting and meaningful proposals. They appreciate the innovative, well-perspective, and community-extendable of our work. Our responses are as follows:

CHOICE OF MASKING PATCHES & RATIO:
Thanks for the suggestion by reviewers R1 and R3. Itâs interesting to discuss the way of masking patches. An appropriate patch masking strategy could further improve HySparKâs representation learning capabilities. In the paper, we apply random masking of the patches and we will add the masking detail in the camera-ready version. In addition, for the masking ratio, we utilize the MedNeXt+ViT as a HySparK pre-trained network with three different ratios (i.e. 25%, 50%, and 75%). We believe that exploring the optimal mask ratio of HySparK is a meaningful topic, but it may depend on different hybrid network architectures, data volumes, and distribution which requires large computing resources to search. We provide a coarse-grained masking ratio scope in the paper and it shows an interesting trend. We believe these proposals could open the way for the community to research various patch masking strategies and more fine-grained mask ratio searches for HySparK.

PERFORMANCE:
Thanks to the reviewers for their comments on performance. In the BTCV dataset, we use MedNeXt+ViT as our pretraining network, which achieves an average Dice of 78.58% without pretraining, only 0.63% lower than MedNeXt pre-trained by SparK. We believe it is a hard task to further boost performance under high-level segmentation results. Compared to without pre-training, our HySparK could achieve an average 2.09% improvement and achieved meaningful results in some difficult segment organs, such as the bladder (5.45%), the stomach (3.50%), and the adrenal glands (2.39%). In MSD datasets, compared to SparK, HySparK improves Pancreas and Hepatic Vessel lesions by at least 1.81% and 1.60%. Moreover, we believe that downstream performance partly depends on the specific network design. HySparK serves as the foundational upstream method that can be pre-trained with any hybrid architecture with top CNN and bottom ViT and we believe it can inspire more interesting hybrid network design and other extensions downstream tasks discussions in our community.

DATASET:
Thanks to reviewers 2 and 3 for their suggestions on datasets. It is a very prospective proposal to apply our proposed Hybrid Sparse masKing method to other modalities. HySparK is a universal method that can be quickly extended to ultrasound, MRI, and other modalities that could maximize the advantages of hybrid architecture. In addition, we think HySpark in cross-domain issues that pre-training on large-scale CT and performing downstream tasks on completely different datasets (e.g. MRI) still need to be explored. Moreover, we will release the official code and weights in the camera-ready version, welcome everyone to follow and inspire more interesting community work.

OTHER CONSTRUCTIVE COMMENTS:
Thanks to all reviewers for their constructive comments on the paper description (R1, R2), grammar (R1, R3), and experimental settings and analysis (R1, R2, R3), which is very important for us to improve the readability and quality of the paper. We will correct, revise, and improve those in the camera-ready version."
https://papers.miccai.org/miccai-2024/396-Paper1184.html,N/A
https://papers.miccai.org/miccai-2024/397-Paper2701.html,"We thank all reviewers for their valuable and insightful reviews. They described our method as âquite innovativeâ (R3), âsolid studyâ (R4), and âthorough comparison to state-of-the-art architecturesâ (R1). Here we address their main concerns:

Motivation of Instance-aware Guided Module (R4)
Using FFT to analyze feature correlations in the frequency domain can enhance the contrast of CCTA images by distinguishing between broad anatomical structures (low-frequency features) and fine CAC details (high-frequency features). However, not all low or high-frequency information is contributing to CAC segmentation. Therefore, we propose an Instance-aware Guided module that adaptively determines which frequency information should be retained. We agree with R4 on providing feature examples to explain how FFT affects segmentation, which will be explored in an extended version.

Recent study on CAC segmentation in CSCT(R1)
1) We agree with R1 that deep learning methods in CSCT should also be discussed, such as the methods [a, b, c, d]. Specifically, [a] proposed a two-stage CNN pair to identify CAC from coarse to fine; [b] uses Unet to segment CAC; [c] and [d] focus on coronary artery segmentation and then combine coronary artery information and voxel intensity values to identify CAC. We will discuss these methods in the final version;
2) References that are miscited or standard operations should be edited in the final version.

Clarity and notation of the method (R1&3)
1) In clinical settings, CACs are evaluated individually, which implies that CAC segmentation should be formatted as instance segmentation. Additionally, small CAC instances tend to be overlooked in semantic segmentation (R3);
2) The position-wise feed-forward network applies a fully connected network to each position in the sequence, adding non-linearity and enhancing the modelâs ability to capture complex patterns (R3); 
3) Sorry for the confusion. The ViT box in the instance-aware guided module box should be image embedding. We will revise Fig 1 in the final version;
4) The weight of InstanceViT is obtained by a series of two 1 Ã 1 convolution layers, BN and GELU (R3);
5) Inputs of ViT and InsViT are the same (R3); 
6) The notation errors and inconsistent formats will also be corrected with minor modifications (R1&3).

*Data annotation and Analysis (R1)
1) Our data were independently annotated on CCTA by a radiologist and a cardiologist via 3D Slicer. The Dice between each annotation and their union measured annotator preference. Inconsistencies were rechecked. We will revise the data annotation in the final version;
2) Accurate total CAC volume estimation correlates with a higher Dice score, which indicates better overlap between predicted segmentation and ground truth. We will revise Tab.1 by adding total CAC volume analysis for clinical CAC quantification in the final version.

*Experimental details and Reproducibility (R1&4)
We will release the code in the final version, ensuring the reproducibility of the work (R1&4). We will revise experimental details in the final version to clarify the following points:
1) The encoder-decoder block of our method is the same as the standard Unet [3] (R1);
2) The âlung trunkâ is the major bronchial structure. We used a threshold range from -224HU to 600HU for coarse-segment the lung, then used the seed-filling algorithm to fine-segment the lung. Subtracting the lung from the original image can eliminate noise to better segment CAC (R1);
3) The pixel threshold for detection is 0.5 (R4);
4) Two datasets are used separately for training/testing. In the ablation study, when validating the InsViT module, we used the combination of Unet and InsViT. We replaced InsViT with ViT when validating the Guided module (R1).

[a] 10.1016/j.media.2016.04.004
[b] 10.1148/radiol.2021211483
[c]10.1007/s10554-014-0519-4
[d]10.21037/qims-21-775"
https://papers.miccai.org/miccai-2024/398-Paper0495.html,"We are grateful to the reviewers for their feedback and approvals of our methodâs novelty, strong performance, and potential benefit to the pathology community. Our aim was to propose/plant a seed work to open the door for survival analysis in the immunohistochemistry (IHC) modality; weâre glad the reviewers found our results promising and interesting. We also appreciate the suggestions to improve clarity and will incorporate them into revisions.

Evaluation
[R1,R3,R4] Single Dataset - The most salient critique was that our evaluation involved a single IHC dataset. We understand this is suboptimal. However, when no publicly available IHC datasets exist, we worked hard to curate an extensive dataset for evaluation and future public release (after legal approvals). Our data targets pancreatic cancer and has over 10TB of images from a cohort of ~1000 patients, each with multiple IHC images. We ensured diversity in ages, genders, TNM staging, tumor differentiations, and pancreatic parts. The final dataset was curated after filtering for quality and represents one of the first largest IHC survival datasets to be.
[R3] Public Comparisons â We thank R3 for the suggestion to adopt TCGA benchmarks. Our main reasons for not doing so are: 1) our method targets the IHC modality while TCGA only contain H&E images, and 2) some proposed components (e.g., cell count feature enrichment) use priors exclusive to IHC images which may prevent direct fair comparisons.
[R4] Significance of Results â We thank R4 for suggesting stds in our main evaluations and will add them: 0.0117 for CD8 & 0.0142 for CD4+CD8. All of our CI scores in Tab. 1 are statistically significant (using a paired t-test between our CI and the 2nd best) with p values far below 0.05. We also observed score variance, however, with stronger regularization and tuning, we were able to keep std to under 1.5% CI across different random seeds. We also highlight that stratification studies are more stable and our method is the only one to achieve statistically significant separation.
[R1,R4] CD8 for Single Stain â We used CD8 for two main reasons: 1) CD8 was a stronger baseline to compare multi-stain against (CD4: 0.5716 CI), and 2) CD8 has more evidence in literature to be a potent prognostic predictor given its role in quantifying tumor infiltrating lymphocytes while CD4âs prognostic value originates from its interaction with other immune-targeted stains.

[R1] Method Motivations - Our method for patch feature extraction via clustering is commonly used (e.g., AttnMISL) to select representative patches while reducing the number of patch inputs. Although patch embeddings contain spatial information, we are still tasked with making patch processing tractable, maintaining spatial context around patches, and introducing useful implicit biases (region-level processing) in the face of sparse learning signals. Our initial experiments ran without clustering (random patch subsampling and selecting the most cancerous patches), both were worse than k-means and substantially worse than our proposed clustering scheme. Further, we agree that IHC-specific features via pretraining may improve performance, but our paper focused on a survival framework where patch features may be extracted in various ways. The use of ImageNet features is common in literature (e.g., CLAM). We also tried using HIPT features but was significantly worse (0.5566 CI for CD8), likely due to the lack of IHC stain colors in H&E images.
[R3] Clustering Novelty â Other pathology works have utilized coordinates directly to inform clusters. However, we use them to formulate similarity as affinity scores between patches via spectral clustering which allows for more fine-grained control over semantic and distance weighing.
[R4] N_k Choice â We selected N_k=400 patches per cluster based on a target region size of 2.5x2.5 mm which was deemed appropriate by clinicians to balance cell-level and tissue-level information (stated in 2.1)."
https://papers.miccai.org/miccai-2024/399-Paper3797.html,"Response to Reviewer #1
Concern on Data Leakage:
Reviewer #1 highlighted the criticality of data splits due to the multiple views and masses in mammographic exams. We confirm that our dataset was split at the patient level, ensuring that all images, views, and masses associated with a single patient were contained within the same subset, thus effectively preventing data leakage and maintaining the integrity of the modelâs evaluation.
Reproducibility and Data Access:
Our study utilises in-house data, and we are committed to providing access to this data and the computational code upon reasonable request. This approach maintains confidentiality while upholding transparency necessary for scientific validation.

Data Description:
We curated our dataset to ensure high quality and control, processed according to BI-RADS standards by expert radiologists. A more detailed account of our data collection, cleaning, and ROI extraction was indeed included in our paper to provide clarity on our rigorous data management.
Statistical Analysis Clarity:
We performed T-tests to assess the statistical significance of our model comparisons, with p-values such as 1.22Ã10^â12 for the IHRRB-DINO vs. CAM with Resnet50 indicating significant differences. These details underscore our commitment to rigorous statistical evaluation.
Limitations in Technology Used:
We acknowledge the use of 2D over 3D mammograms due to page constraints, and will ensure to address this discussion more thoroughly in future discussions to provide clarity.
Combined Response to Reviewers 4 and #3
Explanation of CDN and Noise Addition:
Both reviewers noted the need for a clearer explanation of CDN and our noise addition strategy. CDN is defined by two hyper-parameters, lambda_1 and lambda_2, which control the noise scale for positive and negative queries, enhancing the modelâs accuracy. This bifurcation is critical, as positive queries aim to reconstruct ground truth boxes, while negative queries predict âno object,â helping to refine model training against irrelevant anchors.
Evidence of DINOâs Effectiveness:
Questions regarding the proof of DINOâs impact on model performance were addressed by detailed comparisons in our manuscript. Results from our tests, such as a T-statistic of a p-value of 1.59Ã10^â17 comparing IHRRB-DINO vs. ACOL with Inception v3, demonstrate significant improvements and validate DINOâs efficacy.
Response to Reviewer #4
Clarity and Novelty in Methodology:
Reviewer #4 raised concerns about the clarity around the DINO component and its naming, which might be confused with a known self-supervised learning method. DINO, in our context, stands for Data-Driven Instance Noise, a unique adaptation for object localization in breast mass detection. It enhances the model by introducing instance-level noise during the training phase, significantly improving detection accuracy.
Baseline Comparisons:
Our studyâs primary objective was to explore the impact of integrating various pretrained models(swin-l, â¦ etc) with our novel approach. We believe this focus allows us to contribute valuable insights into the potential enhancements these models can bring to breast mass detection methodologies. While we recognise the merit in comparing with existing tools, our intent was not to establish superiority but to highlight how pre-trained models can be effectively adapted within our framework to improve detection capabilities.
Conclusion
The comments from all reviewers have been invaluable in refining our presentation and explanations of complex methodologies. We have addressed each point raised, ensuring that our research contributions are clear and well-supported by empirical evidence. The discussions on CDN, data handling, and statistical validations are aimed at providing a comprehensive understanding of our workâs robustness and innovation."
https://papers.miccai.org/miccai-2024/400-Paper0484.html,"We thank the reviewers for their constructive feedback and positive assessment!
We will publish our code with the camera ready version.

Maximally informative images (R1).
We generate graphs for each class separately, then select the same number of images from each graph. This process is explained in the text and the Algorithm 1.
We train a class-conditional diffusion model and generate synthetic images with labels. We do not generate pixel-wise labels for segmentation.

Privacy protection (R1, R3).
The diffusion model is trained on real data and learns the data distribution. It can be trained locally once without sharing it publicly.
Then we can generate synthetic images, but these images are different from any real images, even though they contain some realistic features. Therefore, there is no personal information in synthetic images.

Infomap algorithm (R1).
The Infomap algorithm is detailed in [3]. It is a community detection algorithm capable of identifying communities in a graph.
The representation space refers to the space where images are projected to features. The graph is constructed using these features, which are obtained from the output of the penultimate layer of a classifier or through UMAP. In the graph, the nodes represent image features, and the edge weights correspond to the Euclidean distance between nodes.
After constructing the graph, the Infomap algorithm is executed to generate communities (or clusters). Subsequently, we uniformly select images with high modular centrality from each community.

Modular centrality (R1).
The scalar score of modular centrality combines two scores to quantify both the intra-community and inter-community influence.

The size of X_g (R1, R3).
Our goal is to capture the input data distribution as comprehensively as possible. To achieve this, we generate a large number of synthetic images and then select a small subset of representative images.

Results table (R1).
Some results are missing because the corresponding papers do not provide the results for this metric.
We will differentiate between the two âdistillationâ methods in Table 1.

Hard coded hyperparameters (R3).
We conducted an ablation study on these hyperparameters and selected the best combination to report the results.
We opted not to include the table in the paper because of redundant evidence.

Computation cost (R4).
We will expand on the computation cost in the final paper."
https://papers.miccai.org/miccai-2024/401-Paper2884.html,"To Reviewer #1:
a) We used ReLU and sigmoid as activation functions of the neurons in the model.
The ReLU function designed in the modularity-guided interactive network facilitates faster learning and helps avoid the vanishing gradient problem. We also attempted to use the leaky ReLU function with a leak rate of 0.33 as a replacement for ReLU. However, this led to a significant decrease in performance. Our choice of the sigmoid function in the MLP-based attention module is intended to softly map the attention weights to the range (0, 1), with higher contributions above 0.5 and lower contributions below 0.5.

We are able to furnish additional empirical studies, such as convergence curves and reconstruction accuracy metrics, to substantiate the effectiveness of the Adam optimizer within the context of your model.
As the reviewerâs suggestied, we will provide the training and testing curves in the supplementary materials.

Due to the page limit, we omitted the details of cross-validation. We conducted 5-fold cross-validation to validate model performance. We ran 10 runs of cross-validation and reported average and standard deviation of cross-validation accuracies. To avoid overfitting, we used dropout layers after the readout layer and between two fully connected layers in the output layer. Moreover, we adopted L2 regularization with a weight setting of 0.001 and utilized early stopping to prevent overfitting. We will add this information in the final version.

To Reviewer #2:
Our modelâs generalization capability was demonstrated on both HCP and PPMI datasets. As the reviewer suggested, we plan to apply the model to ABIDE 2 and COBRE datasets for autism disorder and schizophrenia classifications in future work.

a) Rename the results section
We will rename it as âExperiments and Resultsâ in the final version according to the suggestion."
https://papers.miccai.org/miccai-2024/402-Paper3212.html,"We thank all reviewers for their valuable input and constructive criticism. Their feedback has provided us with additional insights that will help us improve our work. We are happy to address the concerns and inputs of the reviewers:
Reviewer #1 suggested strengthening the motivation and value of INRs in the motion correction process:
INRs are valuable for MRI motion correction because they enable unsupervised instance-specific optimization, eliminating the need for large datasets and reducing the risk of sharing sensitive patient information. Unlike deep learning-based methods, INRs prevent hallucinations and preserve critical anatomical information, ensuring high fidelity in motion-corrected images. The kLD-Net enhances this process by grouping motion-corrupted lines into n detected movements, varying for each instance. Accurate prediction of these motion groups is crucial for overall performance. Future work will study the impact of each part in an ablation.
Reviewers #3 and #4 suggested discussing the ablation of different encodings and INR Architectures:
We plan to investigate various encoding methods for motion representation. Currently, hash-grid encoding is preferred for static intensity prediction due to its resistance to spectral bias and ability to quickly fit high frequencies, enabling faster optimization. This reduced computational complexity is advantageous for extending to 3D motion correction. In future work, we plan to explore other encoding techniques, such as Fourier Features encoding, which may be more suited for interpolation. Additionally, we aim to study the potential combination of different encoding techniques for spatiotemporal data. For instance, hash-grid encoding could be used for spatial encoding (2D), while Fourier features could be employed for the smoother temporal direction.
Reviewer #3 suggested an ablation of gradient entropy vs. total variation (TV):
Both regularizers showed similar denoising effectiveness in previous experiments. We used Gradient Entropy because it resembles the Autofocusing algorithmâs entropy-based metric.
Reviewers #1 and #4 suggested discussing the Computational Complexity, Runtime, and Extension to 3D:
We did not include runtimes in this manuscript due to space constraints. The inference time for IM-MoCo on a 2D MRI image is approximately 20-30 seconds. Extending our approach to 3D would result in runtimes within the range of several minutes, which remains clinically acceptable. This extension would require minor architectural adjustments and evaluation of different encoding strategies. Further research will assess IM-MoCoâs effectiveness in handling more complex motion patterns in 3D data, including correcting non-rigid movements such as breathing in cardiac and abdominal MRI.
Reviewer #3 mentioned that training details for the kLD-Net could be missing:
We would like to refer to the âImplementation Detailsâ subsection within the Experiments section for specific training information on the kLD-Net:
â¦ Implementation Details. The kLD-Net was trained using the Adam optimizer with a learning rate of 1eâ4, for 4200 epochs, and a batch size of 4 â¦
Reviewer #4 encouraged future work on multi-contrast motion correction:
We did not consider employing different contrasts in this work, but this approach is interesting. It could benefit the 3D case where low-resolution out-of-plane views and complementary contrasts are employed (as in the mentioned paper). Assuming all contrasts are affected by the same motion artifacts, complementary high-resolution in-plane views could be used to fit the motion INR to a common motion model in all planes, guiding a multi-contrast image INR to learn motion-free high-resolution 3D scans.
Repository Access:
We apologize for providing a non-functional link to the code repository. The correct link is https://anonymous.4open.science/r/MICCAI24_IMMoCo-671E"
https://papers.miccai.org/miccai-2024/403-Paper3757.html,"Dear Reviewers, Area Chairs and Program Chairs,Â 
Â 
We thank the constructive feedback provided by all the reviewers on our application manuscript. In this rebuttal, we group and address the major critiques that the review process detected and provide clarification on decisions previously unclear in the text.Â

We thank Reviewers #1 and #3 for their observations and questions regarding using the PLIP foundational model in analyzing multiplex immunofluorescence (mIF) images. Closely relevant researchÂ conducted by Yu et al. (âA Multi-Granularity Approach to Similarity Search in Multiplexed Immunofluorescence Imagesâ, 2023) supports using PLIP for feature extraction and similarity search in mIF images. Regarding the concerns about transfer learning, to validate our selection of PLIP, we used PCA and t-SNE to study the feature embeddings extracted by PLIP from mIF images, which showed distinct clustering patterns of the image embeddings. The dispersion along the principal components suggested that PLIP can capture significant variance and underlying structures within the mIF images, indicating that the abstract features learned from H&E images (e.g., cellular morphology and tissue architecture) are transferable to mIF images. We did not include these details in the original submission due to the 8-page limit and considered that the validation of PLIP on mIF images is not the main goal of our research.

Regarding the applicability to H&E-stained Whole Slide Images (WSIs) mentioned by Reviewer #3, while our current pipeline is optimized for mIF, it contains flexible components that can be adapted to different staining techniques. These elements include pre-processing stages that can be tailored to handle the specific characteristics of H&E staining, thereby extending the utility of our workflow to a broader range of pathological analyses. Moreover, the PLIP modelâs existing proficiency with H&E WSIs complements this flexibility, ensuring our methodology remains versatile and broadly applicable.

As to the validation of the Regions of Interest (ROIs) selection (Reviewer #1), we agree with the reviewer that the end goal is molecular profiling. However, without a clear optimization goal, the process can result with many candidate ROIs. Before digging into such complexity, we chose to start simple and retrospectively assess the pan-cancer automation of the process. Generating an AI-based spatial molecular profiling is within our overarching goals, but it requires more time for molecular assays, and it is outside the specific scope of this manuscript. Our study is designed to use expert-selected ROIs as a benchmark to learn from pan-cancer and clinically relevant data, enhancing its applicability. We show that AI-selected ROIs closely match pathologistâs selections in terms of consistency and coverage, this being a crucial step in molecular profiling, where precise ROI selection impacts the detection and quantification of biomarkers.

On the specifics of the training details (Reviewer #3), we used a YOLOv8-S model with a batch size =64, an image size of 1280 pixels, learning rate =0.01, momentum =0.937, and a weight decay = 0.0005 for 1000 epochs with a patience=200. After multiple runs with different values, we achieved the following training scores: box loss of 0.523, class loss of 0.399, and distribution focus loss of 1.001. The validation scores were box loss =1.686, class loss =0.985, and distribution focus loss =1.51. The images were tested at multiple magnifications (20x, 10x, and 5x), with 20x achieving the best results in the main submission.Â

After approval, these changes can be added to the main submission if needed. By addressing these points, we aim to clarify our application, underscore its robustness by applying SOTA models within a rich dataset, and highlight the potential of our study."
https://papers.miccai.org/miccai-2024/404-Paper0885.html,"We thank all reviewers (R1,R3,R4) for their comments. The main concerns are grouped and clarified as below:
Q: Technical Contribution and Method Discrimination(R4)
We introduce DGCI to model high-fidelity airway in the continous space. Our contribution are: 
1) Proposing intrinsic topological consistency within the same class to regularize implicit shape modeling via a reversible deep correspondence flow.
2) Utilizing implicit geometric regularization to promote smooth and high-fidelity representations.
3) Generating airway shapes with state-of-the-art topological fidelity, beneficial for downstream applications (skeletonization and breakage repair).
Unlike DeepSDF, which focuses solely on implicit surface reconstruction, DGCI addresses underlying topological constraints within the same category, particularly for fine-scaled airway structures. Our reversible correspondence flow leverages the inherent topological consistency of airway structures, which are single-connected tree structures. In addition, while DeepSDF only constrains SDF values, DGCI incorporates implicit geometric regularization to ensure smooth surface reconstruction without sacrificing fidelity, allowing precise modeling of fine structures.
Q: Ablation Study (R1&R4) 
A: The proposed DGCI is inspired by DeepSDF, further enhanced with the novel reversible correspondence flow and the implicit geometric regularization. Without these modules, our method degrades to DeepSDF. Thus, the results of DeepSDF serve as an important ablation study. Tab.1 reports proposed modules boost more than 10% improvement of TD,BD, and Dice. Fig.4 qualitatively presents the improvement in airway modeling.
Q: Clinical Significance and Experimental Setting(R3)
A: For clinical significance, automatic preoperative planning for the navigation of endobronchial interventions can save significant effort and provide valuable reference for radiologists. The discontinuity disturbs automatic preoperative path planning algorithms, leading to interrupted trajectories and wrong results for radiologists. This motivated us to design DGCI for smooth, high-fidelity representations of fine-scaled airways. For training, we trained on a Linux workstation (Intel Xeon Gold 5119T CPU @ 1.90GHz, 2 NVIDIA Geforce RTX 3090 GPUs). During inference, our method only needs ~10s / per-case, revealing that the computational complexity is not heavy.
Q: Framework Workflow(R3&R4)
A: In this work, we introduce the DGCI to model fine-scaled airways in continuous space, achieving high-fidelity performance. DGCI not only enhances the continuity of airway modeling but also aids in its skeleton extraction and breakage repair. Hence, Sec 2.1 and 2.2 detail the design and optimization of DGCI, while Sec 2.3 discusses its use in downstream applications. For the airway breakage repair, we design the implicit fine-tune framework. The main innovation of this fine-tune framework lies in a unique data refinement scheme that prevents overfitting to fracture patterns. While disconnectivity is not a new issue in airway segmentation, our approach addresses it from a novel continuous perspective. We have checked the results from the paper suggested by R4, our method achieved better perfomance on the same dataset, with an increase of over 2% in TD and more than 8% in BD, and also achieved competitive DSC. This can be attributed to the topological regularizations proposed in DGCI.
Q: Evaluation Metrics(R3&R4)
A: A non-manifold vertex (NM-V) is a vertex connected to three or more non-coplanar faces. A non-manifold edge (NM-E) is an edge shared by three or more faces. A non-manifold face (NM-F) refers to faces arranged in such a way that a closed and continuous body cannot be formed. Fewer non-manifold properties indicate better connectivity and smoothness of reconstructed shapes. TD and BD measure the topological completeness of reconstructed airways, defined in [25]. Detailed description of the metrics will be added in the revised version."
https://papers.miccai.org/miccai-2024/405-Paper3427.html,"We thank the reviewers for their feedback. For brevity, we are not addressing all of the minor issues in this rebuttal, but all are noted and will be addressed in the final paper.
R#1 raises concerns about generalisability of our model with only 48 patients: we would like to emphasise that this is exactly why our work innovates in data augmentation. Nonetheless, more data will soon be obtained from another site to provide further assessment of our methods in future work. In agreement with the reviewer, the novel tracer cannot be revealed in order to comply with MICCAI anonymity requirements at this stage.
R#4 requests a motivation of the usage of the DenseNet and clarification of the definition of the G-mean metric. We chose DenseNet because of its established performance on a wide range of benchmarks. For G-mean, R#4 asks that we perhaps use the geometric mean definition instead. It will be a minor update of our paper to use the terminology âG-mean scoreâ instead, otherwise leaving it defined exactly as is in the present paper. 
R#5 asks that accuracy be either removed or replaced with balanced accuracy. In our view, no single metric can give a comprehensive assessment of the modelâs performance for our dataset, hence we present a variety of metrics. Whilst a balanced accuracy is a simple calculation to make (we had in fact done this in an earlier version of the work), we believe the model can be more accurately assessed using the 5 metrics we have provided. As a minor update, we will briefly describe the complementarity of these metrics (such as how TNR and TPR show how the model performs with type I and II errors respectively).
R#5 queried if the 11K augmentation could be worse than the m* augmentation due to the 11K augmentation not containing the wrist. We investigated this before submission, and yes, to some extent the lack of wrist augmentation does affect the results , but it is not the complete story, as this was a secondary finding of our work it was not included in the initial manuscript. We can add a single line stating the effect.
R# 5 prefers the terminology âcross validationâ instead of âcross foldsâ: we agree and will make the update to clarify that the models were trained using five-fold cross validation . The numbers in Tables 1 and 2 correspond to the mean and standard deviation of the performance metrics, found over 5 different training runs, please see our further clarifications below. We will clarify this in a minor update to the paper.
R#5 requests clarification regarding parameter initialisation for our models and the use of any hyperparameter optimisation in the Perlin augmentation. All models were trained from Kaiming uniform initialisation. The hyperparameters were chosen as a trade-off between computational cost and variability of the segmentations.  We will state these in a minor update of the paper.
R#5 requests a statistical analysis given our small dataset and some of our results statements. R#5 asks us to not overstate our results, quoting our paper in their review âPerlin variation increases performance over the non Perlin version in all metrics except TNR which decreases slightlyâ. We can easily clarify in this line that these results are promising even if not yet strictly established to be significant.
R#5 does appear to misapprehend the size of the test dataset and the results presented. To clarify, the models were trained in a five-fold cross validation, each test dataset being 20% of the data ~10 patients, ~20 images per test dataset, but all results are the mean performance over the five models evaluated on the separate five datasets, so the results are based on all 48 patients, 96 images, not just 7 samples as R#5 seems to indicate.
R#5 requests that figure 6 be updated to use grid lines and zoom in on the relevant sections and that we rephrase an aspect of the paper regarding an increase in specificity levels. We agree with R#5 on these points and will make these minor changes."
https://papers.miccai.org/miccai-2024/406-Paper0915.html,"We sincerely thank the area chairs and anonymous reviewers for their invaluable time and efforts spent on our work. We are deeply grateful for their constructive feedback, and will diligently take all the feedback into account to improve the quality of this work."
https://papers.miccai.org/miccai-2024/407-Paper1234.html,"Dear Reviewers,

We greatly appreciate your efforts and constructive feedback on our manuscript. We have carefully considered each of your comments and would like to address the key concerns as follows:

Two reviewers (1 and 5) raised concerns about the clinical feasibility and anatomical correctness of the generated images, noting potential issues with generalizability across different CT scanners and diverse patient populations and clinical scenarios. We fully agree that demonstrating such generalizability is indeed a critical step for clinical deployment. However, we believe this is beyond the scope of the current study, given its initial phase in the development process. We will focus on addressing generalizability in the next phase, accommodating scanner variability, patient population diversity, and practical clinical scenarios. Advanced data normalization techniques, including multi-frequency processing, are currently under investigation in conjunction with the diffusion-based approaches.

We would like to emphasize that the superior performance of the proposed model compared to existing models has been clearly demonstrated in this study. Although the proposed method consistently showed improved performance over existing models and techniques, we acknowledge the concern raised by one reviewer regarding the potential mismatch of actual Hounsfield units (HU) and anatomical structures with the ground truth, which could cause issues if used for treatment planning. However, our goal in developing this model was not to completely replace planning CT in clinical practice. Instead, it aims to enhance the quality of daily cone-beam CT (CBCT) images used for patient positioning verification, allowing for more accurate alignment. This improvement is intended to better align the patient and monitor overall changes during the treatment process, such as tumor size reduction and patient weight loss. For these purposes, the emphasis on accurate HU is less critical than for planning and dose calculation. Therefore, we believe that this model can be effectively used in the decision-making process for treatment progress monitoring and determining the need for adaptive planning, providing clinical utility without direct dose calculation.

We also acknowledge the suggestion to collaborate more closely with clinical experts to gain a deeper understanding of clinical requirements.

Additionally, Reviewer 4 pointed out the absence of a discussion on the MICCAI SynthRad challenge and validation against its data. We appreciate this comment. Unfortunately, we became aware of the challenge dataset only recently and did not have enough time to incorporate it into our study. However, we fully expect that the proposed method will consistently outperform other approaches.

Kind regards, Author"
https://papers.miccai.org/miccai-2024/408-Paper3487.html,"We thank the reviewers for their comments and their time. We address their questions and remarks below.

We organized the paper and allocated space to highlight our main contribution in the new application domain, which seemingly resulted in an under-representation of the methodological aspect of the work. The main methodological difference between FetalSynthSeg and SynthSeg lies in the use of meta-labels (merged target segmentation labels) on which we perform intensity-based splitting tailored to deal with fetal super-resolution domain shifts. This differs from cardiac SynthSeg, which splits original segmentation labels into subclasses.

R1. Meta labels link to overcome domain shifts. The meta-label-splitting strategy is aimed at accurately mimicking tissue heterogeneity and super-resolution (SR) reconstruction artifacts and errors, thus mitigating domain shifts caused by these factors. Furthermore, using meta-labelsâ subclasses helps overcome the limitations of a small number of generation classes, which can create artificial boundaries between brain regions leading to intensity borders aligned with ground truth segmentation labels.

Preliminary analysis comparing FetalSynthSeg to the baseline model and SynthSeg showed a consistent Dice improvement across all tissues and splits with an average increase of 5.16%Â±3.71 and 4.84%Â±2.71 respectively. Notably, the only case where FetalSynthSeg performance is slightly lower (54.9Â±15.9 vs 54.5Â±17.7) than the baseline is the GM segmentation on KISPI-MIAL, which is explained by the low quality of the ground truth segmentations on this split, as mentioned in the paper.

We will shorten the supplementary material section to include a table with these detailed results, as it is important for all reviewers and provides a more detailed view of our experiments.

R4. Ablation experiments. Although in the current paper we have used the same model architecture and training schedule to ensure comparability between the experiments, we acknowledge the importance of ablation studies and architecture optimization, which are intended for an extension of this work.

R4. Fairness of Comparison (7000 images vs 35):

Both approaches are based on the same 35 real images for training on each split. FetalSynthSeg uses 7000 synthetic images generated offline from ground truth segmentation labels of these 35 images. Whereas the baseline model directly uses the original 35 T2w intensity images and performs on-the-fly data augmentation. The same augmentations are used for both models, and both models have the same amount of supervision signal and training schedule. This comparison is similar to the one in the SynthSeg paper, which evaluates whether using synthetic data generated from segmentations rather than corresponding intensity images helps in achieving domain generalization."
https://papers.miccai.org/miccai-2024/409-Paper1552.html,"We greatly appreciate the reviewers for the effort and insightful comments regarding our submission. We are encouraged by the reviewersâ positive feedback in terms of technical novelty, workload, study design, and writing. We will further correct errors and clarify all the concerns of the reviewers in the final version. We respond to the major points raised by the reviewers as follows.

Q1. The clinical relevance of generated mammogram in pCR prediction
A1. Despite the advancements of MRI in breast imaging, generated mammograms continue to play a crucial role in therapy response prediction. A multimodal approach, incorporating both mammogram and MRI, provides the most comprehensive evaluation, enhancing the precision and reliability of breast cancer management.

Q2. Pathological information features extraction
A2. This process is achieved through the application of a linear layer and the Exponential Linear Unit (ELU) activation function, to allow for the integration of pathological features.

Q3. Multi-view (CC/MLO) mammograms in pCR prediction experiments
A3. For each patient, mammograms from both the CC and MLO views are used. The extracted features from the CC and MLO views are combined to form a comprehensive representation for each patient for therapy response prediction."
https://papers.miccai.org/miccai-2024/410-Paper2899.html,"We thank the reviewers for their critical assessment of our work. 
In the following we address their concerns.

The main concern of all three reviewers is the lack of information given on the network structure and hyperparameters.
In the paper, we do indeed only state the high-level elements of our approach and provide limited details.
We wanted to focus on the novel approach to predicting diffusion coefficients rather than on the precise details of the networks we used.
Nonetheless, we fully agree with the reviewers that these details should not be omitted.
All details can be found in the source code which we will release upon acceptance of the paper.

The encoder part of our network consists of five standard convolutional layers with GeLU activations. This network is trained separately by minimizing an autoencoder L2 loss using an Adam optimizer with a learning rate of 5e-4 and a batch size of 512.

The predictor network uses a 1D ResNet with seven ResNet blocks and an initial kernel size of seven-time steps. The ResNet output is average-pooled and linearly transformed. The final layer outputs two numbers, which are mapped to positive by f(x) = 1 + ELU(x).
The first of these numbers is the diffusion coefficient D and the second is the estimated error sigma.
The loss [Eq. (2) in main text] ensures that sigma is the optimal Gaussian noise standard deviation.
This network is trained using an Adam optimizer with a learning rate of 0.01 and a batch size of 50.
As we use synthetic data, we generate data on-the-fly and never show the network the same data twice.

The reviewers expressed concern on how we defined the zero-knowledge baselines.
Our synthetic datasets consist of videos with known diffusion coefficients sampled from a uniform range [D_low; D_high]. 
Our baselines are simply the numbers that result from always predicting the mean (D_low + D_high) / 2.
In the main text, we give the example L1= <|x-0.75|> = 0.375 for the dataset with D sampled from [0;1.5], i.e. L1 = 1/1.5 * \int_0^1.5 |x - 0.75| dx.

As mentioned above, the error estimate sigma is just one of two outputs (D, sigma) of our prediction network, both of which are forced positive by a non-linearity.
This output is trained to be an accurate predictor of the error as needed to minimize the Gaussian likelihood loss (Eq. 2 of main text).

Reviewer 3 asks about the sizes of the quantum dots and fluorescent nanobeads. 
The quantum dots and nanobeads have 25 nm and 50 nm diameters, respectively.

Reviewer 4 asks to the generality of our synthetic simulations.
The main limitation is our focus on microscopic videos in which there are no other objects visible.
This restriction can possibly be relaxed by training on masked videos.
Furthermore, we have tuned the intensity of the QDs, the scanning speed, etc. to that of our experimental setup, but such parameters are simple to change in our approach.

Reviewer 5 asks if we can compare to more previous literature.
To our knowledge, the determination of diffusion coefficients in previous literature on nanobeads is always done using particle tracking (which we do compare to).
Our work is fundamentally different from this approach, and as we show, does not only improve the accuracy but also demonstrates a completely different scaling with density of particles.

We use PyTorch for both training, model specification, and simulations.
All is run, end-to-end, on GPUs.
As already mentioned, we will release our source code upon acceptance."
https://papers.miccai.org/miccai-2024/411-Paper2830.html,"We thank all chairs and reviewers for their time. We are encouraged by the positive comments on our novelty and experiments. We believe that our work is a strong submission to MICCAI this year. We highly appreciate the ACâs careful reading of our paper and favorable consideration of it.
â R1
Q: How sensitive the work is to the actual guideline, vs simply having the MLLM process the image?
A: Our model shows better generalizability over all baselines on a test set with distribution shifts. We attribute this to our introduction of clinical guidelines since there are similar conclusions in existing works(Liu et al., NeurIPSâ23; Dai et al., NeurIPSâ23). More experiments are interesting for future study.
Q: The pre-trained weight of baseline and MLLM
A: We apologize for the typos. We used ImageNet pre-trained weights for our baselines. âInitialized stateâ refers to LLaMA pre-trained weights, not a random state. We will rephrase these.
â R3
Q: There was no comparison of the number of parameters and training data with the existing methods. 
A: Our PI-RADS score classifier has the same parameter size as the baseline. During training, we distill features from MLLM into a lightweight classifier. Upon deployment, only the lightweight classifier is used. Training MRI data is the same for all baselines. 
â R4
Q: Why adapt the MLLM for prostate cancer PI-RADS scoring & Why propose the two-stage training process
A: (1) PI-RADS scoring is crucial for prostate cancer diagnosis, which requires not only effective but also reliable solutions. Existing methods make reliable decisions with network modifications and extra annotations. As clinicians score images according to guidelines, we naturally adapt MLLM to encode the textual guidelines to guide the image-scoring task. (2) Our two-stage training is designed for our clinical task. The guideline has different rules on various MRI modalities. Thereby, we added a stage teaching the MLLM distinguishing MRI modalities, before encoding the guidelines.
Q: Differences with previous works
A: Unlike others built from scratch (Liu et al., NPJ Digit. Medâ23) or focused on clinical text generation effectiveness(Li et al., ICMLâ23), we sit on a guideline-aware reliable solution and consider the limited computational resources in the clinics. We use ârulesâ to regularize the image feature space with texts and distill these features into lightweight models. Besides, we also suggest a scheme to teach the model to discriminate different MRI modalities in the first stage. Our novel two-stage training meets 3D volume needs (most MLLM inputs are 2D) and requires no extra annotations.
Q: Provide an experiment on a public dataset
A: Due to space limits, we prioritize presenting our modelâs generalizability. We trained our model and all baselines on a public dataset (Natarajan et al., 2020) and evaluated them on our in-house dataset. Table 1 shows our modelâs remarkable performance on the unseen dataset. 
Q: Contrasting with other methods that integrate clinical guidelines & statistical significance of the reported improvements
A:(1) Comparisons with existing rule-based models might require extra annotations, which risks an unfair comparison. According to the results we show in Table 1, although without any additional labels or modification of network structures, our proposed method shows impressive generalizability over all baselines. This validates the effectiveness of the proposed method. (2) Table 1 reports the average and standard deviation of model performance over three runs. We will include dataset statistics in the final version.
â R5
Q: Include a more comprehensive analysis of the modelâs interpretability
A: Our model shows better generalizability over all baselines on a test set with distribution shifts (Table 1), which is a side-proof of the model integrating guidelines in the decision. We will consider a more comprehensive analysis in future works."
https://papers.miccai.org/miccai-2024/412-Paper0983.html,"Title: Inject Backdoor in Measured Data to Jeopardize Full-Stack Medical Image Analysis System

We thank the reviewers for recognizing our approach as âinteresting idea/topic â, âclear descriptionâ, and âwell-organizedâ. The writings and unclear descriptions have been revised and will be updated in the next version.

@R #1 and @R #3 It would be even better if the code will be open-sourced.

Thanks for your suggestion. We intend to make code available after acceptance.

@R #2 It would be beneficial if the authors could provide a threat model section.

Full-stack medical analysis system (FMAS) consists of imaging and analysis models. The scanned data is the measurement data, such as the sinogram data, rather than the image data. Hence, the measurement data is required be passed to the reconstruction module to show patientsâ anatomical information. In the pipeline, a hacker can threaten an FMAS and control the downstream model in two ways: by injecting a trigger into the scanner or during the transmission process. Previous works assume that the hacker has to gain unauthorized access to the downstream model, which is typically well-protected locally. Therefore, compared to previous works, our assumption is more relaxed.

@R #2 and @R #3 Please elaborate on the distinction between the invisible backdoor attack in the CV domain.

There are significant differences between our approach and the invisible backdoor attack in the CT domain. To our knowledge, existing works focus on injecting triggers into images, which can be considered post-imaging attacks, as Reviewer #1 noted. These methods do not consider injecting triggers into measurement data, which is quite different from the CV domain. Due to the physics of the reconstruction process, triggers designed by these methods may not remain invisible in the image domain when injected into the measurement data, as the results shown in our paper. Our approach avoids introducing visible labels by injecting triggers in the measurement domain without requiring handcrafted prior. Our approach can be regarded as a pre-imaging attack technique, and, to the best of our knowledge, this is the first attempt in this field. This method effectively assesses the security of FMAS, thereby addressing a critical gap in the existing literature.

Besides, we conducted experiments on the methods mentioned by the reviewers, which also introduce visible triggers in the image domain if we inject the triggers into the measurement data. We will add related experiments in the final version, and we will highlight our novelty compared to existing invisible backdoor attacks.

@R #2 The robustness of the proposed method.

Thanks to your question. Our method automatically learns and generates the invisible trigger without any prior knowledge. Hence, similar to most backdoor attack methods, our method is a model-agnostic framework. To support our point, we will include robustness-related experiments, such as ResNet, in our camera-ready version.

R #3 The contribution of this work is limited since there exists many articles that targeted the same topic.

We agree that many papers address security issues in the medical field, highlighting the importance of evaluating the security of medical systems. However, our approach differs significantly from others as it employs a pre-imaging attack method, whereas others use post-imaging attack methods. This distinction raises significant security concerns regarding the imaging process, which existing works have overlooked. Consequently, our work comprehensively validates the security of an FMAS.

R #3 The authors should discuss the algorithmic steps in detail and present the cost analysis of the proposed model.

Thank you for your suggestion. We will add a pseudo-code style algorithm in the final version to help readers better understand our method. Additionally, as promised, we will release our code to ensure proper implementation."
https://papers.miccai.org/miccai-2024/413-Paper0298.html,"We thank the reviewers for their insightful suggestions. We would like to address the major critiques as follows:

[R1, R3, R4] LLM fine-tuning implementation: Implementation leverages the TRL-Transformer Reinforcement Learning GitHub repo to fine-tune Llama2. We utilized the TRL library incorporating 4-bit QLoRA for efficient training. Fine-tuning was conducted with a batch size of 4 per device with maximum 10,000 steps. Learning rate was 2e-4 with a constant scheduler and maintained a maximum sequence length of 512. All other parameters align with those specified in the TRL GitHub repo.

[R3, R4] Visual translator implementation: Visual translator is implemented based on the Wizaron/instance-segmentation-pytorch GitHub repo. We utilized a ResNet50 backbone for our instance segmentation network, training separate models for lower (553 images) and upper (486 images) eyelids on CRC data annotated with gland masks. Each model was trained on 256x256 resized images for 300 epochs using a batch size of 8 and a learning rate of 1.0, employing the Adadelta optimizer with a weight decay of 1e-3. Other parameters align with those specified in the GitHub repo.

[R3, R4] LLM-based summarizer details: The summarizer uses the GPT-4 API in the Erol444/gpt4-openai-api GitHub repo, with a unique seed to control uncertainty. We input raw morphology data along with a structured task template (Fig. 3) to produce clinical reports.

[R3, R4] Make code, model & data public: We will release code and model weights on GitHub and Hugging Face after peer review. Datasets will be made available upon request for research purposes with appropriate Data Transfer & Use Agreements for sharing protected patient medical data.

[R3] Dataset train/test split: 
Train/test split is 90%/10%. Training set has 1903 metadata-only and 1257 image+metadata cases; Test set has 198 metadata-only and 155 image+metadata cases. There are a total of 878 subjects.

[R3] MGD implies DE, classification tasks may be flawed: The presence of MGD does not imply that the patient has evaporative DE. While MGD is often an important etiological factor in evaporative DE, that is not true for all cases (Galor, 2014). MGD, DE and blepharitis are distinct conditions, albeit often with similar symptoms. In our model, we defined independent labels for these conditions based on the TFOS 2017 DEWS II Definition and Classification Report (Craig, 2017). DE is defined by loss of tear film homeostasis, ocular surface damage and symptoms. MGD is defined by ductal stenosis and quality of glandular secretion. Blepharitis is based on eyelid margin inflammation, debris, and collarettes.

[R3] Rephrasing title with âEyelid Diseaseâ: Slit lamp and OCT would certainly be within the scope of future work for additional ocular surface diseases, however clinicians do not refer to MGD and blepharitis as âeyelid diseasesâ. The TFOS definition of âocular surfaceâ is that it comprises the structures of the eye and adnexa, including cornea, conjunctiva, eyelids, eyelashes, tear film, lacrimal glands and Meibomian glands (Craig, 2017). Therefore, in alignment with accepted literature, our method addresses a subset of ocular surface diseases.

[R4] Lack of diverse datasets, performance for different datasets:  It is important to note that our datasets do come from diverse study populations. Data from the CRC and DREAM (a major clinical trial with 11 meibography sites across the US) are combined. Our distributions are mostly similar to US Census statistics for age, sex, and race. Our dataset also covers a wide range of disease severities. Work is ongoing to obtain additional data on much younger and older populations, male subjects, and those of African ethnicity.

[R1, R3, R4] Suggestions to revise and requests for additional material: We appreciate the reviewersâ insights, however changes to the paper and inclusion of new data, experiments, or results in this rebuttal is specifically prohibited by MICCAI guidelines."
https://papers.miccai.org/miccai-2024/414-Paper2694.html,"Summary: R1(R), R3(A), R4(WA). We thank the reviewers for their valuable and encouraging feedback! We are pleased they find our work adaptable (R1), high improvement (R2), and novel(R4). We clarify the main concerns below.
(R1) Novelty: We would like to emphasize that no paper has successfully achieved proper nuclei instance segmentation using SAM with only weak point annotations. All the papers suggested by the reviewer have fundamentally different settings from our method, making direct comparisons difficult. While CellViT replaces the image encoder of Hovernet with SAM, it still requires a segmentation mask label. Also, SAC and PromptNucSeg equally require masks and only binary segmentation is modeled. In contrast, our framework enables parameter efficient fine-tuning (PEFT) using a SAM adapter, introduces a pseudo-labeling process for weakly supervised learning, and proposes a point-based instance segmentation method. Aside from supervised learning-based methods, Guided prompting-SAM uses box annotations for binary segmentation and reports lower performance than All-in-SAM despite having the same settings (Table 1&2). Herein, we reiterate the benefit of our proposed method that only requires weak point annotations and still achieves higher performance over box-based approaches. 
(R1) Point annotation scenario setting: We generated annotations by extracting the center points of the masks from the training data similar to prior art. For the Shift a-b scenario, points were randomly extracted from the region between a and b based on the center point of the mask. As mentioned in the Pseudo Labeling section, to segment the k-th nucleus, we use p_k as the positive point and randomly select 4 points from the remaining points as negatives for the prompt. Even if this number is decreased or increased, the method still works, but if there are too many negative points, the training time increases significantly, and the foreground region becomes smaller. Additionally, random negative point selection allows the model to learn from various contexts, resulting in a more robust model. In fact, Guided Prompting-SAM also reported the highest performance when using 4 negative points, thus our setting is reasonable and fair. 
(R1) Limited experimental data: Due to paper page limits, we focused more on experimental results that highlight the benefit of our method across different tasks i.e., few-shot, cross-domain, and shifted points rather than adding additional datasets. Previous works like MIDL, MixedAnno, SPN+IEN, and PROnet were also validated on two datasets, including MoNuSeg. While we could not present additional results in the rebuttal due to new guidelines, similar performance gains were observed on other datasets.
(R1) Insufficient evaluation metrics: In the proposed method, we achieved state-of-the-art performance with PQ scores of 68.35 on CPM and 56.45 on MoNuSeg dataset, which is consistent with the differences observed in AJI scores. If accepted, we will include this information in the final version of the paper.
(R4) Performance difference according to label: To demonstrate the effectiveness of the pseudo labeling process, we compared the results in Table 3 using cluster- and Voronoi labels (as used in existing methods), pseudo instance maps, and the ground truth. Our pseudo instance map trained model showed a slight decrease in AJI (1.5% ) on CPM, and 2.2% on MoNuSeg compared to fully supervised learning. This demonstrates that our method significantly reduces the burden of label creation while maintaining the same level of performance."
https://papers.miccai.org/miccai-2024/415-Paper1786.html,"We thank the reviewers for their constructive feedback and suggestions. We are pleased that they appreciated our work as very useful (R1), a strong result (R1), an interesting and important topic (R3), and that integrating clinical knowledge in a CBM setting is novel (R4).

Why perturbation-based method (PBM)? (R1): We use PBM for the following reasons: 1. PBM is a widely adopted approach for explaining model predictions (arXiv:1311.2901, 1602.04938 with 15000+ citations). 2. It offers a flexible framework for handling complex, non-linear mappings between concepts and classes, and is applicable to any model architecture. Alternatives like weight constraints or priors in probability space often only work well with linear mappings, relying on predefined thresholds that are challenging to justify and require extensive tuning (DOI: 10.1016/j.patrec.2021.06.030). 3. The importance scores obtained from PBM not only guide the model to align with clinical priorities but also offer insights into each conceptâs contribution to the final prediction, thereby enhancing interpretability. Alternative methods lack this capability.

Inadequate literature review (R4): We value the feedback and agree that citing modern CBM variants helps readers understand the broader context of our work. We wrote the related work primarily focusing on integrating clinical knowledge into models, which we consider the main novelty of our approach, rather than improving CBMs for better performance. In this regard, we note that the papers listed by R4 do not introduce clinical prior knowledge into the models. To our best knowledge, there is no existing work that incorporates clinical knowledge into CBMs.

Relevance to Shapley value & generalization to numerous concepts (R4): Shapley value calculates the average marginal contribution of features acrossall possiblecombinations, which is computationally expensive. Our method can be viewed as a simplified version of this, measuring concept importance through individual concept removal and observing resulting changes in probabilities. Our approach greatly reduces computational complexity, while it remains effective for our objectives of maximizing or minimizing concept importance based on expert rankings via alignment loss. In this way, our model can generalize well, even in settings with numerous concepts (>50), at a significantly lower computational cost than Shapley values. Our promising results on datasets with 11 (WBC) and 22 (skin) concepts demonstrate significant improvements in OOD performance, indicating the potential for our model to efficiently handle larger concept sets.

Comparison with SOTA CBMs (R4): While we wish to simply perform additional experiments with some SOTA CBMs, the rebuttal rules this year prohibit it. However, we note that our alignment loss can be easily plugged into any CBM models, and we believe our methods can benefit other CBMs, including Post-hoc CBM (PCBM). Our loss focuses on the class predictor (c->y), where we guide it to make predictions based on concepts prioritized by experts. PCBM differs from vanilla CBM by utilizing concept activation vectors or multimodal learning to learn concept representations without annotations (x->c), yet it still employs a class predictor for final predictions based on these concepts (c->y), which can benefit from our method.

Comparison with Distilling BlackBox [Ghosh et al, MICCAI23] paper (R4): While we agree it is interesting to compare, we are afraid that the comparison may be unfair as the method needs fine-tuning on OOD datasets and still requires a small portion of class labels from OODs. Our approach improves the performance on OOD without further training or fine-tuning.

Additional figures (R3, R4): We will include a heatmap plot of importance scores with cliniciansâ rankings and examples showing how our method corrects incorrect predictions. This is easy to do, and we do have some space left in the supplementary."
https://papers.miccai.org/miccai-2024/416-Paper2165.html,"We thank the reviewers for their constructive criticism and positive evaluation. We will make the code available after acceptance.

[R1] Motivation on choice of benchmark datasets. The selected three datasets contain gigapixel-scale WSIs with significant heterogeneity in tissue types and structures. The BRIGHT dataset, with its six breast tumor subtypes, highlights this morphological diversity. Pathologists consider both local cellular patterns and broader tissue architecture, as well as correlations between different areas when diagnosing. Thus, modeling long-range dependencies is crucial for WSIs, as critical diagnostic information may be distributed across various regions of the slide.

[R1] Clarification of experimental results. We ran these baseline models on the same extracted features of WSIs from ResNet50, strictly following the experimental settings outlined in the original papers. Our experimental setup and data usage for the RCC dataset were similar to [28], using 940 WSIs with a different splitting ratio. In [28], TransMIL achieved 87.6 ACC and 97.2 AUC. In our experiments, we achieved comparable performance with 90.2 ACC and 97.7 AUC. The discrepancies with the original TransMIL results mainly stem from differences in the dataset and data splits. Specifically, TransMIL used a different number of slides (884 WSIs) and employed cross-validation, whereas we used a randomly selected test set and averaged the results over three runs. These variations in data quantity and evaluation methods significantly impact the results.

[R1] Experiment with any graph pooling approaches. We tried to integrate the SAGPool graph pooling method into our proposed IGT framework using global pooling architecture or the hierarchical pooling architecture. However, we did not observe any performance improvements.

[R3] Reason for adopting GenConv as the GNN component. The choice of GNN component is indeed flexible. We compared GenConv with GCN and GIN when developing the IGT framework. GenConv achieved the best performance with acceptable complexity, largely due to its aggregation function design, which approximately performs attention pooling of instance-level features within local graph neighborhoods, enhancing the modelâs effectiveness.

[R3] Clarification on edge features and terminology. We thank the reviewer for pointing out the potential sources of confusion. In our current setup, the adjacency matrix is binary, with no weighting or features used for the edges. We will correct the description and equation in Section 2 that might have implied otherwise. Additionally, we will revise the manuscript to consistently use âGNNâ when referring to general graph neural networks throughout the section to ensure clarity and coherence.

Detailed Queries.
[R1] Size of âNâ. âNâ varies across WSIs. On average, it is 10,720 for TCGA-NSCLC, 12,397 for TCGA-RCC, and 7,157 for BRIGHT.
[R1] Clarification on Features. In Section 2.1, âeach featureâ should indeed refer to âeach patch.â
[R1] White Background: We removed white background patches by applying a saturation threshold of < 15
[R1] Thresholding on Distances in kNN: We do not set a distance threshold and always include the 8 closest neighbors, following the same pipeline as Patch-GCN[6].
[R3] Figure 1: We will correct âadjacent matrixâ to âadjacency matrixâ and remove the âAddâ icon for clarity.
[R3] Section 3.2: We used a linear layer to downscale feature vectors.

[R4] Detailed explanation of the IGT framework: The IGT framework enhances WSI classification by integrating GCNs with efficient Transformer-based attention mechanisms to capture both local and global features. The novel Graph-Transformer Integration (GTI) block processes node features through GCN and Global Attention layers in parallel, integrating their outputs to form comprehensive feature representations. This effectively models spatial relationships and long-range dependencies, addressing the limitations of existing GNN methods."
https://papers.miccai.org/miccai-2024/417-Paper1263.html,"Thanks for all the valuable comments.
R1, R4: Hypergraphs and Graphs
The hypergraph is an extension of the graph, where an edge in a graph can only connect two nodes, thus limiting its ability to represent correlations. In contrast, a hyperedge in a hypergraph can connect any number of nodes, providing a stronger capability to represent complex correlations. We empirical employ the Inductive strategy, where during training, the model aims to learn a general representation to make predictions on unseen data. 
R1, R4: Motivation
The proposed method is motivated by two key factors. Firstly, cognitive activities in the brain involve complex interactions between different regions. Such complex interactions cannot be represented by graph structures, whereas hypergraph structures can effectively represent these group correlations through hyperedges connecting multiple nodes. Secondly, ASD and healthy subjects exhibit complex group-level high-order correlations, such as differences in behavior and neural activity. Understanding these correlations through a group-level hypergraph structure is crucial. Therefore, this paper integrates individual (Intra-hypergraph) and group (Inter-hypergraph) high-order correlations within a framework and aims to enhance ASD diagnosis and provide biomarkers from a high-order correlations perspective for clinicians.
R1, R4: Proposed Method
The proposed method employs a two-stage training optimization process.
(1) In the first stage, individual subjects are modeled with different brain regions (segmented based on the AAL template) as nodes in the hypergraph structure, and the fMRI signals of brain regions as node features (i.e., X). This establishes a higher-order association structure H among different brain regions. The goal is to learn individual brain function embeddings (Evi in Eq. 4) guided by high-order correlations and to compute individual hypergraph prediction scores (Result_intra in Eq. 7). Notably, LASSO is introduced in the hypergraph modeling to preserve from ROIs with Î±i valuesï¼0 and form a hyperedge with the i-th ROI. Note that negative values are not deeply explored in this paper, but it is an interesting idea because interactions between neurons involve both activation and inhibition. This kind of relationship may also exist between different brain regions, and this line of thought will continue to be explored.
(2) In the second stage, all subjects are modeled together, with each subject as a node. The intra-embedding generated in the first stage is used as the feature to establish high-order correlations among different subjects. The goal is to learn functional and behavior differences at the group level and to generate group-level prediction scores (Result_inter).
(3) The final results are obtained by weighted fusion of the two (Eq. 7). Additionally, the experimental results show that our proposed two-stage method outperforms the end-to-end method. This is mainly because the quality of intra-embedding significantly affects the results of the second stage, and generating a better embedding in the first stage leads to a superior group hypergraph structure.
R1, R3, R4: Training Architecture and Parameter Settings
The proposed method uses cross-entropy as the loss function in both stages. The Intra_model consists of two hypergraph convolutional layers, followed by pooling and two fully connected layers. Dropout is set to 0.5, the optimizer is Adam, with a learning rate of 1e-5, and 100 epochs. In the second stage, the learning rate is set to 1e-2. We will open-source our code after the paper is accepted, along with the ABIDE subject IDs and cross-validation parameters used.
R1: Comparative Experiments
The methods compared in this paper follow the training protocols mentioned in the cited literature. The graph methods use validated features: graph edge weight matrices as node features. The input features for hypergraph methods and our method are the same, all being fMRI signal information."
https://papers.miccai.org/miccai-2024/418-Paper3839.html,"We genuinely thank all the reviewers for their positive feedback and constructive criticism. We appreciate the reviewers recognising this work as innovative and novel, which could help deepen the understanding of drug mechanisms in discovery endeavours. We address a few points below:"
https://papers.miccai.org/miccai-2024/419-Paper1877.html,"We thank R1, R3, and R5 for their valuable feedback and positive assessment. In this work, we address the âimportant problem of blurry image generation in cardiac MRI while maintaining latent interpretabilityâ (R3, R5). To this end, we propose AR-SIVAE, which âcreativelyâ combines attribute regularization with adversarial autoencoders (R3). Our experiments demonstrated that our method is âeffective in overcoming the limitations of blurry reconstruction while preserving latent space interpretabilityâ (R1, R3, R5)

Main points raised:

[R1, R3] Missing evaluation of the interpretability in downstream tasks: In this work, we focus on overcoming the blurriness of VAE-based methods while preserving latent space interpretability. This could be for instance of high interest for downstream applications where detailed reconstruction ensures that subtle differences in ventricular morphology are captured accurately to differentiate between subtypes of cardiomyopathy or other cardiac conditions. Currently, there is no consensus on XAI approaches [A] and rigorously evaluating the true interpretability of downstream tasks requires thorough exploration, which was not feasible within the allocated space constraints. Prior work [3] demonstrated the capability of Attri-VAE (the baseline in our paper) to perform well for the interpretable classification of cardiac disease. The rigorous evaluation of the downstream applications was not in the scope of this manuscript, but we agree that it is an interesting avenue for future work and we plan a thorough clinical-oriented analysis.

[R3] Complementary information on related works, that will be added to the paper. The advancement of VAE generation capabilities can be categorised into approaches that focus on enhancing the networkâs architecture, integrating more robust priors, introducing regularisation techniques or integrating adversarial objectives [4]. The latter has the benefits of combining the generative capability of GANs and the inference capability of VAEs, which is needed to add attribute regularisation. SIVAE has been demonstrated to be state-of-the-art in this family of approaches [4]. Concerning the attribute regularisation, its main benefit is that it can handle continuous variables. It allows to have a structured latent space and allows to have correspondences between an attribute and a specific dimension in the latent space[11]. One limitation of this approach is its capability to regularise highly correlated attributes.

[R3] Counter effect of attribute regularisation on deblurring: Adding the attribute regularisation indeed has a minimal counter effect on the quality of the reconstruction. The delimitation of the cardiac structures and of those in the background are less sharp, but we gain interpretability. This comment will be added to the figure analysis.

[R1] Difficulty of convergence: Despite mentioning that SIVAE is hard to converge, detailed convergence analysis and theoretical upper bounds are provided in the original reference. We use their observations to obtain stable training and conduct a hyperparameter search around those values. The attribute-regularization loss was then added to the training and the related hyperparameters were chosen empirically. We wanted to highlight with this sentence that the proposed method has a higher complexity than the baseline.

We thank again the reviewers for their valuable feedback and suggestions to improve the paper. We will incorporate their comments into our revised manuscript to the best of our abilities.

[A] Adebayo, et al. âSanity checks for saliency maps.â NeurIPS (2018)"
https://papers.miccai.org/miccai-2024/420-Paper0919.html,"We gratefully acknowledge the insightful comments and questions provided by our esteemed reviewers.

Main issues.
A). Selection of tasks and datasets. HCP and OASIS datasets are two publicly available resources that can be easily accessed. Since many similar studies (as referred in our experimental results) validate their methods on these two datasets, using same datasets will facilitate us to compare our method with other similar approaches. We totally agree with Reviewer 1âs suggestion to focus on MCI predictions. Currently, our work does not include enough MCI subjects from OASIS, as we are primarily processing MRI data from NC and AD subjects to construct brain networks. We will prioritize processing data from more MCI subjects and will include MCI predictions in our future studies.

B). Illustrations of the Eqs. 7 and 8. The embedding of the effective network at T+1 (i.e., G^{f}(T+1)) by using the T+1 layer is the increment of this dynamic model.

C). Model interpretability. The optimal \lambda for different tasks is determined based on parameter analysis experimental results, which are presented in our supplementary materials. The question proposed by Reviewer 4 about the discussion of information directions in the effective network is very important, which is exact the content of our next paper. Due to the page limitation, we really might not include this part in this MICCAI paper.

A few other issues. 
D). The definitions of c and b are the dimensions of features. We explain this shortly in our camera-ready version. 
E). Since we cannot obtain node feature matrix from fMRI data, we randomly initialize (using Gaussian distribution) the node feature matrix as in our previous studies. We do not include functional brain networks in this work. We utilize fMRI yielded BOLD-signal to construct effective brain networks in this work. 
F). The length of time segments may affect the performance. We will include the discussion on this in our future studies. 
G). We utilize whole brain networks (all brain regions, 82 regions for HCP data and 132 regions for OASIS data as presented in the data description section) for classification and regression tasks. Our model can automatically generate the most important brain connectomes as well as corresponding brain regions to each different prediction tasks as shown in the section 3.4. 
H). For the optimal structural network construction. There are different methods to construct brain structural networks, however, there is no optimal one. This conclusion was indicated in our previous study [1] which compared 9 different structural network construction methods for different prediction tasks. In this paper, HCP structural network was constructed using probabilistic tractography (FSL probtrackx) and OASIS structural network was constructed using deterministic tractography (FACT) and these two structural networks have different resolutions (82 vs. 132). We intentionally employed various methods to reconstruct brain networks to demonstrate that our new framework is independent of any particular brain structural network.

Reference
1) Title: âComparison of nine tractography algorithms for detecting abnormal structural brain networks in Alzheimerâs diseaseâ
DOI: https://doi.org/10.3389/fnagi.2015.00048"
https://papers.miccai.org/miccai-2024/421-Paper1325.html,"We thank the reviewers for their constructive feedback. They appreciated our interpretable time-to-event model as highly clinically relevant and liked that it captured clinically relevant changes. Regarding the critical feedback:

Trade-off between performance and interpretability [R1, R3, R5]: Importantly, the goal of our study was not to introduce a model with new SOTA performance, but an interpretable model with near SOTA performance. Given that there is typically a clear trade-off between performance and interpretability (as also pointed out by R3), our model performed surprisingly well, almost reaching the performance of non-interpretable SOTA models. The reason for this is likely that its inductive bias fits the task structure very well: AMD lesions are localised, so the localised processing of BagNets did not incur large performance loss.

Clinical expertise of annotator [R3]: We apologize for this omission. The annotator is a senior resident in ophthalmology with four years of experience and research experience in AMD. We added this information.

Generalizability to other problems [R3]: The model will generalise well to other clinical time-to-event problems for which image data is relevant, if the disease-related lesions are small, including progression modeling for diabetic retinopathy. We added a sentence to the discussion.

Novelty [R1, R5]: The survival setting has received much less attention than disease detection. Therefore, using sparse BagNets in this setting with the CoxPH model contributes two aspects: (a) we obtain an inherently interpretable model without the need for post-hoc methods and (b) we can predict risk over time from a single image. In contrast, most other models for survival prediction are not interpretable without post-hoc methods and treat time-to-event prediction as independent classification problems for each time point at the discretization available in the data. We clarified this in the text.

Conceptual similarities to other approaches [R3]: Few deep learning models provide explanations of their inner workings. (a) Prototype models learn prototypical image parts and provide them for interpretability. Their main drawback is that the explanations are coarse and spatially imprecise, which is problematic for diseases characterized by multiple small lesions, such as AMD. (b) NAMs are conceptually similar to BagNets but have to our knowledge been mostly used for tabular data, generalizing spline-based GAMs. NAMs for images like the EPU-CNN, would not yield contributions of pixels, as the BagNet does, but contributions of previously defined concepts/features. None of these methods have been used in image-based survival settings. We added references to these alternative approaches.

Other architectures [R5]: R5 recommended using transformers in this framework. Currently, we are neither aware of any works combining vision transformers and survival models, nor is it immediately clear how this would yield inherent interpretability. Also due to rebuttal rules, we think it is beyond the scope of the current paper.

Interpretability [R3, R5]: To distinguish the BagNetsâ interpretability from analysing CNN filters (R3), we clarified in Methods: âStandard ResNets learn potentially global features and their interactions, while the BagNet learns the local evidence in an image patch. This eliminates the need for post-hoc saliency maps or the post-hoc analysis of convolutional filtersâ. R5 pointed out that the explainability âstays at the image levelâ and âwhether the model can output the features contained in each patchâ. Unfortunately, we are not sure what exactly is meant. We want to stress that we clearly show that the identified local patches contain clinically meaningful tissue lesions.

Minor points: We thank the reviewers for pointing out redundancies and small issues with the figures. We will address these in the camera-ready version."
https://papers.miccai.org/miccai-2024/422-Paper3581.html,"We are pleased that the reviewers recognized the novelty and technical strength of our work, particularly highlighting our innovative approach to using Neural Radiance Fields in conjunction with a hypernetwork for multimodal 3D/2D registration in neurosurgery. R1 noted the novel formulation and application of NeRF, along with its clinical feasibility, while R3 appreciated the rigorous evaluation on clinical data and the methodâs potential to be used in clinical practice to enhance surgical precision. R4 also acknowledged the originality of our separation of learning anatomy and appearance. We value the constructive feedback provided and address the major points of critique below.

EVALUATION AND COMPARISON:
The reviewers pointed out the need for a more rigorous evaluation of the hypernetwork and a more extensive (quantitative) comparison with a broader range of existing methods.
Our main goal for the hypernetwork was to show that it can be effectively used - even on little data - to go from MR to RGB appearance with sufficient similarity to the intraoperative target to enable registration with conventional loss functions. To show that, we focused on evaluating the downstream task rather than the synthesis quality of the hypernet, as an extensive exploration of features, architecture, and interface of hypernets with radiance fields was not the primary focus of this paper.
Our evaluation for the clinical cases with corresponding target images was qualitative since manually labeled 6 DoF ground truths are difficult to acquire and come with low inter-annotator agreement. Instead, we opted for an extensive quantitative evaluation on synthetic targets with accurate ground truth poses and provided rotation and translation errors.
For comparison, we limited ourselves to SOTA methodologies that come with the same simple prerequisites (single RGB image from the surgical microscope) as our method. Other SOTA methods (such as skull-based ICP) require additional data acquisition in the OR and were therefore not considered.

GENERALIZABILITY AND OVERFITTING (R3, R4):
Whereas the reliance on vessel features is an inherent limitation of our approach that we will clarify, the danger of overfitting the hypernet on limited data can be solved in future work with 1) more elaborate data synthesis and 2) additional data acquisition. We refrained from employing 1) since a larger synthetic dataset would still not have guaranteed that we capture the ârealâ distribution of brain surface appearances. For 2), data acquisition is ongoing.

FAILURE CASES:
While the clinical feasibility of our method was acknowledged, reviewers suggested providing a more detailed discussion on failure cases. In an additional future work section, we will elaborate on how we can make our method more robust and propose solutions to failure cases.

COMPUTATIONAL REQUIREMENTS:
Reviewers raised concerns regarding the computational intensity. Since all training is done preoperatively, the computational complexity is heavily concentrated in that phase and has a limited impact on intraoperative registration. We will add details on the computational requirements in the final version.

FUTURE WORK:
The reviewers pointed out that there is no dedicated future work section. We will elaborate on this in the final version. This will cover potential extensions of our method to handle non-rigid deformations, exploration of the hypernet, and a better pose solver that can go beyond pixel-wise loss and instead operates on the whole image to allow for more robust loss functions.

We will add technical details on data dimensionality, scalability, and data acquisition process in the final paper.
The source code, documentation, and a dataset will be made public."
https://papers.miccai.org/miccai-2024/423-Paper2798.html,"To R1: Thank you for your feedback.
We acknowledge the concern regarding potential bias in image selection. To address this, we ensured that our image selection process was as unbiased as possible by employing random selection techniques.
Figure 1 illustrates the key aspects of the teacher-student techniques and the integration of CNN and ViT models. By breaking down the components and their interactions in a more digestible manner, we aim to provide a clearer understanding of the methodologies employed.

To R3: We appreciate the opportunity to clarify and improve our work.
We realize that the justification for employing the teacher-student model for fetal ultrasound segmentation needs further clarification. The teacher-student model is particularly advantageous in this context because it allows the model to leverage a large amount of unlabeled data effectively. The teacher model, trained on labeled data, generates pseudo-labels for the unlabeled data, which the student model then uses for further training. This approach enhances the segmentation performance by improving the modelâs ability to generalize from a limited set of labeled images to a broader set of clinical scenarios, which is critical for the variability encountered in fetal ultrasound images.
Because the limited labeled data is not reliably stable for training the network model, directly inputting the unlabeled data into the pre-trained model to produce hard pseudo-labels may result in a lot of noise (i.e., incorrect predictions introducing noise). As training progresses, these errors may accumulate. By setting a high threshold, the quality (i.e., correctness) of pseudo-labels can be ensured. However, a series of dynamic threshold methods have pointed out that overly high thresholds discard many uncertain pseudo-labels, leading to imbalanced learning between categories and low pseudo-label utilization. Dynamic thresholds lower the threshold in the early stages to introduce more pseudo-labels for early training, but the low threshold in the early stages inevitably introduces low-quality pseudo-labels. Therefore, to reduce errors (noise) in hard pseudo-labels and focus on challenging areas without labels, we utilize a sharpening function to generate soft pseudo-labels in this paper.

To R4: Thanks!
In Figure 1, EMA stands for Exponential Moving Average.
We have already specified in the article that E represents the Mean Squared Error (MSE) loss, and D solely represents the dataset.
Due to the space limit of the paper, we did not give standard deviations and p-values, and the ablation experimental results on parameter Î¼. In fact, the value of parameter Î¼ = 0.1 is optimal, which is consistent with previous research, therefore, we do not give relevant results (we set Î¼ to values of 0.01, 0.1, 0.5, and 1.0 while keeping the other weight parameters constant at Î± = 0.5, Î² = 1.0, and Î³ = 3.0. The experimental results showed that the Dice Similarity Coefficient (DSC) for PSFH were 0.889, 0.893, 0.891, and 0.890, respectively).
In our article, the comparison methods and their corresponding years are as follows: Mean Teacher (MT): 2017; Deep Adversarial Network (DAN): 2017; Deep Co-Training (DCT): 2018; Uncertainty-Aware Mean Teacher (UAMT): 2019; Cross Consistency Training (CCT): 2020; Cross Pseudo-Supervision (CPS): 2021; Cross Teaching Between CNN and Transformer (CTCT): 2022; Interpolation Consistency (ICT): 2022; Self-Integration Method Based on Consent-Aware Pseudo-Labels (S4CVnet): 2022; and Collaborative Transformer-CNN Learning (CTCL): 2022.
We apologize for the error. With only 20% labeled data for training, our DSTCT achieves 89.3% DSC performance, only 6% inferior to the upper bound performance. The unit for ASD is indeed millimeters (mm).
We first conducted ablation experiments and then performed experiments on the balancing weight parameters. Therefore, in the comparison of methods, we selected the best results from Table 3, rather than from Table 2."
https://papers.miccai.org/miccai-2024/424-Paper2519.html,"We thank reviewers for their insightful and supportive comments. Here we address the main review points.
1.Novelty compared to other SAM-based works in medical field (R#1&R#4)
Some works also introduce SAM into medical field. Compared to them, our paper differs in following aspects:
(1)Data modality. We focus on using SAM for the segmentation of non-Euclidean data i.e. IOS represented by meshes. However, other works focus on Euclidean data, e.g. CT, MRI, X-ray, endoscopy, ultrasound, pathology, etc. These structured data can be seen as a single image or multiple images stacked together, which can be processed by SAM more naturally and directly than non-Euclidean data. To the best of our knowledge, we are the first to generalize SAM to IOS segmentation.
(2)Training cost. We freeze SAM and only use weakly annotated prompt learning to exploit the âtooth shapeâ knowledge embedded in SAM. While most other works fine-tune whole SAM or design complex adapters. These methods require a large amount of labeled data and expensive computational cost.
(3)Automation. Our work automatically generates prompts for SAM without any manual prompts. However, some other works utilize SAM to facilitate labeling by performing segmentation interactively.
2.Performance (R#1&R#4)
We acknowledge our performance is competitive but not completely outperform the fully supervised SOTA. However, it is worth to note that our performance is achieved based on weakly annotated data (the amount of annotation is far less than full annotation) and native capabilities of SAM (without any domain-specific fine-tuning). Our method shows better results compared to the recent method specifically designed for weakly supervised 3d segmentation (refer to the 10xFewer in Table1&2). In addition, our method has a better understanding of tooth shape, resulting in more accurate masks both internally and at the edges (refer to the visualization in Fig.3). Through the analysis of failed cases, we believe that improving the accuracy of FDI prediction in cases of missing teeth have the potential to further improve the performance. We will add discussions about this as future work part.
3.Clarity of table1&2 (R#3)
The methods presented in tables can be sequentially categorized into 3 types: fully supervised general segmentation, fully supervised tooth segmentation, and weakly annotated tooth segmentation. Our weakly annotated work outperforms weakly annotated method, and is even competitive to fully supervised SOTA. Following the suggestion, we will reformat tables (coarse grid lines for division) and add more explanations to emphasize our weakly annotated setting.
4.Computational requirements and efficiency (R#3)
In the paper, we mainly focus on the feasibility and effectiveness. We agree efficiency is also important. With simple parallelization, our inference time is less than 8s on RTX3090 server. It is indeed insufficient for real-time applications. More optimizations, e.g. rendering acceleration and faster SAM version, may facilitate real-time applications. In practice, IOS devices can directly obtain RGB and depth images, and integration with hardware can skip the rendering step.
5.Organization of paper e.g. related work section (R#4)
Due to the page constraint, we had to forgo a separate related work section, but we reviewed related works in the introduction (2nd&3rd paras.). We reviewed most relevant 3 kinds of work i.e. learning-based IOS segmentation, IOS segmentation with weak annotations, and visual foundation models. We agree that a related work section aids in understanding. We will improve the organization following the suggestion.
6.Adding a link to code (R#4)
We agree that the importance of open-sourcing. The program chairs prohibit authors from providing links to external material in the rebuttal, but we will make code available after acceptance. Our paper provides implementation details and parameter settings, which also help in understanding and reproducing."
https://papers.miccai.org/miccai-2024/425-Paper1958.html,"We warmly thank the reviewers for their positive and constructive comments. They say that our method is ânovelâ (R3, R4), âimpactfulâ (R1), and our paper is âwell-written and easy to followâ (R1, R3), and âexperiments comprehensiveâ (R3, R4). In future research, we will conduct experiments using more pre-trained SAMs and extend our evaluation to other datasets to further demonstrate our proposed methodâs effectiveness. We thank the reviewers for pointing out the typos and we will correct them in the final manuscript. More experimental details will be included in the upcoming code release."
https://papers.miccai.org/miccai-2024/426-Paper3180.html,"For reviewer #3:
We will like to thank the reviewer for the time and energy spent reviewing our submission, and for the comments made. We fully agree with the reviewer that an additional (open-source) medical dataset would have been ideal. However, we did not manage to find one. Finally, thank you for the grammatical corrections, those are nice when writing the camera-ready version

For reviewer #4

We thank the reviewer for the comments and time taken to read our submission. With regards to the limitations of the method, we do have a brief discussion about it in the discussion section. Most notable is the lack of possibility to include prior knowledge about a physicians/students skill. Below we address your questions. 
1) We employ two different pre-trained encoders (one for each dataset/domain) for the skin lesion diagnostic dataset it is a ResNet50 trained to embed the images according to diagnostic label, and in the Duolingo dataset it is a DistilBERT model used for autoencoding sentences. 
2) Long skip connections is something we tried and saw that it increased performance slightly, see table 5 for ablation study.
3) Hyperparameters or model choices were selected through the ablation study results shown in table 5.
4) We are slightly unsure about what is meant with this question on the significance of balanced accuracy. However, we chose this as the primary metric to counter for class imbalances in the two datasets.

For reviewer #5:
We would like to thank the reviewer for the nice comments, and appreciate the time taken to review our submission.

It would be interesting to investigate the interpretability of the model e.g. through attention weights. However, we would venture the guess that it wouldnât be very understandable for non-domain experts.

We think that the possibility of integrating feedback into the model is an interesting avenue of research. One could look into online-learning or something similar. Simple retraining as the number of available samples increase is also a possibility."
https://papers.miccai.org/miccai-2024/427-Paper0901.html,"We appreciate the reviewersâ comments and insightful suggestions. They consider our proposed idea of bridge data augmentation with the mainstream classifier novelty (R1, R3). The problem we addressed is significant (R1, R3). The paper is well-written (R4). Below we address the reviewersâ concerns.

R1:
We appreciate the positive comments regarding novelty, effectiveness and contribution for the weak accept recommendation in Q12. However, we noticed that the rating given in Q11 showed Weak Reject. We kindly request clarification of this inconsistency and hope you can adjust the rating accordingly.

R1Q1: Discuss computational efficiency and scalability.
For computational efficiency, our IOIS method increases the computational cost a little, approximately 40% longer time and 30% more GPU memory for iterative image generation than the compared imbalanced methods. However, it is worth noting that our method is efficient, as it solely utilizes the inference stage of diffusion model.
For dataset scalability, we have conducted experiments on two datasets of varying scales to demonstrate the generalizability and scalability of our method. These experiments effectively show the applicability of our method to larger datasets, highlighting its potential for dataset scalability.

R1Q2: Compare with more methods.
We argue that our paper includes comprehensive experiments. We have compared seven state-of-the-art methods specifically designed to address the imbalanced classification issue. These methods contain three main solutions to imbalance problems, including re-weighting, re-sampling, and GAN-based synthetic methods.

R3Q1: Whether accuracy is a good metric.
We agree that accuracy may not be a suitable metric for imbalanced classification task, as it often fails to reflect the performance of minority classes. A few wrong predictions affect a little for accuracy while they may impact the minority classes a lot. Consequently, to better assess the performance on imbalanced datasets, we apply Macro-F1, Balanced Accuracy, and MCC. These metrics provide a more comprehensive understanding of the modelâs performance by considering the performance of all classes, including the minority ones.

R4Q1: Motivation needs citations.
We add corresponding citations to support our motivation and make further explanations.
Firstly, separating the image generation and downstream classification tasks may lead to overfitting due to the lack of synthetic diversity or model collapse problem [1]. The high similarity of synthesized images makes the classifier easy to distinguish the training set, while showing poor results for test. 
Secondly, the fixed portions of synthetic images for each class may not align with the dynamic requirements of the classifier during training [2], because the difficulties in classifying different classes vary for each epoch.
Moreover, we have conducted ablation studies in Table 1, where the experiment of âOurs (+offline)â separates the generation and classification and fixes the synthesized image size during classifier training. The results demonstrate our claim.
[1] Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review. Journal of Imaging, 2023.
[2] OnlineAugment: Online Data Augmentation with Less Domain Knowledge. ECCV, 2020.

R4Q2: Compare with large language-based data augmentation methods.
We argue that comparing our method with large language-based data augmentation methods is not suitable for our application due to two reasons:
Firstly, directly using large language-based models to generate medical images yields poor performance due to the lack of domain-specific knowledge [3]. Secondly, training these large language-based models requires significantly more computational resources than our method, which leads to an unfair comparison.
[3] A domain-specific next-generation large language model (LLM) or ChatGPT is required for biomedical engineering and research. Annals of Biomedical Engineering, 2024."
https://papers.miccai.org/miccai-2024/428-Paper1365.html,"We want to express our sincere gratitude for the reviewerâs positive feedback and appreciation of our work. Your insights serve as a motivating affirmation of our efforts. And we thank the reviewersâ valuable suggestions in helping improve the paper.

Due to the space limit, we couldnât include all training details in the paper, but you can find all the information about the model, training process, and dataset preprocessing from the code link we provided. We will include the size of small, medium, and large lesions in Tab.3 to the camera-ready paper. Regarding training and testing data, we evaluate our method using 2D models, as commonly done in previous works to simplify experimentation. For this purpose, from every 3D image, we extract slices 70 to 90, that mostly capture the central part of the brain. For model training, from the slices extracted from the training subjects, we only use those that do not contain any tumors. For validation and testing of all compared methods, from each validation and test subject, we use the slice that contains the largest tumor area out of the 20 central slices. Adam optimizer is used and learning rate is set to 1e-4. We train the model for 80000 iterations. The training of the Unet for the first step is the same as the main model, and the only difference between these two models is in the model structure, where this model for the first step has only one input channel. Our models are currently trained and tested only on 2D slices, but we have a 3D version coming soon!

Regarding what is the best threshold with shaded gray cells in Table1,2,3, it is explained in the âHuman-AI Collaborationâ section. The results with the best threshold for IterMask2 are obtained by determining the optimal threshold for each image during the iterative process, demonstrating the best performance the model can achieve through human-AI interaction. While the non-colored cells for IterMask2 show the result using the same threshold for the entire dataset (the threshold comes from the healthy validation setâs error map). For baseline methods, the gray-shaded best threshold refers to the optimal threshold per-image when computing the Dice score from the final error map for a fair comparison. And the non-colored cells for baselines use the same threshold for the test set which achieves the maximum dice.

We also want to clarify that the sensitivity and precision trade-off we mentioned applies not only to diffusion models but to all anomaly-segmentation methods that involve initial distortion of information followed by the reconstruction or regenerating of the missing information. In these cases, more distortion amplifies the reconstruction errors of anomalies, thereby improving segmentation. However, it also increases the reconstruction error of normal areas, leading to false positives. And this trade-off serves as the main motivation for our work."
https://papers.miccai.org/miccai-2024/429-Paper1351.html,"We thank the reviewers for giving a positive consensus on our work. We further address your concerns below.

R1 & R2 & R3: Reproduction & Source code.Â 
We thank all the reviewers for raising the important issue of reproducibility. We will release the code along with the camera-ready version to benefit the community and to help others build upon our work. Regarding the concerns about the training configurations and specific hyperparameters, we will also provide complete details in the open-source code.

R1: Discrepancies of results in Table 3.
Thanks for pointing out the discrepancies. As mentioned in 3.3, âDue to the high memory demands of transformer models, Table 3 adopts a tile-based testing strategy (256 Ã 256), resulting in a marginal decline in denoising performance compared to Table 1. To be specific, for the models evaluated in Table 3, the block-testing strategy was adopted across all cases to ensure a fair comparison. To clarify, the tile-based testing strategy involves splitting the full-sized input image into non-overlapping tiles of 256 x 256 resolution, processing each block independently, and then stitching the outputs back together to reconstruct the full image. In contrast, for Table 1, the models were evaluated on the full-sized images directly during the testing phase without splitting them into tiles.

R1: Details on model parameters and computational cost.
For our framework, the main increase in computational cost originates from the fusion network. Our advantage is that during testing, if only the segmentation result is needed, we do not need to go through the inference of the fusion network, thus avoiding any additional computational overhead. Compared to the classic U-Net, our fusion network introduces additional convolutional layers for the transformation of affinity features, as well as two instance-aware embedding modules (IEMs) to perform pixel-wise attention between image and semantic features. The total number of parameters of the fusion network is approximately 1.981M, and its computational complexity is around 27.220G FLOPs when using a 256x256 resolution image as input.

R1 & R2 & R3: More explanation, correction of grammar errors, and more clear illustration.
Thanks for your careful review. We will fully revise the manuscript to correct all grammar errors and typos, add more explanation of the design and related work, and clarify the illustration as suggested in future versions.

Again, we appreciate all reviewersâs suggestions, which will help us further improve the quality of the paper."
https://papers.miccai.org/miccai-2024/430-Paper1368.html,"We thank the reviewers for their insightful feedback. We are pleased they appreciate the ânovelâ (R1) and âinnovative and biologically well motivatedâ (R4) concepts of our paper which âhave the potential to enhance performance of several other clinically relevant computational pathology tasksâ (R4). We are encouraged by positive feedback on our extensive experiments (R3, R4) with âconvincing resultsâ that âincrease performance beyond benchmarksâ (R1).

@R3 âThe paper writing is badâ: We were surprised by R3âs sentiment given that the other reviewers described the paper as âvery well writtenâ (R1) and âeasy to readâ (R4). Nonetheless, we would appreciate specific feedback on which parts were hard to follow, so we can improve the clarity of our paper. Further, in response to R3âs incorrect claim that no open access source code is available, we kindly point to the bottom of page 3 which contains the link to an anonymised version of our repository.

@R3 âThe contribution is very smallâ: We disagree. Our contribution of weakly-supervised joint multi-task learning including tumor microenvironment (TME) tasks shows âimprovement over SOTA single task modelsâ (R4) for âthe important clinical problem of MSI and HRD predictionâ (R4). We are pleased that R1 and R4 recognize the significance of our contribution, describing our idea as âgood, novelâ and âinnovative and biologically well motivatedâ.

@R3 âresults are not convincingâ and âfew related comparison methodsâ: On the contrary, we compared with 16 different methods across 4 datasets. In our opinion, echoed by the other two reviewers, this constitutes a âcomprehensive benchmarkingâ (R4) with âconvincing resultsâ (R1). The results âincrease performance beyond benchmarksâ (R1), âdemonstrate marked improvementâ and âgeneralizabilityâ (R4). R3âs initial statement that our âextensive experiments on multiple datasets demonstrate the effectiveness of the proposed frameworkâ conflicts with the sentiment of the rest of their review. We would appreciate specific feedback causing R3 to find the results unconvincing.

@R3 âArchitecture is not clearâ and âFigure 1 is not clearâ: The architecture that we used is based on the widely-known standard Vision Transformer (ViT). All deviations and extensions from the standard ViT are explained in the methods, and visualized in Figure 1. Moreover, the class tokens (and, by extension the regression tokens) are randomly initialized learnable vectors, as is standard in ViT-style architectures. We thank R1 and R3 for their suggestions for improving the clarity of Figure 1 (adding colored classification/regression scores and expanding the caption to explain the classification/regression tokens), which we will incorporate in the camera-ready version.

@R1,R4 confidence intervals: We will include 95% CIs for Table 1 and 2 from the 5-fold experiments in the camera-ready version, which were previously removed due to table size and page limitations.

@R1,R3 explain balancing methods: We will further formalize the type of task balancing optimization that is performed, how the approaches differentiate from each other, and which layers of the architecture are affected in the methods and the caption of Table 1 (R1) and Figure 1 (R3).

@R4 choice of cancer types and targets: We thank R4 for suggesting a dataset for evaluation of HRD in ovarian cancer (OV). We compared to El Nahhas et al. (2024) for HRD, which did not include OV experiments. HRD prediction in OV from histology is still not convincing (AUCs 0.51-0.56; Ahn et al. (2024)). The evaluation of our framework on OV will be mentioned as future work in the conclusion (R1). We will also expand on our choice of cancer type and TME-relevant tasks (R4).

@R1,R3,R4 conclusion: We will reduce the length of the current conclusion (R3), add more information regarding the interpretation of the task-balancing outcomes (R1), and extend the perspective to future work from a technical and clinical perspective (R1, R3, R4)."
https://papers.miccai.org/miccai-2024/431-Paper1998.html,"We are grateful to the (R)eviewers for their considered and informed reviews as well as their constructive comments to better this work. We categorize and address the principal concerns below:

Generalizability: To balance the range of settings and the robustness of results, we made several design choices while selecting the final 315 experiments summarized in this manuscript. We appreciate the reviewersâ concerns about the applicability of our findings beyond the explored settings, including other metrics, SSL methods, tasks, and sample sizes. Wherever possible, these tradeoffs were informed by experimental findings (e.g. whether there were informative changes in trends using other metrics) and on previous literature (e.g. MoCoâs comparable performance to other SSL methodologies on a range of relevant downstream tasks including anatomy segmentation and instrument presence detection [17]). Still, we share these concerns and have tried to emphasize this by tempering our claims and being transparent with our results (through the release of checkpoints and code). Extensions of this work would build on this extensive (noted by R1,3,5) foundation to explore and bolster the recommendations made.

Novelty & Value: Note that this work is positioned as an application of SoTA methodology (i.e. SSL) to a new problem (i.e. leveraging diverse surgical data), addressing the application track of MICCAI. As such, while not methodologically innovative, we strongly argue that our work does present scientific novelty of value to the MICCAI readership. While previous studies have demonstrated the value of scaling SSL methodology to use diverse surgical data, this work is the first to systematically explore the impact that dataset composition can have on performance. This is relevant as it highlights important limitations in previous work (including at MICCAI, e.g. where only procedures significantly represented in the pretraining datasets were tested [6,22]) and provides indications for improvement. Broadly, as noted by R1,3, it provides practical insights into how to leverage diverse (and accessible) sources of data, particularly in low-label settings (e.g. feasibility studies, rapid prototyping, and few-shot adaptation) and in the shift toward unified foundation models.

Metrics: We would like to specify that we calculate F1-score for surgical phase recognition by averaging across videos to reflect the variability across different videos.  We thank R1 for highlighting the need for clarity, as this has previously caused confusion in the literature. Due to space constraints, we will provide additional metrics, balanced accuracy for CVS and accuracy, precision and recall for phase on github.

Clarification on experimental setup for Stage2,3: As R1 noted, fixing the number of videos in Stages 2 and 3 doesnât address workflow variations within and across centers and procedures. Instead, having a large number of cases per procedure/center and many procedures/centers helped us represent this variability. We will revise the text for clarity.

Clarification on stage 2,4 results: R1 points out that the relatively modest boost between Laparo420 with and w/o LC in stage 4 may contradict the claim that procedure-specific initializations improve results. First, we would like to note that this is inline with the modest increase in chlecystectomy pretraining representation from 0 to ~10%. Note that the two pure-cholecystectomy pretrainings perform markedly better in almost every setting. Finally, we are not advocating that other procedures should not be used but rather that it may not be trivial, illustrated by the wide range of performance boosts for different procedures in stage 2.

For brevity, we have omitted minor corrections that we will address in the manuscript such as typos, selection criteria for hyperparameters, and the inclusion of computational budgets."
https://papers.miccai.org/miccai-2024/432-Paper0877.html,N/A
https://papers.miccai.org/miccai-2024/433-Paper3454.html,"We appreciate the reviewersâ constructive comments. While R4 considers our work an âinnovative approachâ and R3 states it provides âvaluable data to the communityâ, R1 and R3 believe the main weakness of the paper is its lack of novelty. Additionally, R3 and R4 consider that the major contribution of the paper is the use of a fiducial marker for camera pose estimation, which is not what we intended to convey. We hope this rebuttal provides some clarification.
Video-based Surgical Navigation as proposed in [16] already utilizes a fiducial marker (WM) that is implanted in the anatomy for the purpose of camera pose estimation. The contribution of this work is, as explained in the last paragraph of page 2, the demonstration that it is possible to perform â3D surface reconstruction and registration without using any instrumentation for digitizationâ in the context of arthroscopy. To the best of our knowledge, there exists no system or method capable of registering a 3D model with bone anatomy solely from arthroscopic footage and without the aid of instruments such as touch probes or structured-light devices. As stated in the last paragraph of Section 1, our only ârequired instrumentation is a WM rigidly attached to the anatomyâ whose implantation does not disrupt the normal course of the medical procedure as it takes less than 30 seconds. Answering R4âs questions, the WM is a metal 3mm cube with an attached thread that is screwed into bone and provides submillimetric tracking accuracy. Its implantation is invasive but by being placed in bone (and not in cartilage or soft tissue), surgeons are not concerned about it causing any damage to the anatomy. 
Contrary to R3âs statement that there exist systems that perform SLAM without the aid of any fiducial markers, the literature reports that previous attempts to perform SLAM in arthroscopic footage were unfruitful [16]. For this reason, instead of going completely markerless, we decided to keep the WM and start by removing the probe from the procedure to accomplish touchless registration, this being the main contribution of our work. Given the recent advances in keypoint matching in challenging and low-textured scenarios, and since classical feature extraction approaches perform poorly in such conditions, this paper assesses the performance of different learning-based matchers in the task of 3D reconstruction from arthroscopic footage that is dominated by low-texture, floating debri, moving tissue and specularities. These adverse conditions cause existing matchers to provide many wrong matches, which are filtered out by the known epipolar geometry retrieved by accurately tracking the WM. Without the WM, the extracted correspondences would be highly contaminated with outliers, hampering 3D registration. 
Another important contribution of our work is a new deep-learning model for semantic segmentation in arthroscopic video. Such model allows to identify regions in video frames that correspond to bone and cartilage such that correspondences in other anatomical parts such as ligaments or tissue that pass the epipolar verification are filtered out. 
In conclusion, it is demonstrated, for the first time, that it is possible to accomplish touchless registration in the context of arthroscopy by using recent learning-based feature matchers combined with accurate camera pose estimation and semantic segmentation. This opens the way to important applications in the medical field.
Addressing particular comments:
[R1] âParameters [â¦] not discussedâ - We will include details on the parameters used in each method.
[R3] â[â¦] bottleneck [â¦] attributed to feature matching [â¦]â - This work demonstrates that the recent advances in the literature of feature matching now make 3D reconstruction and registration in arthroscopic environments a possibility, when combined with accurate camera pose estimation and semantic segmentation."
https://papers.miccai.org/miccai-2024/434-Paper3080.html,"We sincerely thank all reviewers for their constructive comments and recognize the advantage of our work.

R1Q3 & R3Q1 & R4Q2: Confidence intervals and repeat times
We randomly split the dataset and repeated all experiments multiple times. Due to space constraints, we only reported the mean values. We will clarify the description.

R1Q1: Difference from other deformable cross-attention methods
We thank the reviewer for pointing out recent approaches, which are either uni-modal deformation (Sangwon et al., CVPR 2023) or resampling of raw data of each modality with concatenated multi-modal information  (Liu et al., ICONIP 2023), or computing attention between tokens within pre-set windows (Chen et al., International Workshop on MLMI 2023). While effective, these may be less applicable to our study given the significant heterogeneity between genes and WSIs. In contrast, our method integrates histological and genetic features to deform the attention on WSIs. To the best of our knowledge, this is the first work of cross-modal deformable attention in multi-modal (Gene and WSI) cancer analysis.

R1Q2: Convergence properties of the optimization strategy
Our CG-Coord module adjusts the contradicted and less-confident gradient, dynamically avoiding subspace conflicts during training, which accelerates convergence. Table 2 shows this module improves performance within a fixed number of epochs, demonstrating its effective convergence ability.

R1Q4: Ablation study of survival prediction
We appreciate the reviewerâs insightful suggestion. Due to the page limit, we did not present all ablation results, which will be added in the final version.

R3Q1: Dataset size
We leverage a meta-dataset with over 2,000 high-quality slides from IvyGAP and TCGA, the largest open-source WSI dataset for glioma as far as we know. Consistent results of multiple tasks across our multi-cohort could validate our modelâs generalization ability to some extent.

R3Q2: Disease beyond glioma
Glioma, as a representative tumor with remarkable heterogeneity, is an excellent testbed to evaluate our method. We would thank the reviewer for this constructive comment - future studies warrant validation on pan-cancer research.

R3Q3: Meta dataset variations
TCGA GBMLGG itself is a multi-cohort dataset, containing WSIs from multiple institutions. For our meta dataset, we performed stain normalization to harmonize the data, as per (Chen et al., IEEE TMI 2020). Details will be added to the final version.

R3Q4: Separating tumor- and TME-related genes
Tumor and TME provide crucial information for cancer analysis while presenting significant variances in genes and WSIs. Inspired by this, we separate genes to deform the attention on WSIs regarding each subspace. Comparisons with other methods without gene separation in Table 1 prove our methodâs effectiveness. We will improve the discussion to elaborate on this.

R4Q1: Model Interpretation
Mounting research (Rebeca et al., Cancer Letters 2019; Karin E. et al., Cancer Cell 2023) has revealed the crucial importance of characterizing tumor and TME features for a deeper cancer understanding. We built our method on this biological prior knowledge, learning clinically relevant multimodal features in each subspace. Due to the page limit, discussions on the motivation and outcomes are brief. We thank the reviewer for this constructive comment and will add more discussions.

R4Q3: Top 30% genes
Highly Variable Genes indicate high signal-to-noise ratio information within an organism, allowing for more biological information captured with smaller dimensions. The 30% of genes is an empirical choice according to previous studies (Akhilesh et al., Nature Communications 2020). We appreciate the reviewerâs suggestion and will add a discussion in the final version."
https://papers.miccai.org/miccai-2024/435-Paper3702.html,"We thank the reviewers for their valuable feedback and address their major concerns below. We will release the code repo including preprocessing and add all feedback and results in the final version.
Motivation(R5,R6): The proposed retrieval model is used to automatically select relevant cases (mammogram-report pairs) from 100,000s of cases for training radiology residents. Hand-picking a set of cases is time-consuming, challenging, can introduce sampling bias, and is unlikely to match the desired distribution needed for adequate training of residents (pg 2, L4-L14 in paper).
Contribution(R6): We propose aninnovative and non-trivialâ(R1) method to train VLMs on radiology data by an efficient mini-batch sampling(SS) approach to address therealâ(R5) challenges of contrastive learning in the medical domain: 1) high-class imbalance introduces false negatives within a mini-batch, 2) underrepresentation of rare groups. We extensively validated the method on `in-domain and out-of-domain VLMsâ(R5) under zero-shot, few-shot, and supervised setting. Design choices (batch size, ratio R of freq.:rare groups, #freq. vs. #rare groups, mini-batch shuffling) are extensively studied. MedCLIP-SS (I2R: R@10=23.1, R2I=59.6) works better than MedCLIP(I2R: R@10=9.9, R2I=5.5) with smaller batches(B=8). Applying SS on larger batches (B>64) requires non-trivial solutions, extensive study and is future work (Q6,C4).
(R1, R5): Data Preprocessing: We use a binary mask of thresholded pixel values to identify the largest connected component in the image and use its bounding box coordinates to crop the breast tissue area. The cropped bilateral images are concatenated, zero-padded for maintaining aspect ratio, and resized to 512x512 pixels.
Reports are cleaned by lowercasing, punctuation removal, and extra spacing removal. The text is then split into sentences, each examined for key concepts: density, calcifications, asymmetry, architectural distortion, mass, and additional features. Negation sentences are ignored. If a sentence contains a key concept, the report is marked accordingly. Each key concept is detected separately and then combined to form discrete groups.
We appreciate the reviewersâ suggestion to study the relative distance of groups and will explore this in future work. For simplicity, the current approach treats all groups equally.
R1: ITM:image-text alignment. We corrected the notation inconsistencies in the paper. The supplementary material was excluded due to formatting issues by organizers. We apologize for the inconvenience and have addressed the suggestions in the main manuscript.
R5,C2: Increased mammogram recall rates indicate incomplete information for diagnosis, requiring the patient to return for further testing by the expert reader. We will reword the sentence. 
R5,Q6: Hard-negative mining: The proposed knowledge-grounded grouping of image-report pairs ensures sampling of true negatives within a mini-batch, i.e., no examples within a mini-batch come from the same group (sec 2.3, L1-L6). This further ensures to contrast against examples from groups closer to the anchorâs group, e.g., ABC vs ABCD are hard negative examples for each other. Thus, the proposed grouping benefits our sampling approach to take care of hard-negative examples. 
R5,C3: Primary 5 groups: Breast composition, calcification, asymmetry, mass, surgical change. Secondary groups: architectural distortion, intramammary lymph node, skin lesion, solitary duct, skin and nipple retraction
R6: Statistical Analysis: We performed t-test to compare pairwise similarity scores from MedCLIP-SS and MedCLIP on the external test set. With the alternative hypothesis that MedCLIP-SS is better than MedCLIP,i.e., we obtained t-statistic=7.47, one-tailed p-value=5.90E-14 < 0.05 for image-to-report, and t-statistic=10.54, p-value=1.33E-25 <0.05 for report-to-image. This supports the significance of our results that the proposed selective sampling helps the model for the retrieval."
https://papers.miccai.org/miccai-2024/436-Paper1321.html,We thank AC and all reviewers for their insightful comments. Below are our point-by-point responses.
https://papers.miccai.org/miccai-2024/437-Paper3796.html,"We thank all reviewers for their constructive comments. We have accordingly addressed all comments one-by-one below. We will make our codes publicly available upon acceptance of paper.

To Reviewer #1: 
Q1: Concerns about TSPIRIT kernel.
R1: We appreciate great comment. Given the novelty of using kernel to generate a new SDE for spatiotemporal exchange, we change the sampling pattern to obtain high-quality TSPIRIT kernel for more robust and improved reconstruction. As for LORAKS kernel, we will explore it  in our future work.
Q2: SSIM values are unusually high.
R2: Our experience suggests that SSIM values closely correlate with the data. We consistently calculate SSIM across all the experiments using the built in function torchmetrics.StructuralSimilarityIndexMeasure in Python. Specifically, for each slice, we independently perform max normalization and SSIM calculation for different frames, and then average them. 
Q3: Fig.4 looks very impressive.
R3: It is attributed to TSPIRiT kernel. It can effectively capture temporal correlations. Self-consistency correction in k-space at each time step leads to better image generation.
Q4: Selection of the comparison method.
R4: Sorry for confusion. The VE-SDE in our paper refers to (Chung et al., MIA 2022), which combines VE-SDE (Song et al., 2020) with ESPIRiT. Besides this method, we did not find other diffusion methods combined with DL-ESPIRIT, although we tried all efforts.

To Reviewer #3:Q1: The approximation problem in deriving the SDE.
R1: Thank you for great comment. \mu can be expanded in Taylor series as: \mu={I+1/2\int_0^t\eta(s)\Phids + 1/(2!)(1/2\int_0^t\eta(s)\Phids))^2 + â¦}x(0). Due to the use of self-consistent prior for guaranteeing \Phi(x(0))=0, all the terms on the right of I in the above equation result in 0 when inputting x(0), therefore, \mu=x(0).
Q2: Why do we only consider PC sampler?
R2: PC sampler is a common sampling solver in SDE, and its effectiveness has been demonstrated in MRI reconstruction (Chung et al., MIA 2022). Therefore, we adopted the PC strategy. Using more other efficient sampling strategies will be our future research.

To Reviewer #5: 
Thank you for great comments to improve description of our paper.
Q1: Eq.2 is confusing. 
R1: The formulation of Eq.2 extends Eq.9 in SPIRiT (Lustig et al, MRI 2010) for linear interpolation of any k-space point from adjacent 2D+t k-space, maintaining self-consistency. Here, G is the linear interpolation operator, allowing Gx^hat=x^hat due to the self-interpolation capability of k-space data x^hat. Eq.2 is the constrained optimization to find a solution satisfying self-consistency and data consistency in Eq.1.
Q2: Reasons to avoid learning inherent spatiotemporal relationships using deep networks.
R2: We avoid capturing temporal correlation by deep networks, since individual cardiac motion patterns can vary significantly. Purely relying on deep networks to capture this inherent relation requires high generalization of the model. Fig.4 confirms our strategy.
Q3: The derivation process from Eq.2 to Eq.3.
R3: The convex constrained optimization Eq.2 can be written in Lagrangian form:
argmin_{x^hat} |Gx^hat-x^hat|^2+\lambda|Ax-y^hat|^2 (2), where \lambda is the Lagrange multiplier related to the data consistency term. The iterative gradient descent for Eq.2can be expressed as Eq.3 with step sizes of \alpha_k and \beta_k=\alpha_k\lambda. The gradient of the L2-norm self-consistent term requires the conjugate transpose (G-I)^H.
Q4 & Q5 & Q6: Problems with Eq.4.
R4 & R5 & R6: a) Thank you for pointing out imprecision in Eq.4. \Omega_\Phi is an operator defined as: \Omega_\Phi: zâzï¼for any z \in \mathbb{C}^{n_xn_yn_t}, where z*=argmin_z|GFz-Fz|^2. b) In Eq.5, dw is discretized as Gaussian noise z, so \Omega_\Phi(z) corresponds to the diffusion term. c) Based on a) and b), \Omega_\Phi is a deterministic operator, so the gradient of the score function (\Omega_\Phi) remains a deterministic distribution."
https://papers.miccai.org/miccai-2024/438-Paper2217.html,"Dear reviewers,

Thank you for the detailed and constructive feedback. In the following, we will address the raised concerns and suggestions.

Reviewers #1 and #2 suggest further experiments to validate the label merge-and-split approach. We are currently conducting further experiments including: 1) symmetry-based label merging suggested by reviewer #1, and 2) application of the method to brain images with lesions. Due to the limited available space, we plan to include the results in a journal extension.

Reviewer #1 suggests reporting an upper bound performance based on perfect merged label predictions. While this experiment was not included in the manuscript, we found that with the chosen distance and volume-ratio thresholds the merged labels are split perfectly so that the ground truth labels are fully restored across the testing sets. This implies that differences in segmentation accuracy compared to the baseline without label merge-and-split can mostly be attributed to the learned CNN weights. However, a distance threshold that is small compared to the spatial variability of structures in the test set will indeed result in faulty label splitting and reduced segmentation accuracy.

Reviewer #1 suggests refining the baseline predictions by using an atlas prior. While we consider this interesting idea out of scope for this work, we would like to refer to a recent publication that explores this approach (Fidon, et al., 2024). The authors demonstrate that the atlas prior can indeed be combined with the CNN output to increase the robustness of the prediction.

Regarding reviewer #1âs comment on Eq. (1): The tilde is used to indicate the one-hot encoding. Y (without tilde) is defined as the corresponding integer-encoded representation.

For Fig. 2, we will adopt reviewer #1âs sensible suggestion to use more than two colours for the graph in the camera-ready version.

Reviewer #2 points out correctly that the IXI_merged model performs better than the IXI_orig model in terms of relative volume error (RVE) but worse in terms of Hausdorff distance. Considering the small differences (relative to the standard deviations) we expect that this observation is due to statistical fluctuations resulting from the randomness in the training process. Additional experiments are required to confirm whether the label merge-and-split method is favourable/disadvantageous for specific metrics."
https://papers.miccai.org/miccai-2024/439-Paper1997.html,"We thank all reviewers for their comments.

R3 motivation of knowledge distillation (KD). What is distilled?
It was pointed out in [14] that most existing methods used a frame-level loss which did not fully leverage the underlying semantic information and dependency in the output space, leading to suboptimal performance. We argue that the ground truth (GT) labels can not only be used to supervise network training but also provide semantic structure hints for the intermediate layers. Thus, we designed a new KD mechanism which exploited label structures when training phase recognition network. This was done by first measuring the correlation between a sequence of lower-level frame features and that of higher-level label semantics among phases, and then by temporally reassembling the informative label embeddings for dynamic adaptation. It turns out that the student in such a KD mechanism learns significantly better than when learning alone in a conventional supervised learning scenario, as shown by our ablation study results in Table 2. By designing a KD training procedure to transfer structure label information, we are able to capture the underlying semantic information of a surgery, and as a result boost performance at test stage.

R3 temporal modeling (TM) in our method and clarification of performance of SwinV2. 
We would like to point out that the KD mechanism in our method was implemented on top of TM. In both teacher and student networks, the key element to achieve TM is our self-attention layer (SAL) as shown in Fig. 1-(e). The input to SAL is a sequence of frame features or label embeddings. SAL is designed to aggregate a certain range of temporal information for feature enhancement. In our paper, we empirically chose the range to be 500 frames. Please note that our baseline model also included TM. The accuracy (acc) of 90.9% was not from SwinV2 but rather from our baseline model (SwinV2 + TM). Acc is computed at video level, while other metrics are at phase level.

R4 comparison with state-of-the-art (SOTA) methods
For fairness, we used official data split and relaxed metrics proposed in M2cai challenge to compare SOTA methods. However, not all SOTA methods use official data split or metrics. For example, Opera used different data split while ARST did not use relaxed metrics. This is why we did not compare with Opera and ARST. Our method achieved an equivalent acc to SKiT (93.4% of SKiT vs. 93.3% of ours).

R1&R3 clarification on training procedure
Our method is trained in 3 stages. First, we train VFE for 100 epochs with L_ce loss, taking phase annotations as GT labels. Then, we train teacher network for another 50 epochs with L_all (Eq. (3)). Finally, we further train student network for 50 epochs with only L_dis (Eq. (4)).

R1, R3, R4 reproducibility
We will release our PyTorch source code.

R3&R4 related works
Due to page limitation, we focus on more relevant related works. We will cite the two papers suggested by R3&R4.

R3 why not use SCL in the student model?
We have tried such an option but did not generate satisfactory results. By incorporating SCL in the student model at the third stage training, we obtained an acc of 92.4%. In contrast, our method achieved an acc of 93.3%. We argue that adding SCL may lead to a trade-off between SCL and KD, resulting in sub-optimal results.

R3 performance of teacher model
By taking GT labels as input to LFCT blocks, the teacher model obtained an acc of 97.8% at test stage.

R4 efficacy of our method
We agree with the reviewer that Fig. 3 itself is not enough. Additional results in Fig. 2 show a consistent improvement of our method over baseline when evaluated on complete videos.

R4 how about design LFCT with a simpler block
As we explained above, the attention mechanism in LFCT was designed to explore the correlation between lower-level frame features and higher-level label semantics among phases. With a convolution or linear layer-based block, this may not be achievable."
https://papers.miccai.org/miccai-2024/440-Paper2377.html,"We thank the reviewers for their kind words and valuable feedback on our work. Below, we address the main issues raised by the reviewers.

Novelty

Following the remark of R4 about the technical contribution of our work, we would like to clarify what we consider its novelty:
1) the adaption of PointNet++ message passing to projective geometric algebra (PGA), in particular, identification of a suitable element of G(3,0,1) to replace relative-position conditioning,
2) the interpolation module, in which we define a convex combination of multivectors that provably confines the output to the convex hull of its components, a property that is important yet non-trivial in PGA, and
3) the addition of a geometric class token which greatly reduces computational overhead for mesh-wide regression.
As the reviewer indicates, these contributions are combined into a general transformer model that we believe can have a substantial impact on the analysis of large (bio)medical meshes.

Results section

We recognize the potential added value of metrics beyond mean absolute error (MAE), tests for statistical significance, and Bland-Altman plots for baseline methods (suggestions by R4). For objectivity, we have chosen to use the metrics reported in the baselinesâ original publications, rather than re-implementing the models ourselves. This means that we do not have access to sample-wise performance metrics for the baselines and are limited to aggregated metrics, such as MAE. We will address this by including the standard deviation values for our results in the camera-ready version and upload sample-wise metrics alongside our code. This will enable detailed analysis in future studies by colleagues in the field.
R4 questions the lack of vertex-level evaluation in the cardiovascular experiments. We chose to use the same global metrics and visual presentation as the baseline papers (refs [23, 24] in paper) for direct comparison. We found our estimated wall shear stress (WSS) fields to be visually indistinguishable from the ground truth. Thus we agree with R4 that visualisation of local error (via âun-wrappedâ surface maps) would be valuable. If this conforms with the rebuttal guidelines, we would like to add it to Figure 2 in the camera-ready version.

Memory efficiency

R4 asks for a comparison of memory usage between GATr (ref [4] in paper) and LaB-GATr. Both âuse[â¦] memory- efficient attention [18] with linear complexityâ (p. 2 in paper) proportional to the number of tokens n. LaB-GATr allows us to reduce n arbitrarily. While preparing our paper, we verified the linear scaling experimentally, but did not include it in the submitted manuscript.
R1 and R4 correctly point out that the learnable pooling and interpolation introduce additional parameters. This should lead to a trade-off between memory usage (GATr) and training time due to parameter overhead (LaB-GATr). However, we found that computing self-attention dominates runtime and far outweighs the parameter overhead. We will add this information in the camera-ready version.

GA estimation

We concur with R3 that the performance on gestational age (GA) estimation that we mention in the Discussion requires additional study. R1 asks if we have tried other models besides LaB-GATr that have also underperformed in GA estimation. We have in fact studied an ablated LaB-GATr without the geometric algebra. Its performance was inferior to LaB-GATrâs. Our current hypothesis is that while postmenstrual age (PMA) can largely be estimated based on the geometry of the brain, GA is better explained by vertex-specific biomarkers such as myelination. In our dataset, these features were provided on sphericalised and subsequently sub-sampled brains. Back-projection to the cortical surface erased some of their spatial context (due to the sub-sampling). We will adapt our Discussion accordingly in the camera-ready version."
https://papers.miccai.org/miccai-2024/441-Paper1056.html,"We thank the AC and reviewers for their time. Most reviewers are positive and supportive, highlighting our idea as âwell-motivatedâ. Our work is âlogical and innovativeâ, experiments âeffectively address the problemâ, and âperformance improvements are impressedâ.To R4:Regarding R4âs concern about the modelâs generalizability, our model fuses global textual and local organ features for trauma detection. The design considers relations between the global and local features, it can potentially generalize to similar clinical scenarios, in which both global and local contexts are important, such as brain age estimation (He et al, IEEE TMI21), and reconstruction (Huang et al, ICCV21). We will extend our work across various organ types for future work.
R4 asks for a discussion with existing works, like LLaVa-Med, LViT. Our work uses ViT-B-32 with CLIP for text embedding and GPT-3.5-turbo for trauma descriptions. Compared with LLaVa-Med and LViT, our work focuses on 3D medical volumes, whereas they both focus on 2D image-text tasks. Furthermore, LLaVa-Med is tunned from LLaVa by instructions, while our work adopts the idea of text embedding. Particularly, compared with LViT which also uses text embedding, we further novelly fuse text embedding on local and global scales, which better leverages medical information. In addition, we use the text encoder empowered by CLIP to align text and image features more efficiently.
R4 requests statistical analysis. We have provided the mean and std. of the result. Due to space issue, we didnât put p-values in our manuscript. Here, we list the p-values when comparing with the top-2 best baselines (Huang et al, and CBAM) on the case and organ accuracy on private and public datasets, which are 2e-4, 5e-3, and 1e-6, 5e-4, indicating the significance. We will include the full results in the final version.
To R6:We thank the R6 for suggestions on writings and figures. We have followed your suggestion to correct the typo and further improve the organization and figure caption. We will further simplify sentences and include these revisions in our final version.
R6 requires clarification of the model inference. The Language-Enhanced Module uses organ-wise and category-wise prompts. During inference, only organ-wise prompts are used, as they are based on organ names, not labels. Category-wise prompts, generated from labels, are only used during training to compute the KL-loss and guide training.
Regarding clarification on Eq. (1). We apologize for the oversights. âdâ is the dimension of the key, not âQ, K and Vâ, We will correct this in the final version.
R6 requires discussion on the backbone selection. For the vision encoder, we tested ResNet18, DenseNet121, and DenseNet169, which had lower case accuracy (by 5%, 2%, and 2%) than our chosen ResNet50. We didnât present these results due to space constraints. For the text encoder, we selected ViT-B-32 with CLIP for its proven effectiveness and wide applicability (Liu et al, ICCV23, Li et al, ICLR23). For comparison with other LLMs (e.g. blip2), please note that our work focuses on 3D medical volumes, whereas blip2 focuses on 2D image captioning, which is hard to generalize to 3D data. In this regard, we propose to generate text from labels by using GPT-3.5-turbo, which can generate the clinical description based on labels, without vision inputs.
To R8:Regarding R8âs question on the input shape. We apologize for any confusion. Our methodology is based on 3D scans, not 2D scans. All vision encoders are 3D models, making our approach suitable for practical 3D applications.
R8 asks for the discussion on information embedding. We clarify that straightforward text has intrinsic semantic relationships, which prior works (Liu et al. ICCV23, Li et al. ICLR23) have shown effectiveness in segmentation tasks. In addition, we fuse organ information with vision features and use trauma semantics to guide training, which shows more effectiveness from our ablation study."
https://papers.miccai.org/miccai-2024/442-Paper1339.html,"Thank you for your time and feedback, which we will incorporate in the final version.

= Preprints =
Preprint papers, including arxiv, should not be considered by reviewers to assess the novelty of submissions, and we refrain from comment on this point raised by R1/4. We ask the AC to take this into account.

= Reproducibility =
We will release our codebase upon publication, which contains many of the requested details that did not fit in the paper, e.g. numbers for Fig. 2-3 and Table 3.

= Contribution to Laplace approximation (LA) (R1, R4) =
Indeed, the Hessian approximation we use exists in a form that scales to larger images, but did not apply to models with skip connections. We solve this by presenting a suitable block diagonal structure for the Jacobian.

= R1 =
Indeed, Active Learning (AL), next to out-of-distribution detection (OOD), is an important application of epistemic uncertainty. However, AL requires more from the uncertainty estimates than OOD: To gather informative new samples, you donât just want high epistemic uncertainty, you also want low aleatoric uncertainty. AL is thus a natural second, but not first, validation step.

= R3 =
1) Notation for entropy and mutual information from [32] were used to save space and avoid additional notation. The formulations are equivalent.
2) y is binary.
3) S is the number of pixels.

4-7) Please note that âmeanâ in Eq. 10 refers to the mean network: At this step we discard the variance head of the mean-variance network, going from t+2(T-t) to t+1(T-t)=T parameters, indicating thetas in this space with a ââ. Eq. 11 operates in this space and the samples of the distribution can be seen as mean networks, see different gray shades in Fig. 1. Sampling T parameter mean networks requires taking the remaining parameters of the variance head from the MAP estimate vector (deterministic). The definition of Hbelow Eq. 11 should be with \theta* for the nabla operators.

8) In Eq. 12, []l and \nabla{\theta_l} are indeed the same.
9) In Eq. 12, H^(L) denotes the Hessian of the loss function w.r.t. the neural network output. Explicitly, given an output v=f_\theta(x) we have H^(L)=diag_i(\frac{e^{v_i}}{1+e^{v_i}} - (\frac{e^{v_i}}{1+e^{v_i}})^2).
11) The ID testset used for normalization is given by the data split.

12) The 5% and 10% thresholds provide intuition on how effective each method is at assigning epistemic uncertainty, via its relative sensitivity to OOD data. In future work, statistical tests could provide more meaningful thresholds.
13) Functions f are in wrong order; there is no difference between J_{\theta} and \nabla_{\theta}.

All requested details will be incorporated!

= R4 =
Epistemic uncertainty vs OOD: Since epistemic uncertainty should be high for OOD samples, following [15], we utilize OOD as a downstream task for validating epistemic uncertainty. Here, the role of aleatoric uncertainty, captured by the SSN, is to improve the estimated epistemic uncertainty. We will clarify this.

Baselines: a) The LA depends on the aleatoric component only via the loss function. Thus, an alternative aleatoric component via test time augmentation wouldnât change the results. b) Implementing the Hessian approximation for the SSN loss function of the mean-variance network is a promising next step, but we emphasize that our simpler version is faster as it circumvents the sampling to evaluate the SSN loss. c) MCMC on the mean network could be an alternative epistemic component. While this is interesting, it would be a novel method, not an existing baseline.

ID/OOD classification: Following [15], AUROCs were calculated with sklearn using per-image ground truth binary labels (0-ID, 1-OoD) versus uncertainty scores as target predictions.

We also note that our conclusions are unlikely to change much with architecture, if the uncertainty methods and measures are identically applied. We will include a discussion of these limitations and possibilities in the final version."
https://papers.miccai.org/miccai-2024/443-Paper2260.html,"First of all, we would like to thank the reviewers of our manuscript for their detailed and valuable feedback. We have carefully considered their comments and will now address the key points.

Data publishing (R4): Unfortunately, patient privacy will not allow us to publish the dataset. However, we will make the model trained on the dataset publicly available to foster more research and hopefully improve clinical infant treatment in the near future.

Model architecture (R1):  We have chosen this specific neural network architecture due to its simplicity, which will make it easy to adapt by a wide range of users â even if they donât have a GPU, PyTorch, CUDA etc. installed, our shallow fully-connected autoencoder can still be downloaded quickly and easily integrated into other libraries. For instance, the few matrix multiplications required to encode and decode an infant mesh could be easily performed with numpy on the CPU. We hope that the published model will further resolve remaining questions about architectural details of our model.

Model and training details (R1, R5): In the camera-ready version of the paper, we will include more details about the architectural choices of our model, hyperparameters, and training details for reproducibility purposes. We also note that the description of the multi-nonlinear model can be confusing. The biggest difference to achieve the expression and age disentanglement lies in the training scheme, which is fully unsupervised and detailed in equations (2), (3), and (4). The scheme uses the fact that the babies were often scanned multiple times on the same day and over multi-month intervals â additional visuals provided in the supplementary video may help to understand the specifics.

Robust model performance evaluation (R1, R4): There are no publicly available 3D datasets of infants to test our model on. However, we have intentionally designed our test set to include diverse high-quality infant data from both of our partner hospitals. All our quantitative results were computed on this test set. Due to privacy constraints, the visual examples shown in the paper are indeed more restricted. Nevertheless, the paper still features three different babies from our dataset exhibiting typical artifacts and two more patients with cleft lip (one of them is only shown in the supplementary video). Additionally, in Figure 6, we have applied our model to the publicly available Infanface dataset, featuring uncontrolled 2D images of infants. We hereby showcase the modelâs applicability towards varied real-world conditions to the utmost of our ability.

Dataset diversity and labeling (R1, R4): Unfortunately, our dataset features only a small number of non-Caucasian infants; detailed demographic information was not collected. Automated labeling methods could help streamline the process of diversifying the dataset to eliminate this racial bias in the future.

Error distribution of 3D reconstruction (R5): In Figure 6, we include a heatmap of the 3D reconstruction error from single view, averaged over our test set. We cannot provide any heatmaps for the visual examples, since there is no ground truth 3D mesh associated with these 2D images.

Related infant models (R4): BabyNet is a work that focuses on 3D reconstructions of infant faces from monocular 2D images that are synthesized via BabyFM (âSpectral Correspondence Framework for Building a 3D Baby Face Modelâ by Morales et al.) â the very first infant face model introduced in 2020. Compared to BabyFM, which was never published, IFACE is built on a dataset that is an order of magnitude larger, covering a vast range of different expressions. We demonstrate that this advancement allows IFACE to reconstruct 3D geometry from real-world monocular 2D images captured in uncontrolled settings, rather than relying on synthesized samples as BabyNet does."
https://papers.miccai.org/miccai-2024/444-Paper0380.html,"We appreciate the reviewersâ insightful comments. Our data is available with a data-sharing agreement.
A. Clinical Utility (R3,R4): Treatment planning involves selecting prescriptions and inverse planning (IP) to calculate a dose distribution (DD) and delivery machine parameters. Prescription selection in multi-lesion SABR (ML-SABR) is challenging (Intro para. 2). If the prescription is too high, the DD does not meet constraints, requiring iterative prescription lowering and IP. If the prescription is too low, patients may receive insufficient radiation or even miss out on treatment. LDFormer allows ROs to bypass IP to compare prescriptions, and therefore IP only needs to be performed for one prescription, saving time. To illustrate, a patient at our centre was initially treated for bilateral lesions. He then developed 2 new lesions, which were treated with 35/5 each. One lesion recurred, and the RO decided against further retreatment. Retrospectively, we showed that a 3rd treatment with 35/5 to the recurrence was possible and created an acceptable plan in the Eclipse treatment planning system. We also showed that the two original progressions could have been treated to 55/5, possibly preventing the recurrence. To prospectively validate clinical utility, we are integrating LDFormer directly into Eclipse and will randomize patients to be planned with or without the model. We will collect planning times and determine whether escalated treatment was possible. We will better highlight clinical utility and provide more detail on the prospective study in the Discussion. The predicted DD can also be used as the optimization target in IP (Discussion para. 1). This will allow us to assess the deliverability of predictions.
B. Comparison to Existing Work (R1,R3,R4): To our knowledge, there are no other ML-SABR dose prediction models to compare against. Existing models were created for single-lesion (SL) plans [5,13,14,25] or same-fraction plans that do not account for the radiobiological impact of different fractionation schemes [12,28], with many created for OpenKBP. In OpenKBP [4], up to 3 PTVs were treated synchronously with 3 dose levels over 35 fractions. PTVs do not overlap because intersecting regions were relabeled to the highest dose during data curation [4]. In contrast, our ML-SABR dataset is more complex as it contains up to 5 PTVs treated with varying doses and fractions, overlap, and asynchronous treatment. A major limitation of applying existing models to ML-SABR is that using a single mask for PTVs [5,12,14,25,28] is not possible due to overlap (i.e. retreatment). We will expand on the discussion of existing work in the Introduction. While LDFormer can be used for existing datasets [4], it may not beat all SOTA models on simpler SL or same-fraction plans, as the error caused by lossy VQVAE encoding may outweigh the benefit of self-attention. Finally, to clarify, our previous work is an adaptation of DoseGAN [14] for ML-SABR.
C. Conformality Metrics (R3): The meanÂ±SD of the HI, D1cm (Gy), and D2cm (Gy) of the test set ground truth doses are 1.58Â±0.35, 113Â±52, and 80Â±49 for all PTVs, and 1.67Â±0.32, 126Â±55, and 119Â±46 for overlapping PTVs. These will be added to Tab. 1.
D. Ablation Study (R3): The latent of a combined PTV mask is prohibitively large since the mask must be encoded in 3D to preserve relative PTV locations. During development, we experimented with removing the IDE to reduce sequence length. We found LDFormer could not correctly place hotspots without the IDE, and so did not pursue a formal ablation study.
E. Model Design (R4): Data encoding via VQVAE (first stage) is necessary because transformers operate on integer sequences. Both the dose and IDE are encoded/decoded with the same VQVAE. The two 5s in Fig. 1 are multiple copies of the same vector in the IDE latent. The first 3 is an OAR latent vector, and the second 3 is the PTV fraction. Position encoding allows for differentiation between token meanings."
https://papers.miccai.org/miccai-2024/445-Paper0263.html,"First, I would like to thank the reviewers for their time and the feedback they provided for our paper. Their insights will be crucial for the extended version of this work. For the rest of the rebuttal response, we will refer to reviewers in the order of appearance, such as #R1, #R2, and #R3. We are also pleased to hear that all three reviewers think our paper presents a promising idea with great potential, especially #R1 and #R2.

[1] Zeghlache, Rachid, et al. âLongitudinal self-supervised learning using neural ordinary differential equation.â International Workshop on PRedictive Intelligence In MEdicine. Cham: Springer Nature Switzerland, 2023."
https://papers.miccai.org/miccai-2024/446-Paper2135.html,"We sincerely thank reviewers for their valuable feedback. We address some of the major points as follows:
1) Datasets(R3): ISIC17 and ISIC18 datasets are widely used in skin lesion segmentation papers, representing 39.4% and 21.2% respectively. ISIC18 is the largest publicly available dataset for skin lesion segmentation. Despite some similarities, the datasets are sufficient to validate the effectiveness of the model. Despite having some similarities, the datasets are adequate to validate the effectiveness of the model.
2) Data split(R3): 1. This is a common approach to splitting data in medical image segmentation, especially for skin lesion segmentation, as seen in various previous papers such as UNeXt(MICCAI2022) and EGE-UNet(MICCAI2023) 2. To make better comparisons with previous sota, we utilize the dataset provided by EGE-UNet .
3) Loss function(R3): 1. We use a hybrid loss function combining binary cross entropy and dice loss, commonly used in image segmentation. We adopted the same weights as UNeXt for these loss functions. 2. We introduce additional region loss and boundary loss to improve the accuracy of segmentation. This was validated through ablation experiments comparing the baseline with the baseline+PMA in Table 2.
4) Motivation(R4): Accurate lesion segmentation is vital for rule-based skin lesion diagnosis, both in clinical settings and on smartphones. The commonly used ABCD diagnostic algorithm(Asymmetry,Border,Color,Diameter) relies on accurate segmentation results. Nowadays, thereâs a global shortage of specialists particularly in rural areas, and consultation costs are rising. Our model can assist diagnostic software on smartphones achieve more accurate diagnoses, reducing patientsâ time and costs. Additionally, our model can assist doctors in the diagnostic process.
5) Difference between LB-UNet and related works(R4): Thank you for your suggestions. We will cite these papers and specify the differences.
1.Grouping channel: Depthwise Separable Convolution in MobileNets [a] performs separate convolutions on each channel. However, considering the parameters, GSA only divides the channels into four groups and construct shared memory for each group to perform linear attention. Additionally, the Group Shuffle operation is employed to better capture information.
2.Deep supervision: Usage methods and purposes differ. In the 3D Deeply Supervised Network [b], the results of certain hidden layers are up-scaled using deconvolution to match the label size. However, given that deconvolution significantly increases the number of parameters, we create feature maps through 1x1 convolutions and resize them to match the label size using bilinear interpolation. AD Diagnostics [c] uses deep supervision in upper layers for fine-tuning the model, while we employ deep supervision in intermediate layers to enhance boundary perception.
3.Boundary information: The methods for generating ground truth boundary maps differ. BAT [d] uses circles with radius of 10 to calculate the lesion area, which is not suitable for images of different sizes. However, GA-based boundary algorithm proposed by us does not have this issue. This is also a major point of innovation.
6) Applications(R4): Thank you for your suggestions. We will introduce the âDermAssistâ(Nature Medicine 2020) and âSkiniveâ(Dermatology Review 2022) apps in the introduction.
7) Fig. 2(R4): Thank you for your suggestions. We will add stages in Fig. 2.
8) Evaluate metrics(R5): Mean Dice and IoU are the most commonly used metrics in skin lesion segmentation. Sensitivity and specificity are crucial for clinical diagnosis but are more commonly used in classification tasks. Inference speed is closely tied to the GFLOPs used in experiments, which indicate computational complexity.
9) Limitations(R5): Thank you for your suggestions. We will incorporate the limitations. LB-UNet is proposed only for skin lesion segmentation, and we intend to extend our efficient design to other tasks."
https://papers.miccai.org/miccai-2024/447-Paper1602.html,"We sincerely appreciate the reviewersâ insightful feedback and recognition of the significance of our study. Below, we address the major concerns raised and outline the enhancements in the manuscript.

R1#: Thanks. We will refine the language as advised. Once approved, the code will be publicly accessible. Also, weâll enrich the discussion, exploring clinical implications such as assessing the lower limb gravity line and planning orthognathic surgeries.

R3#: 1.Generalization Across Different Medical ModalitiesCurrently, our focus is on landmark detection in 2D DR images. To expand our scope, we intend to extend our work to 3D and validate our findings across various medical modalities, including MRI and CT scans. 2.Potential for OverfittingRigorous validation experiments have been conducted to ensure the generalizability of our model to unseen data. We tested it on two new datasets and two independent public test datasets from the ISBI 2015 cephalograms dataset, as depicted in Sup Table 2. Techniques such as data augmentation and regularization were utilized to mitigate the risk of overfitting. 3.Enhancing the Comparison with Manual Methods Beyond Performance MetricsIn the final paper, weâll complement performance metrics with diagnostic comparisons, including femoral and tibial angles and other relevant parameters for the lower limb dataset. Both FRGCN and manual methods will be used for evaluation. 4.Limitations and Future WorkWhile our paper validates that the graph structure via Fiedler regularization outperforms manual designs, questions about its optimality persist. In future work, we aim to provide a comprehensive mathematical derivation and proof method to identify the optimal graph structure, integrating perspectives like control theory.

R4#: 1.The TAE and SAE Components in FRGCN Seem Loosely ConnectedAcknowledging your observation, we recognize the significance of developing a more efficient connection method. We will explore this direction further, building upon existing methodologies [1,2]. [1] Xu X. Structure-Enriched Topology..TMM, 2022. [2] Dai Y. RSGNet: Relation based skeleton graph..AAAI,2021. 2.Spatial Attention is Considered Old-Fashioned and Computationally ComplexAcknowledging spatial attention as a classic method, we recognize the potential for improvement with more advanced attention mechanisms. In our forthcoming research, we intend to explore advanced methods such as cross-spatial attention or spatially separable attention[3,4], while maintaining the core focus on a Fiedler regularization-based sparse graph representation for skeleton reconstruction. [3] Guo F. B2c-afm.. cross-spatial attention.. TIP 2023. [4] Xiangx C. Twins: Revisiting..Spatial Attention.. NIPS, 2021. 3.Outdated Baselines and Missing Ablation Study for TAEOur choice of baselines aims to underscore the superiority of FRGCN over manually designed structures. While GCN serves this purpose effectively, we also incorporate a comparison with a novel baseline, Vitpose, which utilizes transformer architecture. Additionally, we acknowledge the necessity of conducting ablation studies for TAE and commit to including these results in the final manuscript. 4.Highlighting the Novelty of This WorkWe will cite the articles you mentioned to highlight the novelty of our work. While Fiedler regularization has been studied in spectral graph theory, our application in optimizing skeleton structure for landmark detection tasks introduces an practical optimization scheme. We appreciate the clarification regarding the focus of the mentioned article, and we will ensure appropriate citations. [5] Chung,F.R. Spectral graph theory. American Mathematical Society, 1997. 5.Efficiency AnalysisYour suggestion to include an efficiency analysis is duly noted. In our future work, we will consider evaluating running speed, model agnosticism, extendability, and generalizability to enhance the comprehensiveness of our study."
https://papers.miccai.org/miccai-2024/448-Paper0250.html,"Thank reviewers for their valuable feedback. Overall, reviewers consider the paper is well-written (R4/R5), and our method is novel (R3/R4/R5), attractive (R3), and inspiring (R4). Also, they appreciate our methodâs effectiveness and superior performance (R4/R5). Below, we address reviewersâ concerns. We will revise accordingly and add necessary content.

[Q2] Clinical significance.
Even with 6 views (Fig. 2-4), lungs and bones are reconstructed with precise boundaries. Hence, Extremely Sparse-View Scanning (ESVS) can be utilized in intraoperative navigation, where boundaries of organs are often used for registration (Sec. IV.A in [33]) during surgery. Extremely reduced radiation dose enables more frequent ESVS, allowing for frequent registration and thereby minimizing errors due to patient movement. Also, ESVS can be performed in dentistry (e.g., orthodontics [34]) or orthopedics [35] for surgical planning, as bone surfaces can be extracted from the boundaries.

[33] Evaluation of registration methods on thoracic CT: the EMPIRE10 challenge. TMIâ11.
[34] Tooth model reconstruction based upon data fusion for orthodontic treatment simulation. CBMâ14.
[35] Pre-operative planning and templating with 3D printed models for complex primary and revision total hip arthroplasty. Journal of Orthopaedics 2022.

[Q3] Training/utilization of Gaussians.
Gaussians are generated from projections (Eq. 2-3) and used for feature querying (Eq. 4-5). The whole framework (including generation of Gaussians) is trained end-to-end. Due to limited space, we cannot present all the details of Gaussians here (and in the paper). We will release code later.

[Q4] Gaussians v.s. CNNs.
3D CNNs (as decoders) require high memory and computational consumption and are not feasible for reconstruction with scalable output resolutions. 3D Gaussians have been proven to be a powerful explicit representation, represented by a set of sparse points (with properties) and supporting point-based feature querying (Eq. 4-5). Hence, 3D Gaussians can be effectively integrated into sparse learning and inference, showing memory and computational efficiency.

[Q2] Implementation/efficiency of TTO.
TTO: network is optimized using Adam (LR=1e-7). In each iteration, one projection view is selected (i.e., batch_size=1). For a view, randomly select 512 rays (512 points sampled in each ray). Loss converges after 60 iterations. Efficiency: 0.465Â±0.005 s/iter, and 28 seconds per-sample optimization.

[Q3] Name of method: DIF-GS => DIF-G.

[Q4] DRRs: Digitally Reconstructed Radiographs."
https://papers.miccai.org/miccai-2024/449-Paper4008.html,"We appreciate the comments of all the reviewers (R3, R4, and R5) on all review points (Q4-Q14). We have tried to address all feedback and believe our manuscript has been strengthened as a result. Please find the authors (A) detailed responses below.

Q6: âThere is a lack of information about the dataâ¦â (R3), âThe paperâs structure is not well-organized. It devotes excessive space to describing the dataset, while providing insufficient detail on the methodology. Many crucial implementation details are missing, making it difficult for readers to grasp the specifics of the proposed method. (R4)
(A) Supplemental Table 1 provides additional descriptive statistics. Section 2 describes the architecture and weight freezing. Section 3.2 describes the data split.  Section 3.3 describes learning rate schedule, loss, and augmentation. Subsections of 3.3 describe concept correction and cancer head architectures. Section 4 describes hyperparameter optimization and Supplemental Table 2 provides the search space. The planned code release will also provide clarity.

Q6: Critique of the lack of visualized predictions and BI-RADS lexicon labels. (R4)
(A) We have created an additional figure with concrete predictions, annotations, BI-RADS lexicons, and biopsy labels on the testing set which addresses this comment, as well as R4âs comments in Q10 and Q12 vis-Ã -vis visualization.

Q6: Critique of the lack of k-fold cross validation (CV) for estimation of generalization. (R5)
(A) We have a relatively large dataset (R4) and thus felt CV was not necessary to obtain accurate performance estimates. This work would require nested CV and a full hyperparameter search, presenting computational constraints. We are collecting more data for future work showing generalizability.

Q6: Requested explanation for having several images per woman. (R4)
(A) Our clinical data are collected opportunistically. The examining sonographer captures images they feel are necessary. These images may be at different angles and/or positions.

Q6: Requested limitations statement be added. (R5)
(A) We have added a limitations statement. Briefly, the limitations are: limited demographic information, lack of evaluation alongside an expert reader, and lack of a geographically-distinct testing set.

Q10: Requested justification for choice of ResNet-101 FPN and Mask RCNN. (R3) Suggested future work in automatic concept discovery. (R5)
(A) ResNet-101 is presented in the Mask RCNN paper and is a standard pre-trained FPN. Mask RCNN extends Faster RCNN to perform both detection and object segmentation and is familiar to the authors. Future work could extend to more advanced models. We thank R5 for the suggestion and, though it is beyond current scope, is planned in the future.

Q10: Request for information on BUS examination time added by model use. (R3)
(A) Model predictions are available to the examining sonographer instantaneously; optional concept correction may add negligible additional time (we estimate 5-15 seconds).

Q12: Requested additional information on data distribution and architecture. Requested distribution and performance disaggregated along the BI-RADS lexicon. Requested data inclusion/exclusion flowchart. (R5)
(A) See Q6 responses for architecture clarification. Supplemental Table 1 provides BI-RADS lexicon distribution in each data split. Splitting along case-control groups helps to maintain balance between âmalignant-lookingâ and âbenign-lookingâ lesions. We do not present subgroup performance due to lack of power due to limited sample size in some subgroups (i.e., not parallel lesions). Unfortunately, due to page limitations, we are unable to provide a visualization of the inclusion/exclusion process.

Q*: General critique of manuscript and abstract writing style, citing lack of clarity.
(A) We have revised the abstract and description of the model architecture as well as training procedure in Section 2 to enhance clarity.

We thank the reviewers for their comments."
https://papers.miccai.org/miccai-2024/450-Paper1667.html,N/A
https://papers.miccai.org/miccai-2024/451-Paper0103.html,"We thank the reviewer for their valuable feedback. In the following we answer the questions raised by the reviewers.

For the clarity of the reviewer and readers, we restructured the introduction section of the paper. The restructuring is based on a funnel approach where first, we explained the problem statement through recent statistics from world health organization. Secondly, we explained reference method and explain their limitations. Additionally, we highlighted the technical contribution of our work in bullet points. The quantitative results clearly show our proposed network perform better than the reference method.

We explain the clarity the concept of data wasta and wrong exemplar below. 
Data Waste: 
Traditional co-training methods typically involve splitting the dataset into several subsets and training multiple networks concurrently (hence the term âcoâ). Each network utilizes only one subset of the data. Clearly, since each network trains on a different subset, the features extracted by them are distinct. The issue arises because, in pursuing different features, each network does not have access to the entire dataset. Typically, two networks are used, with each training on half of the labeled data, meaning each network effectively wastes half of the labeled data. We define this as Data Waste. In contrast, our proposed method uses data augmentation to obtain different features, allowing each network to utilize the entire set of labeled data, thereby eliminating the problem of data waste.
Wrong Exemplar: 
Traditional co-training often trains two networks simultaneously. If one network makes a high confidence but incorrect prediction, using contrastive learning to make the features produced by the other network more similar to those of the erroneous network can lead us further away from the correct answer. Traditionally, using two networks is a necessary compromise because the labeled data is divided into subsets. If divided into too many subsets, each network would receive insufficient data. 
For example, if the total labeled data consists of 120 images, traditional co-training with two networks allows each to access 60 images; with three networks, each will only get 40 images. To ensure that each network receives enough data, only two networks are used, although when one network errs, the other is likely to err as well. Our approach employs more networks to mitigate the impact of individual network errors on overall training. 
Why donât we use even more networks? Because we need substantially varied augmentations to ensure significant differences between the features learned by each network. For each dataset, we can employ more networks as we find better combinations of augmentations. However, for this paper, to ensure a fair comparison and minimize network adjustments for each dataset, we use a common set of augmentations.

The experiments mentioned by R3, and R4 in the Supplementary Material are general medical image segmentation tasks, which do not include surgical instruments. Itâs important to note that surgical instruments segmentation and general medical image segmentation are similar yet distinct tasks due to the vastly different features of surgical instruments and biological tissues. Our results demonstrate that contrastive learning, which focuses on identifying and distinguishing different features, achieves superior outcomes. Our method aims to further enhance contrastive learning through co-training. However, our results reveal that co-training does not hold a clear advantage over other types of semi-supervised learning methods. Nevertheless, by addressing the issues of âData Wasteâ and âWrong Exemplar,â our proposed method significantly improves the precision of surgical instrument segmentation. Given the critical importance of accurate surgical instruments segmentation in contexts like Robotic-Assisted Surgery, we argue that our proposed method has a high potential clinical impact."
https://papers.miccai.org/miccai-2024/452-Paper3246.html,"We thank all the reviewers for their valuable comments and feedback. The reviewers acknowledged the methodological novelty and the extensive evaluation of our paper with an early acceptance.

The reviewers raised concerns related to the limitation of the method and comparison with additional datasets: 
(i) Limitation of the Dataset (R3,R4,R5): Our scans were collected from 1000 patients through multi-institutional collaboration. The irreversible nature of wet-AMD as a degenerative disease is the most suitable to our task. Additionally the public datasets have less frequent visits, compared to our dataset (24 visits per patient). Even though our dataset is temporal, in the input, we process only 2 time points with large time difference, unlike the video specific models (R5). Finally we agree with R3,R5 that multiple medical tasks would enhance the strength of TC. We leave this as an extension to our method.
(ii) Lack of comparison against the other contrastive methods (R4): Our method has been extensively evaluated on multiple equivariant methods. All of them and our method can be trained with any other contrastive method. We chose VICReg for its popularity, but our loss terms are compatible with SimCLR, DCL. In the camera-ready version, we will add contrastive methods more extensively in the introduction section as the reviewer (R4) suggested.
(iii) Limitation of TC (R3): The major limitation of our method is that it relies on the assumption of irreversible disease progression. Even though the most degenerative diseases fell into this category, it limits the general use of the future prediction module. We will extend the conclusion section with the limitation."
https://papers.miccai.org/miccai-2024/453-Paper1759.html,"We thank the reviewers for 1) appreciating our workâs novelty, clarity, superior performance on an understudied new problem, and supplementary material, and 2) the constructive comments.

Q1 Limitation cases & insights (R3)
Weâll include some limitation cases & discussion.

Q2 âIntra-â & âcross-modalâ segmentation (R4) 
Following existing works like [35], we define the segmentation of an organ in modality A as intra-modal if the organ is labeled in modality A, or as cross-modal if it is not labeled in modality A but in B.

Q3 Domain gap & atlas construction (R4)
In this work, the domain gap between modalities is predominantly in image appearance attributes like intensity, contrast, and texture; it has been a central problem of many medical image domain adaptation works [3,9,15]. Meanwhile, organsâ location, shape, and size are statistically consistent across modalities. Thus, we average each organâs binary training labels (spatially registered across modalities) to obtain an organ-wise probabilistic atlas (Fig. S1). The organ-wise atlases of all organs compose the cross-modal atlas. Note the operations happen in the label space, which, as noted by R4, is modality-agnostic. So, despite the significant domain gap in appearance, images of different modalities statistically align with the atlas regarding organsâ location, shape, and size.

Q4 Motivation of AMAN (R4)
We agree the label space is modality-agnostic. Thus, weâll rephrase our expression to be more rigorous: AMAN aligns the cross-modal segmentation softmax output with the atlasinthe label space, instead of aligning the label space itself.
As clarified in Q2 & Q3, the cross-modal segmentation output would be noticeably poorer than the intra-modal due to the domain gap, thus less aligned with the atlas. A discriminator judges whether a segmentation is cross- or intra-modal based on how well it aligns with the atlas. This discrimination is perceptive rather than objective. Meanwhile, the segmentation network produces cross-modal segmentation that better aligns with the atlas via adversarial training. The ablation study (Table 2) confirms AMANâs efficacy. Lastly, cross-modal alignment of segmentation output in label space has been previously explored and validated, e.g., [29].

Q5 Initialization of \bar{a}_c (R4)
Experiments show zero and random initialization are similar in convergence speed (~400 v. ~390 epochs) and results (cf. Table 1).

Q6 Validation strategy (R4)
Thanks for suggesting cross-validation. However, we cannot incorporate it in this study due to rebuttal constraints. Weâll certainly consider it in future work.

Q7 Loss weight lambda_3 (R4)
We concur that early unreliable feature prototypes could destabilize the training process. Alternative to warming up lambda_3, we address this issue by updating the prototypes only with reliable features in correctly predicted regions. In addition, we use an EMA update. Our experiments show no notable signs of instability. We also try to warm up lambda_3 in our framework (linear increase from 0 in the initial 10% epochs). The convergence speed (~400 epochs) and final results are similar to ours.

Q8 Spleen position (R4)
Weâll reorient the figures to follow clinical convention.

Q9 Alignment step (R5)
We concur that the alignment is vital. However, even with the straightforward alignment, our method improves significantly over existing ones. We attribute the superiority to how we use the atlas. Instead of directly using it for supervision, we design three modules that learn to use the uncertain prior in the atlas wisely. In addition, Table S1 indicates a high tolerance to the atlasâ quality. That said, more robust registration methods could improve atlas formation and potentially boost performance, which we envisioned for future work in conclusion.

Q10 Use additional datasets & modalities (R5)
We plan to do so when extending this work.

Q11 Loss weights in Eq. 4 (R5ï¼
They are empirically determined on validation data."
https://papers.miccai.org/miccai-2024/454-Paper3398.html,"We appreciate the Reviewersâ feedback and address their comments here, which we will apply in the camera-ready if accepted. We are pleased that each Reviewer recognized the novelty of our method.
R1 (Performance matches SOTA, 3D CBCT not challenging enough) Although our performance matches existing methods, DeCode has a lower generalization error and is statistically significantly better than unconditioned models. Tooth delineation in CBCT scans is challenging due to variability in tooth shape, artifacts, and noise, with differences between scans from various centers hindering generalization. While current methods achieve high DSC results, root segmentation is crucial for clinical applicability, which makes any improvements valuable. Secondly, compared to the similarly performing VNet, DeCode is 10x lighter and trains 4x faster. We also plan to apply DeCode to vertebrae 3D segmentation.
R1 (DeCode robustness) In DeCode, an unconditioned residual connection bypasses the conditioning layer, making it more robust to annotation errors and noisy shape features (Fig. 1-Di). It helps the model prioritize image features if conditioning is uninformative regarding segmentation loss, ensuring it performs no worse than one without conditioning in worst-case scenarios. Secondly, DeCode learns shape feature embeddings from many samples, reducing the impact of individual annotation errors. Our study improves the utilization of existing labels and aims to inspire further research in this field.
R1 (Method limited to structures with consistent relationships) We appreciate this point and agree it is a limitation; however, physiological structures like teeth, spine, and organs are consistent in shape. This consistency, as seen for teeth in Fig. 2, is used to condition in DeCode. For structures such as tumors, characterized by unpredictable shapes, our approach may face constraints which we will address in the discussion.
R1 (Simple model) Our findings highlight the strength of simplicity. DeCode excels in decoding learned shape representations, outperforming statistically significantly unconditioned models on unknown data. Our model has only 4M parameters and requires 3h training to achieve a DSC of 93.83 on an external test set. At the same time, ResUNet34 (DSC of 93.71) barely fits 3D data patches on an 80GB GPU (impossible for SwinUNETR), requires 11h of training, has 70M (17x more) parameters, and yet falls short of DeCode.
R1 (Missing citations) We will add citations to related works.
R3 Thank you for pointing out typos and for strongly supporting the acceptance of our work. We will mark oracle configurations more clearly.
R4 (3DeCode and CBCT qualitative results) Due to space limitations, we presented quantitative results only because they clearly demonstrated better generalization with DeCode. We will add 3D segmentation results in our git repository.
R4 (Synthetic dataset needs conditioning) The Reviewerâs point about the need for conditioning in the 3DeCode dataset is valid. This dataset was designed for that purpose. Having confirmed the feasibility of conditioning, we show through extensive quantitative analysis that conditioning with learned radiomics shape features improves generalization to real CBCT data. Similarly, Jacenkow et al. (MICCAI20) showed that without conditioning, they could not segment the desired image quadrant in synthetic 2D data, and demonstrated how such conditioning with the cardiac cycle phase improves segmentation performance.
R4 (Shape features explanation) We use PyRadiomics to calculate shape features for each tooth: sphericity, volume, elongation etc. Such morphometric descriptors analyze size, form, and shape, thus closely linked to the morphology of the segmented objects. Incorporating shape features aims to decode morphologically accurate masks. We will provide details on shape features for camera-ready.
R4 (Private CBCT measurements setting) Camera-ready will include more details about CBCT dataset."
https://papers.miccai.org/miccai-2024/455-Paper3328.html,"We thank AC and the Reviewers for their constructive comments. All Reviewers recognize our proposed histopathology-enhanced bone lacunae segmentation method as ânovelâ (R3, R6), âinteresting and soundâ (R4), and find âthe achieved results significantâ (R6), âconvincingâ (R4), and âthoroughâ (R3). Unanimously, they appreciate that âthe approach does not rely on paired data, has the potential to be used on other image types/modalitiesâ (R3) and that it âcan have an impact in clinical practiceâ (R6). Yet, they require clarifications about (i) modalities measuring different features and noise (R3), (ii) visual insights (R4) and model generalization (R6), (iii) model nonfunctional characteristics (R4), and (iv) code-data release (R4, R6).Â

i) We apologize for any misunderstanding. Our aim was not to suggest complete diversity between histopathologies and SR-microCTs but to highlight their multifaceted yet interdependent functions in downstream applications exploiting osteocytes/lacunae segmentations. Discussions with biomechanical researchers and a review of related literature underscore how osteocyte variations can yield insights into bone remodeling or disease processes (Dreyer Vetter et al.). Differently, lacunae can predict healthy/diseased bone microcrack generation and progress (Buccino et al.). Besides, Carter et al. observed that osteocytes and lacunae morphology and distribution depend on each other, as osteocytes lie inside lacunae. This brings, from the functional side, dependence between their different roles, while, from the image feature side, similar semantic characteristics. Therefore, R3âs assumption is correct. The studentâs role is to approximate the teacherâs prediction, although trained on similar yet harder-to-understand features (Hinton et al.). The resulting non-zero noise between their features could manifest as reduced prediction certainty of the student compared to the teacher due to low informative features or student prediction errors caused by some ânon-useful signalsâ from the student image modality. Table I monomodal results prove this: despite the teacher and the student models being the same, the SR-microCT model performs worse than the histopathology one.
ii) We agree that characterizing the predicted segmentation across methods could provide insights and enhance model interpretability. Conversely, a qualitative evaluation would be arduous due to microscale morphological differences among segmentations and lacunae/osteocyte density distribution that would require sample-level analysis and quantitative parameters (i.e., stretch, oblateness, and density) extraction to prove the qualitative findings. Besides, showcasing the effectiveness of our method across diverse image domains holds significance for broader adoption and community validation. Regrettably, time and page constraints precluded us from conducting additional experiments for this paper. Nonetheless, we are committed to comparing our predicted segmentations qualitatively and quantitatively against other approaches and testing our strategy on different domains (i.e., radiology) in follow-up publications.
iii) In the revised manuscript, we will add the details on model trainable parameters (3,016,883) and average inference time for patch lacunae segmentation (0.0063Â±0.0198s), for SR-microCT WSI (12.27Â±0.02s), and DeepLIIF image patches (0.0054Â±0.0190s). We employed an AMD Ryzen 7 5800X @3.8 GHz with a 24 GB NVIDIA RTX A5000 GPU for development and evaluation on 512x512 size patches and 3100x3100 size WSI.Â 
iv) Recognizing the importance of reproducibility, we will add a link to the code repository in the final manuscript. While we cannot disclose our clinical bone dataset yet, we are working to facilitate its accessibility. Nevertheless, our method demonstrates versatility with the DeepLIIF dataset (Ghahreman et al.), which is publicly available.
Finally, we will fix the typo (R3) and include the visual results (R6) in the final paper."
https://papers.miccai.org/miccai-2024/456-Paper1957.html,"Thank to both two reviewers for their professional comments and for acknowledging the innovation and practicality of our work. Below are clarifications regarding aspects of the paper that were not clearly articulated, accompanied by some summaries:

#Reviewer 1:

Insufficient detail in data, methods, and results:
1) Anatomical labels are assigned based on anatomical knowledge on chest x-rays, such as âright lower lungâ. Disease labels are based on observable abnormalities and inferred diseases in chest x-rays, such as âpneumothoraxâ.
2) To facilitate performance comparisons with existing methods, our train, validation, and test sets are split in the same ratio (8:1:1) as existing methods such as EKAID. The dataset MIMIC-CXR-Diff is now available on Physionet.
3) FasterRCNN is pre-trained on Chest ImaGenome data (anatomical labels and bounding boxes). We use features from intermediate layers (before the ROI Head) as anatomical features. Since MIMIC-CXR-Diff does not provide ground truth bounding boxes, FasterRCNN was not evaluated on MIMIC-CXR-Diff for performance.
4) Due to the limitations of the ResNet input, all images cropped by the FasterRCNN bounding box are resized to 224x224 to obtain disease features.

Why not use the latest YOLOv9 or ViT for feature extraction: FasterRCNN+ResNet for feature extraction is the most widely used approach in general VQA methods, and we referenced their pipeline. In the future, we will consider switching to more advanced feature extraction, especially introducing cross-modal pre-training like BioMedCLIP.

Lack of analysis on disease severity: We agree that disease severity can impact model performance. However, there is currently no good method for quantitatively analyzing âseverity.â This requires further organization and analysis of the dataset and is a direction for future research.

#Reviewer 3:

Insufficient analysis of existing methods: Among the three baseline methods we compared, MCCFormer is a simple encoder-decoder structure designed for change captioning tasks and did not consider the factor of perspective changes in chest x-rays. As a result, it performed very poorly across all metrics, almost failing to capture differences in chest x-rays. IDCPCL introduced contrastive learning pre-training on top of the encoder-decoder structure, resulting in a slight performance improvement. EKAID, although considering perspective changes, lacks pre-training in medical visual language, and its text generation performance is not as good as our proposed method, especially in terms of the CIDEr and METEOR metrics. Further analysis will be included in the revised version.

Simplify method description: Thank you for your suggestion. We will simplify the technical descriptions and provide more intuitive explanations or visual aids for complex concepts in the final version.

Validation on a broader dataset: Collecting image data from the same patient at different times and having it annotated by experts is a very challenging task. We are actively seeking collaborations to apply our proposed method to other medical imaging contexts."
https://papers.miccai.org/miccai-2024/457-Paper2268.html,"We thank all reviewers for their constructive comments and encouragement on the paperâs strength, including novelty (R1, R3, R4), significance of the solved problem (R1, R3, R4), demonstration of method effect (R1, R3), and clear writing (R1, R3). Below we respond to raised questions.

Limited comparison (R3, R4): Methods for selecting an initial annotation set are rather limited, this is why most active learning (AL) algorithms start with random sampling. Diversity sampling (DS) starts typically also from the 2nd AL cycle but has been recently used to select the initial annotation set using powerful pretrained encoders. DS is widely adopted in the latest AL publications, often in combination with other strategies (Wu et al. MICCAI22, Tang et al. MICCAI23, Qu et al. MICCAI23, Chen et al. MIDL23, FÃ¶llmer et al. MIDL24, Chen et al. CVPR24). The work of Lou et al. (mentioned by R3) also used DS to select representative annotation patches, while also encouraging the selection of patches with high intra-patch consistency to stabilize subsequent GAN training that creates synthetic training samples. We agree that the extended approach forms relevant baselines for the specific application of nuclei detection, but it is less general and not suited for tasks where consistency is a less relevant predictor for informativeness, e.g., mitosis detection. Therefore, to retain generalizability, we focused on the basic form of DS without application-specific add-ons. The basic form further allows a direct comparison with our proposed method, since the same pretrained models are used for feature extraction to make the differences in the effects of different methods (i.e., assignment of regions to clusters or identified class prototypes) visible.

Impact of caption quality (R3) / prototype retrieval (R1, R4): We expected that the quality of images, captions, and image-caption pairing differs in the ARCH and OpenPath databases. For ARCH, images were sourced from textbooks and publications, and captions were manually reviewed, while OpenPath is larger (176,373 pairs) but less curated with automatic text cleaning and image quality sampling tests only (still, PLIP pretrained on OpenPath has been effective in various evaluations). Therefore, prototypes retrieved from OpenPath may contain (more) noise. This allows us to evaluate the robustness of our approach in different settings. Interestingly, we see no drawback of using OpenPath and/or noise can offset by a stronger pretrained model (see Fig. 2a-b - experiments on CAMELON16; OpenPath was not used on MITOS_WSI_CMC since the image-caption pairs are not directly accessible, which we require to crop the mitotic figure for feature calculation). We prioritized this âreal worldâ comparison over an analysis with artificially added noise patterns. We see value in assessing the quality of the prototype set by including different numbers of prototypes or using a more adaptable way of applying a similarity threshold as suggested by R1, and reserve both evaluations for future work.

Application to rare classes with little open resources (R4): Some cases/classes may indeed be underrepresented in open ressources; still, our proposed approach may help identify sparse but existing characteristic samples and may be used to identify prototypes from related diseases based on prior knowledge. Concerns about finding rare class prototypes are further alleviated by ongoing efforts in creating larger and more diverse histopathological image-caption databases to build powerful all-purpose models. Still, bias and underrepresented diseases are valid concerns, which we will emphasize in the revised paper.

Rely on existing methods (R1): The agnostic design of our approach to models or region selection methods allows it to be seamlessly applied to any new method.

Cluster number K in diversity sampling (R1): We follow most related works [9, 12, 20] and set K to the number of selected regions."
https://papers.miccai.org/miccai-2024/458-Paper1502.html,"Dear Reviewers, 
We appreciate your time and effort in reviewing our paper. We would like to ask you to consider that the space limits of this conference paper also limit the number of methods and metrics we can include in our analysis, and we therefore had to focus. However, we feel that we still provide a comprehensive comparison. In the following, we address the major concerns.

Novelty and Limitations  (R1): The reviewer correctly notes that reconstruction-based UAD is not new. However, the approach fundamentally depends on the anomaly scoring. Our work introduces a novel anomaly scoring method, which addresses limitations of reconstruction-based UAD and can substantially improve their performance, as demonstrated in our experiments. Therefore, we believe that our work is an important contribution to the field of UAD. Furthermore, the reviewer is correct that UAD is typically limited to binary segmentation. We will include this general limitation of UAD in the discussion.

Comparison with Classical Methods  (R1): The reviewer has raised an interesting point regarding the potential performance of deep learning-based methods compared to âclassicalâ methods. Our results revealed that deep learning approaches outperformed the compared âclassicalâ covariance model (CM). Consequently, our comparative analysis primarily focuses on state-of-the-art deep learning-based methods. However, we agree that further work could include further methods.

Discussion of the Chosen Distance  (R1 and R4): The MHD is designed to measure the distance of a point from a reference distribution. Particularly for multivariate outlier detection, the MHD is a common choice due to its ability to consider covariances, a feature not present, e.g., in the Euclidean or Manhattan distances. We chose MHD as it aligns well with our problem, i.e., to test whether a pixel value is an outlier / abnormal.

Methodological choices and Ablations (R1, R3 and R4): We agree that further ablation studies can be performed, and we do have further results that we could not present due to page limitations. When choosing the number of reconstructions, we observed a moderate improvement in performance up to N=10, after which performance plateaued (tested up to N=30). Hence, we used N=10. We also considered MHD for different DDPM-based models (DDPM, pDDPM and cDDPM). The results show that all benefit, but due to their overall superior performance, we chose to present the results only for cDDPMs. We agree that it would be interesting to evaluate using MHD beyond DDPM-based generative models in the future.

False Positives  (R4): We agree that the claim of reduced false positives is not supported by a metric in the paper. We did observe a decrease in the false positive rate (FPR), but we focused on discussing the DICE and AUCPR metrics. As we donât see how to fit the quantitative results for FPR in the paper, we suggest revising the introduction and results sections by removing the remarks regarding false positives.

Description of the MHD calculation  (R4):  The Mahalanobis Distance (MHD) is calculated for each pixel, considering the covariance of all pixels across the N reconstructions. After reshaping the MHD map to the input image shape, each pixel in the MHD map is a scalar representing the MHD of the corresponding pixel in the input image. The final anomaly map is obtained by a per-pixel multiplication of the MHD map with the initial anomaly map. We will clarify the mathematical description in our revised manuscript.

Relevance of the symmetry analysis (R4): This analysis aims to reveal correlations, leading to non-zero covariances among image pixels. These covariances are considered by âMHD_fullâ but overlooked by âMHD_diagâ. Hence, this analysis provides insights into the information that can be leveraged by MHD_full, thereby providing a potential explanation for its superior performance.

We hope that our responses address your concerns. 
Kind regards, Anonymous Authors"
https://papers.miccai.org/miccai-2024/459-Paper3350.html,"Thank you for your valuable suggestions, which are greatly helpful for our future research. Below, we address the main concerns raised:"
https://papers.miccai.org/miccai-2024/460-Paper0813.html,"Dear Riviewer#1,

Dear Reviewer#2,

Dear Reviewer#3,"
https://papers.miccai.org/miccai-2024/461-Paper0827.html,"First, we really appreciate all the reviewers for their valuable suggestions. We first address some common questions (CQ).

[CQ1@R3, R4] Is setting of Light-weight Gaussian Splatting (LGS) reasonable and practical?
(I) Common acknowledgement: Compressing Gaussian Splatting (GS) is an important step for implementing GS in practice, the practical value of which is widely acknowledged by CVPRâ24 when considering accepted papers [i, ii].
(II) Does distillation weaken the real-time reconstruction/rendering?: First, we will clarify in revision that the terms âreal-time reconstructionâ are often mixed with âreal-time renderingâ rather than the efficient training in common practice [10, 14, 31], and there is also basically no work that can achieve real-time training. After this misconception is dispelled, it is worth noting that the knowledge distillation used only increases training time, without adding to inference time.
(III) Is clinical practice need LGS? Due to the development of applications deploying medical AI models in surgical robotics and point-of-care scenarios, efficient memory usage and real-time rendering capabilities for compressing Gaussian scattering to achieve 3D representations play a crucial role on these edge-low resource scenarios.

[CQ2@R3, R4] Different values in Tables
The different results of LGS in Table 1 and Table 2 lie in different number of gaussians in initialization. The initial number of Gaussians in Table 1 is 3k, while 30k in Table 2.

[CQ3@R3, R4] How performances of related methods obtained? 
(I) We reproduce the related methods using their released code and apply the same setting as in [14, 29, 31]. 
(II) NeRF is suitable for multi-view scenes, and specially for EndoNeRF on dataset SCARED, we use the same setting as [14, 29].
(III) EndoNeRF has only open-sourced two scenes.

Then we response to the other specific comments from each reviewer.

[R3 Q1] Novelty regarding theoretical insight.
As you acknowledged, theoretical analysis of methods offers insights into fundamental principles and novelty. Actually, all of our designed modules are exactly derived from theoretical insights. (I) With a theoretical analysis on each Gaussianâs importance on deformation, Deformation-Aware Pruning minimizes memory usage by limiting the number of Gaussians. (II) Gaussian-Attribute Pruning derives insight from the rendering principle of 3D GS and reduce redundant attributes. (III) FFC is designed based on the key component (Spatial-Temporal Structure Encoder) of 4D Gaussian and compresses the representation of space-time state adjacent Gaussians.

[R4 Q1] Clarification about results
(I) Qualitative: Please note that the view of SCARED is dynamic. LGSâs well performance on SCARED in Table 1 shows its ability to achieve the same accurate depth as EndoGaussian. Thank you for your advice, and visual results will be included in future work.
(II) Quantitative results for h_sh equal to 3 are shown in Table 2 line 2, indicating performance inferior to that of h_sh equal to 2 across all metrics.

[R4 Q2] How about memory usage and training time for LGS?
(I) Memory considerations discussed in our study do not pertain to the GPU memory utilized during training or rendering, but the storage memory on device. 
(II) The training LGS takes about 3 minutes.

[R5 Q1] Effect of teacher loss
LGS without teacher loss performs worse than full LGS. This is also the common setting of knowledge distillation [9]. As the output of teacher model is more easily imitated by student model, supervision from a mixture of teacher output and GT is essential.

[R5 Q2] Explain for Equation 2
The calculation occurs during the differentiable rasterization of Gaussian Splatting. Specifically, as we render the color for each pixel, we record the number of pixels each Gaussian hit.

[i] Lee, Joo Chan âCompact 3d gaussian representation for radiance field.â, CVPRâ24
[ii] Niedermayr, âCompressed 3d gaussian splatting for accelerated novel view synthesis."
https://papers.miccai.org/miccai-2024/462-Paper3351.html,N/A
https://papers.miccai.org/miccai-2024/463-Paper1629.html,"We appreciate all the reviewers (R1, R3, R4) for their thoughtful comments and constructive suggestions. We also appreciate the recognition of our work as a ânovel networkâ and for addressing a very significant problem (R2, R3).
Novelty and Contribution @R1: This work proposes a precise LIver tumor DIAgnosis (LIDIA) network on multi-phase contrast-enhanced CT for real-world scenarios. We design a novel Iterative Fusion Module (IFM) to address the incomplete phase issue and employ Asymmetric Contrastive Learning (ACL) to increase discriminability for rare tumor types. Comprehensive internal and external validations verified LIDIAâs superiority compared to strong baselines.
Experimental Comparison @R1: We have included nnUNet in the comparisons (Table 1). It is generally considered that nnUNetâs pipeline is well-established thus further adjustment yields marginal performance gains. We also compared with strong baselines, Mask2Former and PLAN, in which LIDIA shows superior performance. Ablation studies and comparisons with other fusion methods are detailed in Tables 2 and 3. Networks such as nnUNet, Attention UNet, and Swin UNETR were primarily designed to enhance segmentation performance, yet their classification capabilities (tumor diagnosis) may be limited, as evidenced by the results of nnUNet in Table 1. Due to rebuttal policies, we cannot provide new results here, but we will compare with more SOTA methods in our extension paper.
LiTS dataset @R1: LIDIA focuses on a clinically relevant task: differential diagnosis of liver tumor types using multi-phase CT. However, the LiTS dataset has only one lesion type annotated and contains only one CT phase. It is designed for liver and tumor segmentation without diagnosis, thus may not be suitable for LIDIA, compared with our two datasets with 2749 samples, 8 tumor types, and 4 CT phases.
IFM Design @R3: Yes, IFM starts with the NC phase and iteratively combines other phases. It is inspired by the clinical prior of the multi-phase imaging progress, while offers the potential to handle cases with arbitrary missing phases. We agree that it is beneficial to study how each phase contributes to the model and plan to supplement in the extension paper.
Tumor detection @R3: We evaluated on tumor-positive cases in this paper. However, LIDIA can also handle tumor-free cases without needing to modify its structure, if such samples are given in the training set. We will investigate the situation in our future work.
Statistical Significance @R3: t-test results show LIDIA is significantly better than all baselines in patient-wise diagnosis, but nnUNet and Mask2Former is significantly better than LIDIA for pixel-wise Dice (only 0.6% better in avg Dice). LIDIAâs strength in classification is aligned with our key objective.
Limitations and Future Work @R3: LIDIA still has room for improvement in rare and hard tumor types such as ICC and âothersâ. Our future work has been noted in questions above. It is also interesting to apply LIDIA to other organs.
Discussion about rare types @R4: Due to the limited number of rare types, it may not fully address the challenge of significant diversity. However, it is foreseeable that ACL helps distinguish between common and rare types, increasing the likelihood of rare cases being classified into âothers,â which is beneficial.
Delay Phase Ratio @R4: The delay phase is recommended to be scanned in liver tumor diagnosis guidelines, but it also depends on each centerâs routines. The ratio of delay phases is 2/3 in our internal dataset while less than 10% (53/828) in the external dataset. Our method achieved the best AUC in both datasets, confirming its effectiveness despite different ratios of the delay phase.
Efficiency @R4: LIDIA requires more computational resources (~12G, ~110s/epoch vs. nnUNetâs ~8G, ~70s/epoch), a tradeoff we deem justified given its gains in accuracy."
https://papers.miccai.org/miccai-2024/464-Paper1854.html,The following responses tackle the following six issues.
https://papers.miccai.org/miccai-2024/465-Paper0771.html,"Thank the reviewers (R) for the recognition of our work.
Clinical Application(R1):
Our method improves surgical imaging in low-light conditions, offering clearer visuals crucial for precise procedures.  Its lightweight nature ensures that it can be integrated with existing medical imaging systems without requiring significant hardware modifications, making it a practical solution for real-time applications in busy clinical environments.

Robotic da Vinci surgery dataset (R1):
We would like to clarify that although we used synthetic data from the da Vinci robotic systems, we do not specifically target at the da Vinci robotic systems. Instead, our research addresses a general case scenario applicable across various domains, including robotics. While existing fluorescence-guided techniques indeed provide a controllable approach in managing low-light conditions in surgical settings, they do not completely solve the illumination challenges faced in robotic surgery. Our aim is to explore broader computational methods that can complement existing physical solutions to enhance visual accuracy under diverse operational conditions. We appreciate the feedback and will ensure to highlight the general applicability and relevance of our findings beyond the specific context of surgical robotics in our revised manuscript.

ESD surgery (real-world) dataset (R1 R3 R4):
For our ESD real-world dataset, we manually selected 61 low-light images from 20 real robotic ESD surgery videos and verified them by the doctor. Using porcine models for endoscopic experiments is primarily because the anatomical structures and size of organs in pigs closely mimic those of humans. Prior to human experimentation, live pigs were best suited for use as research subjects for digestive tract surgery and are now widely used as research. This similarity allows for more realistic training and testing of medical devices and surgical techniques that are intended for human use. And this case also verified from EndoVis17 and EndoVis18 which also collected from pig endoscopic surgeries. Our dataset is doing in the same way as EndoVis17 and EndoVis18 dataset.

Leap between Super resolution and Low light Images (R1):
It is important to note that the diffusion models have proven to be excel in a vast of generation tasks other than denoising or super-resolution. Clarify that diffusion models excel at learning robust feature representations of images effectively across different imaging conditions. In surgical applications, especially under low-light conditions, the robust feature representation learned by DDPMs can be crucial. While the transition from enhancing low-resolution images to restoring low-light images may seem like a leap, both processes fundamentally rely on reconstructing and enhancing underlying image details that are obscured, whether by resolution loss or inadequate lighting. Thus, our approach with DDPMs is not only about handling noise but also about enhancing the quality of surgical images under varied conditions, making it a promising tool for improving the clarity and utility of endoscopic video data in real-time surgical environments. It is noted that the capability of diffusion models in enhancing low-light images has also been confirmed by other LLlE methods compared in our study.

Comparison with other light-weight LLIE methods (R3):
We did compare our method with other lightweight LLIE methods such as MIRNet v2 and PyDiff on both performance and efficiency (FPS) in Table 1.

Real-time augmentation(R4):
While our approach made a significant leap forward, there remains a gap to achieving true real-time performance (24+ FPS). In our ongoing and future work, we are committed to conducting extensive testing to better quantify this time delay and further optimize our model.

We will also fix the minor problems for better clarity. All the suggestions will surely be considered and added to the final manuscript."
https://papers.miccai.org/miccai-2024/466-Paper3416.html,"We found the feedback to be thoughtful and constructive. Some provided future directions. We are grateful to the reviewers. We appreciate the comments on âuniqueness,â ability to address reconstruction (recons.) challenges, good readability, dataset of radiologist annotated paired CT-US liver scans, and evaluations that âshow the effectiveness of the method.â
Reproducibility (R1, R4): We will release the code and weights for further research. We will make the dataset available and an API for 3-D visualization of the liver.
Novelty (R1, R4) with references to Balshova et al. 2019, [12], and [17] (R1): [17] uses 13 full-view CT slices of the left ventricle. [12] and Balshova et al. use x-rays taken from CT machines, with full view of the liver. We use only 3 partial US scans of the liver and use an SSM constructed from annotated CT scans. Our method, therefore, is novel and has high utility in the clinic. Clinics use repeated liver volume measurements. While CT scans are accurate, their use is limited by radiation exposure: US scans are used despite their noisiness and blurred boundaries. The liverâs large size and partial visibility in US make 3D recons. useful for radiologists. We agree that the individual components of our system are not new. The novelty is in the 3D liver recons. and volume computation using only 2 or 3 US scans, a significant improvement over the commonly used Childsâ method [3]. Contributions: i. 3D recons. and volume computation using only 2 or 3 unclear US scans in which the liver is onlypartially visible, ii. dataset with paired annotated CT and US scans of the liver (134 scans), iii. better liver 3D recons. accuracy (CD and MSD) compared to Ground Truth (GT) CT volumes and volume accuracy better than Childs (US liver volumetry with highest accuracy so far).
End-to-End Training (R1): Our MLP and TransUNet segmenter are end-to-end trainable. We will have a fully DL based end-to-end trainable system by replacing the SSM with a DL module in future. 
Segmentation models âare outdatedâ (R1): and requests to compare with LeViT-UNet and H2Former. The model that we use is TransUNet (2021), a well-cited transformer-based segmenter. We ran tests with LeViT-UNet and H2Fornmer: RMSE slightly worsened. We conclude that our method is agnostic to the segmenter so long as it is transformer based, as stated on p. 5. Tab. 1 justifies TransUNet use. Also, we wish to state that segmen. is not our focus. 
Why Three US Scans (R4): Our radiologists have carefully selected 3 scan planesâmidline, mid clavicular, and anterior axillary lineâas they can be easily replicable. Childs et al. use 2 of the planes (except anterior axillary): ablations show even 2 US scans are satisfactory. 
Supp. material Tab. (R1): We have made a mistake in Tab. 1 (liver volumes calculated by various methods): we have interchanged the headings of Our Vol. and US Vol. We apologize. With this correction, our volumes can be seen to be closer to CT.
Visualization (R1): âvis. in Fig. 3 lack reliabilityâ as we have just overlapped the GT with our reconstruction. Despite visualizations, our recons. are accurate (low CD & MSD in Tab. 1). However, we agree and have re-created Fig. 1 with recons. error.
Ablation Clogged (R1): This was to conserve space.  RMSE is not much affected by no. of principal comp. We will add descriptions.
US image noise effect (R4): We experimented by adding Gaussian noise and observed that it did not affect the volume much. We will further investigate. 
Literature review and comparisons (R1): We will cite the papers (e.g., 3D Organ Shape Recon.) and compare results . 
Evaluation of Dataset (R3): We will design evaluation tasks. Intra- and interârater reliability studies will also be made available. 
Limitation on Atypical Livers (R4): We have not tested with atypical livers (fatty, Riedellâs lobe, fibrosis, cirrhosis) and are collecting datasets. Note that our dataset too has variations (liver sizes), and our system handles them."
https://papers.miccai.org/miccai-2024/467-Paper0286.html,"-Common Question 1: More Experiments (as requested)
More experiment results comparing our LMa-UNet with SOTA including UMambaBot, UMambaEnc, and SegMamba on the Brats2023 dataset are shown below.

Conclusion: LMa-UNet consistently exceeds all SOTAs by a clear margin.

We will add these results in the final version.

-Common Question 2: Efficiency Analysis
We show the parameters and inference time of SOTAs and our LMa-UNet. LMa-Unet has less parameters and is more lightweight and faster, which facilitates clinical deployment. We will add these results to the paper.

Responses to Reviewer #1
Q1 and Q4: SegMamba and More Datasets
Thanks for the suggestions! Please see âCommon Question 1â above.

Q2: Mamba Design Novelty
The novel design of our paper includes (1) bidirectional Mamba, (2) Patch-level SSM (PaM), and (3) Pixel-level SSM (PiM). The bidirectional Mamba design is a reasonable design for the vision task, while PaM and PiM utilize Mamba as large-window kernels to attain larger receptive fields, which provide a novel perspective on the role of Mamba in vision tasks. We argue that while our method is concurrent work of other combinations of UNet and Mamba (e.g., U-Mamba and SegMamba), our performance is the bestï¼demonstrating our design novelty.

Q3: Efficiency Analysis
Thanks for your suggestions! Please see the âCommon Question 2â above.

Responses to Reviewer #3
Q1: Visualization
Thank you for the suggestion! We will include visualization results, which will also demonstrate the superiority of our LMa-UNet.

Q2: Efficiency Analysis
Thanks, please see âCommon Question 2â above.

Q3 and Q4: Future Work Suggestion
Thanks! We will explore these aspects in future work.

Responses to Reviewer #4
Q1: Effective Receptive Field
We visualize the Effective Receptive Field (ERF) for CNN-based methods (SegResNet), Transformer-based methods (SwinUNETR) and Mamba-based methods (UMamba and ours LMaUNet) at an anonymous link âimagehub.cc/image/1.bgaLYeâ. If the link is not allowed to include, we also calculate the ratio (%) of Active Pixels of after-training ERFs (192 x 192) under different thresholds (More âActive Pixelsâ mean âLarger ERFâ; â).

The Active Pixel Count (our LMa-UNet has most Active Pixels):

Conclusions:
(1) CNN-based methods focus more on local feature extraction.
(2) Transformer-based methods (SwinUNERT) have a wider range of effective receptive fields.
(3) Concurrent Mamba-based methods (like UMamba) utilize Mamba to obtain globally effective receptive fields but weaken some local receptive fields.
(4) Our proposed LMa-UNet with large window Mamba achieves larger effective receptive fields both in global and local aspects.

We will add this analysis in the final version.

Q2: Compare with UMambaBot
Thanks for your suggestions! Please see the comparison results in âCommon Question 1â and we will include it.

Q3: Notation on Figure
Thanks for the suggestion! We will add the mentioned symbols to the figure."
https://papers.miccai.org/miccai-2024/468-Paper2173.html,"We deeply appreciate the reviewersâ time and effort in providing detailed feedback and suggestions. We have addressed the key points raised by each reviewer below.
(R3)
Manually generated prompt : Since we use a fixed text encoder, the text features used for guidance vary depending on the input prompt, making the optimal prompt crucial. We found that combining all clinical information into a single sentence prompt is more effective than applying each piece individually (e.g., âa photo of {information}â). However, as you mentioned, performance could be improved with learnable prompt techniques (automated prompt generation), which have been extensively researched recently. Thus, it would be more appropriate to consider our manually generated prompt as a type of prompt generation methods with room for improvement, rather than at the concept generation level.
Dropped precision or recall : Except for the internal validation of the model using all three modalities, we presented results where AUC, accuracy, and recall values increased while precision slightly decreased. The excellent performance in AUC and recall suggests that the text-guidance and the addition of modalities contribute to higher performance. The slight decrease in precision can be understood as a typical trade-off between precision and recall. Additionally, our proposed method demonstrated improved precision and recall in external validation, proving the overall effectiveness of our model.
Reproducibility : We will release our code after final acceptance to provide sufficient information for reproducibility.
(R4)
Typo and detailed comments : We will actively incorporate them and provide more detailed explanations to aid understanding.
CLIP as text encoder : We treated CLIPâs text encoder as an LLM. CLIP can extract matching information from both text and images. Thus, we used the CLIP model with the expectation that the features extracted from our clinical prompt would effectively capture the matching clinical information in image modalities through cross-attention. Other LLM models could also perform well since they extract the semantic meaning of text more effectively than simply converting clinical information into a single vector.
(R5)
Novelty : First, unlike most multi-modal classification models, such as references 20 and 21, that simply concatenate extracted features from each modality without specific guidance, our proposed model uses cross-attention modules within the MAM to align text and image features, enhancing generalizability by exploiting text information, which has less distribution disparity between internal and external datasets. The LLM effectively extracts the semantic meaning of text, and CLIPâs training on text-image pairs helps the model better understand the visual elements embedded in text. Additionally, we ensured that the model does not become biased towards the text information alone and prevents any loss of information by re-attending to text features with unique information from the image modality. When we used a list-based vector alignment instead of CLIPâs text encoder for text input, the AUC was lower (0.829) than ours (0.877), further proving the efficacy of the LLM. Second, as a clinical aspect, our research has also novelty in its use of CT, pathology, and text for 5yOS prediction, which has not been attempted before. Each modality provides unique information for 5yOS, such as TNM stage from CT and lung cancer cell type from pathology images. Although some recent studies have used FDG PET, inflammatory, or genomic data, our studyâs strength lies in using essential yet more commonly available data.
Comparison with SOTA : As mentioned earlier, to our knowledge, there are no SOTA algorithms directly comparable in the exact context of our study. Most multi-modality research employs parallel or serial concatenation of extracted features from each modality. Therefore, we compared our method against such approaches, as shown in Table 1."
https://papers.miccai.org/miccai-2024/469-Paper1851.html,"We would like to thank the reviewers (R1, R3, and R4) for their valuable and constructive comments, which have significantly improved our manuscript. In response to the concerns, we have provided the following responses.

Methodology Concerns:
The motivation of the AE module (R3 and R4). R3 concerned about why it is applied exclusively to the CT branch of the network. R4 concerned about how the AE module enhances segmentation performance based on anatomical information.
1) In our work, we aimed to highlight the structural and functional differences between PET and CT modalities. Lesions on CT are required to be more precise than on PET, as PET images often have relatively blurred boundaries in high metabolic areas. Therefore, we introduced the AE module to focus on the CT branch.
2) The AE module consists of two components: multi-scale convolution and CBAM. The multi-scale method captured fine details and overall structure in CT images, while CBAM highlights important features and suppresses noise, leading to improved segmentation performance.

Experimental Concerns:

Reproducibility Concerns:"
https://papers.miccai.org/miccai-2024/470-Paper2275.html,N/A
https://papers.miccai.org/miccai-2024/471-Paper1064.html,"We thank R1, R3, R4 for the appreciation of motivation, novelty, and clinical uniqueness. Below, we summarize and address the major concerns.

Q1: Applicability on patients without stones (R3), and clarifications on modelâs input and automation (R4)
A1: We clarify that our model is not limited to image patches already known to contain stones. The image patches with stones are extracted through a two-step process as given in Sec. 2 Par. 2. Non-stone (NS) patches are randomly extracted from each KUB image, with the coordinates centered on the NS patch box. We extracted 410 NS patches in our 5-fold validation. Thus, the model can be deployed on new patients with or without stones. We will add the clarification upon acceptance.
During clinical deployment, our model first performs course segmentation using a sliding window strategy on a KUB image to identify high-confident NS patches and patches containing suspicious stones. For suspicious patches, the location information is generated automatically, without manual intervention, by calculating the center of the coarse stone region. Finally, the acquired location information and patches are used for automatic fine-grained classification. The process will be added to Sec. 4.1.

Q2: Importance of classifying different types of stones (R3)
A2: Reference [18] shows that current diagnostic accuracy is low. Identifying the accurate type of urinary stone is critical in precision treatment planning. For example, renal stones can be treated with medical expulsion therapy or surgical intervention, while phleboliths are common benign pelvic calcifications that often require treatment of venous thrombosis to cure. Thus, accurate classification of each individual stone is essential. We will emphasize motivation in Sec. 1, Par. 2.

Q3: Citing more related papers (R3)
A3: For CRE, we identify one paper [19] on medical knowledge-guided regions. For FPD, we find a similar idea [23] in supervised contrastive learning. Although the concepts of each proposed component are related to these methods, LEPD-Net is a distinct approach.

[19] Wang K, et al. âKGZNetâ¦.â, IEEE BIBM 2019.
[23] Khosla P, et al. âSupervised contrastive learningâ NeurIPS 2020.

Q4: Existing methods for comparison (R1)
A4: We did not conduct experiments for the following reasons. First, our model is designed for fine-grained diagnosis with auxiliary coarse segmentation to provide location information. Therefore, comparing it to fine-grained stone segmentation methods is not necessary. Second, the model [9] uses a simple 17-layer ResNet architecture, whereas we have already utilized ResNet18 (as in Table 1) for comparison.

Q5: CNN-based models outperformed Transformer-based methods (R1)
A5: We explain that although transformer-based methods can capture long-range dependencies, their heavy-weight architectures are susceptible to overfitting on our dataset. CNN-based methods, which have fewer parameters, tend to achieve better results.

Q6: Reproducibility (R1)
A6: We have uploaded the code to a GitHub repository, but concealed crucial information within the link due to double-blind review. The full link will be added upon acceptance.

Q7: Replacement of the coarse segmentation network (R1, R4)
A7: The network, which can be any segmentation network, is used to detect potential stone region(s) from the relatively large KUB image before precise downstream classification (explained in Sec. 3.1, Par. 1). For computational efficiency, we employ a lightweight model. We will leave the investigation using different segmentation models in future work.

Q8: Why use SiLU (R3)
A8: We consider location as auxiliary information to enhance image features. Thus, SiLU is chosen to smooth location features within the range of [0,1].

Q9: Strength of online augmentation (R4)
A9: Online augmentation is performed to mitigate the risk of overfitting. The justification will be added to Sec. 4.1.

Other minor comments will also be corrected.

Thank you!"
https://papers.miccai.org/miccai-2024/472-Paper1964.html,"Reviewer #5 questioned motivation of down-sampling the images for model training.

To clarify, we initially experimented with higher resolutions but found that the feature representation ability was not as good as that with down-sampled data. Given the pioneering nature of this study, our primary goal is to ensure accuracy (i.e., longitudinal structural fidelity) in data generation to better facilitate down-stream developmental neuroscience studies. To this end, down-sampling is a practical and effective strategy. Our novelty remains unaffected as we leverage an inherent consistency constraint across preceding and subsequent time points to achieve high-fidelity longitudinal generation.

Reviewer #5 mentioned that our model just focused on age and ignored the identity information.

We believe that there is a misunderstanding of the purpose of our LoCI module. Its excellent ability in extracting individualized developmental features has been demonstrated in our paper (Sect. 3, Fig. 2) thanks to well-learned subject-specific longitudinal information with corresponding age information. Besides, our method helped in the accurate estimation of individual developmental trajectories, pivotal to individual development assessment (Fig. 3). This point has been well noted and acknowledged by other reviewers. Completing data with accurate individual developmental information is exactly the strength of our model. Furthermore, the generation results from different subjects in Fig. 2a-c also clearly demonstrated significant individual identity information.

Reviewer #4 concerned the unfairness in methods comparisons in experiments.

Brain completion requires age information and subject-specific image as identity information to accurately generate data at the target age. Many generative models, e.g., StyleGAN, are not conditional generative models. Therefore, they are not suitable for our aim. We had tried Latent Diffusion, but its performance was even poorer than DDPM. Alternatively, forcing the models relying on a single guidance image to take two brain images as input (e.g., by concatenation) is too crude and lacks interpretability.

Regarding why SADM was guided by 4&7 months while LoCI-DiffCom by 7&14 months, this is because SADM can only utilize images from the previous time points to predict later time point. We had tried providing SADM with the same age guidance and input images as LoCI-DiffCom, but the results were not good. Therefore, we set the image guidance in SADM to the previous time points, as originally designed. Such comparisons again demonstrated the effectiveness of our proposed consistency constraint on longitudinal infant brain completion.

Reviewer #4 mentioned that some unclear statements in the figures made it difficult to understand.

Regarding the ambiguityÂ ofÂ age embedding in Fig. 1, we would like to clarify that the LoCI module was mainly designed to fuse image information from two time points. Age information served as important additional condition, encoded into a feature vector with the same dimensionality as the input image features. In our implementation of LoCI module, we simply added it to both inputs of the LoCI module. Hence, we did not explicitly annotate the age embedding in Fig.Â 1 to avoid overcrowding the diagram.

Reviewer #4 mentioned how to complete the missing data if there are no preceding or subsequent time points.

Our work focuses on the scenario where longitudinal data with at least two time points is available, which is common in studies that collect developmental or longitudinal neuroimaging data. The case where the prior time point is missing would be unusual in our targeted applications, as longitudinal studies typically acquire a baseline scan to establish a starting point. If the subsequent time point is missing, one can still call the participant back for an additional scan at later time point. Therefore, our method is ideal and valuable to most of these studies."
https://papers.miccai.org/miccai-2024/473-Paper2565.html,
https://papers.miccai.org/miccai-2024/474-Paper3369.html,"We thank the reviewers for their constructive feedback. Upon publication, we will share a GitHub repository for our model to facilitate further research. We address the reviewersâ points below, organized by section and noting the specific reviewer (R#) who raised each.

R1 suggests detailing subject characteristics and conducting subgroup analyses (e.g., by age, density categories) during inference. We value this suggestion and plan to collect and incorporate more detailed demographic and risk factor data in future studies to enable these analyses.

R1 highlights the potential impact of variations in the characteristics of patients with more extensive historical data on our results. Thanks for pointing this out. We will address this in our revised discussion and look into it in future studies.

R1 suggests improving the experiments section by adopting the statistical methods used in similar works. Our evaluation method is similar to bootstrapping particularly in how we construct each pseudo test set by randomly choosing one present point from every subject. This approach, combined with 10-fold cross-validation, is designed to enhance the robustness and reliability of our results. We believe this provides a statistically rigorous framework for evaluating model performance. We acknowledge that incorporating statistical tests, such as those used in referenced works, could further be informative. We will include the result of a DeLong test between the fifth year ROCAUC score of the LoMaR (4*) vs. LoMaR (no history) in the revised version of the paper.

R3 notes that the ROCAUC results suggest the model may not be fully utilizing the longitudinal input. Although LoMaR shows up to a 10% ROCAUC improvement over Mirai in the fifth follow-up year for tests excluding screen-detected cancers, as detailed in the supplementary material, we acknowledge the potential for further model enhancement. Our initial focus was on assessing the impact of longitudinal mammogram history on prediction accuracy, which included a grid search to optimize hyperparameters. Moving forward, we will explore alternative architectures, expand data sources across multiple sites, and incorporate more longitudinal data types to enhance LoMaRâs performance. Furthermore, we plan to collaborate closely with clinical experts to conduct a more detailed analysis of why LoMaR demonstrates more substantial performance improvements over longer time horizons.

To address R1âs concerns regarding the relevance and use cases of localizations: We agree that 5-year risk might be dominated by non-localizable features. However, we still believe localization can be informative for two reasons: 1) It can enhance interpretability of LoMaRâs predictions, which can be important for validating and real-world deployment; and 2) It can assist radiologists by highlighting areas of interest, helping to focus their attention, and potentially reducing the likelihood of misreading scans. We will update our manuscript to include these points. We also agree that GradCAM can have shortcomings. We will add these points in our revised discussion.

Regarding the cases in Figure 2, highlighted by R3: We have shown a representative set of test subjects with breast cancer where historical mammograms corrected LoMaRâs predictions. In future work, we intend to perform a comprehensive analysis of localization results across TP/TN/FP/FN classes, with input from clinical experts.

Regarding the feedback from R3 and R4 about the lack of detail concerning the observed trends from the GradCAMs: We acknowledge this oversight and will extend our discussion in the revision.

R1 and R4 recommend that our conclusion discuss clinical merits and shortcomings. Thank you for pointing these out. We will include them in the revised discussion.

Finally, we will implement the minor changes and clarifications suggested by R1."
https://papers.miccai.org/miccai-2024/475-Paper3725.html,"We thank all reviewers for comments and recognition of our contributions, including 1) âoffering novel methodology in addressing longitudinal consistency challengesâ (R1, R3, R4); 2) âtackles the complex task which have not resolvedâ (R3) and âholds potential for various extension tasksâ (R1); 3) âleading to more robust and interpretable predictionsâ (R3, R4).
R1 requires detailed discussion of longitudinal consistency. Our network ensures consistent predictions over time through a temporal consistency constraint (Fig. 2). It yields predicted trajectories closely aligned with ground truth and mirrors actual developmental patterns, unlike comparison methods, highlighting our approachâs superior effectiveness. The unique decrease in cortical thickness observed in individual 1 may be due to individual differences, and our method captures these individualized patterns, demonstrating its ability to adapt to individual variability and provide precise predictions.
R1 mentions the inferior results on sulcal depth compared to CSUNet. CSUNet might slightly outperform our method on sulcal depth, but has inferior performance on cortical thickness, surface area, and myelin content. One possible reason is the trade-off among 4 cortical features introduced by averaging their temporal consistency losses, aiming to minimize variability across time points. This approach may lead to a greater emphasis on dynamic features, while placing less focus on comparably stable features like sulcal depth. Consequently, our method may underemphasize the importance of sulcal depth during training, which can be solved by adjusting weights of different features in DTW loss. Notably, our method has smaller variation with the same PCC values, indicating its greater stability compared to CSUNet.
R3 concerns the difference with work [23]. Our framework distinguishes itself by generating consistent results stemming from independent inferences. Our objective is to achieve precise predictions while minimizing discrepancies across multiple longitudinal results for the same individual, a key distinction from previous approaches (including [23]), and the visualization of the predicted trajectories presents solid evidence for maintaining the longitudinal consistency in Fig. 2. Additionally, our framework simultaneously predicts multiple cortical features, leveraging their implicit mutual correlation, differing from [23] which separately predicts each cortical feature. Further, our approach employs a lightweight autoencoder with an end-to-end training strategy, contrasting to the memory-intensive stage-to-stage training strategy in [23]. 
R3 and R4 concern the experimental details. 1) For the parameter setting, we prioritize the magnitude and significance of different losses. Specifically, the DTW loss is thousand times of others, so we manually decreased its weight. Besides, to obtain realistic predictions by separating the age and identity features while maintaining the longitudinal consistency, we tuned the weights of losses using validation datasets based on their relative importance and contributions. 2) For the computational cost, we have adopted a lightweight autoencoder framework with reduced channel numbers to minimize parameters. Our training process utilized a 3090 RTX GPU with the batch size of 16, requiring about 5 hours for 100 epochs.
R4 points out the limitation of dataset and comparison methods. We apologize for the confusion on dataset description. We excluded 30% from the dataset as the hold-out test dataset, the 5-fold cross validation was conducted using the remaining 70%. We will include other longitudinal datasets to further validate our framework. Besides, we are striving to compare state-of-the-art methods. However, most works do not work directly on cortical surfaces. We will address this limitation by referencing more recent works (e.g., Ha et al., TMI, 2022 and Fawaz et al., biorxiv, 2023.10.16.562598) that can be adapted for cortical surface."
https://papers.miccai.org/miccai-2024/476-Paper1379.html,"We sincerely thank all reviewers and ACs for dedicating time and effort to review the paper and providing insightful comments and advice. We would like to address questions as follows.

To Reviewer #1
Q1: Include a statistical analysis in the abstract.
A1: Thanks for the advice. We will add quantitative results to the abstract and simplify its content in the revision.
Q2: Clarify the ablation study from experimental design.
A2: Sorry for the lack of clarity. The ablation study evaluates three loss function terms (Section 2.3) to verify their effectiveness. We will revise for clarity.
Q3: The main limitation is needing both contrast and non-contrast images in a clinical scenario.
A3: Sorry for the confusion. Single modality data is actually sufficient for clinical applications. Training with temporal and modality consistency reduces reliance on fine annotations. During testing, the lesion area is predicted by mapping the highest lesion probability patch onto the original CT image.
Q4: Include intro of results, define SOTA, and rename experiments section.
A4: Thanks. We will revise according to your valuable advice.

To Reviewer #3
Q1: Section 2.3 title could be misleading.
A1: Sorry for the confusion. We will change it to âComplete Frameworkâ for better understanding.
Q2: Include annotation types and quantities in the table.
A2: Thanks. Annotation types and quantities are detailed in Sections 3.1 and 3.3:
Unsupervised: GradCAM, EigenCAM.
Fully supervised: Sahoo, ResNet50, MobileNetV3-L, RegNetY-128G, EfficientNetV2-L, ConvNeXt-L, ConvNeXtV2-L.
8.3% supervised: Ours (8.3% of bounding boxes used).
We will clarify these in the revised table.
Q3: Integrate the method with other backbones for fairer comparison.
A3: Thanks for constructive advice. We integrated our method with the ConvNeXtV2-L backbone (see âOursâ in Table 1 and 2), showing a 1.4% increase in classification performance. Integration with other backbones also improved performance by 1-2% and showed promising localization results. Due to space limitations, we present only ConvNeXtV2-L results.
Q4: Specify the backbone used for GradCAM and EigenCAM.
A4: We used the ConvNeXtV2-L backbone for both. We will add detailed descriptions in the final version.
Q5: Address dataset availability.
A5: The dataset is being desensitized and reviewed for public release.

To Reviewer #4
Q1: Few related works exist due to the topicâs nature. Please elaborate.
A1: This underexplored domain has few related works. As stated in Section 1, most CRC CT diagnosis methods lack interpretability or require extensive labeling. Our work addresses these issues with loose location self-supervision and mask correction loopback, aiding clinical CRC diagnosis.
Q2: Dataset availability and applying the method to public CRC datasets.
A2: Thanks for valuable feedback. Our dataset is being desensitized and reviewed for public release. We used Task 10 from the Medical Segmentation Decathlon for localization, obtaining bounding boxes from segmentation masks. Testing pre-trained models (ConvNeXtV2-L backbone), our method achieved a P-IoU of 47.09%, compared to 42.83% with fully supervised, validating our approachâs generalizability.
Q3: Ambiguity in choosing the number of annotated slices.
A3: We apologize for confusion. We used 6304 non-contrast CT slices (8.3% of the total) with lesion location bounding box labeling. We also tested with 1%, 5%, and 10% of the annotated data, achieving Accuracy and P-IoU of 87.23%/27.02%, 90.04%/50.86% and 91.25%/63.64%, respectively. Validating 8.3% proved effective. More details will be added in the revision.
Q4: More discussion on limitations and feasibility in diverse clinical environments.
A4: Thanks. The main limitation is that for diseases with significant location variations, such as large cardiovascular curvature, the CT imaging position may deviate, introducing erroneous constraints. Future improvements will consider additional criteria to avoid these inaccuracies."
https://papers.miccai.org/miccai-2024/477-Paper2877.html,N/A
https://papers.miccai.org/miccai-2024/478-Paper0900.html,"We thank all reviewers for their comments, especially in noting our paper is well-written (R1, R3, R4); proposes a novel (R1) and technically sound (R4) method; demonstrates effectiveness with sufficient ablation and comparison experimental evaluation (R1, R3, R4); achieves promising results with low catastrophic forgetting (R1).

Q1: Task label/ID at test-time (R1).
A: Weâd like to clarify that our method does not require the dataset/task ID of the input image in testing. Similar to SUN[11], we also have an output merging module, which uses an entropy-based ensemble to combine the prediction from all tasks (Model Inference in Methods).

Q2: Single dataset upper bound (R1).
A: Yes, they use pre-trained weights on the TotalSegmentator dataset.

Q3: (R4, R3) New insights & differences between our implementation of LoRA and other non-CLâs implementation of LoRA, such as SAMed and SurgicalDINO.
A: SAMed and SurgicalDINO adopted vanilla LoRA to the linear attention layer. In contrast, we implement LoRA to both FFN and 3D Conv layers in the PVT network to solve the more challenging CL task (obtaining the ability to segment new organs while preserving all previously learned organs), we conduct a holistic exploration of the architectural modification. We are the first to use LoRA in continual segmentation and design an effective architecture-based CL framework, which can minimize forgetting, maintain low params increasing rate, and achieve high performance on all sequentially learned tasks.

The rationale/insights of implementing LoRA to the feed-forward network (FFN) is to provide extra feature aggregation capability for segmenting new unseen organs. Further extending LoRA to 3D Conv (Methods 2.3) could help handle the spacing variation in sequential medical datasets, (LoRA matrices in 3D Conv would have different shapes from the original LoRA). The ablation studies of these two new LoRA components are shown in Table 2.

Q4: Difference between CL task and EFT task (R3). 
A:  EFTâs goal is to adapt a pretrained model to a new task yet forget the previously learned knowledge. In contrast, CL sequentially expands the model to an arbitrary number of new tasks while âno forgettingâ for all old tasks â more challenging than EFT.  R3 is concerned that our method is not CL-based as âno learnable parameters are shared across tasks and the model does not leverage stream data for CLâ. However, this is not true. According to [S1], CL is defined as âincrementally learning new information from a non-stationary stream of dataâ and is not related to shared parameters ([S1] van de Ven, G.M., et al. Three Types of Incremental Learning. Nat Mach Intell 2022). Ours is an architecture-based CL method, where it shares the base PVT and adds new LoRA incrementally for new tasks. Our experiment strictly follows the stream data like other CL works (SUN[11] ICCV23).

Q5: Ablation results where no LoRA is used (R4).
A: Without LoRA, it degrades to a vanilla fine-tuning model when learning a new task â causing severe catastrophic forgetting on old tasks. This âfine-tuningâ model performs even worse than the regularization-based CL methods, such as MiB[2] and PLOP[6]. If LoRA is applied only to the attention layers (as SAMed and SurgicalDINO), results are shown in the ablation Table 2 (referred to as âbaseâ), indicating a 5%+ Dice drop as compared to our proposed framework.

Q6: In-house dataset (R3).
A: We use the private chest organ dataset (CHO) because the public chest organ dataset is rare with very few labels, e.g., 4 organs and 60 CTs in SegTHOR. In contrast, our private CHO has 16 organs and 153 CTs. Note that similar private data are observed in other CL work, e.g. SUN [11] (ICCV23) also uses 3 in-house datasets. Yet, we appreciate this comment and will add more public datasets in the future."
https://papers.miccai.org/miccai-2024/479-Paper1160.html,"We thank all the reviewers for their time and comments on our manuscript.

Q1: The absence of supplementary material:
A1: We have revised the supplementary material to ensure compliance and will include it in the updated submission.

Q1: Specific details of off-line classifier.
A1: The classifier uses an image feature extractor (e.g., CLIP) to extract features from k randomly selected images per category as exemplars. During inference, the category is determined by the highest cosine similarity between the test image and the exemplar s of each category. Its high accuracy supports continuous learning expansion.

Q1:More substantial experiments on class-level task. 
A1: Due to time constraints and unavailability of the StructSeg test set, we conducted preliminary evaluations on three datasets: TotalSegmentator->Flare22->BTCV, achiving Dice scores of 87.65, 83.22, and 81.3.

Q2:Why not compare the method of [11] in table2? 
A2: [11] lacks available code, preventing a short-term comparison. We plan to reproduce [11] and include comparisons in the revision.

Q3:Consider LoRA using only W0.
A3:Our design aims to transfer useful knowledge across datasets. As shown in Table 1, the results show that sharing LoRA weights does not degrade performance on distinct datasets and can improve it for similar ones. Incorporating LoRA weights from ACDC and ISIC increased accuracy by 2% on the COVID-19 CT dataset (compared with using only w0), demonstrating the benefits of using LoRA parameters.

Q1:Investigate more MoE methods on CL tasks and more details about the dataset.
A1:We will add the following to the article. âIn [1], the author proposes Lifelong-MoE, an extensible MoE that dynamically adds model capacity via adding experts with regularized pretraining. â [1] âLifelong language pretraining with distribution-specialized experts.â
The COVID-19 CT dataset consists of 100 images, with 50% for train and 50% for test. The partitioning of the BTCV and LiTS datasets followed the guidelines specified by the original dataset authors.

Q2:Architecture-based methods to be compared.
A2:Current architecture-based methods in CL are mainly focused on classification tasks. It would be inappropriate to migrate it to a segmentation task.

Q3:A lack of explanation for i in Eq.(1).
A3:The letter i refers to pixels.

Q4:Experiments using only MoE w/o using low rank.
A4:The results (with MoE w/o LoRA) on ACDC->ISIC->COVID-19 CT show improvements of 0.13%, 0.16%, and 0.55% over the single-task models listed in Table 1. These findings demonstrate MoEâs effectiveness and beneficial interactions across the datasets.

Q1:The reason for using transformers rather than CNN.
A1:The leading medical image segmentation networks mainly utilize transformers, guiding our choice based on established practices. We have developed a method to transfer weights, pre-trained on ImageNet, to 3D medical images.

Q2:Provide compared results in Table 1 and re-implement the ways using the same backbone in Table2.
A2:We provide methods using the same SETR in Table1, such as on LwF, ILT and PLOP, our method outperforms the best of them about 5% average Dice. As presented in Table 2, all methods employ the same Swin-UNETR.

Q3:The lack of results of the rank value and the lack of changing the sequence of CL.
A3:We observe that rank 4 yielded Dice scores of 90.05, 89.00, and 72.68 across the ACDC->ISIC->COVID-19 datasets. Rank 16 improved results to 92.06, 90.68, and 74.02. Changing the sequence of datasets to ISIC->ACDC->COVID-19 and COVID-19->ACDC->ISIC, the dice changes slightly (only about 0.1%), demonstrating robustness.

Q4:Conduct an ablation study of the gating strategy.
A4:In the class-level task, we replaced the text guided way with a fully connected layer for gating. Compared with the original LoRA MoE, the slightly worse (2% average Dice) ablation experiment results reflect the effectiveness of the text guided gating way under the CLIP model."
https://papers.miccai.org/miccai-2024/480-Paper2700.html,"Summary: R1(A),R3(WR),R4(R),R5(WA). We thank the reviewers for the insightful feedback and are pleased they find our work novel(R1,R3,R5), well-motived and relevant (R5,R4). The main concerns include clarifying method details and discussing related baselines in literature, which we address below:

(R1,R3,R4)ââLinear Probe (LP) details and the impact of hyperparameter-Kâ: To clarify, LP is modeled as a single learnable linear (randomly initialized) with dimensions D x D that projects image features (x^D) to D prior to l2-norm for similarity estimation with frozen text-prompts. For pooling, both Max and AbMIL were employed on image features prior to feeding inputs to LP. Indeed, we verified the impact of K prompt-pairs (i.e., K=2,4,8,16) and observed both Ours and CoOp are less sensitive with marginal performance gains (+1%) in the 16-shot setting.

(R4)ââClarify core contributions and motivationâ: Our study aims to address efficient transferability of vision-language (VL) models in the low-data setting as opposed to full fine-tuning that requires large-scale data, especially in noisy labeled scenario for whole slide images (Sec.1 Paragraphs.1 & 3). Herein, we highlight the importance of synergistic adaptation of both language and visual features, without degrading the inherent zero-shot ability of VL models (Sec.1 P4).

(R4)ââReconsider Eq.(5) & paper organizationâ: To clarify, Eq.(5) is a standard operation in VL literature for computing image-text similarity (logits) with softmax; used here for instance re-weighting. In addition, Alg.(1) provides a high-level summary of our training procedure for clarity, and the use of two (+Ours in Tab.3) was intended to show the impact of context-pooling. All typos will be checked and corrected accordingly.

(R3,R5)ââDifferences with prior works and missing comparison.ââ: We agree our context pooling and that of Qu et al[19, TOP] could be viewed as similar but differ in three core aspects: (i) no use of ChatGPT to extend class description knowledge, (ii) TOP employs separate prompt learners for slide- and instance-levels (with diff templates), whereas we only used a single learner, and (iii) using a contrastive loss without additional objectives as in TOP[attention loss]. Our approach is more distinct and efficient as prompt optimization does not require multi-loss balancing between separate models. Moreover, comparison with TOP was difficult as their code was unavailable at submission time, and PromptMIL mainly addresses visual prompting i.e., prefix prompting, different from our study and recent literature that shifted to textual adaptation and post-visual prompting.

(R3,R5)ââComparison fairness and the credibility of AbMILâ: In all experiments, text context vectors (K=4), few-shot data-splits/hyper-params were fixed for all methods with default settings of prior works for fair comparison [Sec.3]. In regards to AbMIL, our evaluation show it as a strong upper-bound when trained with (%100 data)[Tab. 1,2], but was limited in the few-shot regime. To clarify, results in Fig.2 are based on 16-shot models indicating severe overfitting for AbMIL (w/ high FPR). Moreover, while including more complex MIL approaches (DTFD-MIL, DSMIL) would be beneficial; we believe gains from leveraging different approaches can be exponential, and equally require method re-design.

(R5)âReadability of Tab. 1 and interpretation of learned prompts (tSNE)â: We will update Tab.1 with only AUC , including incorrect references as suggested. We agree that showing visually learned words would be beneficial, however note that as reported by CoOp and recent works, learned virtual context do not necessarily map to legible words and is left to future works including more baselines with different pathology VL models."
https://papers.miccai.org/miccai-2024/481-Paper3276.html,"We are grateful to the reviewers for their time and effort. Their constructive comments have been very helpful in revising our manuscript and improving its quality. Addressing the reviewers concerns (below) further emphasized the simplicity yet effectiveness of our proposed model. (Code & Supplementary results requested by the reviewers will be available on Github post-acceptance)
Q1 Comparison with temperature scaling.
R1, R2, R3: We used temperature scaling on the Chaoyang & MHIST datasets with ResNet34/50, and our method showed the best calibration performance. Experimental details will be shared post-acceptance.
Q2 High computational cost.
R2: Training LS+ has the same cost as standard LS. Our method does not require multiple validations. V^{acc} are the predictions from a hard label (HL) model, computed only once before LS+ training, with negligible cost at inference.
Q3 Sensitivity to class imbalance.
R1, R2: In case of data imbalance, while the accuracy of rare classes tends to be low, the predicted probabilities are generally high. LS+ reduces the predicted probabilities by aligning them with validation set accuracies using KL to achieve better calibration. We have included experiments on an imbalanced dataset (ISIC; Classes=7) (Page No. 6) to show that LS+ improves model calibration. The retention curves in Fig. 1 and the results in Table 1-D3, indicate that despite data imbalance, the model exhibits more reliable behavior and is less sensitive to data imbalance.
Q4 Missing implementation details
R1, R2: For LS+, we used alpha=0.5; Cross entropy loss function is used in the first training step; Other implementation details are referenced from [7, 18]; We will provide the complete code including ablation studies requested by the reviewers, after acceptance.
Q5 Comparison with standard metrics for calibration including ECE KDE.
R2: In our experiments, we used standard metrics to assess calibration (ECE, SCE, ACE, Brier, NLL) as cited in 2023, MICCAI [1, 2, 3]. Following the reviewerâs suggestion, in the given time limit we also compared the ECE KDE metric on the MHIST dataset, and our method demonstrated the best calibration."
https://papers.miccai.org/miccai-2024/482-Paper1926.html,"We sincerely appreciate the diligent and professional work of the reviewers, which has given us the opportunity to provide our rebuttal.

With regard to the reproducibility of the paper (R3, R4), We have made the source code accessible and intend to include additional content, which could not be accommodated in the paper due to space constraints, in future research. It is important to note that there was an issue with the anonymous URL provided for the code in the paper, and the âLSSNet-FE31â in the URL needed to be modified to âLSSNet-8C86â.

About the motivation of the study (R3, R4, R6), we want to design a model that can balance the segmentation accuracy and network complexity and prioritize the segmentation accuracy. We would like to supplement local features and polyp boundary features in the network to improve the segmentation accuracy. Therefore, we propose a Local Feature Supplementation Structure consisting of SGR, IAF, and MFE modules and a Shallow Feature Supplementation Structure consisting of the SFS module and the Convolutional branch. In terms of the order and positioning of the modules (R4), the role of the Local Feature Supplementation Structure determines the order and position of its constituent modules SGR, IAF, and MFE. SGR is responsible for receiving the local features supplemented by the previous layers and reducing the inter-layer semantic gap. IAF fuses the features of the current layer with those of the previous layers. MFE performs the extraction of multiscale features from the fused features, so these three modules are progressive. The SFS module requires the prediction map to calculate the fuzzy area, and its position is also fixed. We will further clarify the research motivation in the paper.

The generalization performance of the network (R4, R5) may not be described clearly enough in the paper, and the generalization ability experiments are consistent with PraNet. Three invisible datasets, ETIS, ColonDB, and EndoScene are used to verify the generalization ability of the network. From Table 1 in the paper, LSSNet improves the mDice on these three invisible datasets by 2.65%, 1.08%, and 0.62%, respectively. In future research, cross-validation will be added to further validate the generalization performance of the network. About the comparison experiments (R4, R6), we can only illustrate them under limited conditions as we are not allowed to add new experimental results. Due to the proprietary characteristics of polyps, it is fairer to compare polyp segmentation-specific networks rather than medical segmentation-generic networks (e.g., nnUNet and SwinUNet). Moreover, the compared polyp segmentation networks all outperform their compared SOTA methods, which makes the experimental results more convincing. U-Net is compared because LSSNet is an improvement of U-Net. We cite the experimental results of Polyp-PVT and SSFormerPVT from the paper of the comparison network CASCADE. The mDice in EndoScene, ClinicDB, Kvasir, ColonDB, and ETIS datasets are lower than CASCADE in both Polyp-PVT (88.71%, 93.08%, 91.23%, 80.75%, 78.67%) and SSFormerPVT (89.46%, 92.88%, 91.11%, 79.34%, 78.03%), and of course, lower than our LSSNet, so we do not compare these two networks. In terms of network efficiency (R5), while the Params and FLOPs of LSSNet have increased by 1.687M and 2.286G respectively compared to CASCADE, the FLOPs remain lower than the average of all the networks under comparison. The performance of LSSNet has been significantly enhanced, despite a slight increase in parameter and computational costs, making it highly valuable for practical applications.

Due to the length limitations of the paper, certain descriptions may not have been sufficiently clear (R3), and there were some detailed errors (R3, R4). We carefully reviewed the paper multiple times, rephrased unclear sections, and rectified detail inaccuracies."
https://papers.miccai.org/miccai-2024/483-Paper0562.html,"Dear Reviewers and Chairs,

Thank you for all your insightful comments.

WSR Texture Issue for Reviewer #1 & Reviewer #4Due to the physical imaging characteristics of CT, the values in a CT image reflect tissue density. Within the same Region of Interest (ROI), density is typically uniform, especially considering our detailed division into 112 distinct areas. Therefore, the WSR module does not reconstruct texture, which does not significantly impact the reconstruction loss; differences in density between different ROIs are substantial, which more critically influences reconstruction loss. The primary goal of the WSR module is not to achieve perfect reconstruction, but rather to use reconstruction to facilitate higher accuracy in identifying ROIs, which represent purely structural information.

WSR applied for LDCT Issue Reviewer #1 & Reviewer #4The parameters of the WSR module reflect the physical density properties of ROIs. Since both NDCT and LDCT operate on the same imaging principle and yield consistent density measurements for the same structures, the WSR module trained on NDCT can be directly applied to LDCT without adaptation issues.

U-Net Alternative for Reviewer #1In domain adaptation tasks, loss functions are crucial for effective adaptation and a good model architecture fully leverages the potential of these loss functions. For instance, GANs employ adversarial loss and FVP utilizes semi-supervised cross-entropy loss. Our innovation lies in proposing a new reconstruction loss tailored for the universal model. We chose a Fourier-based U-Net due to its impressive performance in recent FVP study, ensuring a fair comparison of results. While other neural network architectures are feasible, they do not compromise the novelty of the WSR. During testing, target domain images pass through the frozen segmentor and the frozen WSR, and U-Net parameters are optimized by minimizing the reconstruction loss.

Lack of comparison with SOTA Issue for Reviewer #1In our manuscript, we included comparisons with recent UDA methods such as AIGAN and FVP, both introduced last year. AIGAN has achieved SOTA performance among GAN-based and traditional methods, while FVP has set a new benchmark in source-free UDA tasks. Our experimental results demonstrate that our method outperforms these latest UDA methods, affirming its effectiveness and innovation in the field.

Lack of Ablation Issue for Reviewer #3The ablation experiments you mentioned are included in Table 1. This table shows that the NDCT model performs well on LDCT images without adaptation (first row), due to our enhanced dense label annotation. However, for more challenging tissues like vessels and abdominal organs, our LUCIDA model significantly improves performance from 84.0% and 87.7% to 91.7% and 93.8%, respectively. These enhancements are critical as the higher model accuracy benefits various downstream tasks, including report generation and kinetic analysis of PET/CT, thus facilitating further research in medical imaging.

** Why do LDCT and NDCT adaptation for Reviewer #3**

Formulas 2 and 5 for Reviewer #1 & Reviewer #4We appreciate your comments and will correct this typo in the manuscript.

Sincerely,
The Authors"
https://papers.miccai.org/miccai-2024/484-Paper0979.html,"We sincerely thank to all reviewers for the valuable feedback. We elaborate on their comments below.
1.Typos and errors (R1, R3 & R4) Re: The manuscript has undergone thorough examination and revision to increase the languageâs clarity [ f_pre and f_post in page 4, acronyms in the article (MRI & VQA), L_cont in the Fig.1, improper words, and sentence revise]. Also, we have reformatted the equations.
2.Missing data details (R1, R3 & R4) Re: We have provided patients information [cancer molecular subtypes (HER2+, HR+/HER2-, and triple-negative), age, cTNM stage] in each dataset. MRI scannersâ manufacturers are also involved (Cohort A-Philips 1.5T Achieva and 3.0 T Ingenia, GE 1.5T Optima MR360, Siemens 3.0 T Verio; Cohort B-GE 3.0T SIGNA Pioneer). For enrolled 579 patients, each patient includes MRI before and after treatment, WSI biopsy and pCR label. The number of pCR / non-pCR cases in training and validation are available. The details of pCR labels and interobserver variability are provided in the supplementary files.
3.Data preprocessing (R1) Re: Beyond dimension, we have done N4 bias field correction, resampling, histogram normalization and z-score on MRI data. Further, tissue regions in WSI are automatically segmented by OTSUâs method following Faisal et al.
4.Network and Training Info (R1, R3 & R4) Re: The Network about bilinear pooling or attention scores for WSI features are elaborated. All of the experiments were conducted on the same training/validation split under python language. The numbers in the tables are average results based on five runs and SDs are calculated.
5.Work objective (R1) Re: Patients achieving pCR could benefit from breast-conserving surgery, even omitting surgery instead of breast mastectomy. Accurate assessment of pCR before surgery is essential for tailoring surgery plans and could select patients with good prognosis in advance, which is an urgent need. However, the gold standard of pCR depends on the pathological results of surgical specimens. So the main objective of this work is to predict pCR based on pre-surgery data and assist guiding surgery strategy. Proposed M2Fusion to handle multi-time multimodal data is unique to this work, adding a fresh perspective to the field.
6.Comparisons (R3) Re: HMCAT (Li et al.) in Table 2 is an attention-based fusion methods, which is not superior than M2Fusion. Earlier, we also compare another attention-based model (Zhou et al.), whose results are 0.7037 and 0.6949, lower than M2Fusion. For other works in pCR prediction, MLDRL (Yue et al.) and Concat (Shah et al.) are conducted. The former did not converge (NaN loss) and the latter is shown in Table 2. MoCo and SimCLR are based on self-supervised learning. However, we hope to end-to-end predict pCR. Thanks a lot for R3âs brilliant idea, we will explore it in our future study.
7.Evaluation (R3 & R4) Re: Due to page limitation, we did not show ROC curves in the article. We will supplement them. AUCs are compared by Delongâs test. We also show Grad-CAM heatmap on pre and post MRI to confirm focused regions.
8.Insufficient discussion (R4) Re: Compared to other multimodal fusion methods and single modality methods, M2Fusion yields statistically significant improvement (Delongâs test) for pCR prediction with an AUC of 0.7346 in the internal validation set and 0.7992 in the external test set. Besides, FPVs are 0.2941 and 0.2759 respectively, lower than other methods. We have carefully check incorrect prediction cases. Some non-pCR cases are indeed identified as pCR. Breast cancer experts were consulted. They admitted that these are truly tough to identify. After NAC, tumors show regression and residual tiny tumors are hard to detect on MRI. Also, the surrounding tissue of the tumor may undergo significant changes after NAC, such as fibrosis, or necrosis, which may mask or mimic residual tumor tissue, making detection harder. We also have revised the conclusion and provided insight that what is given in Sec.4.2."
https://papers.miccai.org/miccai-2024/485-Paper1472.html,"Reviewer #3
Q1: Regarding the major differences and novelty of our method compared to the Dynamic Routing paper, could you please provide further clarification?

A: The major differences and novelty of our method compared to the Dynamic Routing paper are as follows:

Q2: From Table 2, it appears that the STU-Net-L baseline outperforms our proposed method on both AMOS-CT and AMOSCT+MRI datasets, and is only slightly inferior to M4oE on FLARE22. Could you explain this contradiction to the original target of a âfoundation modelâ?

A: While STU-Net-L performs better than our proposed method on the given datasets, it is important to consider the context and limitations of the comparison. STU-Net-L is trained on a single dataset in each task, while our method leverages multiple datasets for training in multi-tasks. Models trained on a single dataset tend to capture subtle features specific to that modality, while utilizing multiple datasets emphasizes the relative balance of features across different modalities. Additionally, the significant difference in model size between STU-Net-L and our model can impact performance. Comparing our multi-modal foundation model directly to STU-Net-L might not provide a fair assessment of the effectiveness of our approach. We appreciate your feedback and the opportunity to address these concerns.

Reviewer #4:

Q1. How is the proposed model considered a foundation model if the sample size of the datasets used is not extensive enough?

A: The current sample size of our datasets may not meet the criteria of a traditional foundation model due to computational constraints. However, we plan to address this limitation in future work by scaling up M4oE, incorporating more data, and increasing the modelâs capacity to align with the definition of a foundation model.

Q2. Could the authors provide more rationale for selecting the baselines and clarify if they are state-of-the-art models for multimodal medical image segmentation?

A: The baselines were chosen based on their relevance to multimodal semantic segmentation and the availability of evaluation datasets. While not all baselines are specifically categorized as foundation models, they serve as suitable benchmarks for comparison in the context of multimodal medical image segmentation.

Q4. Why didnât the authors include any unsupervised baselines in their comparison, considering that M4oE is a self-supervised model?

A: Although our focus was on supervised methods in this study, we appreciate the suggestion of including unsupervised baselines in the comparison. This addition will provide insights into the performance gap between supervised and unsupervised methods. We will address this point in the revised manuscript to offer a more comprehensive evaluation."
https://papers.miccai.org/miccai-2024/486-Paper2097.html,"We thank all the reviewers for the thoughtful feedback. We are encouraged by their recognition of the novelty and feasibility of our proposed method (R1, R3). We will incorporate all of the valuable suggestions in the camera-ready version.

R1
We greatly thank you for your appreciation. About âbidirectionalâ, our proposed MAdapter highlights the bidirectional interaction between textual and image features, distinguishing it from previous unidirectional text-guided medical image segmentation methods. 
We will provide a clearer statement in the next version.

R3
Q1: Method logical flow.
We will revise the method section to keep it clear and logical.
F_v & F_l: the vision and language features output by the encoders. The size of F_v is H_iÃW_iÃC_i, where i denotes a certain stage.
f_v & f_l: features after interaction. We apologize for the typos of f_t, which should be corrected to f_l. Before interaction, we perform a resize operation to keep the sizes of f_v (i-1 stage) and F_v (i stage) the same. Suppose this same size is H1ÃW1ÃC1, then f_l is projected to Lâ Ã C1 (Lâ is the number of tokens after projection). And the consistency of C1 is necessary for MHCA. 
The image features, extracted from a CNN-based encoder, are inherently pixel-wise and do not involve partitioning. In MHCA, the processing of features includes matrix multiplication and attention score calculation for each head.

Q2: Questions about f_g.
âf_gâ is created by pooling the text sequence from multi-stage into a vector of size Câ. f_g is a global representation, providing sentence-level information that f_l lacks. In our ablation study, we do not use f_g to achieve global alignment. Since this is the only difference between our decoder and a normal segmentation head, the effectiveness of the decoder inherently proves the necessity of f_g. 
Introducing global visual features does not enhance the recognition of image structural details.

Q3: Comparison methods.
Unet and Unet++ are widely used for single-modal medical image segmentation in recent MICCAI papers. Following prior works, we take them as our baselines. The mentioned methods are tailored for polyp segmentation and may not be suitable for another lung infection segmentation task. Moreover, our method significantly outperforms PRANet and SANet, and surpasses PVT and Polyper on some datasets, demonstrating the versatility and superiority of our multi-model method.

Q4: The visual-related noise?
Image noise is fine-grained and at the pixel level. Typically, low-level visual noise has negligible impact on higher-level text features. In practice, we should focus more on text noise, which derives from the subjectivity of doctors and would introduce huge bias in segmentation.

Q5: Other questions.
Our trainable parameter is 51.9M. In addition, we will calculate the average evaluation metrics across the five datasets, and modify the figures while ensuring their consistency with the text.

R4
We will carefully revise the issues about the paper writing.
Q1: Different encoders or pretraining strategies.
Our components can be integrated with most pre-trained encoders. We have used other vision and language encoders (ConvNeXt-seg, Vit-B, PubMedBert, etc.). Under different settings, our MAdapter improves segmentation performance compared to previous feature fusion methods.
Our focus is the feature interaction during fine-tuning rather than pretraining strategies.

Q2: Novelty.
As acknowledged by R1 and R3, our novel feature fusion scheme and significant annotation work are valuable. Here, we highlight our innovations: the MAdapter addresses the impact of textual noise by facilitating bidirectional interaction between multi-level features. A flexible decoder refines the global alignment by introducing sentence-level features. Also, we introduced a novel annotation scheme and applied it to polyp datasets. These contributions significantly expand the research in this field."
https://papers.miccai.org/miccai-2024/487-Paper3255.html,"We would like to thank the reviewers for their thorough review of our work. We are delighted that the reviewers found our novel and interesting idea (R #1,3), sufficient experiments (R #1,3), and experimental results convincing (R #1,3).

To Reviewer #1
Thanks for your detailed review and valuable feedback on our paper.

To Reviewer #3
Thanks for your detailed review and valuable feedback on our paper.

To Reviewer #4
Thanks for your detailed review and valuable feedback on our paper."
https://papers.miccai.org/miccai-2024/488-Paper0926.html,"We would like to thank the reviewers for the valuable feedback and suggestions. Adhering to the guidelines, we will not
add any experiments to the paper. We will add the â-â sign in Eq 1 and only move the results table closer to the results
section. As some of the reviewers asked to compare with pubmed based pretraining models, we did experiments but will not
include them in the paper. We also talked about them briefly in the rebuttal.

Mammo-Factor aims to find  the channel unit which encodes a certain attribute. For e.g, our aim is indentify the channel
units that encodes mass. So, first from the report, we get the sentences with mass and get the embeddings of 
these sentences. Next, our loss pulls all the representations closer where mass is present and pulls apart where 
mass is not present. This effectively extracts the units which encodes the mass information.

The paper  uses both CC and MLO images for training using the following
strategy. x^I and x^I~ are the original image and augmented variant of the image. If a patient has both CC and MLO
views, x^I is CC and x^I~ is MLO. If a patient has only CC or MLO view, x^I~ will be an augmented variant of x^I. We
mention the augmentations in the paper in detail.

We appreciate the reviewerâs suggestion. We have compared against PMC-CLIP during the rebuttal period. 
Respecting with the guidelines, where we
are not allowed to add new set of experiments, we are not including the updated results here. The performance of our
model is better than PMC-CLIP in all the metrics atleast by 16% as PMC-CLIP is trained with PubMed
documents with low quality images. We pretrain our model with high quality real life patient data.

While we developed Mammo-CLIP for mammograms only, it can easily be extended to other x-ray imaging domains.

We did not synthesize any reports in our work. We only use the reports during the
pretraining. Adhering the guidelines, we are not allowed to add new set of experiments. We perform ablation studies on
using only image text pair (x^I, x^T) instead of the augmentations and it reduces the objection detection (w/ finetuning
the image encoder) performances by 15%, classification (w/ finetuning the image encoder) performances by 13%.

We acknowledge the reviewerâs request for a more comprehensive discussion on the nuances of applying VLMs in medical
imaging. Specialist VLMs are tailored for specific medical tasks or domains, such as mammography or pathology. These
models are trained with highly curated datasets that focus on particular types of imaging or diseases. The primary
advantage of specialist models is their enhanced accuracy and reliability in specific contexts due to their tailored
training, which deeply encodes domain-specific nuances in their parameters. Generalist VLMs, on the other hand, are
trained with more diverse datasets that cover a broader spectrum of medical conditions and imaging types. These models
aim to provide a more flexible and scalable approach, capable of handling various tasks without needing retraining for
each new application. While they offer greater versatility, they might not reach the same level of precision as
specialist models in certain specific tasks.

Yes, we will correct this.

The summation notation in Equation 2 is intended to convey the comprehensive self-supervision across all combinations of
the original and augmented pairs. To clarify, the summation should cover all possible pairs of original and augmented
image and text representations, excluding identical pairs.

The function \pi in the context of the Mammo-FActOR module is designed to measure the similarity between the MLP output 
and the attribute representation t_k. The notation suggests that \pi is a similarity function,
and in our case, it is the dot product between the output of the MLP applied to the image representation and the textual
representation of the attribute."
https://papers.miccai.org/miccai-2024/489-Paper2226.html,"We thank the reviewers for their insightful comments and for highlighting the main contributions of our paper, which include:
1) Simulating MRF signals depending, among other, on microvascular tissue properties (CBV and R), in an âefficient wayâ (R1, R3, R4). This efficiency stems from the fact that dictionaries do not need to be entirely stored.
2) Estimating 6 quantitative parameters within a âreasonable timeâ (R1) using a âsuitableâ (R4) BiLSTM neural network.

There were concerns raised by Reviewers 1 and 3 regarding the use of quantifiers to evaluate our method.

We fully acknowledge the need for additional validations of our method as pointed in our discussion.

There are several results that could be easily added to the paper such as:
1) an in-silico study of the dictionaries to assess the sensitivity of the MRF sequence to the different parameters;
2) Correlation plots between the results of the different models and those obtained using dictionary matching. All these plots are already available.

However, concerning the validation of our quantitative vascular estimates: our main objective was to show a proof a principle in human volunteers. In this case, obtaining ground truth values is difficult because of the injection of Gadolinium. Given the encouraging results obtained here, we will now start a new validation study in tumor patients that will receive contrast injection as part as their clinical exam.

While Reviewer 1 commended the clarity and organization of the paper, Reviewer 4 expressed concern that our attention to details might reduce understanding of the key messages.

We will consider emphasizing overarching concepts in the final version, yet we think that technical details are essential
1) For reproducibility (as pointed out by R4);
2) Due to the complex MRF setting considered in this paper, with the inclusion of microvascular parameters, necessitating a 2-step dictionary generation process. 
Regarding this specific vascular dictionary procedure, Reviewer 1 requested additional explanations on the procurement of realistic vascular 3D voxels. Briefly, we use public datasets of whole-brain healthy mice (due to unavailability of corresponding human datasets) microscopy. A segmentation pipeline applied on this data allows us to obtain a binary representation of vascular networks with realistic geometries. The CBV and R in each of the produced voxels are characterized and used as parameter dimensions in the dictionary. The complete process is detailed and discussed in an article, that will be published soon, as well as in past conference abstracts, which will be referenced in the final version.

Furthermore, Reviewer 3 raised concerns regarding the over-smoothing of CBV and R maps reconstructed with our BiLSTM.

We wanted to point out that the WM/GM ratio of 2 obtained in the CBV maps using the BiLSTM is actually expected in the human brain. It is however possible that the neural network reconstructions smooth the parameter maps, and we believe that additional factors can contribute to this issue:
1) Insufficient sensitivity of current MRF sequences to vascular parameters, motivating the design of new sequences;
2) Use of non-optimal vascular vessel geometries for simulating our signals. As explained in Section 4, we hope to replace our 28,000 structures by âmore realistic and diverse vascular vessel geometriesâ.
We aim to improve those points in future works.

Lastly, Reviewer 3 suggested acquiring the B1 parameter separately to speed up dictionary matchings. Although we attempted to use pre-acquired B1 maps to enhance matching time and BiLSTM reconstruction quality, we encountered two challenges that we definitely aim to tackle in future work:
1) Acquired B1 maps contain artefacts, which adversely affects reconstructions in the associated zones;
2) Non-negligible slice profile effects were observed with a fixed B1, despite efforts to mitigate these effects in our signal simulations."
https://papers.miccai.org/miccai-2024/490-Paper1246.html,"We thank the reviewers for the thoughtful feedback, and will clarify all the concerns from the reviewers in the final version. Our responses to the major points raised by the reviewers as follows.

Reviewer #1

Q1: Minor comment: in Fig. 4, it might be easier for readers to observe with colored images instead of the grey-scale visualization for perfusion maps.
A1: We understand that colored visualizations can enhance readability and make it easier for readers to observe the details. We will incorporate your suggestion and update the figures accordingly in the final version.

Reviewer #3

Q1: While diffusion (resp. residual diffusion) is an established and widely concept in the general deep-learning community by now, it is still limited within the medical context. The text relies on a working knowledge of it, limiting its audience in the wider community.
A1: We acknowledge that, while prevalent in deep learning, these terms might be unfamiliar to a wider medical audience. Weâll explore incorporating a concise explanation of diffusion and residual diffusion within the medical context, tailored to the specific application in our paper. This will improve accessibility for readers with varying backgrounds. Alternatively, we will reference a well-established paper titledãDiffusion models in medical imaging: A comprehensive surveyãwhich effectively explains these concepts. This approach avoids redundancy and streamlines the paperâs focus. Weâll carefully evaluate these options to strike a balance between conciseness and clarity for a broader readership.

Q2: The work hints at clinical applicability but lacks key characteristics for this like timings and computational overhead.
A2: The running time and computational efficiency of the diffusion model is often related to the setting of the time step t. The average running time of our model for various settings will be incorporated into the table of results. Additionally, the accelerated sampling methods for diffusion models are continuously improving, which makes the time and computational overhead of our algorithm controllable.

Q3: Unclear what statistics where used, i.e. are the shown results averages over the dataset, or runs etc. While common, comparing models based on single instance is not a valid approach due to the stochastic nature of training.
A3:  We agree relying solely on averages from a single dataset and model run can be misleading due to the inherent stochasticity of training. We will consider this suggestion to incorporate additional model runs with the reported statistics reflecting the average and standard deviation across these runs. This will provide a clearer picture of the modelâs performance variability.

Reviewer #4
Q1: The paper would benefit from citing other relevant work in this area to achieve low-cost to high-cost mapping of imaging data using generative models. 
A1: We agree that our paper would benefit from discussing and citing other relevant work in this area. We will properly cite and discuss the references you provided to enhance our paperâs context and depth."
https://papers.miccai.org/miccai-2024/491-Paper0762.html,"We sincerely thank ACs and all reviewers for their efforts. We are grateful that reviewers appreciate the novelty and effectiveness (R#4), good results (R#1), and solid experiments (R#6) of our work. We will revise our paper accordingly and release the code upon acceptance.

All reviewers
1) Point Prompt: Following SAM-Med3D, to simulate the clinical scenario of interactive segmentation, one point per iteration is randomly sampled: from the foreground in the 1st iteration, and from the error region between the coarse mask and GT in the subsequent 9 iteration, totaling 10 points in 10 iterations. The updated prompt embedding E_P is thus generated by feeding the newly sampled point into the prompt encoder at each iteration. 
2) Training/Inference: The above point sampling strategy is used for both training and inference in our experiments, while in real clinical use, it operates interactively with physicians. During training, we calculate the segmentation loss and update the model parameters only after the last (10th) iterative refinement. 
3) Writing issues: Thanks for pointing these issues out and we will fix them all in the revised version.

R#1, R#4
STD values: Our results in Tab1 are averaged across 5 runs, with Dice STD less than 0.3% on all datasets. Due to the rebuttal policy, the complete std values will be included in the revised version.

R#1, R#6
1) Before/After in Tab.2: âbeforeâ means both training and testing on target dataset, while âafterâ means training on source and testing on target dataset. A smaller difference between âbeforeâ and âafterâ indicates less performance drop, i.e., better transferability. The inconsistent results for LiTS in Tab.1 (liver tumor + organ) and Tab.2 (liver tumor only) are because Tab.2 focuses on transfer performance between LiTS and MSD datasets, where only tumor data is available in MSD. 
2) crop-or-pad: We apologize for the mistake in the original description. The correct one is ââ¦and applying cropping for dimensions exceeding 128â and weâll fix it in the revised version.

R#4
1) Baseline in Tab1: Most baseline results on the 5 datasets are from published sources. For their missing results on specific datasets (e.g., KiTS19 and LiTS results for UNETR), we reproduce them using the same hyperparameters reported in the original papers for consistency. 
2) Transferability between different tumors: attempted transfer learning between lung and brain tumors but observed unsatisfactory results (over 19% drop), likely due to the entirely different expertise required for different tumor types. However, this does not affect our core contributions. We have designed a feasible solution to construct a foundational tumor lesion segmentation model by extending our method to a Mixture of Experts (MoE) framework to handle different tumors with different experts. We will explore this in our future work.

R#6
1) MEA vs. CA: We have previously tried cross-attention for interaction but found limited improvement over raw SAM-Med3D (4% less than MEA). It may be due to MEA realizes precise point-to-point fusion, while CA introduces unnecessary or noisy interaction. 
2) Metrics: Since dist-based metrics (like HD) are sensitive to outliers and may skew clinical evaluations, we follow SOTA methods to report commonly-used Dice and IoU. Moreover, the average HD metric values on the five datasets are 11.45 for nnUnet and 9.23 for our method (lower is better). 
3) Statistical analysis: We use t-test to compare our modelâs average Dice across five datasets with current SOTA nnFormer and obtain a p-value of 7.64e-10 which rejects the null hypothesis and indicates the significant improvement of our model over nnFormer.
4) More details: a) N_I and N_P are 384. b) As we have marked in Fig.1, except for the SAM image encoder (accounts for 79% of the total parameters), the rest parts are tunable. GPU memory is 26G with batch size 8. c) Training:validation:test is 6:2:2 following SAM-Med3D."
https://papers.miccai.org/miccai-2024/492-Paper0914.html,"We thank all reviewers for their valuable comments and appreciate the constructive feedback. Reviewers recognize that our method is novel and valuable (R4), important (R5). Extensive experiments are presented (R6).

[R4]Comparison with Other StudiesWe clarify that the main contribution of our method is to interpret the off-the-shelf target model by understanding the concept of each neuron. TSI (Network Dissection)[11] is the most relevant study, which is the reason that we selected TSI as a baseline. CBM[1, 2] is studied to design interpretable architecture, which needs retraining. We appreciate that CBM studies are important but the objective and scope are different from this study.
[3] explains input samples with words. CAV[4,5] needs additional human annotation for naming each concept vector which is expensive in the medical domain. CAV is also hard to identify related input regions for the concept. Our method can interpret neurons of medical models without additional training and these concept-annotated neurons and their activation maps can be used by looking at important neurons (e.g., Shapley value in Fig.3) to explain the decision in terms of âwhere and whatâ.

[R5, R6]Experimental SettingsTSI used expensive mask-labeled datasets. Since both concept sets include ground truth labels, it is much easier for TSI (less number of concepts) to match ground truth concepts. Nevertheless, MAMMI showed better performance in Table 4. Also with the same concept set, MAMMI achieved better performance than TSI.
In Table 5, all methods used the same MIMIC CXR report dataset as a concept set. Therefore comparisons were fair.

[R5]Model and DatasetOur method can interpret other architectures (model-agnostic), which is a merit of the method compared to TSI (only applicable to CNN due to the feature map matching with mask annotation). We also compared three CLIP models. We believe that the paper proved the validity of the method and Experiment section was appropriate for MICCAI.
Also, we selected the CXR task where VL models (i.e., CLIP) are widely studied and the source for concepts (i.e., Reports) is publicly well established. MAMMI can be used for other domains with suitable VL models and concept sets.

[R5, R6]Concept Set Construction/AnnotationWe focus on interpreting neurons in deep layers where neurons represent high-level objects that can be interpreted by the nouns. For other layers, it can use other suitable concept sets. To construct concept sets, nouns are automatically extracted from MIMIC CXR reports by the TextBlob library.
We clarify that a concept is a single word, but neurons can have multiple concepts that exceed Threshold Î¸_concept which is computed by the formula in Section 3.3. Our result shows that 35.2% of the penultimate neurons (361/1024) have multiple concepts in DenseNet.

[R6]Faithfulness of ExplanationsWe clarify that concept annotation globally interprets a well-trained model[25] and it is evaluated by mpnet cos, F1 score, and hit-rate in addition to CLIP cos. Explanation is analyzed on correct prediction in Fig.3.

[R6]Clinical ImpactInterpreting neurons of hidden layer concepts can help clinicians understand the model and basis of decision. This study shed light on the more reliable and interpretable use of DNN.
Note that applying CV domain methods directly to CXR is not satisfactory (Table 1-2). This is due to the class-imbalance dataset and lack of clinical terms, which shows the importance of this study (adaptive image selection and medical concept set construction are important).

[R5]Questions on FiguresFigure 2(b) shows the effectiveness of adaptive image selection, which is better than w/o adaptive selection[1,14] by considering class imbalance.
In Figure 3, size of neuron activation map was 77 and we upscaled it to 224224. Our goal is to provide explanations to clinicians on regions of important neurons based on model interpretation not segmenting ground truth regions."
https://papers.miccai.org/miccai-2024/493-Paper0290.html,"Thank you for your valuable feedback and suggestions. We appreciate the recognition of our experimental results [R3,R6,R7] and proposed pre-processing methods [R3, R9]. We address the reviewersâ concerns below:

Lack of Reproducibility [R3, R7, R9]: We plan to release our code upon paper acceptance to address reproducibility concerns.

Need for Meijering Filter [R3, R9]: We hypothesize the need for a ridge filter in Section 2.1. Ridge filters convert X-ray images into forms suitable for reconstruction, addressing the fine-grained nature of medical data. We use the Meijering filter output to guide our random masking. The ablation study, mistakenly referenced to the supplementary material, is in fact in Figure 4 of the main paper. This strategy outperforms other masking strategies like MAE (random masking) [10], AttMask (region masking) [17], and AutoMAE (learnable masking) [3], showing a significant AUC improvement of 9.08 over random masking. The training here follows MaskVLM and only the visual masking has been changed for fair comparison. We will correct out the reference error as this has in fact already been in the main paper and not in the supplementary.

Lack of Novelty [R9]: We stress our innovations at the beginning of Section 2. âIn Figure 1, we outline our approach, adopting MaskVLMâs [18] architecture. Our innovations primarily lie in visual and textual data pre-processing, rather than the architecture itself, which serves as a foundation for our training methodology.â Preprocessing significantly enhances existing strategies, evidenced by Figures 4 and 5 (ablation tables) in the main paper. While Section 2.2 acknowledges our foundation in MedKLIP, with notable similarities, it is in Fig 3 (b) and Figure 5 (ablation) where we detail our modifications and improvements, specifically over the KE-Triplet outputs from MedKLIP.

Comparison with original MaskVLM [R6]: In MaskVLM, the authors use random image and text masking followed by multimodal alignment. In Fig 4., the ârandom maskingâ numbers would correspond to MaskVLM, but with our proposed report generation used for text masking. Without our report generation and using existing reports, using random masking directly on the reports, we get 58.87, 14.96, 66.69 (AUC, FI and ACC) respectively, which is much lower than the proposed M&M. We did not include this in order to properly ablate our contributions with the limited space of the paper. We will highlight this in the final version. Thank you.

Comparing with vanilla MAE [R7]: We ran the vanilla MAE for 800 epochs, but the reconstructions were subpar. The vanilla MAE typically excels with datasets containing millions of images, a scale our datasets donât match. We tried different masking ratios to no avail. Our ridge filter-based masking decreases the need for such extensive pre-training, achieving comparable results in just 100 epochs. Extending training to 300, 500, or 800 epochs did not improve accuracy. We will clarify this.

Discussion of Limitations and Errors [R7]: We will discuss limitations, particularly concerning the variability in medical images and potential failure modes. We will also correct the missing citation for MaskVLM and fix table numbering errors.

Paper writing and size of images [R3]: We will first motivate the approach before discussing it to improve clarity. Apologies,Ï was used for reducing complexity of the equations. We should have mentioned this clearly. We followed notations used by MaskVLM. This will be fixed, for example, Ï_txt = g^de_txt(g_txt(f_im(I), f_txt(T_m))). About the size of the image, we use 224x224 for fair comparison with other approaches such as MedKLIP. Larger sizes may result in unfair gains in our approach.

Clarity Issues in Figures 1, 2 [R9]: We apologize for any confusion and will improve the resolution and captions in these figures to clarify that âvâ and âv_mâ refer to extracted and masked visual features, respectively, and âwâ and âw_mâ to text features."
https://papers.miccai.org/miccai-2024/494-Paper3032.html,"We greatly appreciate reviewers for their detailed feedback and insightful suggestions. Below, we address each of the raised concerns:

Use of Maximum Log-Poisson Loss and Network Output (R1, R3):
Under the X-ray imaging flux we studied, the system output counts follow a Poisson distribution, as detection events occur independently at a constant mean rate. This is a standard assumption in the field [1]. In our study, the Detector Net predicts the full spectrum, while the ASIC Net calculates expected bin counts. We will add more annotations in Figure 1 for clarity.

Concerns about Improvements Shown in Table 1 (R1):
Our method focuses on accurately modeling the PCCT system and reducing bias and structural artifacts. Denoising is not our main focus. As Table 1 shows, our method can achieve a significant reduction in bias, maintaining within 1 HU.

Monte Carlo Model, Open Source Availability & Reproducibility(R3, R4, R5): 
Our Monte Carlo model integrates elements from several established methodologies. We generate the initial spectrum using [2], incorporate bowtie-filter physics from clinical settings, and utilize energy deposition parameters from [3]. Charge transportation and signal generation modeling follows [4-5], and active-reset ASIC architecture is based on [6]. We are open to providing additional guidance to researchers attempting to replicate our work. To support community research, our simulation code is available upon request via email.

Optimization and Parameter Estimation Methodology (R3, R4): 
For estimating parameters such as dead time in Equation (7), we use calibration data with known material depths to find the optimal parameters that minimize the Poisson loss. Similarly, for estimating material depths in Equation (8), we identify the optimal material depths that minimize the Poisson loss. We utilize the Adam optimizer in PyTorch for these tasks. The full spectrum of PCCT is derived using the physics parameters estimated (such as bowtie filter depth) through this process.

Calibration Frequency and Time Costs (R4): 
In typical clinical environments, PCCT systems undergo calibration every 2-3 weeks. Our model supports simultaneous calibration of all detector pixels, with each model being compact and independent (approximately 1Mb). Material decomposition follows a similar rapid process. Ideally, calibration process should be finished within 1 hour and material decomposition should be finished within minutes. However, current practical implementations of our model require approximately one hour for both calibration and material decomposition due to the preliminary nature of our parallel processing tool, which needs further optimized.

[1] Wang et al. Sufficient statistics as a generalization of binning in spectral x-ray imaging. IEEE TMI 2010
[2] Punnoose et al. spektr 3.0âA computational tool for xâray spectrum modeling and analysis. Medical physics 2016
[3] Jan et al. GATE V6: a major enhancement of the GATE simulation platform enabling modelling of CT and radiotherapy. Physics in Medicine & Biology 2011
[4] Lai et al. Modeling photon counting detector anode street impact on detector energy response. IEEE TRPMS 2020.
[5] Taguchi et al. Spectral, photon counting computed tomography: technology and applications. CRC Press, 2020.
[6] Knoll et al. Radiation detection and measurement. John Wiley & Sons, 2010."
https://papers.miccai.org/miccai-2024/495-Paper1538.html,"We warmly thank the reviewers for their positive/constructive comments. They say that our method is ânovelâ (R3&R4), âan interesting oneâ (R5), and âwell-writtenâ (R3). Here we address the main points in their reviews.
ï»¿"
https://papers.miccai.org/miccai-2024/496-Paper1340.html,"We thank all the reviewers (R3, R4, R5) for their constructive comments, which have been carefully addressed as follows:

Q1: Disparities between our approach and reference [1]; Missing citations. (R3) 
A1: The disparities between our MCAD and TauPETGen proposed in [1] are two-fold. First, and most importantly, the aims of the two works are different. Specifically, TauPETGen aims to increase the availability of tau PET datasets via image synthesis. Accordingly, it utilizes textual descriptions and MR images as input to predict tau PET images. In contrast, our method focuses on reducing radiation exposure while ensuring the image quality of PET imaging. Therefore, we employ tabular and LPET images as input to generate SPET images. Second, the texts used in the two works are different. Concretely, TauPETGen uses short descriptions such as âgenerate a tau imageâ to describe the quality of the generation target, whereas our MCAD uses tabular to offer additional patient information that remains consistent for both LPET and SPET images. We will strengthen our novelty and cite these papers in the final paper.

Q2: Implementation details of the baseline model. (R3) 
A2: We would like to clarify that the SR3 blocks used in our model are inherently 2D. We only modified their channel dimensions in this paper.

Q3: Caption and explanation of Fig. 1. (R4) 
A3: We will provide a detailed explanation of each sub-figure in the final paper.

Q4: Robustness and generalization. (R5)
A4: As a pioneering work, we investigate the feasibility and advantages of introducing clinical tabular data in PET reconstruction in a relatively ideal setting. We will address the robustness and generalization to complex real clinical scenarios (e.g., different populations, scanner types, varying levels of image quality, and incomplete data) as our future research direction in the final paper.

Q5: Other concerns raised by reviewers. (R4&R5)
A5: Thank you for pointing out. Concerns including the validation of adversarial training, clinical quality of output images, and the statistical significance of the results will be addressed in the final paper."
https://papers.miccai.org/miccai-2024/497-Paper2333.html,"We thank reviewers for the positive comments:

We also thank reviewers for their constructive feedback and will revise/address them in the camera-ready version, including grammatical errors (R4) and model naming (R5). Detailed responses by subheadings follow:

Value of multimodal training (R3,R4): Multimodal training is not intended to significantly outperform single-modal training on, e.g., single-modal CXR classifications; it is intended to perform similarly while enabling new cross-modal retrieval and zero-shot avenues of research that better reflect the inherent multimodal nature of medicine. Interestingly, we see some improvements in single-modal tasks in retrieval (Tab 2) and zero-shot (Fig 3), perhaps due to cross-modality regularization. However, the benefits are highlighted in cross-modal retrieval tasks (Fig 2), classification (Tab 3), and downstream LLM integration (Tab 4). Tab 3 highlights MEDBindâs cross-modality zero-shot performance, where the model predicts a CXR label using ECG and vice versaâwithout training. Diagnosing conditions across different modalities is a growing area of research. For example, [1] uses traditional supervised learning from CXR to predict echocardiogram-derived disease labels. Our model is the first to perform cross-modal CXR/ECG tasks with zero-shot classification.

Clarifications on EMCL (R3): While EMCL is like TMCL, its novelty lies in its approach to directly bind CXR and ECG, which has not been explored. In the term $-mlog(m/n)$ in EMCL: $n$ is a constant representing mini-batch size; whereas $m$ fluctuates, representing the number of CXR-ECG matched pairs within a given minibatch (not all patients receive both a CXR and an ECG). Dynamic weight adjustment through $âmlog(m/n)$ in EMCL normalizes the contribution of negative anchors in the denominator of infoNCE loss across such variable batches, ensuring better consistency in EMCL loss. We will explore using different weights between EMCL and TMCL in future work.

Clarity on LLM (R4): We follow the broader definition of LLM as large pre-trained language models on a vast corpus of text, which includes BERT (masked); see [2, 3] for usage. While not generative, BioBERT may be more suitable for classification tasks as an encoder-only LLM. We will revise Sect 3.4 to improve clarity. The role of the LLM in Sect 3.4 is to comprehend the text and integrate it with CXR and ECG embeddings. Tab 4 shows how MEDBindâs CXR and ECG tokens can act as âsummariesâ of imaging modalities and be incorporated with medical text into an LLM by training a classification head.

Text Truncation (R4): We truncated text to the first 100 words (120 tokens) during pre-training, as 97% of CXR and ECG reports were under 100 words. This speeds up training, but longer lengths could be used in the future with larger GPUs.

TMCL Considerations (R5): The InfoNCE loss used in TMCL was first proposed in Supervised Contrastive Learning. Our primary contribution to TMCL was applying it to modality-text pairs to ensure accurate binding and address concerns where identical texts (e.g., âThe ECG is normalâ) could lead to incorrect binding if not managed by TMCL. In Eq 1, t_j->z_j denotes text-to-modality loss, and z_j->t_j modality-to-text loss. Following CLIP, we use both losses in training to enforce binding consistency in both directions."
https://papers.miccai.org/miccai-2024/498-Paper2311.html,We thank all reviewers for their valuable input. Our code will be made public upon acceptance. We address the main concerns and misunderstandings below:
https://papers.miccai.org/miccai-2024/499-Paper4230.html,"We thank the reviewers for their positive feedback. Kindly find the responses to the specific queries below:

Reviewer-1:

Discrepancy in UNETR results: Please note that UNETR has reported results on all twelve organs of synapse with a 24-6 data split. In our case, we use an 18-12 data split (refer to experiments section) with 18 train samples and 12 test samples and report scores on eight challenging organs. For a fair comparison, we train all models with the same number of epochs on synapse with the same 18-12 data split.

Reviewer-3:

Results on nnUNET: Our approach has been comprehensively validated with both transformer (UNETR, SwinUNETR, nnFormer) and CNN (PCRLv2) models. As per Reviewerâs suggestion, we have validated our approach on nnUNET as well. We are committed to open-sourcing all our code and models for the benefit of medical community (including nnUNET evaluation scripts). Due to strict MICCAI policy, we cannot post any links to results or code at this point.

On Novelty: Kindly note that MAE and DINO-v2, while effective, are self-supervised learning methods that necessitate extensive datasets for pre-training. Such huge and diverse datasets are often unattainable in the medical imaging field due to factors like high costs, privacy concerns, etc. Hence, our approach presents a single-stage end-to-end training framework to boost the performance of medical imaging models in low data regimes. In addition, while masking has been introduced in the realm of self-supervised pre-training, our proposed approach stands out as the first in the medical imaging community which uses volumetric 3D tube masking strategy in a student-teacher distillation framework at the fine-tuning stage in an end-to-end pipeline. Furthermore, our volumetric masked tokens are learnable which can effectively learn the contextual knowledge through reconstruction in output segmentation space. We hope this explanation provides an insight into the effectiveness of our proposed approach in the medical imaging problems.

Reviewer-4:

Comparison with other training frameworks: Please note that in addition to comparing our proposed MedContext with pre-training finetuning paradigms (Table 5 and 6), our method shows superior results compared to baselines (Tables 1,2 and 3) when trained from scratch. We have further shown the effectiveness of MedContext in the few-shot scenario in Table 4. We would also like to draw the attention of the reviewer to Table 2 of supplementary material where we integrate MedContext with a relatively new framework MedNeXT (MICCAIâ23) and show performance gains on this framework.

Exploration of loss function: We kindly refer the reviewer to the Table 2 of supplementary for comparison of different loss functions. Our choice of using normalized L2 loss between the masked student output and teacher output stems from the fact that the normalized L2 objective is scale invariant which is beneficial for 3D segmentation tasks, where the magnitude of logits can vary. He et al. [1] further shows that L2 objective with normalized logits as the reconstruction target improves representation quality.

Similarities with USCL: Kindly note that USCL is a pre-training method providing pre-trained backbones for downstream medical tasks. Our proposed MedContext, on the other hand, is a single-stage, end-to-end training solution that directly utilizes downstream data for efficient volumetric segmentation, without any pre-training. Our approach combines self-supervised and supervised objectives in an end-to-end framework. MedContext is the first to use a volumetric 3D tube masking strategy in a student-teacher distillation framework during fine-tuning. Our learnable volumetric masked tokens effectively capture contextual knowledge essential for volumetric segmentation.

[1] He, Kaiming, et al. âMasked autoencoders are scalable vision learners.â CVPR 2022."
https://papers.miccai.org/miccai-2024/500-Paper0867.html,"We sincerely thank all reviewers (R) for the meticulous review. Below, we address the questions (Q) and comments (C):

[1] Interpretation [R4-Q1/C3, R6]
Following R4, we will add GT bounding boxes for ROIs in Fig.4. The comparison in Fig.4 shows that integrating cross-layer information enhances the Swinâs capability to understand contextual information (Swin + IF concat). Additionally, the SAF module as a plug-in, effectively propagates essential information (Swin + IF SAF). The proposed LGT module enables the model to extract crucial contextual information by focusing on the ROI, though it still slightly extends its focus beyond the ROI, which the SAF module addresses (LGT + IF SAF). These findings indicate that MedFormer (LGT + IF SAF) attends to essential information necessary for diagnosis.

[2] Different metrics for datasets [R4-Q2]
Aligning with other SOTA models [8, 12, 14, 4] that used only AUC for evaluation on the NIH dataset, we utilized AUC for fair comparison. ACC was the primary metric for comparison on the BM and DM datasets, and we used the same metric for fair comparison. We value the feedback and can report both AUC and ACC for all the datasets for our model, though other SOTA methods only report one metric on the three datasets.

[3] Information loss in ViT & Swin. Why can Med-Former address it? [R4-Q3]
As the network depth increases, there is a loss of information from earlier layers in both ViT and Swin. Techniques like patch-merging layers in Swin, can worsen this issue, leading to subpar performance. MedFormer addresses this by combining information extracted by preceding layers (phase/stage) through the SAF module. The connection between different stages also enhances information propagation within the network. This is supported by ablation results in Table 3, where models with these connections (Swin + IF concat, LGT + IF concat) show improved performance compared to those without the connections (Swin, LGT). Performance is further enhanced with SAF modules (Swin + IF SAF, LGT + IF SAF), evidencing that SAF modules facilitate essential information propagation within the network.

[4] Comparisons on BM dataset[R4-C2].
BM is a new dataset in 2023. At the time of submission, only two published methods are available for comparison.

[5] Details on architecture [R5-Q1]
Window construction/size: Smaller windows capture finer details, while larger windows capture global contexts. Combining information from different window sizes enhances contextual understanding. Window Shifting: In Fig.2, LGT module has two layers, âlâ and âl+1â. In âlâ, windows are in a grid-like pattern, while in âl+1â, windows shift horizontally and vertically by a certain number of patches, enhancing the modelâs ability to capture relationships across window boundaries. SAF: SAF combines the output feature map of the current stage (fA) with that of the previous stage/phase (fB). It resizes fA to match the dimension of fB, computes attention maps for both, combines these in channel dimension, and forwards to the next stage. 
Small Window Sizes: The MedFormer employs a hierarchical structure, allowing the capture of global context even with smaller window sizes. Shifted Window mechanisms aggregate information across adjacent windows, capturing broader contextual information. We acknowledge MLPs also capture long-range dependencies, and their size is particularly crucial for larger images. Relation with CNN: Our model can be seen as a type of CNN with the added capability to handle long-range dependencies and enhanced global contextual extraction. Comparison with MLP Mixer: Transformers are superior in extracting contextual information (i.e, shape and texture) compared to MLP mixers (arxiv:2106.13122).

We will adopt the following suggestions: providing more details on datasets [R4-C4, R5-C1], updating Fig. 1 and 2 with more architecture details [R5-Q4], comparing the number of trainable parameters [R5-Q5], and simplifying the introduction"
https://papers.miccai.org/miccai-2024/501-Paper2150.html,"We express our gratitude to all the reviewers for their insightful feedback. We hope that our clarification can eliminate any doubts and gain enhanced recognition.

Response to Reviewer #1:

(1) Clarification of binary code encoding: The motivation for binary code encoding in multi-modal hashing retrieval lies in handling large-scale, heterogeneous datasets. By learning hash functions that maintain the semantic relationships between data points, binary encoding ensures that similar items remain close in the Hamming space, which improves the accuracy and speed of data retrieval across diverse modalities.

(2) Review of notations: We have carefully revised all notations to ensure their correctness in our manuscript.

Response to Reviewer #3:

(1) Clarification of symbol Q: Category Q represents the disease labels for each image-text pair in the medical multi-modal dataset.

(2) Citation of relevant works: We have cited these relevant works in our new version, ensuring a comprehensive understanding of multi-modal hashing retrieval.

(3) Modification the sequence of local token embeddings from âzâ to âZâ: To prevent being misunderstood, we have modified the sequence of local token embeddings from âzâ to âZâ.

(4) Clarification of Figure 2: We have also added a brief description of each operation in the figure caption to provide more context for readers.

(5) Explanation of evaluation metrics: To make a fair comparison, we follow the evaluation metrics provided by [22] to verify the effectiveness of our method compared against various baseline methods.

[38] Liu, Y. et al: Multi-granularity interactive transformer hashing for cross-modal retrieval. In: Proceedings of ACM MM. pp. 893â902 (2023)

Response to Reviewer #4:

(1) Modification of Tables 1, 2, and 3: To make a fair comparison, we follow the evaluation metrics provided by [22] to verify the effectiveness of our method compared against various baseline methods.

[38] Liu, Y. et al: Multi-granularity interactive transformer hashing for cross-modal retrieval. In: Proceedings of ACM MM. pp. 893â902 (2023)

(2) Different settings of alpha and beta: We have explored different values of the parameters alpha and beta to examine their impact on the performance of our multi-modal hashing retrieval system. By systematically varying alpha and beta, we aim to identify the optimal settings that maximize retrieval accuracy and efficiency.

(3) More ablation studies and parameter analysis: Due to page limit constraints, we only place the results for Open-I using 16 bits with 20% noise rate.  In practice, our MCPH significantly outperforms all state-of-the-art baselines with different settings.

Response to Reviewer #5:
(1) More details about implementation details: We have included additional details about the implementation in our supplementary materials. Furthermore, the code will be made publicly available upon acceptance of this paper.

(2) Complexity analysis: Following your kind suggestions, we would add more complexity analysis to verify the computational feasibility of our MCPH framework in the supplementary materials."
https://papers.miccai.org/miccai-2024/502-Paper4012.html,"We appreciate all the reviewers for their valuable comments and suggestions. We are encouraged that they find our motivation practical (R1, R4), our representation clear and well-organized (R3, R4), our method novel and effective (R1, R3, R4), and our experiments extensive (R4). We address the reviewersâ concerns below.
To R1: Thanks!
(1) Novel limitation: FedDG [8] pre-banks multiple amplitudes and then interpolates these amplitudes. This bank requires collecting amplitudes from different clients resulting in less flexibility. In Fig.1 of our work, we show the difference between high- and low-frequencies of the amplitude spectrum in 2D and 3D datasets. This will help the reader to clearly see that the variance inconsistency between high- and low-frequencies and introduce our method. Unlike [8], our method is less data-dependent and more flexible. The comparison with [8] is unfair as far as performance is concerned. In Table 1, while [8] generalizes experimental results from multiple domains, our RAS4DG achieves superior performance using only one source domain.
(2) About random mask shuffling (RMS): We introduce two key innovations in Section 2.2 (RMS and RSD). RMS is more robust to noise and local changes for images.
(3) RSDâs explanation: RSD is used to enhance the capture of stylistic information after RASS and the representation of features within the region shuffled after RMS. The RSD consists of spatial and channel reconstruction. For spatial reconstruction, features are categorized into high- and low-information groups based on weights and cross-fused. For channel reconstruction, we use a âseparate-transform-mergeâ strategy: channels are divided, with one part enriched via depth-wise and point-wise convolutions, and the other part supplemented with point-wise convolutions. The enriched and supplemented features are merged using the SKNet (CVPR 2019). We promise to clarify the details in the final version.
(4) Compared DG: Our RAS4DG not only proposes two strategies, RASS and RMS, but also uses plug-and-play RSD to add the bottleneck layer. Due to guidelines, the experimental results are not allowed to be provided here.
To R3: Thank you very much for your recognition!
(1) Motivation: The answer can be found in R1 (1).
(2) Mathematical details and hyperparameters in RASS: For space reasons, we directly obtain the amplitude and phase spectra. Our equation 1 and 2 are the key to RASS, where Î´ is used for random perturbations for different amplitude frequencies. Î´[m,n,p] is sampled from the Gaussian distribution of [1,Ï^2[m,n,p]]. We further discuss the selection of hyperparameters Î±, Î², and Î³ in function Ï. Section 2.1 noted that linear scale perturbations may not be sufficient to distinguish different frequency components. We use an exponential function to increase the level of perturbation. Experiment visualization and hyperparameter results are provided in Fig. 1 and Table 2 of the Supplementary Material.
To R4: Thanks!
(1) Method: We mention the differences from [8] in R1 (1) and elaborate on Section 2.2 in R1 (2) and (3). It is worth noting that RMS only shuffles image pixels.
(2) Experiments and results: âPloyâ learning rate policy, where lr = lrinit Ã(1âepoch/epochtotal)^0.9. It is used in many medical image tasks, such as CCSDG and ASC (MICCAI 2023). The difference in results between normal and abnormal samples in Table 1 may be due to unbalanced data distribution and inconsistent feature representation. We also report experimental results on DA, MSDG, and SSDG methods. DA and MSDG methods use more data during training, and theoretically provide better results than SSDG methods. But we can see that our RAS4DG is more effective compared to these methods. Figure 3 shows a qualitative analysis of the results. Our model outperforms other methods in terms of both local details and structural integrity. For the more obvious differences we have labelled them using boxes. Thank you for your advice on statistical analysis."
https://papers.miccai.org/miccai-2024/503-Paper3619.html,"We appreciate valuable comments from all reviewers and will consider them in revised manuscript.

R#1
Q1: Lack of generalizability
A1: Our experiments are mainly conducted on the MIMIC-CXR and IU X-ray datasets. We will conduct more experiments on other CXR datasets and other medical data to prove its generalizability.

Q2: Potential biases or limitations in the training data
A2: In our method, the fine-grained alignment based synthesis module consists of a frozen LLM and VQ-GAN model to synthesize images, which are pre-trained on the CXR dataset. This might limit the synthesis module to the training data. In future work, we will utilize more diverse CXR data to pre-train the synthesis module and avoid the bias or limitations of the training data.

Q3: Scalability and efficiency of our method
A3: For scalability, our method includes about 3 billion parameters and can be scaled up by replacing the LLM with a larger model with 7 or 12 billion parameters, making it more powerful in language understanding. For efficiency, it takes about 2.24 seconds for the whole pipeline, indicating its practicability.

R#3
Q1: Conern about novelty
A1: There are several different aspects between our method and [1]. 1)The goal of [1] is to achieve fine-grained image-text alignment through cyclic generation, making the fine-grained alignment more accurate and explainable, while our method focuses on generating high-quality CXR images with accurate anatomical and pathological details. 2)Their approaches are different. [1] proposes AdaMatch for patch-word aligment and AdaMatch-cyclic for cyclic generation. Our method innovatively devise anatomy-pathology prompting to automatically generate descriptive and reasonable reports with anatomical structures and pathological conditions, and utilize an image synthesis module to reconstruct images from generated reports that can be replaced with any suitable synthesis model.

Q2: Performance shown in [1] and our paper
A2: There is a difference in the input data between [1] and our paper. [1] directly tests on the original CXR dataset by using the real report as input, while ours uses anatomy-pathology prompting to randomly generate reports and applies those to the synthesis module. This difference in input data means the experimental results cannot be directly compared. Our paperâs quantitative results thus provide a more accurate assessment of the image synthesis performance of different methods.

Q3: Reproducibility and cost of GPT4
A3: Due to the significant performance of GPT4 in medical field, we employ it as effective tool in our work to generate medical reports. We acknowledge that it may lead to issues of reproducibility and high cost. To avoid this, we explored the use of other effective and open-source large language models like LLaMA3. Our previous experiments showed that LLaMA3 achieved comparable performance to GPT4. To ensure the reproducibility of our method, we will replace results obtained with LLaMA3.

R#4
Q1: Selection of LLM and VQ-GAN
A1: We use the dolly-v2-3b LLM, trained through instruction tuning for high-quality instruction following. Since images are challenging for LLMs, we use VQ-GAN to encode and decode image tokens, as it is an effective and efficient network for learning the image-token mapping, outperforming VQ-VAE. Current model selection is simple yet effective.

Q2: Other usage of our method
A2: Our method aims to generate faithful and useful synthetic medical images. For faithfulness, we recruited radiologists to confirm the practicability of synthetic images. For usefulness, the synthetic data can be used for medical AI model training, to augment data and avoid privacy issues. We had used synthetic images to improve performance on downstream tasks like disease diagnosis, COVID-19 prognosis, and report generation.

Q3: Clinical importance
A3: Our method can generate medical images with various diseases to support radiology education while avoiding patient privacy concerns."
https://papers.miccai.org/miccai-2024/504-Paper0333.html,"Dear Area Chair and Reviewers,

Thank you for your valuable feedback. We have carefully considered your comments and addressed the key points below:

Comment 1: Lack of Discussion with [1,2]
Response: We will include a discussion about [1,2] in the camera-ready revision. It is worth noting that the motivations behind MediCLIP and [1] are different. MediCLIP utilizes normal images and a set of carefully designed anomalous synthetic tasks to optimize CLIP for medical anomaly detection tasks. In contrast, [1] uses real anomaly images and corresponding masks.

Comment 2: Motivation and Upper Bound Performance Analysis
Response: In practice, medical images of specific parts, such as the oral cavity, neck, lateral pelvis, and bending positions of the spine, are often difficult to obtain. Additionally, privacy concerns often restrict the collection of sufficient normal samples needed to train full-shot models. MediCLIP is suitable for few-shot scenarios with a lightweight design that is easy to train and deploy, making it ideal for a wide range of real-world applications. When the number of available normal images is 32, the anomaly detection performance of MediCLIP almost reaches the upper limit. This is attributed to MediCLIPâs very few learnable parameters, enabling it to generalize well even with a very limited number of available samples.

Comment 3: More Application Scenarios
Response: In future work, we will explore scenarios where a small set of real abnormal samples is available to improve MediCLIP, enabling it to adapt to various real-world situations.

References:
[1] Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images.
[2] Medclip: Contrastive learning from unpaired medical images and text."
https://papers.miccai.org/miccai-2024/505-Paper1626.html,"We thank the reviewers for the high-quality reviews and will carefully revise our submission accordingly, correct typos, enhance our qualitative analysis and discussions, upload the supplementary material, and release the codes and models in the revision.

Q1. Difference between MedMLP and Other Adaptative Operators(R1)
A1. We thank the reviewer for the insightful suggestion. AdaFC operates on the weight tensor, while previous adaptive operations focus on the feature maps. The primary difference lies in the efficiency of GPU utilization. Previous operators, such as pooling and convolution, employ a sliding window operation, which can reduce GPU utilization efficiency due to a mismatch with the GEMM CUDA core implementation. In contrast, the MLP layer requires only matrix multiplication, aligning better with the GEMM CUDA implementation. However, due to the simplicity of the MLP operation, it struggles with feature maps of variable spatial resolution, as the weight tensorâs dimensions are fixed and sensitive to transformations. The main novelty of AdaMLP is its ability to operate on the weight tensor to adapt to variable feature map dimensions. We will include this analysis in the revision, along with the corresponding references.

Q2. Experimental results on Tab 4 (R3 & R4) 
A2. We thank the reviewers for their comments. MobileNet was excluded from Table 4 due to its significantly smaller model size. As the performance of MedMLP-B2 is already comparable to ResNet, we initially chose not to include smaller models. However, we agree that adding the performance of smaller models would be informative. We will include results for models with mobile settings in the revision.

Q3. Zero-shot and domain generalization (R3 & R4)
A3. (1) The term âzero-shotâ in our paper refers to MedMLPâs ability to adapt to images with different resolutions without requiring fine-tuning. This characteristic sets it apart from other MLP-based networks. We will clarify this in the revision. (2) Due to time constraints, we are currently testing on PACD. We plan to explore other diseases in the future to better understand the domain generalization of the AdaFC-based model.

Q4. Experimental datasets (R3 & R4)
A4. (1) We selected SCES and SINDI due to their larger domain gap, which allows us to better test our modelâs robustness. (2) By combining SCES, SINDI, ICC, and ISF, we achieved a significant 20.1% increase in model testing accuracy on an out-of-distribution dataset. For the purpose of our analysis, we chose two datasets for illustration. In the revision, we will clarify the dataset selection process and include results for all datasets in the supplementary material.

Q5. Differences between MedMLP and ViP (R3)
A5. The Vision Permutator (ViP) does not support variable input dimensions, limiting its use to images with the same spatial dimensions as those used for model training. In contrast, MedMLP, through its implementation of the AdaFC layer, can handle images with variable dimensions, providing greater flexibility and adaptability.

Q6: Justification of the contributions.
A6: The main contributions of this paper are two-fold: (1) We propose a novel operator termed AdaFC, which can process images with variable spatial dimensions. This capability is not possible with previous MLP-based models. (2) We introduce a new family of pre-trained foundation models based on the AdaFC operator, specifically designed for medical image datasets. These models demonstrate better generalization capabilities and faster processing speeds. Both claims are supported by rigorous experimental results. As shown in Figure 1, the MedMLP model family exhibits a significantly better performance curve, balancing model accuracy and running speed more effectively than previous models."
https://papers.miccai.org/miccai-2024/506-Paper2872.html,"Thanks for the reviewersâ useful comments. We address the concerns raised by the reviewers below.

Robustness of the Method [R1] â We clarify that we pre-train our GAN. It provides stable initialization, reducing sensitivity to hyperparameters, and ensuring stable and reliable training across many architectures and datasets. To mitigate model collapse and lower the likelihood of GAN failure, we use Wasserstein loss with gradient penalty, as indicated in Equation(2) of the study, which promotes stable training.

Membership Inference Attack (MIA) [R1 and R4] - We generate the white-box and black-box attacks based on the threat model proposed in [1].  For both, the attacker training set consists of a random 10% of the original dataset (ISIC/Alzheimer) with synthetic fake samples as non-members.  In the white box attack, knowing the target GAN architecture, the attacker inputs the training set to target GANâs discriminator, extracts and sorts the prediction probabilities and uses the highest probabilities to predict the training set members. In the black-box attack, without knowing the target GAN architecture, the attacker first trains a local GAN using the target GAN samples and  carry out the steps typical of a white-box attack. 
The attackâs accuracy is determined by the percentage of correctly identified images from the training set. Lower attack accuracy, below random guessing, indicates increased model security against MIA attacks. Our method defends against MIA attack more effectively because the generative model captures information from a condensed dataset, reducing the attack surface for inference attacks, and preventing leakage of training data.

Missing References [R3]- We appreciate for suggesting references related to medical dataset distillation and will cite them and discuss.

Comparison to Non-Generative Methods [R3] - We clarify that we have already shown in Table 1 that our method achieves significant performance improvement over a non-generative feature matching method, named DM (Distribution Matching) on ISIC 2019 and Alzheimer datasets.  Although we did not present results, our study shows our method performs significantly better than gradient matching method. As suggested, we will compare our method with other non-generative methods.

Training Time [R3] â We used the system with eight Nvidia V100 GPUs installed but used 4 GPUs only. It takes about 30 hours on 4 V100 GPUs on ISIC 2019(about 20k train images).

Ablation study for the two newly introduced modules [R3, R4]- For the first module (Generator pre-training), we validated many GANs (cGAN, WGAN, StyleGAN) with different settings (latent space dimension, loss, training parameters and regularization) and chose attention-based GAN as we observed it outperforms other GANs. For the second module (Fine-tuning the generator), we also validated different network architectures (ConvNet, ResNet, DenseNet) other than ViT for feature extraction as well as on different matching strategies (feature, gradient and logits matching), and presented ViT with logits matching as the feature extraction method as it obtained the best performance for medical datasets.

Two-stage Approach [R4]-   Using a single-stage training approach with random initialization of the GAN may lead to vanishing gradient problem and higher training time due to hyper-parameter tuning. To overcome these issues, we used two-stage approach. Pre-training the GAN on a related medical dataset can provide a good initialization point and improves downstream taskâs performance.

Generalization Ability[R4]- As suggested, we will evaluate the generalization ability on ISIC dataset.

ViT [R4] â Thank you for pointing out. We clarify that ViTâs self-attention mechanism enables the model to capture global interactions among all components of the input image by considering relationships between all positions at once.

1.LOGAN: Membership Inference Attacks Against Generative Models, PET Symposium, 2019."
https://papers.miccai.org/miccai-2024/507-Paper1369.html,"Thank you for your constructive feedback! 
R4: âThe paper â¦ lacks novelty. The application of a Transformer-based architecture is a rather obvious design decision when working with sequential data.â Applying Transformer-based approaches to various types of sequential data isnât always optimal, i.e. CNN-based TimesNet outperforms Transformers in time series analysis tasks, but applying it to MEG data for speech decoding hasnât been explored previously. Our contribution isnât in using the Transformer-based approach per se, but as R5 noted, in developing a hybrid CNN-Transformer architecture specifically tailored for MEG data. This involves incorporating spatial attention layers, and a carefully designed sequence of blocks to accurately capture the nuances of brain activity. This hybrid architecture achieves SOTA performance in decoding speech. Also, we demonstrate improvements with longer speech segments, highlighting its ability to accurately decode complex speech representations linked to high-level perception or even speech comprehension.
R4-R6: âThe paper lacks an explanation or discussion on how this work can be applied to practical use casesâ, âFindings are not discussedâ. Decoding speech directly from brain signals is a relatively new and emerging task. Our primary objective is to demonstrate the feasibility of decoding perceived speech. With the development of larger datasets for speech production, it could be adapted for this task without significant modifications. Thus, our work represents an important step toward building a foundational model for brain recordings. Recent studies have utilized limited vocabularies, whereas our model demonstrates a zero-shot performance with an unrestricted one. The performance of our model (55.08%) and Defossez (39.43%) are based on vocabularies that donât overlap with the training set, showcasing that models can generalise to new words without additional training. In practical applications, such as aiding individuals with speech impairments, our model can be adapted to use a limited vocabulary set, significantly enhancing its performance and making it suitable for tasks like issuing a predefined set of commands. Our approach also can be useful for understanding differences between healthy controls and patients with Auditory processing disorder which can lead to the development of brain interfaces for them [doi:10.3389/fnhum.2014.00151]
R5&R6: âExplanation of the proposed methodology is not very clearâ, âIt is not clear how predictions are madeâ Our method decodes speech from MEG data using a dual-encoder architecture. The audio encoder employs a pre-trained wav2vec 2.0 model, while the brain activity encoder processes MEG signals with CNN layers to capture local spatial features and transformer layers to model long-range dependencies. A spatial attention model focuses on the most relevant MEG channels, and a subject-specific layer tailors the representation for individual subjects, enhancing accuracy. The prediction aligns features from both modalities using a contrastive loss function inspired by Ð¡LIP, encouraging similar MEG and audio representations to be closer together in a shared latent space. During inference, given a new MEG signal, the model predicts the corresponding audio representation by finding the closest match in the learned latent space.
R5: About evaluation metrics. While top-10 accuracy might seem less stringent than top-1 accuracy, itâs crucial for understanding the modelâs ability to narrow down potential matches in a large search space, reflecting its utility in applications where further refinement steps might be employed. Additionally, we included top-1 accuracy metrics to provide a more comprehensive evaluation of the modelâs performance.
Weâll carefully consider your suggestions and make the necessary revisions to figures (R4, R6) and text (R4, R5, R6), and in future work, weâll further explore the impact of dataset size and subject-specific results."
https://papers.miccai.org/miccai-2024/508-Paper1438.html,N/A
https://papers.miccai.org/miccai-2024/509-Paper1764.html,N/A
https://papers.miccai.org/miccai-2024/510-Paper2259.html,"We thank the reviewers (R1, R3, R4) for their comprehensive evaluation of MeshBrush, a neural mesh stylization method for endoscopy to generate realistic views and consistent features. To the best of our knowledge, the proposed work is the first of its kind, supported by R1 and R3âs assessment. The reviewers classified our work as âinnovativeâ (R1), âclever and convincingâ (R3), as well as âinteresting and well articulatedâ (R4). We appreciate the reviewersâ enthusiasm in the proposed workâs methodology as well as their relevant concerns.

In this rebuttal letter, we would like to address 3 main aspects: 1) the applicability to other anatomies and other downstream vision tasks (R1, R4), 2) insufficient evaluation and presentation of our work (R3, R4), and 3) practical concerns regarding realism and data preparation (R1, R3, R4).

1) Applicability and generalizability. Prior to stylizing our mesh, we train an image-to-image (I2I) style transfer network to translate rendered views from CT to realistic endoscopic views, which is used as ground truth for MeshBrush. We claim that the method is generalizable to other anatomies since I2I methods have already been proposed in other anatomies [ex. Aldo et al. Computer Methods and Programs in Biomedicine 200 (2021), Mathew et al. CVPR 2020] (R1, R4). Nevertheless, we plan to extend our work in a journal paper where we provide a more comprehensive evaluation of our approach to other anatomies.

To address R1âs concern about other downstream tasks like depth estimation and camera tracking, we also plan to include these components in the extended work. However, the results from Structure-from-Motion (SfM) are promising in support of downstream vision tasks as SfM directly estimates the camera trajectory and can be used to generate depth estimations.

2) Inadequate evaluation and presentation. Despite our desire to add more qualitative results, the manuscript page constraints led us to include them in only Figures 1 and 3 (R3). To address the inconsistent evidence in Table 1 (R3), we emphasize that a stylized mesh produces distinct textures sufficiently for a much harder task than matching ORB features (which was done using OpenCV, R3), namely Structure-from-Motion, as shown in Fig. 4.

Regarding Figure 2 missing key details (R3), we will clarify variables in the Methods section by adding them to the figure. If accepted, we will correct the notation in the figure and paragraph to be more cohesive, as well as restructure the figures to better organize the paper and improve its readability (R4). We will also add further details for extracting mesh from CT (cropped from CT using 3D Slicer and subdivided to increase texture resolution, R3).

3) Realism and data preparation. Regarding R1âs assessment of realism being an area of future improvement, realism can be improved with more sophisticated rendering strategies since the mesh textures are learned. Furthermore, we kindly urge the reviewers to recognize that our novel work is a foundation for consistent style transfer in endoscopy. To address computational complexity concerns (R1): our mesh is still relatively small with only a few thousand vertices. As a result, rendering a scene is on the magnitude of a few seconds; mesh stylization only took 6 hours on an RTX4090 GPU. Although other anatomies will have various sizes, shapes, and geometric complexities, the training time will still be tractable as MeshBrushâs applications do not require real-time execution. Once the mesh is stylized, it can be used for any downstream task without further modifications. Regarding longer video sequences (R1) and interferences like smoke or surgical instruments (R4), this will only impact the I2I style transfer network training, which our work assumes to be ready on-hand."
https://papers.miccai.org/miccai-2024/511-Paper1687.html,"Thanks for the valuable comments. We appreciate the recognition of our work and have addressed each of the reviewersâ concerns as follows.

Regarding the limitation of single dataset (R1, R3, & R4): We acknowledge that multicenter validation would improve the feasibility and mitigate biases in our approach for clinical practice. However, gathering PET images from Parkinsonâs disease (PD) patients is difficult due to instrument constraints, particularly for CFT PET scans. Indeed, we are actively collaborating with additional hospitals to gather pertinent data for validating the effectiveness of our MetaAD.

Regarding the reproducibility of the model (R1, R3, & R4): We recognize the significance of data transparency and reproducibility in scientific research. We apologize for the omission of the link to relevant code, which is restricted by the double-blind review process. We commit to making our code publicly accessible once our paper is finally accepted. As for the dataset, however, we cannot make it publicly available due to the confidentiality agreements and privacy concerns.

Regarding the improvements of paper writing (R1 & R3): Thank you for your thorough review. We apologize for any occasional spelling and citation errors in the manuscript. We will address these issues and strive for improvements in the final version of the paper, including correcting typos, polishing texts, and enhancing the organization.

Regarding the comparison with more modern pipelines (R3): Thank you for your feedback. In the current era of AIGC, leveraging cutting-edge frameworks like diffusion models for image generation seems like a direct approach. However, this does not apply to our task due to the significant challenge of synthesizing unpaired 3D images. Unpaired image translation remains a challenge for diffusion models. While some recent works have attempted to use diffusion models for unpaired image translation (e.g., CycleGAN-Turbo, Gaurav Parmar, et al, 2024), they are typically based on Stable Diffusion pre-trained on natural images and primarily focus on 2D images, which is not suitable for unpaired 3D medical images.

Regarding the details of the limitations and future work (R3): While our current work is intriguing, it is still in a preliminary stage, thereby leaving considerable scope for further exploration. Due to the limited page length, we only briefly highlight future research directions in our paper, which aligns with ongoing efforts and have yielded promising results. In the final version, we intend to elaborate more comprehensively on these points if additional page is available.

Regarding the analysis of bad performance of other UAD methods (R4): Thank you for your interest. As we elaborated in the latter part of the introduction, many UAD methods aim to learn a compact latent space exclusively representing the normal data distribution. However, optimizing this latent space poses a significant challenge. Conventional reconstruction networks may encounter an âidentical shortcutâ where both normal and anomalous samples are well-recovered, leading to a failure in detecting outliers effectively. To overcome this challenge, we substitute the latent space with CFT images, facilitating easier and more efficient optimization. Additionally, we introduce Abnormal Metabolism Suppression to further mitigate âidentical shortcutâ in the cyclic modality translation workflow, while conventional CycleGAN without it might still suffer."
https://papers.miccai.org/miccai-2024/512-Paper2372.html,"We thank the reviewers for the valuable and predominantly positive (A,WA,WR) feedback. They appreciate the taskâs significance, solid methodology, and rigorous experimentation.
Q1: Dataset details (image resolution, stain distribution, classes) (R1)
Image resolution: 0.4661 Âµ/pixel (20x). Patch size: 1024Ã1024, non-overlapping. Stain distribution: 4642 HER2, 4153 ER, 4361 Ki67, 4139 PR, 4000 H&E patches. Patches from 4 biomarkers were used in training (meta-train+meta-test) and the 5th at inference. HER2 patches were extracted from 64 WSIs, patches of other stains from 56 WSIs. IHC-stained WSIs contain 2 classes (IHC+, IHC-), H&E WSIs contain 5 classes - Neoplastic, Inflammatory, Connective, Necrosis, Non-neoplastic epithelial.
Q2: Failure scenarios/limitations, mitigation (R1)
1) To use consistency-preserving (CP) loss, ~20% of the dataset needs co-registered IHC-IHC patches (e.g. ER-PR, ER-Ki67). However, the I2IT models can only synthesize IHC from H&E. By association, we require ~20% IHC-H&E pairs in the original dataset to achieve IHC-> H&E ->IHC 2) Instead of expert annotations, we use DeepLIIF & HoVer-Net to obtain ground truth cell segmentations & classifications 3) Dataset must exclude low-quality H&E samples to optimize I2IT performance and maximize CP loss potential. Proper QC techniques are needed to maintain high-quality image input.
Q3: Multiple runs (avg, std). Statistical significance. (R1)
We re-evaluated the tasks with 10 random runs for each experimental setting (5-20%) involving Ki67 and ER samples at inference. Due to rebuttal restrictions on presenting new results, the avg. dice, acc with std. will be reported in the âfinalâ supplementary. Our results are statistically significantly different from SOTA (t-test).
Q4: Range of applications unclear (R3)
1) Reduces pathologist annotation efforts and time for cell segmentation & classification on novel stains. 2) segmented cells on samples of novel stains may aid in better downstream analysis (predictive/prognostic models). 3) attain generalizable performance for cross-hospital samples (different staining techniques) 
Q5: What do arrows and squares mean in the figure? (R3)
Fig 1a arrows: maximize the distance between IHC+ and IHC- feature prototypes. Arrows linking 1(a, b): minimize the distance between IHC- prototypes from ER & Ki67 stained samples. In Fig 1d, colored squares represent the spatial arrangement of 4 grids within a patch. 2 differently-stained samples with similar spatial arrangements indicate a +ve correspondence.
Q6: Compare with other domain generalization (DG) methods. Results when the support samples are 0% in expt 1 -> comparison with no access to test domain? (R3)
We compared MetaStain with 2 non-meta-learning DG methods, based on feature augmentation [1] & domain-invariant feature learning [2]. Meta-learning offers clear advantages: i) extraction of superior commonalities (meta-knowledge) from diverse training domains and ii) enhanced optimization by simulating domain shifts episodically. MetaStain outperforms [1,2] with 0% support samples. 
Unlike typical DG, we integrated test-time adaptation into MetaStain to enhance feature generalization to unseen stains, marking a major contribution to our methodology. Even after augmenting [1, 2] with finetuning, they couldnât surpass MetaStain in 5-20% support settings.
Rebuttal guidelines prevent us from providing these results; can be shown in supplementary if AC/reviewers desire.
Q7: Algorithm for I2IT? (R3)
We apologize for citing DeepLIIF inappropriately as it wasnât used for the H&E-to-IHC I2IT task. We adopt the method in MIST [3].
Q8: Ablation study. Code availability. (R4)
An ablation study is already provided in Tab 2 (right) for IHC+/IHC- seg. & classification on Ki-67 stained samples at inference. Due to space constraints, results for other stains were not included. Code will be made public upon acceptance.

References:"
https://papers.miccai.org/miccai-2024/513-Paper2749.html,"We thank all reviewers for affirming the novelty of our method, the usefulness of results for the community, and their constructive feedback for paper improvement. We especially appreciate Reviewer #4âs strong endorsement of the contributions and quality of our manuscript. The primary concerns raised by Reviewers #1 and #3 are addressed in four parts below.

Question 1: Unknown code and dataset availability (Reviewer #1)
Response: Our code will be released duly as mentioned in the abstract of our manuscript and the datasets used are publicly accessible as cited in our manuscript.

Question 2: Contributions need to be further clarified (Reviewer #3), e.g. MetaFormerâs impact (Reviewer #1)
Response: We further clarify our contributions as follows: 
1) We propose a novel MetaUNETR architecture for 3D multi-organ segmentation. It features lightweight TriCruci layers for parameter-efficient token mixing spanning Mamba, Self-Attention, Convolution, MLP, Recurrence, and Global Filter. (MetaUNETR: UNETR-style model with diverse token mixer encoding backbones.) 
2) Based on the extensive comparison of these token mixers, we validate that the capability of these models is derived from MetaFormer architecture and less influenced by specific token mixers. (Original MetaFormer only considered Attention and MLP in 2D natural image domain.) 
3) Comparative analyses using CKA uncover significant similarities and highlight the importance of features within the upper encoder layers while identifying redundant computations in the deeper layers. 
4) Based on these findings, we conducted reasonable layer pruning on MetaUNETR, which achieved heightened computational efficiency and accuracy on three datasets compared to prior arts. (*Reduction on Parameters and FLOPs are explained at the end of Section 3.2)

(*Note: the latest study [1] (released on 26 March 2024) on Large Language Models (LLM) also demonstrates that shallow layers play a critical role in storing knowledge and a high degree of parameter redundancy in the deeper layers of the network. We believe our findings in the vision domain, provide a solid foundation for further research in PEFT by efficiently leveraging parameters in the deeper layers or exploring new structurally efficient architectures.

Question 3: TriCruci layer warrants a little more explanation. (Reviewer #3)
Response: The TriCruci layer, elucidated in Section 2.3, is further explained as follows: The TriCruci layer introduces an artificial inductive bias akin to the locality enforced by large kernel convolutions and Swin Transformer layers. It establishes a mutually cruciform receptive field across three axes (depth, height, and width), each capturing long-range dependencies. For depth mixing, the input X of size HÃWÃDÃC is reshaped to HWCÃD, and diverse paradigms (e.g., linear projections with MLP) are applied to each transverse plane token to mix information. Analogous operations are performed along the height and width axes.

Question 4: The CKA figures need to be more descriptive (Reviewer #3)
Response: The CKA results, elucidated in Section 3.2, are further explained as follows: Fig. 3 depicts the pairwise similarities between corresponding layers of diverse token mixer encoded backbones (e.g., Attention vs. MLP). The x and y axes denote the backbone 4 stages. Similarity scores range from 0.4 to 1, with lighter colors indicating higher similarity. Along the antidiagonal in plots, layer-wise and stage-wise consistencies across backbones are revealed for the top three stages, indicating stage 4 contributes minimally to model performance, rendering it redundant.
Similar to the advance in [1], further understanding of network learning dynamics, such as why stage 4 encodings differ and how to effectively use the deep layer parameters, deserves dedicated exploration in our future works.

[1] Gromov, Andrey, et al. âThe unreasonable ineffectiveness of the deeper layers.â arXiv preprint arXiv:2403.17887 (2024)."
https://papers.miccai.org/miccai-2024/514-Paper2494.html,"To Reviewer #1
Q1: Inception GCN, MLP, and similar single-model methods are mentioned, but it is not explained how these were implemented to process multi-modal data.
A1: Inception GCN uses multi-modal data to construct a single graph, and utilizes GCN to aggregate multi-modal features. The MLP method first concatenates multi-modal features together and then inputs them into multi-layer perceptron for prediction.

Q2: The proposed method exhibits a higher standard deviation than others,
A2: Our model adopts a 10 fold cross validation method. Due to the uneven distribution of features in different training and testing sets, adding SVD may result in some fluctuations in the modelâs performance.

Q3: This work utilizes multi-modal data for representation learning, yet it lacks experiments using only single-modal data. 
A3: Thanks for your suggestions. We will further enrich our evaluation by incorporating the comparison with some single-modal methods.

To Reviewer #5
Q1: What is the meaning of âthe supplementary information in the private information of each modalityâ in this sentence?A1: Here, âthe supplementary informationâ refers to the complementary information of modality. For example, MRI provides the information on the structural characteristics of the brain, while PET provides information on the metabolism and function of the brain.

Q2:  it is necessary to explain how the resulting more compact representation of common information is related to the original information and how it is utilized in subsequent steps.A2: Yes. Using SVD, we can obtain the more compact representation of common information. We will add the discussions of the proposed method w.r.t SVD parameters in revision.

Q3: Why there are no classification results of the MAFGN method on ABIDE, please explain.
A3: MAFGN does not report the performance on ABIDE and the code of MAFCN is also not released. We will try to re-implement MAFGN on ABIDE in our future work.

Q4: I consider that it would be advantageous to compare with advanced multi-modal fusion classification methods, rather than solely focusing on GNN-based methods.
A4: We will include some more non-GNN advanced methods in the future experiments.

Q5: it does not provide a detailed explanation of what aspects of the method.A5: First, we propose to exploit a novel disentangled representation pipeline for multi-modal data learning by exploiting both common information of modalities and private information of each modality. Second, we employ a new contrastive learning module to learn more compact representation of private information for each modality.

Q6: it lacks a thorough demonstration of the clinical feasibility of the proposed method. 
A6: We apology for the unclear claim here. Our proposed method is currently evaluated on several public datasets and we report the prediction performance on these datasets.

To Reviewer #7
Q1: could you elaborate on the computational complexity of this model?
A1:  We will provide the details of computational complexity of our model in revision. We cannot provide them in rebuttal due to the penalty rule (should not include any new experiments) of rebuttal stage.

Q2: Could you discuss how the MGDR modelâs performance might be affected when the number of multi-modal inputs is altered?A2: Yes. We can discuss the modelâs performance w.r.t the number of multi-modal inputs. 
When using only the fMRI modality, the performance is not very well. Adding data from the other three modalities one by one shows a significant improvement in performance.

Q3: it would be beneficial to provide a rationale for choosing these specific datasets (ABIDE and TADPOLE) over other available options. 
A3: (1) They are publicly available and challenging in the field of brain disease prediction. (2) Both of them contain multi-modal data. (3) They cover different types of brain diseases, effectively evaluating the modelâs applicability."
https://papers.miccai.org/miccai-2024/515-Paper0040.html,"We thank Reviewer R1, R3, and R4 for the in-depth discussion, constructive suggestions, and endorsements for our contributions: (1) the novel Model Heterogeneous FL framework, (2) the lightweight global bypass model for information sharing, and (3) our comprehensive experiments. We are encouraged that our framework is ânovelâ (R3, R4), âeffectiveâ (R1, R3, R4), âreasonableâ (R4), andclear and detailed description"" (R1, R3, R4), the experiments arecomprehensiveâ (R1, R3, R4), and the paper is ``well-writtenâ (R1, R3, R4). We will release the code upon paper acceptance.

[R1Q1].Â Global Bypass Model definition, information sharing, and security.
We only share the parameters instead of any subject-specific features of our Global Bypass Model (GBM). The structure of GBM varies in different tasks, but for the same task, the GBM model structure is identical across different clients, and the modelâs functionality is consistent with the local model. Therefore, on the central server, we can aggregate the uploaded GBM model parameters. Since the information is shared only through the parameters of GBM, data privacy can be well secured.

[R1Q2, R1Q5]. Data security/privacy/fairness; Clientâs contribution with limited data.
As our framework only shares the parameters of the global bypass model among various clients, security/privacy would not be a problem with our method. We evenly average the parameters of each client in aggregation, so our framework is fair to each client no matter the data scale difference among the clients.

[R1Q3]. Comparison with feature-sharing methods.
As our framework shares information via model parameters without the needs of sharing subject-specific features, we have fewer privacy concerns compared to feature-sharing methods. Besides, our approach outperformed previous works shown in Tab.1-3. In terms of practicability, our methods only share a lightweight bypass model, which would greatly save computational resource compared to directly sharing features in the dataset.

We appreciate R1 for the insightful questions about data security, fairness, and performance comparison!

[R3Q1]. The weights Î», conclusions are very short, and Table optimization.
We tried a few Î»s and the one we are using is well-balanced and has the best overall performance.

[R3Q3]. Experiment setting details.
Our experimental setup is as follows: We first sort the data based on labels, and we will divide the data into 16 equal parts. Each of the 8 clients will randomly select two equal parts without repetition. This can ensure that each client has a different category and that the data distribution of each client meets Non-IID.

We thank R3 for the constructive comments on the experiment details and conclusions improvements!

[R4Q1]. Details for Fig 1(a).
Fig 1(a) includes three steps: a. Local model training. b. Global bypass model training. c. Upload, aggregation, and download. Local model training: Train the local model with the global bypass model frozen. Global bypass model training: Fine-tune the global bypass model with the local model frozen. Upload, aggregation, and download: Upload the global model to the server, aggregate the parameters, and then download and distribute the aggregated model for the next training round.

[R4Q2]. Framework specificity of medical images.
There are 2 reasons why our design is specifically suitable for medical scenarios. A) Compared to previous methods, our framework eliminates the need for a public dataset, which is highly medical-friendly considering the difficulty of medical dataset collection and publication. B) Our framework does not share patient-specific features, but only the network parameters, which raise fewer concerns about data privacy. This is also very useful in medical scenarios.

We thank R3 for the insightful comments on the specificity of our design in medical scenarios!"
https://papers.miccai.org/miccai-2024/516-Paper1260.html,"We thank all reviewers for their constructive comments. In our approach, we propose a transform-pool-based contrast learning super-resolution framework to effectively improve the modelâs superpartition performance. Here we try our best to clarify the reviewerâs main questions as follows within the scope permitted by the response rules:
Computational complexity [r1] : Although two basic models of brench are required for training during the training of the model, in the actual reasoning process, we only need to use base brench that has been strengthened by comparative learning for reasoning. Under our experimental conditions, the pre-processing of pathological data can reach an average speed of ten to the minus second order of magnitude per patch (there will be certain differences in the actual test of different devices), which is completely feasible in the real application background.
About the transformation pool [r3] : We are sorry for the undetailed explanation in the previous version. In our Settings, we selected rotation and image inversion at different angles for the reversible transformation part, and Gaussian filter, salt and pepper noise, median filter and arithmetic average filter were selected for the irreversible transformation pool. Random sampling of these transformations was performed during the training process.
As for the evaluation of bicubic method, our original intention is to show the results after pre-evaluation with non-parametric interpolation method using low-resolution pathologic patch for the downstream verification task. In other words, it is to show the potential performance limit of various super-resolution images on this task. Therefore, due to space and other reasons, After careful consideration, we concluded that bicubic as an interpolation scheme and various models with trainable parameters were not comparable in the direct representation of super resolution, and finally did not use it as an evaluation method for our comparison.
Method compliance [r4] : First of all, I would like to thank the reviewers for their questions about the method framework, which will be elaborated in more detail in our camera ready version. As for the selection of reversible transformation and irreversible transformation, I have explained in the previous answer. For the selection of irreversible transformation, we hope to select some noise, ambiguity and other problems that may exist in hypercomponent problems. We artificially reproduce part of this kind of data and adopt a contra learning scheme to avoid the same problem caused by our method (extrapolating the negative example set) as much as possible. For the selection of reversible transformations, we believe that all reversible image transformations can be selected for sampling.
About the definition of some modules: due to space limitations, we did not expand some modules in detail in the body, but their definitions are consistent with the original sources, you can refer to the following literature
For HAB block: Chen, X., Wang, X., Zhou, J., Qiao, Y., Dong, C.: Activating more pixels in image super-resolution transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22367â22377 (2023)
For DAT block: Xia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4794â4803 (2022)
On the difference between comparative learning loss functions [r4] : layer 0 is included in the calculation of our contrast function, that is, the comparison calculation of information before the input feature extraction network. In other words, the distribution of image space is also included in the calculation of contrast learning. In order to ensure the aesthetic definition of the formula in this paper, we did not define the contrast between feature space and image space separately."
https://papers.miccai.org/miccai-2024/517-Paper1156.html,"We would like to thank all the reviewers for their constructive comments.

Q1: Reproducibility of paper.
A1: We will release the code and models on GitHub if paper is accepted.

Reviewer#1:
Q2: Only incrementally novel by self-attention structure in generating pseudo mask.
A2: The reactivation attention is also important. Since conventional CAM only focus on most discriminative regions, which is insufficient for precise segmentation. While our reactivation attention can push model to focus on non-predominant features to expand the useful areas of CAMs, and generate fine-grained pseudo masks to alleviate the impact of noisy labels. Compared with previous works, our method is simple but effective.

Q3: Why Fa is teacher network and Fb, Fc are student networks? Why not try segmentation performance of Fb or Fc?
A3: In fact, three networks training independently, each network is supervised by the pseudo mask and the output of other two networks during training. To save paper layout space, we only take Fa as an example to illustrate our method. So when Fa is served as teacher, Fb and Fc will be the students. And the condition is similar on Fb or Fc network. Note that the denoising strategies for Fb and Fc are defined similar too. We already tried the segmentation performance of Fb or Fc. The results of our model listed in Table 1 is the average of three networksâ segmentation results for three repeated times.

Q4: No comparison with different backbones on the mask generation stage.
A4: All backbones listed in Table 1 are applied to segmentation stage, they are not used as classification networks on the pseudo mask generation stage. Actually, the previous SOTAs listed in Table 1 serve ResNet38 as their backbone on the pseudo mask generation stage and so do we.

Reviewer#3:
Q5: Method is a bit complex and the improvements arenât very big.
A5: Although our improvements of performances are not impressive, it has statistical significance (p<0.05) with other methods. And our method also reducing about 10%-30% GPU training time compared with the latest SOTA.

Q6: Benefit from more high level explanations.
A6: ARS is proposed to reactivate the insufficient CAM to obtain more precise pseudo masks. SWV is proposed to highlight correct labels and suppress noisy signals. SRM is proposed to guide models in learning the feature relation difference between pixels, in order to better clarify noises and clean samples.

Reviewer#4:
Q7: Hyperparameters justification and how do they influence training.
A7: We conducted a sensitivity analysis of all hyperparameters, but not fully included in paper due to space limits. The Î» in Eq.(9) is to balance the final loss of segmentation model. We found that the model achieves the best mIoU score 67.87 on BCSS validation set (shown in Table S3 in Supplementary Material) and 76.28 on LUAD-HistoSeg validation set when Î»=0.2. Since there is an order of magnitudes between Lswv and Lst, so the value range of Î± and Î² in Eq.(8) is considered from 10 to 100 in order to balance loss. Moreover, the angle-wise loss can better guide model to excavate useful samples than distance-wise loss, so we give a larger weight to Î² and lower weight to Î±. When Î± and Î² increased, the mIoU score on validation set of two dataset also increased, and reached the best score on two datasets when Î±=30 and Î²=60. As these two values continue to increase, modelâs performance will decrease. Since too large value will bias the modelâs learning ability and bring extra noise to influence final performance.

Q8: Complexity comparison with SOTA.
A8: We already done the complexity comparison but not included in the paper due to space limits. For MixTransformer, ResNet38 and ResNeSt101 backbone, the GPU training time of our method is 3.7h, 2.7h and 3.1h for BCSS, and 2.9h, 2.1h and 2.4h for LUAD-HistoSeg respectively. While the latest SOTA method TPRO needs 4.3h for BCSS and 3.2h for LUAD-HistoSeg. So our method is 10%-30% faster than latest SOTA."
https://papers.miccai.org/miccai-2024/518-Paper2885.html,"We appreciate the reviewersâ time, effort, and valuable comments to enhance our paper. They found our approach interesting and novel (R3, R4), with rigorous experiments (R3) and strong evaluation results (R4), showing good qualitative effects (R1, R3) and clinical application potential (R4). All reviewers (R1, R3, R4) recognized our workâs importance in addressing the critical issue of texture loss or blurring in 3D reconstruction. Below, we provide succinct responses to their major concerns."
https://papers.miccai.org/miccai-2024/519-Paper3852.html,"We thank all the reviewers for their valuable feedback. We appreciate the recognition of our ârigorous experimental evaluationâ (R3), âsolid, thorough analysisâ (R4), and âstrong evaluation methodologyâ (R5). We feel encouraged by the acknowledgment of our âinteresting application of gaze dataâ (R3), âclear demonstration of clinical feasibilityâ (R5), and the relevance of our study as a âvery important topic, pertinent to the MICCAI communityâ (R4). Below, we summarize and address the points raised.

Data Usage: We agree that the gaze metrics we use are standard in eye-tracking research and consider this a strength because the measures are well-established. The innovation of this manuscript derives from tailoring these metrics to 2D/3D registration assessment by computing weighted metrics with image similarity metrics like NCC in the areas of interest (cf. Sec. 2.2 and 2.3). Analyzing these weighted metrics offers a valuable perspective on gaze patterns during human-based assessment of spatial alignment that were not previously documented. We believe this contributes new knowledge to the field, and we will revise the Introduction to clearly highlight the aims and novel contributions (R4, R5).

Prior Work and Application: We appreciate the importance of incorporating relevant literature, and we will update the Introduction to include relevant studies on gaze analysis around fluoroscopy data and related fields (R3). Additionally, we will expand on the application and its relevance to image-guided surgery in the Introduction (R4).

Participant Details: We agree that participant details were insufficiently described. The recruited participants had a background in medical imaging, image processing, or both. We will update 3.1 Gaze Data Collection to state this explicitly (R3, R4).

Data Exclusion Criteria: Despite progress in gaze tracking hardware, reliable data collection remains a challenge for various reasons. Upon inspection of our sample, we had to exclude data from participants where significant portions of the gaze data were incomplete to ensure data quality. We will modify 3.1 Gaze Data Collection to clarify the exclusion criteria (R3, R5). We chose a stationary screen-based eye tracker for its anticipated accuracy in our tasks (i.e., inspecting an image and overlays on a screen) over a head-worn gaze tracker (R3). Despite this, we faced challenges with gaze tracking, mainly due to participants altering their initial position or interference from eyeglasses. We will enhance our Discussion and Conclusions with potential future research methods to mitigate these factors (R3, R5).

Justification for Limited Dataset: We acknowledge the limited size of our dataset resulting from the exclusion criteria. However, even with this constraint, we observed significant effects in our analyses with substantial effect sizes. We believe this study informs future research by identifying possible effects between gaze patterns and user responses, allowing proper power analyses for subsequent studies. We will update the Discussion and Conclusion to clarify this (R3, R5).

Discussion of Non-significant Results: Stationary gaze entropy and gaze transition entropy showed no significant correlation, likely because misalignment cues were presented statically, leaving users to decide where and when to look. Future research could explore dynamic paradigms that sequentially reveal misalignment cues, potentially making entropy calculations more controlled across individuals. We will ensure this discussion is clear and comprehensive (R5).

Discussion of Future Studies: We agree that investigating additional quantitative metrics, various medically relevant tasks, and surgeons with different experience levels is intriguing. We will update the Discussion and Conclusion to reflect these suggestions (R3).

Other Minor Points: We will improve the caption of Fig.2, move the models and analysis details to the Methods, and address the typo in the Results (R4)."
https://papers.miccai.org/miccai-2024/520-Paper0067.html,"We thank all reviewers for their constructive comments, and appreciate that they all agree this is a well-written paper and presents novelty. We notice that all their major concerns are related to experiments. However, we strictly follow the experiments setting used in previous related works published on MICCAI, i.e., the same dataset and same metric for evaluation.
1.R1/R4-experiments only on the BraTS2018 dataset is insufficient:
â(1) BraTS2018 is the most widely used benchmark in recent related works on this same task [2,4,9,11,12,14,15], including the latest SOTA work M3AE[9] we compared to. More importantly, the recent related works published on MICCAIâKD-Net6, ACN12, MFI15, mmformer14, all conducted their experiments on this single dataset. So we follow them to use this same benchmark in order to make a direct and fair comparison and it is also helpful for readers to directly reference their results in their papers. 
â(2) We acknowledge suggestions for other available datasets and we will take them into account in our future work. Meanwhile, our model performance on the BraTS2023 (Adult Glioma Segmentation) has 4.29%, 2.16%, 1.47% Dice gains  (average of all missing modality cases) compared to previous SOTA M3AE with p value less than 0.01 on all cases. Nevertheless, the BraTS2018 should suffice to be evaluated on because it is still widely used despite released earlier and it shares the same acquisition and annotation protocol with later BraTS dataset and the image and annotation quality are the same.
2.R1-marginal improvement: 
Our improvement over M3AE has a significant margin. We have achieved 4.61% Dice gains on average over M3AE for enhancing tumor segmentation, with more than 2% Dice gains on 14 out of 15 missing modality cases. This is a really big margin. Besides, on the harder cases with more modalities or important modality (T1c) missing, our method has even larger improvements. For example, we achieve over 10% gains with only {Flair} or {Flair, T2} for enhancing tumor, over 3% gains with {Flair} or {T2} for tumor core. The signifincant improvements on more challenging tasks and cases demonstrate our methodâs superiority. Moreover, M3AE employs a two-stage training process which takes 15h longer training time compared to ours under same setting.
3.R1/R3- statistical tests and other metrics: 
As most previous related MICCAI papers all used the averaged Dice score as the only metric, we follow their setting. Per the reviewersâ request, we provide more results. For statistical significance, the p values with mmF, MD, M3AE tested seperately under each missing modality case are all less than 0.01 with an average of 6.10e-5, 1.27e-05, 7.42e-4 respectively. The average Hausdorff distance results are 14.12, 12.46, 15.28 on the three types of tumor which are clearly lower (better) compared to previous SOTA M3AE (18.26, 15.41, 16.74).
4.R4-uncompared paper RFNet: 
RFNet was proposed in 2021, and was shown to be outperformed by M3AE proposed in 2023, according to [9]. Since we have shown that our method is better than M3AE, it should also be better than RFNet. Per reviewerâs request, we compare our method with RFNet, and our method has a 7.12%, 2.04%, 1.58% Dice gains on average over RFNet with p values all less than 0.01.
5.R3-Network detail and reproducibility: 
We will make the code public once the paper is accepted. The detail of the feature reconstruction network is a standard transformer block with a depth of 4, each consisting a self-attention layer followed by a feed forward network. The hidden dimension is 512.
6.R3-upper bound: 
The upper bound using all modalities with nnunet is 78.17%, 86.69%, 91.33%. Our results are close to the upper bound which shows its robustness to missing modalities.
7.R1-Figure 3: 
We have labeled the y-axis values on different plots and will adjust the scale to avoid the misleading.
8.Minor mistakes:  We will fix them."
https://papers.miccai.org/miccai-2024/521-Paper1002.html,"We are grateful for the comments, highlighting that our paper is âwell written and motivatedâ (R3,R4,R5), addresses a âvery important concern/topicâ (R4,R5), showing the âmost convincing counterfactuals (CFs)â (R5) with âsignificant/important improvementsâ (R4,R5), and âexcellent experiments and expositionâ (R5).

[Novelty/contribution, R3,R5]
A key contribution of our work is the identification of the attribute amplification problem. While spurious correlations are widely studied in the literature, we are not aware of any works that have previously discussed the amplification problem in counterfactual image synthesis. It was the discovery of the problem that allowed us to devise a simple yet effective solution. As such, we would encourage reviewers to consider the novelty and contribution of this paper as a combination of problem discovery, analysis, and effective mitigation of attribute amplification.

[Why attribute amplification is undesirable, R3]
Attribute amplification creates spurious correlations between attributes that were assumed to be independent, and hence, violates the causal assumption of the data generating process. The reviewer is right, that there can be unknown factors and confounders, in which case the assumed graph might be wrong for the application at hand. Defining an appropriate graph is the fundamental problem of causal modelling. But once a graph is specified for a given problem (typically taking domain expertise and prior knowledge into account), the generative model should obey these assumptions and therefore attribute amplification is highly undesirable.

[Details about attribute predictors, R3,R5]
The attribute predictors (ResNet34) used for assessing CFs are trained on real data with ground truth labels in a supervised fashion. Their reliability is confirmed in terms of high prediction accuracy on real test data (see Table 1). As suggested by R5, we will refer to Fig. 1 earlier to highlight the importance of attribute amplification. We will add details.

[Age attribute, R3]
Attribute amplification concerns variables that are assumed to be independent, however, age and disease are (causally) related. Therefore we do not consider age in this work.

[Measuring amplification, R4]
A change in attribute, such as being âhealthierâ after intervention on sex, is quantified by an increase in the attribute prediction accuracy for disease (Table 1) together with a visual distribution shift in PCA embeddings (Fig. A3). Inspecting logits, as suggested, is indeed another possibility that we have explored (but not included due to space limitations).

[CFs in downstream tasks, R4]
For table 2, the attribute predictors are trained with CFs only. We randomly generate one CF per subject per attribute. We then use the same number of CFs for training as for the predictors trained on real images. We will clarify this.

[CF validation, R4]
The reviewer is right that evaluating CFs is challenging and an open research problem. We opt to use attribute predictors to assess the quality, but visual assessment by experts is part of future work. If CFs are only used for training (e.g., data augmentation), their value may be assessed by the downstream task performance.

[Identity preservation, R4]
This is a common challenge in CF synthesis. We argue that our CFs are of high quality (see also comment from R5), preserving many subtle details and anatomical features (ribs, clavicles) after intervention. But some interventions, such as sex, will naturally affect the perceived patient identity. Additional visuals are in the supplement, and we will add more in our GitHub repository.

[Adding assumed graph, R4]
We agree it is helpful to have the causal graph on page 4. We try to move it to the main text.

[Shortcut learning papers, R5]
Thanks, will add those. We agree that spurious correlations are well studied in shortcut learning literature. However, the link to attribute amplification in CF synthesis was not previously discussed."
https://papers.miccai.org/miccai-2024/522-Paper2540.html,"We thank all reviewers (R1, R4, R6) for the constructive comments. All reviewers acknowledged that our method is mathematically sound and novel, as âobjective functions and optimization methods are novelâ (R1), âthe mathematical derivation in this work is thorough and interestingâ (R4), âMILPâs systematic approach and novelty are notable strengthsâ (R6).

The main concerns:

Additional responses.

To R1:
Q1: Covering radius is not defined.
A1: We give a simple explanation (i.e. minimal angular separation) in introduction. We will add a mathematical formula in final paper. 
Q2: A typo in equation (3a).
A2: Thank you very much. We will correct it in the final paper.

To R4:
Q1: Approximation in Equation 4(a) may not be accurate.
A1: We agree that the approximation of packing density may be inaccurate, but it is necessary to convert the optimization into MILP. 
Q2: Will there be larger artifacts like the eddy current. 
A2: Optimizing the ordering mainly focus on obtaining uniform distributed subsets upon interruption. We could better reduce eddy current after optimizing the polarity. 
Q3: Worse covering radius in the combined shell. Why better fODF?
A3: MILP obtains better angular separation in each shell, and multi-shell SD is used."
https://papers.miccai.org/miccai-2024/523-Paper1311.html,"We thank the reviewers for their detailed and valuable feedback. We refer to weakness section as âWâ in our rebuttal below.

R1,W1: âNon-expert doctors do the screening mammographyâ is highly unlikely anywhere in the world: The said phrase does not appear in our paper. In Section 1 (Screening vs. diagnostic mammography), we wrote, âFor non-expert readers, the screening mammographyâ¦,â in which, we are referring to âreaders of our manuscriptâ without the medical background. We shall rephrase for better clarity.

R1, W2: âhistory is gathered through a cost-effective questionnaireâ is not convincing: The statement is clearly not intended to be generalized to all aspects of medical history taking. The process of eliciting a medical history is an art and no questionnaire can compete with it. However some simple aspects of history- such as whether the patient feels a lump, has a history of discharge etc are within the patients domain to answer, and this is routinely collected in our hospital. This is information for anyone interpreting these studies and holds potential for making deep neural networks reliable.

R1,W3: âOversight of screening renders existing deep neural network models unreliableâ: Our manuscript does not contain the said phrase. In Section 1 (Screening vs. diagnostic mammography), we wrote, âThis oversight renders existing deep neural network models unreliable,â referring to the limitation of DNNs that only use mammograms without patientâs clinical history. We did not imply that screening mammography is unreliable.

R1,W4: âoverall novelties of this work are limitedâ: The comment that cross-attention for cross-modality fusion is standard, itself implies that many works use fusion with different details. Similarly, our work uniquely integrates textual clinical history with visual mammogram data, as noted by Reviewers 2 and 3. We did not claim that our basic architecture is novel. However, our technique for integrating clinical history with mammogram data is novel, and outperforms any other detection approach. Additionally, our proposed method allows to exploit computer vision foundational models, which have been used for small-sized natural images so far, on high-resolution medical images. This was acknowledged as innovative by Reviewer 3. We believe our unique and interpretable approach offers valuable contributions to the community.

R1,W5;R3,W1: more comparison of mammography-related detection methods: To the best of our knowledge, no existing literature proposes a multi-modal DNN for mammography with source code released for comparison. We can show results with image-only models in camera ready, but it will not be fair as our model also uses additional clinical history information.

R2,W1: Application to real-world datasets: Our experimental setup is specific to the medical imaging domain and does not make sense for natural images. In natural image settings, typically, if the image description is available, the image class would also be known, making such a problem irrelevant. If the reviewer could suggest a suitable benchmark dataset for the experiment, we would be happy to perform it.

R2,C1: ablation studies on text modality: In our experiments we also trained MedCLIP encoder based on BioClinicalBERT. We found RoBERTa to be a superior encoder. Adhering to the MICCAI guidelines, we do not disclose the numbers here.

R2,C3-5: finetune the ROIs extraction model?: As mentioned in Section 3.1, we annotated a small subset of our training data with bounding boxes to train the detection networks to extract relevant ROIs from the mammograms. All object detection networks mentioned in Table 3 were trained in the same manner.

R2, W2: how prompts are used during training and testing: For our model we do not include âCancer:{Yes/No}â during the modelâs training or testing phase. This is introduced only in the CLIP training paradigm for baseline comparison. This is a standard setting with no label leakage."
https://papers.miccai.org/miccai-2024/524-Paper1370.html,"Response to Reviewer # 1:
Thank you very much for recognizing our work. Your valuable comments will be crucial for us to improve the quality of our work in the future. Currently, this study on multimodal lymph node metastasis diagnosis for esophageal cancer is unique and rare, and such multimodal datasets are globally scarce and lack attention. Combining these CT lymph node and tumor modalities and annotating multimodal datasets require a significant amount of work. Therefore, at this preliminary stage, due to time and length constraints, we have presented our work to this extent. In future work, as you suggested, we will consider a more comprehensive comparison in terms of clinical feasibility and real-world applicability, including comparisons with other LNM diagnostic methods. Additionally, we will consider multi-center validations and include similar modality data for other lesions. This will deepen our research on the data. For the second point regarding the discussion of limitations, we have added this in the camera-ready version of the paper. Critically, we will further test the impact of backbone network on model performance and further explore the utility of the loss function part in future work. Thank you again for your review effort.

Response to Reviewer # 3:
Thank you very much for recognizing our work. Your valuable comments will be crucial for us to improve the quality of our work in the future. The masks in the CT images were annotated by doctors from Sichuan Cancer Hospital. Regarding the reason of input of three largest masks, we selected the three largest lymph nodes (90Ã90Ã22; 68Ã68Ã16; 48Ã48Ã8) because the remaining lymph nodes are too small and provide insufficient information. Furthermore, the unified selection of three lymph node regions for feature extraction is conducive to the maximum efficiency of the model, and larger lymph nodes can provide valuable prognostic information to the maximum extent and provide minimal redundancy. The gold standard for lymph node metastasis in large dataset is at patient level, which is the same as our paper. Besides radiomics data, other data were clinically collected by Sichuan Cancer Hospital, and the radiomics data were extracted using the Pyradiomics package. We will supplement the details of the data acquisition process and the ablation study in the camera-ready version of the paper and correct any textual errors. Thank you again for your review effort.

Response to Reviewer # 4:
Thank you for your recognition of our work. We will further extend our work in the near future. Thank you very much!"
https://papers.miccai.org/miccai-2024/525-Paper1159.html,"Thanks to the reviewers for your valuable comments of our work. We address the concerns as follows:

Methodological detail and available code (R1.Q4, R4.Q1, R4.Q4)
1.We have already prepared a GitHub repository to share our code publicly. But rebuttal guidelines prevent us from providing external links. Therefore, we will make the repository accessible immediately following the acceptance notification.
2.To enhance clarity in our methodology, we plan to revise Fig 1. It will be split into two separate figs: one showing samples and the other detailing the method. The latter will add a flowchart including Shuffle-Aug and Error-Prompt Pruning. For Error-Prompt Pruning, f_w is our proposed model, utilizes prompt-QAs in its input text, which includes both answered and unanswered questions. In contrast, f_w/o is trained and tested under identical conditions without the inclusion of prompt-QAs.

Questionable Superiority Claims (R4.Q2)
1.We have thoroughly reevaluated MMQL in comparison to MedVInT-TD, acknowledging it as significant work in the field. We will include MedVInT-TD in our references and integrate its results into Tab 1. However, itâs important to note that MedVInT-TD benefits from pre-training on the PMC-VQA dataset (177k samples), while our method does not utilize such extensive pre-training. This significant difference suggests that a direct comparison between MMQL and MedVInT-TD might not provide a fully equitable assessment.

Generalization on modalities, scenarios, and downstream tasks (R1.Q2, R3.Q1):
1.The MMQL leverages all relevant questions associated with a medical image in a single forward pass, enhancing the modelâs ability to recognize inter-question correlations. Its applicability is driven by the inherent one-to-many relationship between images and questions, regardless of modalities, scenarios, and downstream tasks.
2.Our study utilizes VQA-RAD and SLAKE, which encompass 3 primary image modalities (X-Ray, CT, and MRI) and 3 anatomical locations (lung, abdomen, and brain). These modalities and locations are representative of the broader range other datasets like VQA-Med (2018-2020) and RadVisDial. Additionally, our work aligns with previous works (ref 3, 4, 7, 12, 19), which also focus on 1-2 datasets.
3.Previous studies (âOpen-ended medical visual question answering through prefix tuning of language models.â) have characterized the open-ended setting as a text-generation QA. In this scenario, the modifications would be required in the final MLP layer. It would be replaced with a decoder. Additionally, adjustments would be necessary in the Error-Prompt Pruning to incorporate metrics like perplexity, which better suit the text-generation context.

Error analysis (R1.Q5, R3.Q2)
1.We have detailed 3 typical cases in the supplementary material. We observed primary issues such as inconsistency answers (Xmlab105) and language bias (Xmlab469). Error prompt-QAs can lead to cascading mistakes, prompting us to introduce the Error-prompt pruning.
2.Rare cases in Section 3.5 mainly involve language bias, particularly for questions with imbalanced answer distributions. E.g., the 9th question in Xmlab469 has two potential answers, with one being significantly more common. Mix-up helps augment these less common answers, a strategy not employed by our method. This likely explains why our method ranks second in terms of ECE/MCE metrics.

Other suggestions (R1.Q1, R4.Q3, R1.Q3)
1.We plan to enhance both the Introduction and Conclusion sections by incorporating examples that clearly illustrate our methods and their significance.
2.The extra 1/2-page in camera ready version can be used to include the formula for vanilla accuracy in Section 3.1.
3.Ordering questions is a great suggestion. But it needs extra labeling, and most datasets do not meet this requirement. Thus, training questions without ordering is our first option. In this scenario, Shuffle-Aug shows its effectiveness (Tab2), which can prevent over-fitting."
https://papers.miccai.org/miccai-2024/526-Paper0375.html,"We thank all the reviewers and meta-reviewers for their consistent support of our work, as well as providing insightful suggestions. Based on the comments, we have refined our paper to enhance clarity in our presentation. For instance:

Overall, we have carefully addressed the comments and will continue to polish our paper for the camera ready version."
https://papers.miccai.org/miccai-2024/527-Paper0399.html,"We sincerely thank all reviewers for their invaluable comments and approval that our method is novel. The code will be published for reproducibility.

Q1/R1: Interrelationship between 3 stages.
A1: Three stagesâkeyframe detection, keyframe captioning, segmentation and measurementâare trained and evaluated separately because: 1) Each stage has unique objectives, making joint training inefficient and potentially detrimental to performance. 2) Only a few frames in US video are keyframes with caption or biometry labels, so misidentified non-keyframes cannot be properly evaluated in subsequent stages due to absent labels.
Sequentially processing stages in real deployment might lead to error accumulation, so we aim to ensure high accuracy in each stage individually. In the future, we plan to predict uncertainty for keyframes and captions, providing extra information for sonographers to correct the multimodal summary and reduce error impact.

Q2/R1: Insufficient discussion on stage 1.
A2: Recent papers related to keyframe detection have been discussed inVideo summarization' ofRelated Workâ. Early non-deep learning (DL) methods focused on keyframe selection but lacked computational efficiency. Recent DL methods prioritize video clips but are unsuitable for MMSummary because: 1) Clips may include too many frames, complicating documentation. 2) Neighboring frames often appear redundant in US video. 3) They cannot handle the extreme class imbalance in keyframe detection. 4) Video clips cannot be directly used for biometry.

Q3/R1: Ensuring caption accuracy.
A3: Our model includes frozen image encoder of BiomedCLIP, frozen GPT-2, and a mapping network that bridges visual and textual features. This network inputs visual features to generate prefix embeddings, which are fed into GPT-2. By fine-tuning the mapping network, GPT-2 can produce accurate captions for keyframes.

Q4/R1: Frame quality before/after redundancy removal.
A4: There are many redundant frames in pâ, so we design Diverse Keyframe Detection to detect representative keyframes p while removing redundancy. As in sec. `Keyframe Detectionâ and supp. Fig. 2, properly chosen tau&tauâ ensures keyframes capture essential information while minimizing redundancy and information loss. A too-large tau leads to frame redundancy, while a too-small tau risks information loss. A too-small tauâ includes non-informative frames and a too-large tauâ discards useful frames.

Q5/R1: Compared to text-guided methods.
A5: Our goal is multimodal summary generation, addressing all related challenges, with biometry as a crucial part. Stage 2 generates captions that aid segmentation, mimicking the sonographerâs process, so we design a text-guidance method in stage 3. We have shown that text guidance outperforms random prompts in Table 1. Our focus is not solely on text-guided segmentation, so we did not compare with existing ones. But other text-guided methods can also be applicable in stage 3.

Q6/R3: Model complexity.
A6: MMSummary performs inference after the entire examination. Despite its complexity, the model processes a video in seconds across all three stages, ensuring efficient inference.

Q7/R3: Simple stage 3 experiments.
A7: Our ultimate goal is not only segmentation and measurement but also keyframe detection and captioning, to generate multimodal summary. For stage 3, we have compared our biometry performance with two existing ones. Adding more experiments is not feasible due to the rebuttal policy. Given that our target is broader than segmentation and measurement, the current comparisons should sufficiently demonstrate the frameworkâs overall performance. (Also see Q5)

Q8/R4: Video Transformer&GPT-2 are a little archaic.
A8: Video Transformer&GPT-2 remain effective and are still used in recent papers, e.g. ViECap (ICCV23), SMALLCAP and AutoAD II (CVPR23). They are chosen for their efficiency, especially when computational resource is limited and dataset is small in medical field."
https://papers.miccai.org/miccai-2024/528-Paper1678.html,"We gratefully appreciate reviewersâ recognition of our workâs technical novelty (R1, R3, R4) and its practical value (R1, R4). We hope the concerns are addressed as follows.

R3/4: Applicability to infer with limited computation resources. 
The experimental details are elaborated in Section 3 and we will release code/model upon acceptance to support reproducibility. During inference, our model requires 10.742GB GPU memory, which is feasible for a typical single-GPU system. Moreover, our model doesnât require real-time inference, which reduces the demand for computation resources. Its application in our ongoing large-scale brain cohort construction has rescued many data that previously failed quality control due to motion artifacts. The in-house application underlines the merit of our method, especially when itâs not feasible to reacquire the data from the volunteers.

R3: Adaption to real-world data. 
We argue that our method is effective to handle real-world data as already demonstrated in our paper, such as validations on three external datasets with real (not simulated) motion artifacts in Section 3 (Fig. 3, Fig. S2). The in-house application stated above also verifies its adaption to real-world data.

R3: Adaption to different data types. 
Fig. S2 shows the modelâs adaption to data with different scanners and imaging parameters (FOV, TR, TE, etc), proving its efficacy of correcting motion of diverse data. In this paper we focus on a single MRI modality, partially due to the page limit. We agree with the comment, and are working with multi-modal validation. We will soon release results in follow-up papers.

R3: Clarification on Uncertainty Predictor (UP). 
First, we apologize for a typo in Eq. 5, where the loss term in the second line should be same with that in the first line. We will make correction in final version. 
The diffusion model benefits significantly from conditional guidance, and UP provides an adaptive way to improve the guidance precision. As detailed in Section 2.2, UP employs a DBT-equivalent network with three decoders to estimate the guidance and parameters Î± and Î² separately (Eq. S1). The parameters É and Î² are subsequently used to compute the uncertainty map (Eq. S2). The diffusion model reaches its final prediction through guided iterative sampling while the uncertainty map produced by UP refines the sampling process by adjusting loss weights (Eq. 5). In ablation studies (Table 2), we validated that UP can prevent PSNR degradation while improving perceptual quality.

R1: Lack of visualization showing improved consistency in MRI volume. 
We observed enhanced through-slice consistency when checking the 3D volumes. Regrettably, given the rebuttal constraints, we cannot show the results now. We intend to publish the code/model and display visual results on GitHub page after acceptance.

R1/4: Clarification on the benefits of network designs. 
1) The integration of the trainable ControlNet to the frozen Unet in diffusion enhances training efficiency and quality of the corrected images, and mitigates overfitting on relatively small datasets while preserving the knowledge of large model learned from billions of images; 2) AP-Diff relies on the good initialization startpoint provided by DBT, focusing on distinguished features for authentic texture prediction and enhanced visual quality, tackling texture loss and oversmoothness.

R1/3/4: Minor issues on writing.
1)Fig. 3 displays real-world artifact correction across three external datasets; 2)We apologize for any confusion on the UP structure, it is a DBT-equivalent network with three decoders. In the final version, we will revise and simplify Fig.1; 3)The Absolute Error Maps range from 0 to 1; 4)We will check and revise all the details and grammers in the final version; 5)A brief discussion: Besides high performance on diverse motion types, our model, like other diffusion-based models, should also address acceleration and lightweight challenges."
https://papers.miccai.org/miccai-2024/529-Paper1623.html,N/A
https://papers.miccai.org/miccai-2024/530-Paper2265.html,"We appreciate the constructive and thoughtful comments from our reviewers. Below are our detailed responses.

R1, Explanations for variables (e.g., v in formula 3), or lacking explanations for some terms (vicinal distribution, vicinal risk)
We are sorry for the unclear statement. v represents a vicinity distribution, measuring the probability of finding the virtual model f_ij in proximity to existing models f_i and f_j. The empirical risk denotes the average loss function across model samples. Similarly, the empirical vicinal risk indicates the average loss function across constructed vicinal model samples. We will clarify it in the revision.

R1, Many design details are based on empirical assumptions, e.g., the linear interpolation of the convolution parameters between encoders.
Thanks. Our methods are designed based on the prior knowledge that linearly interpolating convolution parameters results in linear interpolation of the relevant features. The effectiveness of this approach is further validated through our ablation study.

R3, Only mixing one convolution layer is weird in the setting. More explanations and experiments are required.
Thanks. In this study, we randomly mixed one convolutional layer, leading to linear interpolation among generated features, while ensuring stable training. We acknowledge that mixing more convolutional layers could be a viable approach and would like to leave it as the future work.

R1,R2, The performance comparison between the proposal and baseline methods seems unfair: the proposed methods which were trained on multiple dataset were compared to the models trained on single dataset.

Thanks. For fair comparison, we try to train compared methods on multiple datasets, using different decoders for each task while sharing a common encoder. However, we discovered that even when trained on multiple datasets, performance is inferior to training on each dataset separately (Table 1, model #5 vs model #2). This performance gap occurs because limited training supervision makes it challenging for a single encoder to generalize across diverse tasks. Therefore, we opted to compare our proposed method with weakly supervised methods trained separately on different datasets.

R2, why the Dice in Table 3 is better when only MSCMR segmentation task is introduced than when ACDC+MSCMR segmentation is introduced?

Thanks. This discrepancy arises because images from MSCMR and MyoPS exhibit enhanced pathological information, whereas ACDC primarily contains structural information. Consequently, the pathology complementarity between MyoPS and MSCMR is more significant than that between MyoPS and ACDC. With the introduction of ACDC, ModelMix can assimilate robust shape priors, leading to improved HD performance, albeit with a slight decrease in the Dice metric.

R1,R2,R3, Source code
Thanks. The code will be released upon acceptance.

R1, The framework should be briefly illustrated in the caption. Also, the font in the figure should be slightly enlarged. Suggest âstagesâ instead of âcomponentsâ.

Thanks. We will revise it in the manuscript.

R1,R2, The âc1â, âc2â, âLcosâ should be clarified. I suppose that it is equivalent to âIc â xâ and the cosine similarity.

We are sorry for the unclear statement. c_1 and c_2 refer to I_c1 â x and I_c2 â x, respectively. Lcos denotes to the cosine similarity. We will clarify it in the manuscript.

R1, Is the ground truth of the scribble denoted as âyâ in eq. 7 and 8?
Yes, we will clarify it in the manuscript.

R1, For Table 1, the task combination is confusing.
We are sorry for the confusing statement. In models with vicinal loss or a shared encoder, they utilize task combination. For instance, model #1 and #2 apply PCE and invariant loss to separate tasks. Meanwhile, models #3, #4, and #5 in the upper and lower columns report results of MyoPS + MSCMR and MyoPS+ACDC, respectively.

R1,R2. Typos.
Thanks, we will revise it."
https://papers.miccai.org/miccai-2024/531-Paper2168.html,"We express our sincere gratitude to the reviewers for their valuable time and effort in evaluating our paper. We greatly appreciate their insightful and overall positive feedback. We are pleased to note that the reviewers have recognized the following aspects of our manuscript: (1) the manuscript is well written and easy to follow (R1, R4), (2) the proposal of MoME is promising and reasonable (R1, R3, R4), (3) the justification of why self-attention is better than co-attention (R3), and (4) our method is effective (R1, R4). Below, we provide our responses to the questions.

We believe that these responses and revisions can address the concerns of the reviewers and improve the quality of our manuscript. Thank you again for your valuable feedback.

[1] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, ICLR 2017.
[2] Multimodal Co-Attention Transformer for Survival Prediction in Gigapixel Whole Slide Images, ICCV 2021."
https://papers.miccai.org/miccai-2024/532-Paper1536.html,"We sincerely thank all reviewers for their valuable comments. We are grateful that our work was recognized as well-written (R3, R4), promising results (R3, R4, R6), interesting (4), and convincing ablation (R3). Regarding the concerns, we address one by one as follow:

Are the splits of training, validation, and testing randomly? Cross-validation recommended (R3-Q2)
The dataset was randomly divided according to a split ratio of 0.8, 0.1, and 0.1. This setting is consistent with the experimental settings in the baseline papers (MAPs and MSPs). However, we acknowledge that cross-validation provides a more robust evaluation method.

The comparisons of the computational resources (R3-Q3)
Thank you for the comment. We compare the computational resources required for our method and other competing methods below. The resource comparison shows that our method is more efficient than competing methods in terms of both memory usage and training time.

Dataset    Method    Memory    1000 training steps
ODIR        MAP        13.0 GB    1.82 h
ODIR        MSP        12.1 GB    1.85 h
ODIR        MoRA      11.6 GB.   1.59 h

CXR         MAP        14.4 GB    1.71 h
CXR         MSP        12.4 GB    1.75 h
CXR         MoRA      12.2 GB    1.58 h

Code not provided (R3-Q4)
We plan to open-source our code.

What would be the performance in extreme cases? Why do you choose the missing rate form text and images between 30% and 65%? (R4-Q1, R6-Q2)
We followed the experimental settings used in the baseline papers (MAPs and MSPs). In our study, Figure 2 illustrates several extreme cases where the missing rate is exceptionally high (e.g., 0.9). For even more extreme scenarios, such as 100% image and 0% text or 0% image and 100% text, fine-tuning is not feasible; only the evaluation of the pre-trained model is possible, which makes it as a zero-shot problem.

Can you get the similar finding or conclusion for the ablation study using CXR? (R6-Q3)
We observed similar results for the CXR dataset; however, we did not present these results due to page limitations."
https://papers.miccai.org/miccai-2024/533-Paper0782.html,"We thank the reviewers for the valuable comments, which help to improve the quality of this paper. We are encouraged that the reviewers found our motivation and idea to be novel (R3, R4, R5), and that our comparisons against varying backbone showed significant improvements (R5). We respond to your concerns as follows.
Detail of Fourier space operations and adversarial learning (R4, R5): Low-frequency components capture style variations across domains due to their high energy distributions, while high-frequency components focus on object structures and identity[3]. Therefore, it is reasonable for the reconstruction decoder to focus less on low-frequency components to produce images of diverse styles. To further enhance this capability, we utilize adversarial learning to generate data samples of more diverse styles that fool the segmentation network, significantly improving segmentation robustness[4], as shown in Table 3.
Novelty clarification (R5): CCSDG[1] employs the same data augmentation method as FDA[2]. Fourier-based data augmentation methods, including FDA[2] and FACT[3], generate style-augmented images by just exchanging the low-frequency components of the amplitude spectrum. MoreStyle utilizes adversarial training and L_{FSD} to encourage the reconstruction decoder to generate images with more diverse low-frequency components, thereby also enhancing the stylistic diversity of the images. As illustrated in Fig 1 (b) and Fig 2 in the supplementary material, MoreStyle is widely spread across the tSNE space, further demonstrating its capability to generate images of more varied styles. We also compare the data augmentation of MoreStyle with FDA[2] and FACT[3] through ablation experiments, demonstrating superior performance. The results are presented in Table 4. 
Faster convergence speed (R4): MaxStyle[4] requires 1500 epochs for network convergence. In contrast, MoreStyle achieves good performance in just 100 epochs, showcasing a much faster convergence rate, as detailed in Section 3.2. MoreStyle adds 3.76M parameters over the baseline but does not increase inference time, maintaining efficiency while significantly enhancing performance, as shown in Table 3.
Discussion about the contributions of related work(R3): Due to page limitations, our discussion on the contributions of related work is limited. We will conduct a fuller discussion about the contributions of related work in the camera-ready paper.
Additional dataset (R4): We conduct comparative experiments on two public datasets. We are willing to test the WIA-LD2ND on more datasets. Due to the rebuttal policy prohibiting new experimental results, we will provide the results on GitHub in the future.
Data augmentation results (R5): As shown in Fig. 1 and Fig. 2 in supplementary material, which show the data augmentation results, they demonstrate that MoreStyle generate images with more diverse styles.
The effects of varying parameter (R4): We conducted a series of ablation studies on varying parameters as shown in Table 1 in supplementary material.
[1] Hu, Shishuai and Liao, Zehui and Xia, Yong: Devil is in channels: Contrastive single domain generalization for medical image segmentation. MICCAI, 2023
[2] Yang, Yanchao and Soatto, Stefano: FDA: Fourier domain adaptation for semantic segmentation. CVPR, 2020
[3] Xu, Qinwei and Zhang, Ruipeng and Zhang, Ya and Wang, Yanfeng and Tian, Qi: A fourier-based framework for domain generalization. CVPR, 2021
[4] Chen, Chen and Li, Zeju and Ouyang, Cheng and Sinclair, Matthew and Bai, Wenjia and Rueckert, Daniel: MaxStyle: Adversarial style composition for robust medical image segmentation. MICCAI, 2022"
https://papers.miccai.org/miccai-2024/534-Paper0906.html,N/A
https://papers.miccai.org/miccai-2024/535-Paper0967.html,Thank the AC and the reviewers for their efforts and constructive comments. We will incorporate the revisions in the camera-ready and journal version of the paper and make further improvements in our future work.
https://papers.miccai.org/miccai-2024/536-Paper2723.html,"To Reviewer #1
1.Thank you for mentioning of STI. According to current research, there are deep learning-based methods (10.1016/j.neuroimage.2021.118376, 10.3389/fnins.2022.837721) using single scan to preserve susceptibility anisotropy. STI requires echo data from six different directions for calculation, making it impractical for clinical applications. Therefore, this study uses QSM as a non-invasive method for assessing brain iron. A significant clinical application is the use of segmentation methods instead of registration to improve the accuracy, as demonstrated by the results in Fig. 3, Tab. 2, and the supplementary materials.
2.Thank you for raising concerns about some claims. A highlight of this study is the multi-task QSM reconstruction, including reconstruction and segmentation. Having these two results allows for brain iron quantification immediately, constituting an âall-in-one solution.â The precision lies in the above mentioned that segmentation-based methods are more accurate than registration-based. The results from the ablation experiments show that the proposed multimodal fusion method indeed enhances model performance.
3.Thank you for pointing out the completeness issues. The QSM reconstruction primarily relies on the TKD method. The justification and analysis of this approach are supported by research on TKD. Therefore, a detailed discussion of the TKD method is not included in this study.
4.Thank you for your concerns about the innovations. The innovations are as follows: itâs the first to propose the concept of multi-task QSM reconstruction, offering readers new insights in QSM reconstruction. This study demonstrates that segmentation-based methods are more effective than registration-based methods for brain iron quantification. This finding, in turn, provides a novel perspective for research in the neurological field.
5.Thank you for your insights on the evaluation of segmentation. This study validates that the QSM quantification of brain regions based on segmentation results shows no significant difference from the ground truth, indirectly reflecting the accuracy of the segmentation results. Regarding the clinical application of QSM, numerous studies in Radiology have reported the use of QSM for brain iron quantification and its application in neurodegenerative diseases. QSM has already been incorporated as a routine examination sequence, providing a non-invasive method for assessing brain iron distribution. Additionally, recent research has introduced sub-voxel QSM methods, allowing for a more detailed analysis of the distribution of paramagnetic and diamagnetic substances. This advancement offers tools for both brain iron quantification and demyelination analysis.

To Reviewer #3
1 & 2. Thank you for pointing out these issues, which will be addressed in the revised version.

To Reviewer #4"
https://papers.miccai.org/miccai-2024/537-Paper1151.html,N/A
https://papers.miccai.org/miccai-2024/538-Paper1471.html,"We thank all reviewers for their acknowledgment of our contribution and their constructive comments for further clarification.
Q1: Detail about loss functions Eq.6. (R1&R3)
A1: The final objective function contains five parts, as follows. (1) Lsur is calculated based on the risk values output by the final survival header, that adopts the NLL function. (2) NLL loss Lcsur is calculated based on the risk values output by an additional survival header (a linear layer), the header accepts the average of C^P and C^G. For (3) the common loss Lc and (4) the specific loss Ls, we employ ground truth modality labels represented by one-hot encoding and use BCELoss to compute them in adversarial learning for modality binary classification (WSI or gene), ensuring the purity of common and specific features. (5) The Eq.5 provides Lor, that further eliminant redundancy through penalize C and S using Euclidean distance. In the final paper, we will add each loss function labeled in Fig. 1 for better understanding and provide details of loss functions.
Q2: Parameters of Eq.6. (R2)
A2: The parameters are established empirically. We set alpha_1=0.1 and alpha_2=0.01 in all datasets, beta=0.7 in BLCA and beta=0.5 in the other two datasets. We will report these settings in the final paper. As suggested, we will explore learnable parameters in the future.
Q3: The second BCAâs contribution and effectiveness of MOFC. (R2)
A3: The second BCA aims to capture interactions of common and specific information that remain consistent and specific respectively, and explores more complementary information. Also, as suggested, we have used the second BCA to integrate S and concatenate (S^P, S^G) retraining our model on BLCA, obtaining an unsatisfied CI result. This performance degradation is due to incomplete information in fusion features. In MOFC, the two FC layers closely cooperate with the discriminator and play pivotal roles in enhancing specific information. This is supported by the ablation study results obtained by our model w/o Specific. Also, the performance degradation brought by the model w/o GAN indicates the vital role of common information.
Q4: How can high/low risk be determined in Fig. 2? (R2)
A4: Our model outputs risk values (probability of an event occurring at a specific time). Following the routine clinical practice  in predicting survival risks, patients were divided into high/low-risk groups based on median risk, and KM curves were built for each group reflecting OS times. 
Q5: Explanation of each ablation variant. (R3)
A5: w/o Lor: to verify the redundancy elimination ability of Lor; w/o Specific: removing the two FC layers of MOFC to verify its ability to enhance the specificity of the layers; w/o GAN: to confirm whether both consistent and reduced redundant information improve the performance; w/o BCA: to verify multi-modalities, multi-granularity integration and interaction ability of BCAs, including i) the integration and interaction between specific genetic features and specific pathological features; ii) the integration and interaction between common and specific features. We will provide these explanations in the final paper.
Q6: The model is complex. (R3)
A6: As a multimodal approach combining large-scale pathological images, our method, with moderate parameters of 7.81M and GFLOPs of 22.12, is not excessively complex. Also, the inference speed of our method is 179848p/s which is far better than that of CMAT of 84407p/s (p/s is the number of processing patches of WSI per second).
Q7: Format and writing. (R1&R3)
A7: We will double-check the paper to ensure correctness of the format. We will refine the writing and improve the introduction of methodology to enhance the clarity of our work.
Q8: Subfigures in Fig. 1 are unclear. (R2)
A8: We will improve the image quality in the final paper.
Q9: Missing citations of two-stage multimodal fusion methods. (R2)
A9: We will cite the mentioned references in the final paper."
https://papers.miccai.org/miccai-2024/539-Paper1255.html,N/A
https://papers.miccai.org/miccai-2024/540-Paper4049.html,"We appreciate the reviewersâ time and effort in providing valuable feedback on our manuscript, Multi-Dataset Multi-Task Learning for COVID-19 Prognosis. Below, we address the concerns and suggestions raised.
We recognize that the manuscript needs a more thorough and clearer analysis of potential limitations, as evidenced by reviewers R1 and R4. Our approach aims to demonstrate that performance improvements are achievable by extracting prognostic information from multiple datasets. These datasets have medical correlations but disjoint labels. However, the applicability of this method is limited by the need to select datasets and tasks that follow a well-determined logic, ensuring they correlate with prognostic outcomes despite different labelling schemas. In future work, we plan to develop a model capable of processing prognostic information from a broader range of heterogeneous datasets. Following the reviewersâ advices, we will emphasize the expressed limitations in the conclusions and possible future research directions.
We were encouraged by reviewer R1 comments to delve into more detail on managing datasets from distinct data sources, particularly on data harmonization, universal annotations, and potential biases introduced by combining datasets. We preprocessed all images from both datasets following the same steps. These included lung segmentation, bounding box extraction of the lungs, square cropping using the bounding box, and image standardization using the same mean and standard deviation. Regarding the annotation systems and potential biases, we used the morbidity label (mild/severe) for the AIforCOVID dataset, while for the BRIXIA dataset, we adopted the original severity score system and applied a relabeling function (Eq. 4 in Section 3.1 of the Manuscript). This step was necessary to mitigate bias introduced by radiologistsâ annotations. Moreover, by simplifying the severity score system, we noticed that the BRIXIA task was more supportive of the AIforCOVID task. We have not found evidence of possible universal relabeling adoptable on prognostic tasks, apart from new Self-Supervised Learning (SSL) paradigms. These paradigms include the possibility of bypassing the problem by adopting new self-labelling strategies that could be beneficial for our context. Future research will explore the generalization ability of SSL pre-trained models to downstream prognostic tasks in zero-shot inference and with parameter-efficient finetuning methodologies.
In response to all reviewersâ suggestions for further analysis of advanced models and traditional MTL methodologies, we are aware that our work focuses on the learning framework rather than a new architecture. Our architecture is well-documented in the Multi Task Learning (MTL) literature (as hard parameter sharing), as evidenced by reviewer R5 referring to MMoE and PLE models. However, our custom loss function presents its novelty in the biomedical field in handling two disjoint datasets with different labelling schemes. We are intent on expanding the applicability of our framework even to more advanced MTL architectures, evaluated on single datasets with multiple tasks.
Reviewer R5 expressed major concerns about the possibility that, during training, all sampled data could come exclusively from one dataset, causing the custom loss function (Eq. 2 in Section 2 of the manuscript) to revert to a standard loss function. This scenario was investigated during the experimental phase, leading to the implementation of a custom sampler to ensure the presence of samples from both datasets in any batch passed to the models. The results obtained are not significantly different from those presented in the manuscript. For this reason, it was not considered during the manuscript drafting. For clarification, we will provide a description of what was previously described in the experimental settings section."
https://papers.miccai.org/miccai-2024/541-Paper0973.html,"We thank the reviewers for their invaluable comments and recognition of our methodâs novelty and its contributions to clinical applications. As stated in the abstract, the in-house FFA database and the source code will be released. Below, we respond to the major comments one by one, whilst the minor ones will be included in the camera-ready paper:"
https://papers.miccai.org/miccai-2024/542-Paper2720.html,"The authors thank the reviewers for their constructive comments on the paper. 
(R1, R4, R5) We acknowledge the reviewersâ concerns regarding the effectiveness of the loss function, multilevel frequency learning block (MFB), and smoke segmentation network (SSN). Due to length constraints in the main paper, detailed evaluation results for these components were not included. However, supportive evidence demonstrating their effectiveness has been presented within this paper. Specifically, the MFB module enhances the detection of mid-to-high frequency information in the smoky images, as evidenced by the frequency domain analysis shown in Figure 1, it shows that MFB improves the learning of mid-to-high frequency information and can help smoke removal and restoration of image details. For loss functions, Appendix 4 provides an impact comparison of varying the loss weights (Î»1, Î»2, Î»3) used in the multi-task learning, these weights, which extend the baseline L_1 loss, have been adjusted to optimize the modelâs performance across various tasks, indicating that the multi-task learning effectively enhance the overall performance. In addition, SSN focuses on the smoke attention and is also associated with the smoke perception loss, its contribution (PSNR/SSIM is increased by 1.5/0.1 as Î»1 is adjusted from 0.01 to 0.8) can be shown in Appendix 4, which validates the effectiveness of SSN in optimizing the model performance. 
(R1) Regarding the dataset issues, Cholec80 is a public dataset and âreal world datasetâ refers to a dataset from our local laboratory, both include laparoscopic surgery images but from different sources and they are used to evaluate the robustness of the model. The synthetic smoke datasets are rendered with the smoke-less laparoscopic images from both the Cholec80 and the local dataset, and the real smoke dataset refers to the smoky laparoscopic images from both the Cholec80 and the local dataset. And we plan to present our dataset during the conference sessions, facilitating in-depth discussions, feedback, and potential collaborations.
(R1) Furthermore, regarding the technical symbol concerns, Î²_t denotes a predefined variance schedule parameter that controls the intensity of the noise addition at each step, using ð¡ could result in less control and potentially unstable noise addition, while exponential decay nature of Î²_t ensures a smoother and more stable transition during the diffusion processes. I_t^sm is the smoke image and is not obtained through diffusion, t is used as an index and corresponds to the paired I_t^sm and I_t^sl."
https://papers.miccai.org/miccai-2024/543-Paper3296.html,"Thank you for your valuable comments on our work. Special thanks to R3 and R5 for accepting the paper directly.

Novelty (R1: âfirst program to achieve an automatic diagnosis of multi-label gastric atrophyâ R5: âa novel technique surpasses SOTA methodsâ);

Sufficient experiments (R3 & R5: âcomparisons with recently published SOTA methodsâ, R5: âjustify the utilization of various componentsâ).
Q&A1: Code and dataset will be released publicly for reproducibility, along with detailed implementation.

Q2: slightly weak experimental rigor, noâ¦confounder, noâ¦ ablation (R1).
A2: 1) Table 1 shows that ours achieves the highest on most average overall metrics (OR, OF1, mAP) and CF1, and competitive scores on other metrics. Highest OR and mAP scores mean that ours achieves lower clinical risk and total average accuracy improvement, demonstrating that it can weaken influence of confounders.
2) In Table 2, adding sample, modulation, and attention in turn bring significant improvements (%): 1.58, 1.26, 1.11 on OR, 0.57, 0.59, 0.51 on OF1. Although slightly performance drop on mAP or CR is observed when removing attention or modulation, using both brings overall improvement in ours.

Q4: Reasoning of structural causal model (SCM) with image features (X) causing CAFs (R5).
A4: Our novel SCM leverages CAF (Z) as true causality from X to labels (Y). Although X is affected by confounder contexts (C), backdoor adjustment with do-calculus cuts off link C-X, hindering influence of C on X pass to Z and ensuring that CAF is insensitive to contextual bias.

Q5: Correlations between prediction of classifier in Eq. (1) and transformer in Fig. 2 (R3).
A5: 1) P(Y|X=x,C=c,Z=z) in Eq. (1) correlates to obtain prediction (Logits in Logit-level Modulation module in Fig. 2) by (classifier in Sample-level Re-weighting module) on (multilevel CAFs) from (Sampling Features). We will label all variables in Fig. 2 on camera-ready version.
2) Notably, parameters of classifiers used for spatial grouping and final prediction are the same.
3) P(Z|X=x)P(C=c) in Eq. (1) denotes progressive casual intervention implemented by sample-level re-weighting and logit-level modulation (right of Fig.2).

Q6: Different from [11] and [R2] (R3).
A6: 1) Our technical advantages include:
-Confouder set building: single feature level in [11], our multilevel confounders are sufficient to represent strong visual interferences.
-Casual Intervention: sample-level attention in [11], our sample-level reweighting and logit-level modulation is effective, as in Table 2.
2) Clinical difference: [R2] only identifies normality but no gastric site.

Q7: Technical justification with [R1] in obtaining CAF using class and patch tokens (PT) (R3).
A7: Different combinations of different features leveraged: [R1] averages intermediate attention weights to multiply PT to refine CAF, we reweight intermediate hidden states in PT path to derive robust CAF that is casual to label.

Q&A8: detailed implementation including:
-Loss function: Binary CrossEntropyLoss with sigmoid (R3).
-Tensor u in logit-level modulation is obtained by the variance of O (R1 & R3).
-u refers to aleatoric uncertainty that captures inherent ambiguity in outputs for a given input (R1).
-$O-p$ is part of O (R3).
-K in Fig. 3 denotes sampling number of patch token paths (R3).
-W/o backbone in 3rd row of Table 2 denotes applying a single class token in transformer (R3).
-ResNet-50+ViT-B_16 is also used in IDA and CCD (R3)."
https://papers.miccai.org/miccai-2024/544-Paper1280.html,"Thanks for your valuable comments. We will proceed with detailed revisions and clarifications.

To Reviewer 1:

Firstly, regarding the TCGA dataset, we employed medical terminologies to describe different subtypes of cases. Regarding the single WSI labels, it is possible that a WSI may correspond to multiple labels, but the likelihood of occurrence could be relatively low, or its proportion within the entire WSI is comparatively low.

Regarding the interaction between gene and pathology image modalities, your suggestion regarding visualizing the impact of genes on WSI is indeed valuable. We propose to illustrate this by visualizing attention maps for both single-modal and multi-modal instances.

About comparison methods, among our eight methods, MCAT, CMTA, PORPOISE, and M3IF are multimodal methods integrating genes and pathology. We will further compare with newer models such as MoCAT, SurvPath, and PIBD.

In Figure 4, we show partial visualizations. For the visualization of cancer region probabilities across the WSI, we have provided Supplementary Files Figure 1 with ground truth annotations from pathology experts.

To Reviewer 3:

In regards to the tumor microenvironment, we have cited several medical articles in the Introduction section. For instance, Reference [8] highlights the importance of lymphocytic infiltration in the tumor microenvironment. Furthermore, Reference [18] discusses the impact of gene expression on the tumor microenvironment and cancer diagnosis.

We appreciate your suggestion and will incorporate additional literature related to the tumor microenvironment in survival analysis.

[1] Wang, Weichen, et al. âThe cuproptosis-related signature associated with the tumor environment and prognosis of patients with glioma.â Frontiers in immunology 13 (2022): 998236.
[2] Brodsky, A.S., Khurana, J., Guo, K.S. et al. Somatic mutations in collagens are associated with a distinct tumor environment and overall survival in gastric cancer. BMC Cancer 22, 139 (2022). 
[3] Tron, Laure, et al. âSocioeconomic environment and disparities in cancer survival for 19 solid tumor sites: An analysis of the French Network of Cancer Registries (FRANCIM) data.â International journal of cancer 144.6 (2019): 1262-1274.

We utilize optimal transport between two tasks and intentionally avoid simultaneous alignment between two modalities, which may blur the impact of subtype classification. Additionally, we believe that if alignment is solely performed between the two modalities, it may result in the loss of information from individual modalities.

To Reviewer 4:

In our study, the subtype classification task provides insights into the tumor microenvironment for survival analysis. We agree that different subtypes could lead to varying levels of risk in survival periods. We do not entirely agree that our method leaks information from the subtype classification to survival analysis. We believe that, compared to other methods, our approach supervises the accurate delineation of cancer regions more effectively to better determine the tumor microenvironment. In other words, the other models could capture the information about different subtypes if they are ideal. In our method, this information serves as prior knowledge guiding the model to focus on the tumor microenvironment. And the âº value is 1.

Regarding your mention of the ablation experiment in Table 2, where patches were randomly selected, it is plausible that these patches may contain a lot of adipose tissue or normal cells, thus ignoring significant tumor-related information and resulting in a severe performance drop. We believe this issue is not directly related to the concerns you raised about subtype classification leaking information relevant to survival analysis.

In response to the external cohort, we are collaborating with hospitals to collect a batch of data to serve as the external cohort. This will help further validate the effectiveness of our experiments in the paper."
https://papers.miccai.org/miccai-2024/545-Paper0334.html,"We appreciate all the comments and recognition of our novelty and superior results. All corrections will be made to the camera-ready.

(Q1) Code release. (R1, R4)
The code will be released upon acceptance.

(Q2) InfoNCE loss (R1, R4)
InfoNCE Loss [R1] is common for contrastive learning. It compares the similarity of samples and encourages the model to identify positive samples among the negatives.
[R1] Representation Learning with Contrastive Predictive Coding.

(Q3) Add STD (R4, R5)
Due to space limits, we reported the mean of 5-fold validation. On average, the STD of all folds are below 0.5%. Detailed STD will be added.

R1
(Q1) Short limitations and future work.
Due to space limits, we briefly mentioned them in the conclusion. The main limitation is the lack of validation on larger multi-center medical datasets, which was planned in our future work.

(Q2) What are w and V represented in Eq.(1)?
They are learnable parameters used to obtain the attention weights for each instance.

(Q3) No statistical analysis.
We provided p-values for the 2 datasets in the lower left corners of Fig. 3 A and B. They are both less than 1e-7, indicating our efficacy.

(Q4) Slight gain with ShaSpec.
Evaluating prognosis using CI and BS metrics is challenging. Compared to ShaSpec, our method shows clear average gains of 0.9% in CI and 0.8% in BS across 4 tasks on 2 datasets.

R4
(Q1) Unclear data preprocessing, e.g., images and keywords to segment reports.
In Sec 2.1, we explain the preprocessing and encoding process for all modalities. Due to space limits, we did not detail the keywords used to segment the reports. They were provided by pathologists through structured text report data, including tumor location, size, grading, etc.

(Q2) Sec 2.4 is hard to understand. How are the losses and the lambda_cen being calculated. 
Survival analysis predicts the probability of patient survival at a series of time points. We divide survival times into intervals and construct a maximum likelihood loss from these discrete labels. A prediction layer is used to regress death hazards and survival probabilities. For uncensored patients, the model can be optimized through accurate labels. For censored patients, we propose estimating hazards beyond the censoring time by predicting risks, assigning hazards, normalizing with softmax, and forming soft labels. A time-dependent Gaussian function is used to weight soft labels in training. lambda_cen is a hyperparameter set empirically to balance the losses.

(Q3) No information for the in-house dataset and no public dataset is used.
In Sec 3.1, we detail the composition and quantity of the 2 datasets. Due to the shortage of public datasets with sufficient cases and complete multi-modality data (images, reports, and clinical notes), we instead experiment on 2 large in-house datasets with 367 and 193 cases.

(Q4) Reporting attention in Fig.3 for a population would be more informative than individuals.
In Fig. 3, we selected 3 typical patients with 4, 3, and 2 complete modalities for illustration. Overall, the weights will be averaged out across the population. It also may not be statistically meaningful to compute the mean when many of the cases are missing certain modalities.

R5
(Q1) Ablation studies on radiology images and reports.
We consider pathology images and reports as essential modalities (all patients have these 2 modalities). But radiology images and reports are often missing, making it infeasible to conduct ablations with only radiology images and reports.

(Q2) Not explicitly consider the uneven and complementary semantic information in various modalities.
Yes, we only learn the weights for the different sources of information (features) via attention mechanism, which relies on the training data and functions implicitly. Thank you for the valuable comment and we will try to integrate the semantic information (maybe as a form of prior knowledge) using more data in future work."
https://papers.miccai.org/miccai-2024/546-Paper1443.html,"We appreciate the reviewersâ thoughtful assessments and valuable insights. They found our work innovative (R3, 5), holding both biological significance and practical value (R3, 4), and well-structured (R3, 4, 5), showcasing convincing advancements (R3, 4, 5). We delve into their critiques and suggestions below.

Evaluation (R3,5): Thank you for your constructive suggestions on deeper comparisons and model generalizability. First, in the ADNI dataset, the clinical tablular data is partially missing by nature, with a missing rate of 30%. In our work, we have compared two kinds of methods for handling missing data, i.e, MIC[10] and mean imputation (MIC+MLP and MLP in Table 1, respectively). Our proposed PMDC outperforms the two comparison methods in all metrics. However, we fully agree that a more systematic evaluation across existing missing data handling methods would be valuable and we will include this in further studies. We also used the ADNI dataset because it is a diverse (multiple sites) well known and widely used dataset for studying AD (enabling benchmarking and reproducibility). We agree that the generalizability should be further studied. However, it is out of scope of this work and we plan to do this using the publicly available AIBL and UK Biobank dataset (work already in progress).

Contributions (R4): We want to clarify our contributions, which are (1) considering heterogeneity of the brain anatomy, a novel image-to-graphical representation module is proposed to construct subject-specific dynamic brain region graphs, as introduced in Section 2.1. (2) For the crucial issue of missing data in clinical records, a novel missing data compensation module is proposed to enhance the modelâs robustness to missing data and expand its applicability. (3) Our work is not the first to use multi-modal fusion, but the first to employ it for MCI progression prediction with missing data handling. The inclusion of image based  graphical feature F_g and the compensated tabular feature F_t (or predicted tabular feature from graphical feature G_T(F_g)) in a multi-modal setup is also novel (as acknowledged by R5).

Clarifying the results in Table3 (R4): We will clarify this in the manuscript. We used two methods for comparison, DGFusion[10] and DGNR[10]. DGNR only takes brain graph data as input, while DGFusion consists of two parts, MIC and DGNR taking clinical tabular data, its corresponding missing vector and brain graph data as input. In Table.3, our method and DGFusion are evaluated under two situations, i.e., partially missing tabular data and completely missing tabular data. For the two situations, each method was only trained once, so DGFusion and DGFusion* shared the same weight. Due to DGFusion only considering partially missing cases, DGFusion works well and achieves a BAcc of 0.8983 under partially missing cases. When clinical data is not available, the MIC module and node attention module proposed in [10] will crash and output random noise features that contaminate the DGNR module. Therefore, DGFusion* just achieves a BAcc of 0.6989, which is significantly lower than the performance of the standalone DGNR shown in Table 2.

Concerning missing rate study (R5): Due to limited space, our method and DGFusion[10] are evaluated only under two missing rate settings, i.e., with missing rates of 33% and 100%. As shown in Table.3, our method presents stronger robustness to missing rate than DGFusion. Based on our in-house study, it is worth noting that our method achieves BAcc above 92% just with slight fluctuations across different rates of missing features. In contrast, the performance of DGFusion drops sharply. These experiments will be part of future work.

Reproducibility (R3,4,5): We will release the code upon acceptance, as we stated in the system during submission.

Thanks again for your comments and suggestions."
https://papers.miccai.org/miccai-2024/547-Paper1153.html,"We thank the reviewers for their constructive comments. We will address all the raised concerns to have the paper accepted.
[Q] Reproducibility (All)
[A] We will publicly release our code and pre-trained model upon acceptance. The ADNI data is publicly available but ROI-wise measures may slightly differ depending on pre-processing pipeline.

[Q] Lack of various analyses (R1,4)
[A] We analyzed the critical influence of specific ROIs at the preclinical stage of Alzheimerâs disease using learned scales and attention scores in our paper. However, due to space limitations, we could not include all of our findings. We plan to extend this paper to a journal version to disseminate more detailed analysis including biological, statistical and clinical relevance.

[Q] Computing resource (R3)
[A]  One NVIDIA RTX A6000 GPU was used for the experiment.
[Q] Why is the highest scale value 1.13? (R3)
[A] The scale determines the magnitude of kernel convolution, and the ROI with the largest scale had 1.13. It means that the ROI required feature aggregation from the largest neighborhood to explain AD.

[Q] How to generate the input graph? (R3)
[A] We ran probabilistic tractography using FSL and FreeSurfer to construct the structural brain networks. 160 regions by Destrieux atlas was defined based on the T1-weighted MR image. Then, we applied surface seed-based probabilistic fiber tractography to the DWI image to generate an anatomical connectivity matrix. For clarity, we will include MRI, DWI and PET as an input in the figure as well.

[Q] Typo, duplicated notation and lack of figure (R3,4,5)
[A] We thank the reviewers for pointing out these issues. We will revise our paper accordingly and polish carefully.

[Q] Limitations of GTAD (R4)
[A] The notable performance and enhanced interpretability of brain networks, particularly in scenarios involving multiple biomarkers, are indeed significant contributions of GTAD. There may be computational limitations, e.g., accurate kernel convolution requires eigendecomposition of Graph Laplacian (O(N^3) where N: number of nodes), but bypassing the diagonalization with polynomial approximation is well-known. Its clinical practicality may be a limit, as obtaining multi-modal imaging measures per subject is challenging in practice. To enhance understanding, we will make a small section discussing these limitations.

[Q] Motivation for GTAD (R4)
[A] In medical analyses, data is often limited due to various reasons. To address this, integrating diverse modality information aims to capture a wide range of features. However, this approach may overlook ROI-wise characteristics in brain analyses. GTAD addresses these issues by learning a transformer-guided diffusion kernel that captures both local and global information simultaneously.

[Q] Why are performance improvements slight? (R5)
[A] Transformer-based methods also achieve competitive performance. However, GTAD showed that, as the number of imaging modalities increases, the difference in performance between GTAD and the second-best method becomes more pronounced. Furthermore, unlike existing transformer-based works, GTAD captures local information by learned scales, enhancing the interpretability of brain networks. This allows GTAD to obtain ROI-wise characteristics across all samples, potentially enabling accurate classification of samples that other models may struggle to classify correctly.

[Q] Why are standard deviations of GTAD similar to baselines? (R5)
[A] The âlow standard deviations (std)â in our paper was meant to highlight the stability of GTAD rather than to claim superiority over other models. As the reviewer probably knows, even if a model yields high accuracy, high std means that the model performance is not robust and may lead to high false-positives. The low std of GTAD, as well as other baselines, means that we thoroughly ran the experiments across different models and implies that the comparative evaluation remains both valid and meaningful."
https://papers.miccai.org/miccai-2024/548-Paper2517.html,"We sincerely thank all reviewers for their valuable comments. We are grateful that our work was recognized as well-written (R1, R4), novel (R1), and providing valuable insights (R3, R4). 
Regarding the major weaknesses that the reviewers identify, we want to clarify all possible misunderstandings as follow:

Availability (R1-Q1)
Since our dataset involves human subjects, we cannot make our dataset publicly available. Instead, we will make our training and evaluation code publicly accessible with detailed documentation. This will enable users to modify and employ our model with their specific datasets.

Why not fine-tune a spatial transformer on domain-specific data? (R1-Q2, R3-Q1, R4-Q2)
As Reviewer 3 recognized, the size of our supervised training dataset is limited for fine-tuning large models. We attempted fine-tuning a spatial transformer, but it resulted in poor performance. Although we have collected large data, the samples with labels for supervised learning are limited. Moreover, the model processes one video which consists of 200-300 frames as a single sample. In other words, the number of videos available for training is approximately 200-300 times smaller than the number of images. Consequently, we opted to use pretrained frame representations rather than fine-tuning the spatial transformer. We acknowledged this limitation in our paper and proposed self-supervised pretraining as a solution for future study.

Lack of confidence intervals (R1-Q3)
End-to-end multimodal learning ROCAUC performance did not fluctuate after training loss convergence. This observation was consistent across different modalities. In contrast, we observed higher performance variation in two-stage approaches. We conjecture this is due to early convergence of two-stage models leading to different solution after optimization. 
Here, we report confidence interval from 10 trials of the two-stage approaches. Note that this is not an additional experiment, but a repetition of the experiment reported in the main paper to add the statistical significance. 
TabTransformer showed (+/-) 0.021~0.045 CI range and TabNet showed (+/-) 0.012~0.025 CI range for embryo-ROCAUC across different modalities. This supports the advantage of the end-to-end multimodal learning approach, which is the main aim of our paper since lower performance variation is preferable. We will add CI with relevant discussion if allowed.

Lack of comparison to [1] (R1-Q4)
Thank you for the comment. We will add the paper [1] to our introduction. It is noteworthy to mention that [1] tackles a similar problem to ours, but the experiment setting is different and, therefore, not directly comparable. While our work aims to combine video modality and EHR data, exploring different modality fusion techniques, [1] uses a single image and EHR data without modality fusion. Moreover, they do not integrate different modalities but rather aggregate the predictions of unimodal models (image model and EHR model) for the final prediction. However, we agree that [1] is relevant to our work, so we will include this work in the introduction.

[1]. Liu, Hang, et al. âDevelopment and evaluation of a live birth prediction model for evaluating human blastocysts from a retrospective study.â Elife 12 (2023): e83662.

Apply data augmentation. (R4-Q1)
We applied rotation and flip augmentation but avoided further augmentations that may change embryo appearance.

Other constructive comments (R3-comments, R4-Q3,Q4)
Thank you for your constructive comments on our direction. As you suggested, we will investigate self-supervised pre-training on larger scale data and techniques for better confidence calibration to improve F1 Scores."
https://papers.miccai.org/miccai-2024/549-Paper1503.html,"First, we sincerely appreciate the insights from all reviewers, who have acknowledged several key strengths of our multimodal learning on Low-Cost cardiac hemodynamics instability detection, incl. technical soundness (R1), data efficiency (R1), clinical usefulness (R3, R4) and comprehensive comparison with a wide range of baselines (R3, R4). More importantly, Both R3 and R4 agree on the novelty and benefits of using non-invasive, low-cost modalities (Chest X-rays and ECGs) instead of high-cost MRIs or invasive catheter for pulmonary artery wedge pressure (PAWP). Such an approach has not been explored before, with its greater practical value to be deployed in low-income countries and medical resource-limited areas. Our research is timely, and also aligns well with MICCAIâs new theme on health equity.

We are also fortunate to receive constructive comments provided by all three reviewers to further improve the clarity and quality of the paper. We have grouped them with our response below.

#Confusion and typos
R1 asks the advantage of the proposed model with unimodal data. We would like to clarify that a key contribution of our paper is its flexibility to support both unimodal and multimodal fine-tuning. Table 2 (Page 7) demonstrates that our method achieves AUROC scores of 0.681, 0.744 for unimodal CXR, ECG downstream tasks, respectively, and outperforms other unimodal non-pretrained and pre-trained baselines. In Table 2, âunimodalâ and âmultimodalâ refer to the downstream dataset, not the pre-trained model. We will revise Table 2 in our camera-ready paper for better understanding.

R3 asks for the description of the latent variable z in the decoder. We apologize for the typo. It should be âlatent space Zâ rather than âlatent variable zâ in the âDecoder Designâ section on page 4. To make it clearer, we will add subscripts under the latent variable z, such as z_CXR for the CXR modality-specific, z_ECG for the ECG modality-specific, and z_(CXR, ECG) for the CXR, ECG shared. These changes will be made in both in-text and equations in the revised camera-ready paper. For the unimodal streams, the decoder uses their respective modality-specific latent variables, but for the multimodal stream, the decoder uses both modality-specific and shared latent variables for reconstruction.

#Recommendations
We will fix the order of references with complete information (R1) in our final version. We also acknowledge the recommendations, such as 1) investigation of semi-supervised learning for better performance (R3); 2) exploration of more advanced multimodal integration methods (R4), and 3) exploration of focal loss for handling complex cases (R4). We will consider these recommendations in our future work.

#Code Availability and Reproducibility
We will publish our code with a link to the source code and pre-trained models in our final camera-ready version, following the guidelines of the reproducibility checklist."
https://papers.miccai.org/miccai-2024/550-Paper2046.html,N/A
https://papers.miccai.org/miccai-2024/551-Paper2758.html,"We appreciate the reviewersâ constructive feedback. We will address all concerns and revise the text accordingly. For reproducibility, our code will be released upon acceptance.

[R3,R4] Novelty justification / Evaluation comparison
Our approach represents a graph as a simplicial complex. It utilizes incidence matrix and Hodge Laplacians for message passing, facilitating explicit learning of node and edge combination. It preserves edge information and captures inter-simplex relationships, unlike other methods, leading to a deeper understanding of topological properties.

[R3] Baseline justification
SVM and GCN are standard conventional methods. CensNet and EGNN are the most commonly referenced and publicly available graph classification for node-and-edge aggregation.

[R3,R5] Lack of references / Syntax errors
We will cite suggested references and correct syntactical errors.

[R3] Rationale / Motivation
NENN [Yang et al., ACML 2020] aggregated information from neighboring nodes and edges, showing improved performance on typical graph tasks. With this rationale, we obtain richer inter-simplex embeddings by mixing L and B.

[R4] Misleading contribution for interpretability
We will adjust the contributions to avoid any potential misunderstanding, stating that our model yields interpretable results âwith Grad-CAMâ.

[R4] No qualitative comparison
Qualitative results from other methods are not included due to page limit. The most influential ROIs differ across different methods as they derive different representations, but several key ROIs, e.g., Putamen, show up in common. Moreover, only ROI-wise Grad-CAMs are available for other baselines, while it can be derived for both ROIs and edges in our method. Last, our result demonstrated symmetric and concentrated ROI/edges (Fig. 3), while the results from other methods show more scattered patterns.

[R4] Position within the entire ADNI classification
There exist various ADNI analyses and they usually cannot be compared one-to-one. Most studies focus on binary classification i.e., AD vs control. Results in [Sheng et al., IEEE JBHI 2022] report accuracy in ~0.90 for binary classification and results in [Kolahkaj et al., Neuroscience Informatics 2023] reports accuracy in ~0.92 for 3-way classification, but our method demonstrates even higher accuracy 0.93 for 5-way classification.

[R4] Number of features and subjects for each experiment
Three types of features (CT, Amyloid and FDG) for four experiments (three individual analyses and a combined one) are used, and each experiment encompassed all subjects possessing the respective features (See supplementary).

[R5] Implementation details
Our model was trained with Adam optimizer, LR 0.001 for 200 epochs on a NVIDIA RTX A6000 GPU. Model performance was evaluated using average acc, macro-precision, -recall, and -F1-score.

[R5] Dual vs. multi-hop aggregation
We will take the result w/o dual aggregation in Appendix Table 3 to the main manuscript (Table 1) and discuss it as the reviewer recommended.

[R5] Class imbalance for SMC?
As seen in Tab 1., high precision, recall and F1-score conclude that class imbalance was not an issue.

[R5] Handling low sample-size?
Indeed, ADNI is the largest dataset in Alzheimer analysis, however, its current sample size of ~2000 is far less than typical graph benchmark datasets such as Tox21 (N=7,831) and Lipophilicity (N=4,200) used in CensNet and EGNN.

[R5] Contributions in the context of related methods
While methods in [Jo et al., NeurIPS 2021] and [Huang et al., IPMI 2023] learn edge embeddings via hypergraph transformation and spectral filtering, respectively, they do not leverage relationships between nodes and edges as we do.

[R5] Computational efficiency / future directions
The computational cost scales quadratically with edge numbers, and addressing this challenge is part of our future work. We will also apply our method to other brain disorders with statistical experiments for journal extensions."
https://papers.miccai.org/miccai-2024/552-Paper1619.html,"We thank the reviewers for the high-quality reviews and will carefully revise our submission, correct typos, add more details and discussions, and release the codes and models in the revision.

Q1. Clinical Values (R1)
We thank for the insightful comments. Alpha matte segmentation offers more detailed and clinically useful information about lesions than binary segmentation. Clinical values of new modules: 1) Detailed information from ambiguous and small regions enhances the accuracy of identifying and characterizing lesions, improving the overall diagnostic process. 2) Clear delineation of ambiguous and small regions reduces the risk of misdiagnosis, ensuring that both benign and malignant areas are correctly identified and treated appropriately.

Q2. Justification of the contributions (R3)
The main contributions of this paper can be summarized as follows: 1) Clinically, our method enhances the accuracy of lesion segmentation by preserving intricate details in ambiguous areas, which is crucial for precise diagnosis and treatment planning. Additionally, the multi-scale feature fusion mechanism ensures that even tiny and arbitrary regions are accurately captured, preserving critical details that might be overlooked by traditional methods. 2) Technically, we introduce a region-aware implicit neural representation that interpolates over larger and more flexible regions, preserving important details missed by conventional locality-based approaches. Our multi-scale feature fusion mechanism integrates features across different scales, enhancing prediction quality and ensuring detailed information capture.

Q3. Motivation of âregion-awareâ contribution. (R3)
Thank you for the feedback here is a clearer explanation. Deformable convolution expands the networkâs receptive field, but coarser scale latent codes (as in IFA and IOSNet) have limitations in capturing fine-grained details, especially in small or arbitrary regions. Region-Aware mechanism addresses this by introducing offset mechanisms on feature maps at each scale, enhancing the capture of detailed information across scales and preserving tiny and intricate features that might otherwise be lost. As shown in the Right in Table 2, this mechanism interpolates over larger areas, capturing more detailed information, which is particularly beneficial for tiny and arbitrary regions. This ensures that even small area features are preserved and supplemented by information from other parts of the image, effectively preserving detailed information across different scales.

Q4. Qualitative Analysis in Fig. 4. (R1&R3&R5)
The IF, LB, FBA methods tend to produce over-segmented or binary mask-like results, missing finer details. Both MM and ours provide a more detailed alpha matte closer to the ground truth (GT). Our method captures the complex texture of the lesion more effectively, maintaining details in ambiguous areas. We will provide additional visual examples highlighting finer details in ambiguous and tiny areas to demonstrate the new clinical values more comprehensively.

Q5. Datasets (R3)
We follow the settings in [23] to use part of two datasets for a fair comparison.

Q6. Baselines (R3)
The medical matting problem is still evolving, meaning related work is limited. We are working on updating our baselines with more recent models in our revision.

Q7. Figures and Captions (R3&R4)
Thank you for your valuable feedback. We will correct the issues in Figures 2 and 3, including the flying arrows and missed key codes, and verify and rectify any mistakes in Equation (4). Subsection titles will be reviewed for consistency. We will revise figure captions to provide more detailed descriptions and ensure all portions of Figure 4 are appropriately cited.

Q8.  Extension (R5)
We agree that further research and evaluation on a wider range of medical images is valuable. We plan to expand the datasets and conduct additional experiments for a more comprehensive evaluation in our revision."
https://papers.miccai.org/miccai-2024/553-Paper3549.html,"We thank the reviewers for their constructive feedback. We address below only the major concerns. Otherwise, we thank the reviewers for detailed feedback on phrasing & clarity (R4) and requested cohort information (R1) which we will include in the paper.

R3âs main criticism was that Mean Imputation yielded only marginal improvements.

R3 questioned the necessity of the work given the relative importance of T2 for lesion segmentation (are multiple sequences necessary?)

R3 and R1 noted the lack of clinical and out-of-domain evaluations, respectively.

R3 questioned the clinical benefit based on the Dice score (median=0.5).

Reviewers highlighted the lack of released code and data, affecting reproducibility.

R1&R3: Lack of visual segmentation results.

R1 asked about the rationale for including contrasts and clarification on âwhich sequences are used whenâ."
https://papers.miccai.org/miccai-2024/554-Paper0621.html,"We thank all the reviewers for valuable comments. We will revise the paper to address grammar and formatting issues in the final version (@R3). In the following, we will address the major concerns one by one.
1.1) @R1: âImage levelâ refers to the analysis of features from individual images, focusing on their spatial and spectral characteristics. This involves understanding the detailed texture, patterns, and spectral information of a single image. âDisease levelâ, on the other hand, involves analyzing the spectral characteristics commonly found in datasets of specific diseases to identify subtle spectral differences unique to those diseases.
1.2) @R1: We resized the images to 256x256 for computational efficiency and feasibility, ensuring a balanced width-to-length ratio for large datasets. This uniform size allows for faster processing and reduced memory usage, which is crucial for training deep learning models. While using high-resolution patches can add complexity and potential loss of context, resizing offers a good balance between efficiency and detail for accurate segmentation.
1.3) @R1: Focus-tuned Learning (FL) incorporates dimensionality reduction but goes further by actively focusing on subtle spectral differences critical for medical hyperspectral imaging segmentation. Unlike standard techniques, FL dynamically adjusts its focus based on the dataâs spectral and spatial features. This adaptive approach captures more relevant features, leading to improved performance with a simple and efficient operation.
4.1) @R4: The proposed SFF module assigns weights to the spectral features of the images, guiding subsequent modules in their spectral focus. Spectral bands with higher weights have a greater impact on the subsequent segmentation process, thereby indirectly influencing the final mask prediction.
4.2) @R4: In Bi-Scale Extractor, intensified and weakened stream is separated by different scales in two adaptive average pooling layers. The feature size from the adaptive average pooling layers represents different levels of spatial granularity. Larger feature sizes (more dimensions) indicate finer spatial details, while smaller feature sizes (fewer dimensions) represent coarser spatial patterns. This approach is feasible as it aligns with the hierarchical nature of spatial feature extraction in convolutional neural networks."
https://papers.miccai.org/miccai-2024/555-Paper1475.html,"We are glad that the novelty (all reviewers), the effectiveness(R#1 and R#5), comprehensive experiments (all reviewers), application and broader impacts(R#1 and R#3) are appreciated. We are pretty excited that R#3 commented our work is ââdefinitely worth publishingââ. In the following, we clarify the reviewersâ concerns and will incorporate improvements in the revision.

Response to Reviewer #1
Q1: Data split of the IU-Xray dataset.
A1: We adopted the same setting used in references [14, 6, 37] to split the IU-Xray for a fair comparison, which has been widely utilized by MRG baselines.
Q2: Experimental setting.
A2: As mentioned in Section 4.1, we reported the average results of five runs with different random seeds.
Q3: Presentation improvements of Tables.
A3: The tables have been updated to include top and bottom borders for improved appearance.
Q4: Code accessibility.
A4: As mentioned in the last sentence of Section 4.1, we will release our code upon publication.

Response to Reviewer #3
Q1 & Q2: Miss baselines on report generation and low performance on MIMIC-CXR.
A1 & A2: Thank you for your insightful references. We will certainly discuss and cite the works youâve highlighted. Specifically, larger models often utilize more advanced checkpoints such as MAIRA-1 fine-tuned Vicuna-7B, while smaller models also leverage additional data based on Large Language Models (LLMs), such as Prompt-MRG and RaDialog. Due to space constraints, we are unable to delve into every baseline in detail. However, our work primarily aims to reduce annotation dependency, making it an unsupervised approach, which is particularly advantageous in clinical settings. In response to your suggestion, we have also compared our method with the omitted references. Despite these methods using relatively advanced checkpoints, our method still demonstrated slightly inferior performance in BLEU-1, METEOR, and ROUGE-L metrics. We will revise all statements regarding performance on the MIMIC-CXR dataset accordingly.
Q3: Macro, micro or example-based F1 scores?
A3: In this work, we utilized the example-based macro F1 score following [6] for a fair comparison, and the results are cited from [6, 15]. We will clarify these details in the revision.
Q4: Suggestions on Presentation.
A4: We have shortened the abstract and adjusted the size of the figures and tables to enhance readability.

Response to Reviewer #5
Q1: Bounding box in Figure 1.
A1: The bounding box in Figure 1 was solely used for illustration. The proposed model doesnât rely on labeled bounding box. For region-level alignment, we continue to use patches to align the tokens.
Q2: Is the choice of the disease prototype a hyperparameter?
A2: Yes, you are right. We will clarify this in the revision.
Q3: Inter-subject interaction in Figure 2.
A3: Your understanding is correct. For simplicity, we depicted only a single input pair in Figure 2. Disease-level alignment projects all input pairs within a mini-batch into a Cross-modal Disease Space for calculating game loss. We will reorganize the right part of Figure 2 accordingly.
Q4: CE loss in Equation 5.
A4: we described the CE(Cross Entropy) loss in Section 2.1 Background of MRG, which maximize pÎ¸(Y|I) by minimizing the negative log-likelihood loss. We will include formal formula in the revision.
Q5: The inference of the model. 
A5: As mentioned in the last paragraph of Section 3, HSA are only active during  training and are removed during inference. Consequently, the inference process is identical to that of using only the backbone model.
Q6: Results on CLIP-like models. 
A6: We carried out supplementary experiments utilizing the MedCLIP backbone. The results for BL-4, MTR, RG-L, and CDr on MedCLIP were 0.174, 0.198, 0.380, and 0.541 respectively. When using HSA with MedCLIP, the results improved to 0.216, 0.235, 0.427, and 0.602 respectively. These findings suggest that HSA can consistently enhance performance, even when paired with more advanced backbones."
https://papers.miccai.org/miccai-2024/556-Paper3289.html,"All 3 reviewers confirmed the novelty of the work and its interest to the medical image analysis community. Our research addresses an unmet clinical need by defining a new set of challenges for computational pathology. The paper proposes two critical advancements:

Prediction of mutational variants. For the first time, we provide a systematic approach for   accurately predicting specific mutational variants from WSIs, going well beyond the traditional binary mutation detection typically reported in the literature. This capability is vital as it directs the selection of treatments based on the specific consequences of gene mutations. For example, conventional approaches might identify a PIK3CA mutation, but our method distinguishes between mutations like p.E545K and p.E542K within the PIK3CA gene, influencing the administration of targeted therapies such as Alpelisib.

Novel morphological signatures. Our findings reveal that each molecular variant exhibits a unique morphological signature within the tumor, which can be utilized to improve the SOTA methods. This novel approach integrates underlying biological processes to enhance the accuracy of deep learning algorithms.

All three reviewers provide supportive and positive statements: R1 â method clarity, novelty, writing quality; R3 â novelty and Experiments were conducted on multiple datasets; R4 â Novelty, clinical feasibility, strong evaluation. This paper represents an initial step towards numerous innovative experiments that introduce a new modality for enhancing performance and improving image-based personalized medicine.

We provide a critical clarification to comments:

R3 - The loss function is not mentioned, besides, no explanation of the structure of three MLPs.

Loss function is described in part 3 Experimental Design and structure of MLPs in part 2.

R4 - Lack of Clinical Integration and Evaluation, sensitivity and specificity analysis missing

The objective of our article is not to market our tool via clinical trials. Instead, our focus is on establishing a new paradigm in digital pathology. The suggested analyses will be comprehensively addressed in future work as done in reference [14] of our paper.

R1 - Need for Ablation Studies & R4 - The proposed MultiVarNet seems to be a combination of three MLPs.

This paper lays the groundwork for further research to enhance our understanding of fine-grained tissue morphology. While exploring various architectures is important, this paper sets a foundational baseline for a new set of challenges. This method is designed to show that each variant mutation within a single gene has a unique signature that can be leveraged. Each MLP is tasked with predicting a specific variant, thereby directing them to identify distinct morphological signatures, rather than a composite of signatures.

R1/R3 - Performance Gains vs. Capacity & R3 - No comparison with other methods in WSI.

To establish feasibility, we demonstrate that even with a less-complex model we can achieve a modest performance improvement. Our results are significant and future efforts will explore more sophisticated architectures to leverage these findings effectively. Another factor contributing to the modest improvement is that we benchmark our results against two SOTA methods ([9, 14]) and focus exclusively on variants. reducing the number of examples and complicating the demonstration of enhanced performance. The analysis of more complex architectures would need to be accompanied with a systematic study of robustness on larger datasets. First, the available data for such a sophisticated task is limited. Second, this analysis would also go well beyond the scope of a conference paper.

R4 - Validation on a Single Data Source.

Our dataset spans a diverse range of cancer types from multiple centers, which is stated as a strong validation by R1 and R3. We also utilized a public dataset, which will be essential for other teams to replicate our results."
https://papers.miccai.org/miccai-2024/557-Paper3730.html,"We thank the reviewers for recognizing our modelâs effectiveness across multiple datasets (R1, R4), our paperâs clarity and organization (R1, R3, R4), and our comprehensive literature review (R1, R4).

The reviewers have also raised some concerns that we will now address explicitly:

MuST on Cholec80 Benchmark (R1): We appreciate R1âs suggestion to add Cholec80, but as required by the program chairs, we cannot include new results during the rebuttal phase. Yet MuST performance is highly competitive on this benchmark, and we will include it in future work.Our paper validates MuSTâs contributions on three datasets with high variability in surgical phase annotations and transitions. In the context of laparoscopic cholecystectomies, we chose HeiChole for its greater challenge compared to Cholec80, which has limited variability in its annotations.

Differences between SKiT & MuST (R1): Unlike MuST, SKiT (citation [17] in our paper) uses a ViT-based backbone that lacks temporal context, extracting features from individual frames. SKiT incorporates temporal information by  building short and long-term feature sequences, processed through transformer layers. In contrast, we sample 4 sub-sequences of a surgical video at increasing sample rates to create a multi-scale pyramid, centering all sequences on the frame of interest (keyframe). This approach captures short, mid, and long-term information. Our video backbone extracts spatio-temporal features from each sub-sequence, followed by cross-attention between the 4 resulting embeddings. Whereas SkiT only has one feature per frame, we extract 4 distinct features. Finally, we merge them into a single, rich context embedding representing each keyframe.

Keyframe concept (R1): Our keyframe concept follows citation [26] in our paper, defining the keyframe as the main frame in an input time window for phase prediction (middle for offline, last for online). We use all frames sampled at 1 fps as keyframes.

Robustness statement (R1): Our robustness claim refers to the adaptability of our multi-scale modeling in recognizing phases of variable durations. We acknowledge R1âs concern about potential ambiguity and will clarify this in the final paper.

Technical Novelty (R1): Our technical contribution is not using a temporal backbone. Instead, our novelty is employing attention mechanisms to combine spatio-temporal features from a single backbone (unlike slowfast) at 4 different frame rates, effectively capturing and relating information across multiple spatio-temporal scales.

Differences between SAHC & MuST (R3): Like  SKiT, SAHC (citation [7] in our paper)  uses a frame-wise backbone to extract spatial features without temporal context, then aggregates them into a feature sequence and applies 1D convolutions to create a temporal feature pyramid. In contrast, our method builds a temporal pyramid by sampling the input sequence at 4 different rates and feeding these into our video backbone, enriching both spatial and temporal contexts. We appreciate R3âs comments and will include this comparison in the final paper.

Temporal Consistency Module (R3): Our temporal consistency module enables consistency by capturing an even wider temporal context compared to our temporal pyramid. This enables finer and more consistent segment predictions.

Experimental Results (R3): As we stated in our paper, we compare ourselves with the recent state-of-the-art methods that have publicly available code, allowing for training and evaluation on other datasets.  Despite the existence of recent works like LoViT and SKiT, they do not have the necessary public code for comprehensive benchmarking.

Standard deviation (R4): We evaluate both GraSP and MISAW on their test set so there is no standard deviation to report. However, for ablations, we perform cross-validation and include the standard deviations.

Computational complexity (R4): Our model has 64.9M parameters and 303.16G FLOPs. We will include this in the final paper."
https://papers.miccai.org/miccai-2024/558-Paper3611.html,"We thank all the reviewers for their supportive feedback, insightful, and valuable comments. We are thrilled that the reviewers find our proposed methodology novel (R4, R7) and easy to understand (R6). Moreover, we are grateful for the acknowledgment of our rigorous (R7) and extensive (R4) performance comparison in a multi-center study and the methodological innovation of partial diffusion (R7).
Impact of Heart Segmentation (R4, R7): We acknowledge the importance of accurate heart segmentation, but our model performance does not entirely depend on it.  Segmenting the heart in LGE imaging is easier than scar segmentation, and commercial software exists for this task. Mitigation strategies, like dilation, can slightly enlarge the ROI at a low computational cost to ensure heart inclusion. Clinicians can use manual landmark or âroughâ segmentation as a starting point and easily verify and correct the segmentation if necessary. We will address this point in the discussion section.
Clinical Impact (R4): Our model could substantially improve LGE image interpretation and quantification. Despite advances in LGE sequences, ambiguity remains in 20-25% of cases regarding the presence, absence, and extent of scars, especially in patients with nonischemic cardiomyopathy. Contrast enhancement could guide readers to high-probability scar areas, enhancing CMR/LGEâs global reach by simplifying image interpretation in centers without CMR experience. Our method also contribute to the standardization and automation of scar burden in LGE- an unmet clinical need. For example, in HCM patients, we use a 15% threshold of scar burden to assess arrhythmia risk, yet scar quantification remains a well-known challenge. We will add insights into the clinical utility of enhanced images and their impact on image interpretation and patient management in the introduction and discussion. 
Implementation Complexity (R4): Our model can be easily integrated into clinical workflow, either on the MRI system as part of reconstruction/image enhancement or on image PACS for a vendor-neutral approach. We have extensive experience in this area and plan to integrate the model into our scanner upon completion of our technical development. Similar methods have been commercialized by Subtle Medical for neuro MRI/nuclear imaging. The model was trained on a single GPU and does not require extensive computational resources. We are making the code and the model publicly available for easy integration by other teams. As suggested, we will discuss the potential of fully automated segmentation methods and the frameworkâs utility in long-term clinical studies to evaluate the impact of the framework on patient outcomes.
Scar Quantification (R6): We acknowledge that we did not fully evaluate the model for scar quantification, as this paper is not about scar segmentation- though scar segmentation is a byproduct of improving contrast enhancement. We believe there may have been a misunderstanding of the framework. Scar segmentation was used solely to generate ground truth and apply gamma correction for the training set, performed manually. The modelâs input is the heart, so no scar segmentation is required. We will further clarify this important issue in our manuscript.
Inconsistency between training and Inference (R6): We follow the typical process for training image-to-image diffusion models, where the denoising UNet is trained using target data to denoise images in the target domain. The model is conditioned on acquired LGE images during training and inference, enhancing its ability to map images from the original to the enhanced domain. The framework was evaluated using a large multicenter, multivendor dataset, as acknowledged by R4, with results reported on unseen cases.
Statistical Analysis (R7): We will include additional statistical tests and adjust the manuscript accordingly. We thank the reviewer for pointing out this important shortcoming of our manuscript."
https://papers.miccai.org/miccai-2024/559-Paper3422.html,"Dear Area Chair, Dear Reviewers,
We would like to extend our sincere thanks to the three reviewers for their valuable feedback and for highlighting the strengths of our work. We appreciate their time, effort and the constructive comments which will help us improve our manuscript. 
We are glad that all reviewers highlighted several strengths including the novelty of utilizing Neural Cellular Automata (NCA) for image classification (R1, R3, R4), its advantages in terms of the inherent explainability (R3, R4), and the simplicity and lightweight nature of our approach (R3). The recognition of our validation efforts across three datasets including our own (R1, R3, R4) and the analysis of parameters (R4) are encouraging, and we are pleased to receive positive comments regarding the clarity and organization of our paper (R1, R4), in particular their emphasis on the detailed explanation of our methodology (R4). 
We also appreciate the feedback from Reviewer 3 and are fully committed to addressing the suggestions to enhance clarity and transparency in our manuscript. We believe that with adjustments to the text and graphics, such as directly linking the NCA architecture description with the corresponding figure 1, we can significantly improve the clarity of our methods. Furthermore, we will reference part B of figure 1 directly in the section after equations 2 to 5 and include symbols used in the equations within the graphic, such as N_c and the perception vector f_p(N_c). Additionally, we will clarify in figure 1âs caption that section B corresponds to equations 2 to 5. Moreover, in the same figure, we will easily address how parameters are distributed between the classifier and the NCA, as well as within the NCA components (R3). 
Regarding the question about the missing values in table 1 (R3), we would like to clarify that both columns of âMatek et alâ and âResNeXtâ are referring to the same architecture and same setup. In the column titled âMatek et al.â, we included the evaluation results reported by Matek et al., while in the column titled âResNeXtâ we trained the same approach on the remaining two datasets to provide a complete report of the performance of the mentioned model on all three datasets. Following the Reviewerâs remark, we will merge the two columns in the revised version of our manuscript, denote it âResNeXtâ, and describe the evaluation results accordingly in the caption, to avoid confusion for the reader. 
We are grateful for the suggestions regarding additional literature (R3) and will reference them while specifying the distinctions to our method in the Introduction and Discussion sections of our revised manuscript. 
With that, we are confident to appropriately address Reviewer 3âs concern about the âclarity in the method descriptionâ.
Lastly, we wish to address the comment about the perceived limited novelty of our method due to the lack of advancements to the NCA backbone (R4). Our focus has been on demonstrating the potential of NCA for image classification, which has not been explored yet and is thus methodologically novel, while being âuseful for medical image analysisâ (R3). With our work, we were able to showcase the advantages of NCA, such as extreme lightweight storage for high accessibility, and generalizability to unseen data, and regard this as an innovative starting point for further exploration and refinement of NCA-based image classification within the research community. We thus believe that we are well within the scope of MICCAI 2024, which explicitly lists âNew methods in medical image computingâ, âAccessible medical imaging solutionsâ, and âGeneralizable machine learning in medical imagingâ as topics of interest.
Thank you once again for your thorough review and insightful feedback."
https://papers.miccai.org/miccai-2024/560-Paper3550.html,"We thank the reviewers for their valuable comments.

(R3, R5) Model architecture and reproducibility: 
The codes and architecture details will be publicly released upon acceptance of the paper. We will also revisit the paper for missing information about the architecture and hyper-parameters.

(R1, R5) Dice score improvement:
Improving brain map reconstruction is beyond this paperâs scope, and the focus is to improve text/brain association via contrastive loss. We checked a posteriori that we maintain brain map reconstruction scores comparable to the baselines. Also, NeuroContext benefits from large descriptive texts, but IBC contrast definitions are typically only 100 characters, making performance differences with Text2Brain less evident. This contributes to having equivalent dice scores.

(R3, R5) Semantics and information loss in averaging:
Our experiments showed that averaging chunk embeddings consistently provides better results. We tested other methods like selecting the top p\% quantile of correlated chunks, and spline quantization with MLP aggregation, yet averaging performed better. Fig.2 shows full body text improves performance compared to titles, abstracts, or random text parts, despite potential semantic changes and information loss from averaging. To handle long texts, the best approach so far is thus to average chunk embeddings. We have detailed this in Section 2.1.

(R5) Technical novelty:
While our approach builds upon existing methods, the innovation lies in integrating and adapting these techniques for automated literature analysis (in our case, brain meta-analysis). By combining advanced language models with a contrastive learning framework, we improve the poor association of text and brain maps in existing meta-analysis methods. Also, we addressed the challenge of processing diverse textual input lengths using an efficient chunk averaging approach. We also showed the importance of leveraging full body text in training our model. Our experiments show that NeuroConText improves association scores by up to threefold compared to existing baselines, marking a significant advancement in brain meta-analysis. It offers a more effective tool for the neuroscience community than current meta-analytic methods.

(R1) DiFuMo dictionary sizes and alternatives: 
Choosing DiFuMo size is based on the reference paper [4]. While alternatives to DiFuMo exist, comparing them is beyond this paperâs scope. We chose DiFuMo for its well-established nature, pretraining on a large dataset (2.4TB), and the detailed, continuous atlases it provides for brain pattern extraction, with clear reference points [4]. DiFuMo atlases range from 64 to 1024 networks. Larger dictionaries provide more detailed brain activation structures, which justifies why 512 dictionary size outperformed 256 by 3\% (Table 2). However, 1024 components led to overfitting due to our small dataset. This trade-off between detail and overfitting explains why 512 performed best.

(R1) Baseline model comparisons (other CLIP strategies):
In this study, we focused on showing the validity and strength of our approach using the original CLIP model. While we acknowledge the importance of comparing with other CLIP strategies, our current scope is focused on this proof of concept. Future research should indeed perform systematic comparisons.

(R5) Text2Brain differences:
Differences between NeuroConText and Text2Brain are addressed in various sections of the paper, such as random text selection vs. long text processing, SciBERT vs. Mistral-7B, high-dimensional brain maps vs. DiFuMo coefficients, and regression via 3D CNN vs. introducing shared latent space via text/image encoders and contrastive loss. We will make the presentation of these points more consist.

(R3) Results in Tables 1 and 2:
They are based on the final NeuroConText model built for the full body text.

(R1) Color bar in Fig. 3:
It is the statistical value for IBC dataset. We will include it in the caption."
https://papers.miccai.org/miccai-2024/561-Paper0811.html,"To all
We are delighted that all three reviewers have recognized the innovation of our work as it mitigates the issue of segmentation discontinuity. Before addressing specific questions, we would like to clarify the following points:
1) For ease of algorithm replication, the manuscript already includes a detailed description. Upon acceptance of the paper, we promise to make the model publicly available. The Zebrafish dataset comes from another concurrent work under review. We will release it following the publication of that work.
2) Our experiment and result evaluations adhere to the standards set by SOTA neuron segmentation works [27,22,23] on IEEE TMI.
3) We have conducted experiments and comparisons on two datasets for the three innovative aspects of our paper. Due to space constraints and the lack of open-sourced comparative work, we did not conduct comparisons with some less important methods as pointed out by R3.

To R1
We are grateful for your recommendation on our work and the insightful comments âclear neuron-morphology-oriented frameworkâ and âthoughtful and well-chosen biases for neuron morphologyâ. We also recognize the importance of further discussion and the quest for improvement methods. Here, we discuss two scenarios observed in our experiments where accuracy was lower and the potential solutions:

To R3
We would like to express our gratitude for your comments and effort dedicated to reviewing our manuscript.

Q1: Method
Essential for DSC: Generating neuron tracing results with App2[24] can ignore minor segmentation disconnections. The DSC increases short-range continuity, which benefit algorithms not permitting discontinuities. It also enhances PRE, SD and SSD, indicating more precise centerline tracing.
Design of WLC: Ignoring the voxels is to reduce the impact of potential inaccurate radius when generating segmentation GT from centerline annotations. If there are gaps, the dilation following the multiplication will not cause interconnection.

Q2: Data augmentation
It diversifies the inadequate samples. The erosion on grayscale images does not affect weak signal. The radius of neurons is relatively uniform (e.g., Fig.3).

Q3: experiment
1.2 [22,27] have already demonstrated the effectiveness of each similar task separately.
1.4 The images are same as [22] for comparison. The strategy you referenced can result in overfitting for the scarcity of images within each subset. The division was completely random.
1.5 Table 2 presents comparison of neuron segmentation. But SD, SSD and MES are based on tracing outcomes (skeleton points). 
1.6 Table 3 conducts ablation study. Quantitative comparison has already been presented in Table 2. Other methods struggle to trace weak signal areas as SGSNet.
1.7,2.3 Table 1 shows ablation studies on Gold166. Zebrafish dataset containing more weak signals, better demonstrates the effectiveness of our design. Most of these methods are not open-sourced, and only results on Gold166 have been reported.

To R4
Thank you very much for your recommendation. The efficiency concerns you have raised will be beneficial in improving the final version."
https://papers.miccai.org/miccai-2024/562-Paper2847.html,"We thank all reviewers for their valuable time and feedback.

Our study makes an alarming and novel discovery: Most 3D segmentation methods introduced in recent years fail to surpass a simple 6-year old U-Net baseline. This suggests a deeply flawed state of the research field, where supposed innovation/novelty is valued more than rigorous validation that would ensure genuine methodological progress.

We believe our contributions align well with the MICCAI guidelines, which explicitly support ânew insights into existing methodsâ, and encourage accepting âa paper [with] a good contribution if you think that others in the community would want to know itâ. If the alarming discovery described above does not qualify as important âinsights into existing methodsâ that âothers in the community would want to knowâ, then what would?

Response to Reviewer 4

R4 recommends rejecting our work because it âdid not propose an original methodâ and instead suggests to âPropose a novel CNN-based U-Net methodâ. There is a certain irony in these comments, as they provide real-time evidence for the novelty bias discussed in our work. It is this exact bias, which has led the field of 3D medical segmentation into a state where the latest methods do not surpass a simple 6-year-old baseline. Our workâs message is that overcoming this state requires a cultural shift in the community and a re-definition of the term ânoveltyâ, such as excellently argued in Michael Blackâs âguide to reviewersâ [1]. Furthermore, the MICCAI reviewing guidelines state: âPlease remember that a novel algorithm is only one of many ways to contributeâ.

Response to Reviewer 1

R1 recommends rejecting the paper, because âthe message of the paper is not newâ and âmany of the pitfalls are well-known.â We kindly ask R1 to reconsider their recommendation based on the following:

While we agree that our study does not invent or discover a novel type of validation pitfall, it is dangerous to assume that the discussed pitfalls are common knowledge and do not need to be explicitly studied. Our study providesempiricalevidence that these pitfalls are not universally acknowledged and demonstrate how they hinder the fieldâs progress. Although the pitfalls may be generally known, method validation is severely neglected in current practice.

R1 also assumes that âthe provided benchmark will not solve the issuesâ. While no study can guarantee a change in common practice, we argue that our work is a much needed first step towards better validation practices. We are the first to systematically describe current pitfalls andempiricallyshow their severity, highlighting the crucial need for action. Besides the public benchmark, our work further provides two concrete tools facilitating meaningful validation in the future: 1) a novel set of standardized state-of-the-art segmentation baselines, and 2) a strategy for measuring the suitability of datasets for method benchmarking.

General Concerns

A similar situation occurred when the nnU-Net paper revealed that many presumed architectural advancements at the time did not surpass a simple U-Net baseline. As publicly discussed, the nnU-Net paper was rejected at MICCAI back then due to a perceived âlack of noveltyâ and later became a milestone for biomedical segmentation. We are genuinely concerned that MICCAI will repeat this history and reject other critical messages regarding a flawed state of research due to a perceived lack of novelty.

Thank you for considering our responses. We hope they clarify the importance of our findings and the need for a cultural shift in the community.

[1] https://medium.com/@black_51980/novelty-in-science-8f1fd1a0a143"
https://papers.miccai.org/miccai-2024/563-Paper1899.html,N/A
https://papers.miccai.org/miccai-2024/564-Paper1511.html,"General Response:
The proposed method aims to apply an MRI diffusion model trained on one dataset for reconstructing images from a different dataset with different noise levels. Such discrepancies can disrupt the pre-trained reverse process at time steps corresponding to noise levels lower than those in the k-space data. To address this, we introduce a simple yet effective attenuation function. We have very extensively evaluated this method.

Concern #1 (Eq. 7 Correctness and Approximation Errors) by Reviewer #1. 
Response:
We thank Reviewer #1 for providing relevant references [1,2], but we disagree with the assertion that Eq. 7 is âtechnically incorrectâ.

The KEY question is whether our Eq. 1 is valid in the context of MRI reconstruction? We firmly believe it is. Firstly, this assumption is widely used in MRI research [3,4]. Secondly, our extensive experiments demonstrate that this assumption leads to robust reconstructions. Thirdly, we have internally tested Songâs approach [2], which did not yield advantages.

Concern #2 (Attenuation Function Design and Hyperparameters) by all reviewers.
Response:
The current design of the attenuation function is simple and effective for our purposes. Some justifications can be found in Fig. 2 of the supplementary material, where noise propagating from the MRI data does not significantly impact the total noise level in the early stages Therefore, we did not scale the data consistency term from the beginning. We acknowledge that the attenuation function can be further optimized, and exploring different shapes for the attenuation function is beyond the scope of this study.

We apologize for any confusion regarding the hyperparameters. Our method includes only two hyperparameters: 1) sigma_y and 2) gamma. Parameters k and b can be determined by \sigma_y and \gamma. For \gamma, we fixed its value at 0.2. We will clarify these issues

Responses to other minor concerns:
The sigma values in Table 2 represent the added noise. We added noise to fully sampled data and then downsampled it to obtain the measurement y as mentioned in Fig.5 caption. For sigma_y, calibration scans are not always available, but it can be estimated in other practical ways, as mentioned in the paper. We will cite all suggested studies.

References:
[1]Chung et al. Diffusion Posterior Sampling for General Noisy Inverse Problems. ICLR, 2022
[2]Song et al. Solving Inverse Problems in Medical Imaging with Score-Based Generative Models. ICLR, 2021
[3]Jalal et al. Robust Compressed Sensing MRI with Deep Generative Priors. NeurIPS 2021
[4]Luo et al. Bayesian MRI Reconstruction with Joint Uncertainty Estimation Using Diffusion Models. MRM, 2023"
https://papers.miccai.org/miccai-2024/565-Paper1262.html,"We appreciate the insightful feedback and suggestions. In response, we have thoroughly considered each comment and carefully respond as follows."
https://papers.miccai.org/miccai-2024/566-Paper0936.html,"We greatly appreciate the reviewers for the effort and insightful comments regarding our submission. We will further correct errors and clarify all the concerns of the reviewers in the final version. And we will evaluate our method on extensive datasets in the following research.

Reviewer#1

Reviewer#3

Reviewer#4"
https://papers.miccai.org/miccai-2024/567-Paper2146.html,"We thank the reviewers for their thoughtful feedback. Our research explores the constraints of diffusion models in medical image denoising, revealing degradation in denoising quality and the emergence of hallucinated image features. We are encouraged by R1âs positive remarks, noting our manuscript as âtimely and well-written,â and their recognition of the communityâs need for studies like ours. We also appreciate R4âs acknowledgment of our thorough evaluation, which tested various noise schedules, sampling regimes, and network outputs, ensuring that the diffusion model was given a âfair chance to perform well.â

Focus on DDM2 (R1)
R1 claims that a non-standard diffusion training framework (DDM2) was used in our experiments, which makes our conclusions misleading. We would like to clarify that we exclusively used the diffusion model network architecture provided by the DDM2 framework, without employing its corresponding three-stage self-supervised training scheme. In contrast, we utilized both simulated data and previously denoised real MR images to enable training with the standard DDPM/DDIM training scheme. This highlights that the diffusion models in our experiments fail despite being trained and tested under ideal conditions. We apologize for any confusion and will clarify this point in the final version of our paper.

Concern of limited data (R1, R3, R4)
R1, R3, and R4 raised concerns about the limited sample size in our experiments. It is crucial for us to use noise-free data to eliminate any influence of noise-corrupted ground truth or other confounding factors. Unfortunately, there is a lack of large noise-free medical imaging datasets. While we replicated our experiments on the ImageNet dataset and observed similar outcomes, we opted not to include these results due to their lack of relevance for medical applications. Our work offers a practical proof of concept, reflecting typical dataset sizes in real-world medical applications, as emphasized by R4. We appreciate R4âs suggestion to employ a pre-trained model and aim to expand our analysis to more relevant datasets and advanced training schemes in future studies.

Noise simulation (R3)
Reviewer 3 requested clarification on the noise simulation in our simulated MRI. In our experiments, we opted for Gaussian noise for two primary reasons: Firstly, the noise in (complex-valued) MRI can be accurately modeled as a Gaussian distribution, reflecting a realistic setting. Secondly, Gaussian noise aligns well with the standard diffusion sampling scheme, representing an ideal setting for the method to perform effectively. Using different noise distributions would necessitate altering the diffusion model training scheme. For our initial proof of concept, we maintained consistency with Gaussian noise.

Model prediction after first iteration (R1)
R1 inquired about the âmodel prediction after the first iteration.â This refers to the initial model prediction for x_0 after the first sampling iteration during inference. Typically, diffusion models perform N sampling steps during inference to arrive at the final model prediction. We will clarify this in the manuscript.

Investigation of generalization error (R1)
R1 raises the important point of investigating generalization error. We have already addressed this in our experiments by closely monitoring the validation loss and implementing a corresponding stopping criterion during training to prevent overfitting.

Synthetic data details (R4)
Reviewer 4 highlighted the need for clarification regarding the number of samples used for training, validation, and testing. We will include this information in the final version of the paper: we generated a total of 720 slices for training and 240 each for validation and testing. For Figure 2 we used a random subset of 50 slices from the test set.

We appreciate the reviewersâ additional suggestions and will incorporate them into the final version of the manuscript."
https://papers.miccai.org/miccai-2024/568-Paper2245.html,"We thank the reviewers for their valuable feedback. We appreciate the recognition of: the methodological novelty; the large dataset to be released; and extensive experiments.

To R3:

To R1: We appreciate the positive comments and valuable suggestions. We will further clarify that the novelty of the method is the tested non-rigid deformation, rather than the practical interpolation method.

To R4: References [8, 10] reported that training with rigid transformation is more sensitive to weight initialisation and learning rate, than training with higher-DoF deformation. This is consistent with what is observed in our application. We will further rephrase the Introduction section. We strongly agree with the reviewer that there is potential for improving the reconstruction performance. We thank the reviewer for the positive comments and constructive suggestions. We will further clarify: 1) Calibration matrix is included in our reconstruction pipeline. 2) The interpolation process has an average speed of less than 1 ms over the dataset, with time complexity of O(N). Further technical details can also be found in the open-source repository."
https://papers.miccai.org/miccai-2024/569-Paper2571.html,"To Rev. #1
We treat the PD-L1 prediction in HE stained WSI slides as a slide-level classification problem, which predicts a label for a whole slide not for small image patches. In clinical screening, a pathologist predicts PD-L1 on an IHC stained slide by identifying PD-L1 positive subregions and then counting the ratio of positive subregions to total ones. Implementing a pathologist-like prediction requires to annotate positive subregions on HE slides to train a patch-level classifier. However, this is hard due to the difficulty of visually detecting PD-L1 positive subregions on HE slides and the huge spatial resolution. This makes us give up such a method, and apply a MIL based framework to directly predict slide-level labels. Although MIL methods require to split a WSI to small patches (instances), no patch-level annotation is necessary. However, without patch-level labels, the training becomes difficult only by using slide-level labels. Thus, we design a new method that exploits novelty detection to select typical patches to guide the training.

To Rev. #4
Using different pretrained models for the two datasets is due to the following reasons. Since the PD-L1 prediction is more difficult than cancer identification, the former task requires a more powerful pretrained model. Thus, for the PD-L1 prediction we select the CTransPath model, which is trained by a carefully-designed self-supervised method on a large pathology dataset. For the Camelyon16 dataset (cancer identification), we follow the other existing MIL methods to use the ResNet50 as a pretrained model for a fair comparison. Note that our method can improve the performance on both datasets no matter what pre-trained models are used. Due to the limited space and schedule, we did not include these results in the paper.

To Rev. #5
1) Our paper is from a perspective of methodology innovation, NOT to mention that we are the first to perform the PD-L1 prediction on HE stained pathological images. Since the medical purpose could be relative new for some readers that have an engineering background, we briefly introduce the medical background. Due to the engineering background of ourselves and not native English speakers, some description for the introduction could be not suitable or precise. We are sorry about this and will revise it. However, we hope this paper could be judged from the methodology innovation.
2) We are dedicated to inventing a new MIL method to predict PD-L1 of non-small cell lung cancer (NSCLC) on WSIs. Previous works cited in our paper are not only considered from the factor of the PD-L1 prediction on HE images, but also from other factors, such as targeted diseases, HE image types and methods. The paper [Gil Shamai et al, Nat Com 22] is out of our citing range due to the following reasons. Their targeted disease is breast cancer, not NSCLC that exhibits more diverse tissue morphology. They use HE tissue microarray (TMA) images (10^3 pixels), not the WSI that has a much larger field of view and is extremely huge in spatial resolution (10^5 pixels). Due to the less diverse disease and much smaller images, they can directly train a ResNet well. Since WSI is more preferred for clinical diagnosis, we use WSI. However, our task is much harder, due to the huge data and diverse NSCLC. We tried a similar method as them, but got a AUC below 0.5. Since this paper did not inspire us much, we did not cite it and report the result. 
3) We find a SOTA work (https://www.nature.com/articles/s41467-024-46764-0) that is published in April 2024 and highly related to ours. They invent a different MIL method for pan-cancer PD-L1 prediction on HE WSIs. They use a larger dataset, test two subtypes of NSCLC (LUAD and LUSC) and report results of two models trained on fresh-frozen & FFPE WSI. Their FFPE model obtains a AUC of 0.71 for LUAD and 0.61 for LUSC (Table S5). We use FFPE WSIs, mix LUAD and LUSC, and report a AUC of 0.724, indicating that our method work properly."
https://papers.miccai.org/miccai-2024/570-Paper1615.html,"We thank the reviewers for their constructive comments and generous scores.

[Q] Reproducibility (R1, R3, R4, R5)
[A] The code will be released, which can be used for any multi-modal neuroimaging analysis with ordinal labels. ADNI data is publicly available but ROI-wise measures may slightly vary depending on pre-processing pipeline.
[Q] Broad title (R1)
[A] Although OCL is a general method applicable to various datasets, we will mention in the title that this paper is for Alzheimer analysis.
[Q] Out-of-date baselines (R1)
[A] The baselines in the current version may be traditional but still serve as benchmark references in most imputation studies. We have found HyperImpute [1] as a recent literature, and we anticipate that it would yield results similar to existing baselines we used as demonstrated in [1].
[Q] Decoupling modality-specific and -independent contents (R3)
[A] While decoupling modality-specific and -independent contents is a feasible idea, the modality-specific contents become redundant in our model as it does not require modality-specific information when generating a target modality from an existing one.
[Q] Potential impact of confounders (R3)
[A] This is a good question. As in other deep models, our model does not directly deal with confounders assuming the disease-specific effect is the most dominant. In the embedding stage, as we discard modality-specific contents only, we expect the confounding effects, e.g., age and sex, to be naturally incorporated in the generation stage.
[Q] Figure clarification (R3)
[A] We consistently use shapes to represent modalities and colors to represent labels in every figure. However, we will clarify this in Figure 1. In Figure 3, while the overlap may not be apparent with numerous (a) training samples, the overlap is more evident with sparse samples in (b) test data.
[Q] Training mechanism (R3)
[A] If different modalities share similar embeddings, the target modality with a similar context can be generated by changing the condition of modality during inference. This information is given in section 2.2.
[Q] Further analyses (R4)
[A] Due to page limit, only preliminary results are presented. We hope to present extended analysis, including group-wise distances from each method per the reviewerâs suggestion, in a journal version in the near future.
[Q] Weighted combination of loss functions (R5)
[A] We experimented with different combinations of losses, but the results were only marginally better than our current unweighted combination.
[Q] Comparison to SCOL [2] (R5)
[A] While SCOL suggests a similar concept, our method is designed in a totally different and more sophisticated way: (1) The mere addition of distance metrics to similarity scores in SCOL may result in the shrinkage or dispersion of the embedding space over prolonged training periods. Conversely, we first assign different temperatures for each negative sample for gradient calculation rather than simple addition. Then, we assign the temperature of positive samples to preserve the embedding space deterministically by gradient analysis as detailed in the supplementary material. (2) While SCOL adopts L2 distance for the label distance metric, we choose L1 distance. In scenarios with numerous labels, adding the distance within the exponential function could introduce errors or lead to overfitting.
[Q] Why âno imputationâ baseline? (R5)
We need the baseline to validate if the imputation helps a downstream task, i.e., no imputation vs. imputation.
[Q] Why more AMY and FDG than CT? (R5)
[A] We have processed a subset of ADNI data to study the effect of PET measures, which resulted in less number of MRI samples. The processing is still ongoing.

[1] Jarrett, et al., âHyperimpute: Generalized iterative imputation with automatic model selection.â, ICML, 2022.
[2] Saleem, et al., âSCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scansâ, MICCAI, 2023."
https://papers.miccai.org/miccai-2024/571-Paper3700.html,"We thank the reviewers for their insightful comments and constructive feedback. We are delighted they recognized the novelty of our work and its value, as sonification is an intriguing yet under-studied topic (R3). They commended our approach for providing the ERM structure from a new perspective through the acoustic model, aiding doctors in qualitative judgments and noted the sounds representing the gaps were easily discernible by listening to the supp. material. (R4). We are grateful for their suggestions and will next address their comments by outlining the changes weâll make based on their feedback.
/Regarding the comments on the extent of the user study, particularly with only one medical expert (R1, R3) and comparison with baseline conditions (R3): 
Sonification is an emerging modality with multifaceted nature to assess. We agree that having surgeonâs feedback is crucial to ensure the systemâs relevancy. Our system has been developed in close collaboration with clinical experts. However, since most surgeons are not yet trained or familiar with concepts of sonification, validating the psychoacoustics and intuitiveness aspects of such methods on non-expert users, while keeping an expert in the study design loop, is extremely important for user-centered solutions. To verify the concept and its face validity for community exposure, in the absence of any prior work for comparison, the current studies were essential, before taking the valuable time of multiple clinical experts and planning extensive clinical experiments. We find it beneficial to share the results of our study with 15 participants, which confirmed primary factors with a high percentage (98.5%) after a brief training session. The expertâs confirmation of the taskâs suitability further underscores the importance of publishing these findings. This would aid in conducting further experiments, aligning with the scope of a CAI-based conference paper introducing a Novel MIC approach to addressing an unmet CAI need. 
/Regarding the question on the reproducibility (R3, R4), we will include the following text at the end of Section 3.3 to further clarify the translation of visuals into audio:
âThe transfer functions fm, fc, and fk map image intensity (I) within the normalized range of [0.001, 1] to model param m (kg), c (N.s/m), and k (N/m). These values are empirically determined to optimize the trade-off between high sound contrast and model stability: fm(I -> [5, 10]), fc(I -> [0.001, 0.01]), and fk(I -> [1, 5]).â
/We appreciate the question about the design motivation (R3) and will paraphrase the last paragraph of Related Work accordingly:
âResearch shows the human brain is optimized for processing behaviorally relevant natural sounds [1]. Accurate representation of objectsâ acoustics requires considering both spectral (frequency mapping, distinct from pitch) and the temporal features. Further, as data dimensionality increases, the freq. mapping becomes more challenging, requiring case-specific design. Physical modeling efficiently handles freq. mapping, incorporating the spatial characteristics of the imaging data, and optimizes temporal mapping with minimal parameters: mass, stiffness, and damping. These parameters affect the modelâs resonant properties, determining sound factors such as attack and decay time and sustain level across the freq. Spectrum.â
/We agree with comments about highlighting the RoI (elevated ERM), differing ERM and other pathological/anatomical structures, explicitly referencing existing methods (R3), and enhancing the acoustic model visualization for better structure demonstration (R4). We will address these by minor updates of the figures and video.
We appreciate the reviewersâ recognition of our approachâs novelty and the positive feedback on the paperâs clarity and structure. We hope that our rebuttal addresses their questions satisfactorily."
https://papers.miccai.org/miccai-2024/572-Paper1977.html,"We thank the reviewers for their detailed feedback and constructive comments on our manuscript. Below, we address the major concerns raised by the reviewers.

Reviewer #1

Major Critique: Figure 3 caption and Lack of Clarity

Response: We apologize for the oversight regarding subplots c and d on page 7. This was a typographical error (it should be Fig. 3a and Fig. 3b), and we will correct it. The revised caption now reads:
âFig. 3: We visualized the impact of perturbation amplitude on model performance, measured by the ÎSSIM metric. Subplot (a) shows that all models experienced a drastic drop in SSIM as the perturbation amplitude increased using worst-case perturbations generated by i-RIM. Similar findings were observed with adversarial perturbations via the ResUnet model, in (b).â

Major Critique: Discussion on Model Instabilities

Response: We appreciate this comment. We have added a discussion about the possible reasons for the observed instabilities in diffusion models. The revised discussion reads:
âOur study suggests that worst-case perturbations in model-based MRI reconstruction can transfer to the independently trained diffusion model. The main reason for this vulnerability is that the perturbed K-space misleads the reverse iterative diffusion process, creating nonphysical artifacts. Classical regularization techniques like total variance regularization might offer better robustness in such scenarios.â

Reviewer #4

Major Critique: Font Size in Figures and Dataset Utilization

Response: We have revised Figure 1 to increase the font size for better readability. Using 80% of the dataset for training and validation follows a standard 80-20 train-validation split, ensuring robust model evaluation. We have clarified this point in the manuscript.

Reviewer #5

Major Critique: Poor Writing and Lack of Clear Introduction

Response: We will thoroughly revised the manuscript to improve clarity and readability. The introduction now reads:
âMagnetic Resonance Imaging (MRI) is essential for medical diagnostics, especially for brain diseases, due to its detailed, non-invasive imaging capabilities. However, MRI faces challenges like long acquisition times and high sensitivity to motion. Recent advancements, particularly denoising diffusion models, promise to accelerate MRI by reconstructing high-quality images from undersampled data. Unlike traditional methods, these models can operate without paired training data. However, our study reveals a critical vulnerability: susceptibility to minimal worst-case perturbations, leading to significant inaccuracies in reconstructed images. Our research explores the robustness of diffusion models in MRI reconstruction, investigating adversarial perturbations and proposing strategies to enhance resilience. We aim to advance reliable diffusion models in clinical settings.â

Major Critique: Confusing Concept Definitions and Limited Comparisons
Response: We evaluate all experiments using both SSIM (structural similarity index measure) and pSNR (peak signal-to-noise ratio), common metrics for evaluating image reconstruction. Due to limited space, we included the pSNR evaluation in the supplement (Fig. S1). Additional visualizations are included in Fig. S2.

Major Critique: Unclear Definitions

Response: The term unsupervised in our study means that paired undersampled MR images and their ground truth are not needed. We follow the usage of âunsupervised reconstructionâ as proposed by Song, Yang, et al. 2023. We will clarify all concepts mentioned by the reviewer in the supplement.

Major Critique: Reconstruction Baselines Are Not Convincing

Response: We selected a Unet-based baseline (ResUnet++) as it is the most widely used CNN backbone in MRI image reconstruction. Our next selection, i-RIM, showed extraordinary success in the FastMRI challenge."
https://papers.miccai.org/miccai-2024/573-Paper0588.html,"We thank all reviewers for their constructive feedback. We are happy they appreciated the novelty and results of our approach (R4,R7) and the paper clarity (R3,R4,R7). We next address their comments.

Motivation
While R4 is enthusiastic and confident about our âinnovative approach to a contemporary research problemâ, R3 requires further clarification. We note that even though âR3: medical operation requires an utmost level of accuracy for body placement, (so) it would most likely require scanning devicesâ our approach would increase the âR4: diffusion of 3D cameras and virtual reality technologiesâ in the medical field, and, as such, advance research in these technologies.
We will improve the introduction by adding this motivation.

Novelty
While R3 claims a lack of novelty âthe main technical difference with SKEL is the inclusion of additional regressorsâ, this statement is refuted by R4 observation âthe novel formulation of a new degree of freedom and the corresponding regressor, which enables the representation of various possible skeletons for a single external skinâ and R7 âfeaturing several key contributions: â¦ segmented individual bones (â¦) development of SKEL+ with adds additional degrees of freedom (â¦) and enhancements in accuracy (â¦)â.

Are multi-label bones a contribution?
Weather the multi-label bone segmentation masks are a considered a contribution (R7) or not (R3,R4), these segmentations are key in three aspects:

Results
We thank R4&R7 for raising the inconsistency in the max values of Tab1 and the wrong scale of Fig2.

Dataset characteristics (R7)
This aggregated information will be added to the revised paper:

Reproducibility (R4)
As mentioned in abstract and intro, we will release: the multi-part segmentation dataset, the SKEL+ fits and the inference code (with regressors).
In addition, to clarify the ambiguity around the lambda weight in Eq3, we will release the fitting code. The lambda value (=0.2) was empirically set to balance the skin and bone losses using the training set.

R4"
https://papers.miccai.org/miccai-2024/574-Paper1540.html,N/A
https://papers.miccai.org/miccai-2024/575-Paper0373.html,"We would like to thank the reviewers for their constructive feedback and provide answers to the following major concerns.

Clinical Application [R1/R5]:

The reviewers have concerns regarding our methodâs clinical value. Dense tissue tracking is necessary for numerous downstream tasks, such as AR for surgical guidance. For instance, in scenarios where information from pre-operative images must be overlayed intra-operatively, dense tissue tracking becomes essential to dynamically update the displayed location and shape throughout the surgery.

Our method, as demonstrated with 3D semantic segmentation, holds promise for such applications. However, we acknowledge the need to address certain limitations and advance its practical utility. Future efforts will focus on enhancing speed, testing on extended sequences, and validating robustness in real-world scenarios encompassing challenges like smoke and bleeding.

While it is noted that utilizing the endoscope pose as input may limit applicability to robotic surgery, we believe it serves as a valuable foundation for further exploration and refinement in clinical settings.

Sampling Anchor Gaussians [R3/R5]:

R3 and R5 express concerns about the random sampling of anchor Gaussians potentially leading to suboptimal modeling of deformations due to underrepresentation of certain regions. While we acknowledge this possibility, our experiments have not shown significant underrepresentation issues.

In our experiments, we subsampled with factor 64 which leads to fairly dense coverage, considering that tissue deformations are typically smooth, except on the borders of anatomical structures. While not mentioned in our manuscript, running our method on each sequence 100 times with different random seeds leads to slightly improved average performance than those reported in Table 1, but induce standard deviations 3.8px for MTE, 5.1% for \delta_AVG, and 5.8% for survival which we consider to be fairly robust. We will clarify this in Table 1. We believe our approach strikes a balance between computational efficiency and effective deformation modeling.

Choice of Baselines [R3/R5]:

We acknowledge the suggestion to include citations for relevant methods like Neural LerpPlane, Orthogonal Plane, and SemanticSuper. However, we maintain that our selected baselines are well-aligned with the focus of our evaluation.

Our method is specifically tailored for robotic surgeries utilizing forward kinematics and stereo endoscopes, distinguishing it from techniques designed for non-robotic surgeries. We emphasize that our intention is not to claim superiority over offline methods, including 4D-GS. Rather, we highlight the value of our methodâs online (incremental) paradigm in facilitating the clinical applications outlined in our above response.

Regarding comparisons with RAFT and PIPS++, we acknowledge potential biases due to missing camera poses or depth information and we will state this more clearly in the revised manuscript. However, we point to our comparisons against methods like [15], which demonstrate the competitiveness of our approach with SOTA methods utilizing the same inputs.

Evaluation and Reproducibility [R5]:

The reviewer expresses doubt about reproducibility of our work, but code and data will be released upon acceptance (footnote p.2).

In our evaluation, we manually selected sequences of 200 frames to capture challenging key moments with various factors such as camera movement, tissue deformations, and camera loops. This length was chosen based on datasets used in previous 3D reconstruction methods (ie.[14]). While our method is not inherently limited to this length, longer sequences may pose challenges for long-term tracking."
https://papers.miccai.org/miccai-2024/576-Paper1838.html,"First of all, we would like to thank all reviewers for their responses and opinions. A common opinion (R3, R4) is that we have not compared our model against DL-based image registration models. Here, we would like to point out several things that could be clarified in the paper: i) our model is not a pure image registration model. The purpose is to estimate missing samples in the image sequence. In the experiment, we want to show that our model shows similar results to two recognized methods in image registration that rely on known observations, and we do not strive to be state-of-the-art in image registration. ii) of course, we could have chosen other DL-based models to compare against. Our choice relies on the work of Krebs, J. et al. 2019. In that study, SyN, one of our comparison methods, shows better results than Voxelmorph (Balakrishna, G. et al. 2018), a recognized DL-based model in image registration. However, a direct comparison against Krebs, J. et al. 2019 is not possible as the source code for that model is not available. But if we compare the result in our experiment with theirs, they are approximately equivalent. This also applies to the comment that the results in the table are âso closeâ (R3). The purpose of the comparison is not to outperform existing models but to demonstrate the accuracy of the modelâs image registration while we can handle interpolation and extrapolation for missing samples.

Another comment we want to highlight is our choice of a Markov process to estimate the motion (R4). Here, we emphasize that Markov processes can capture a wide range of motion types, including cyclic motions (see e.g. ÃstrÃ¶m and Murray, 2008). The key is that by using a state with dimension higher than 1 we can straightforwardly create various oscillatory motion. We see several advantages of using a linear Gaussian state space model as our dynamical model: i) it is simple, with only a few parameters that contribute to fast real-time training; ii) its inference solutions, such as filtering, prediction, and smoothing, are analytically tractable, and finally; iii) stability analysis is possible by studying only the eigenvalues of the A matrix. The last aspect is not included in the paper, as it would have meant violating the 8-page limit. Furthermore, the Markovian variable is the latent state variable z, not the observed variable x. This contributes to an increased variety of dynamics.

Regarding more references concerning longitudinal deformations for image sequences (R3), we will add a brief discussion of this in the final version. An approach that is not mentioned, for example, is vector momentum-based methods (Yang, Xiao, et al. 2017, Pathan and Hong, 2018, Ding, Zhipeng, et al. 2019), where the models are trained supervised, generating a vector momentum sequence using LDDDM and geodesic shooting as ground truth. Those methods have, as far as we know, only been evaluated on brain sequences and not cyclic patterns, which was our primary target in this paper. Other works using adaptive online learning procedures (Sharp, Gregory C, et al. 2004, JÃ¶hl, Alexander, et al. 2020, Lombardo, Elia, et al. 2022, Li, Yang, et al. 2023) were left out since their models rely on e.g. respiratory signals instead of the image sequences. If the reviewer thinks other references are missing, we welcome suggestions.

During online learning, we choose a moving horizon of N=75 (R4). The length was considered a hyperparameter. Our choice is based on a trade-off between a long horizon to cover the most significant motion in the sequence and a relatively small one to adapt the model fast which could be further clarified and evaluated in the paper.

For the comment that scaling and squaring is not âa proven approach to obtain diffeomorphic registrationsâ (R3), we agree. What we mean by this sentence is that using Gaussian smoothing and scaling and squaring layers showed diffeomorphic registrations in the reference work [9,17]."
https://papers.miccai.org/miccai-2024/577-Paper0519.html,"1.[R123] Brief Summary of Contribution
(1) We proposed simple but effective OFG framework to boost the performance of existing unsupervised learning-based registration methods.
(2) During the training, OFG takes the predicted deformation field from current training iteration and optimizes it as pseudo ground truth to explore the benefit of supervised learning in image registration task. The optimizer works as a âteacherâ to provide direct evolving guidance to baseline registration models.
(3) The overhead is limited: ~10% training overhead and ~0.7GB memory overhead. However, as OFG is applied in training only, inference speed is preserved.

2.[R123] Deformation Field Smoothness
Previous methods directly apply smoothness regularization on predicted deformation field to optimize model parameters. Whereas, OFG optimizes models using only L2 loss shown in Eq (3) of main submission. The regularization is applied in the deformation optimization as in Eq (4) to provide smoother pseudo labels for direct supervision. The smoother labels provided by OFG can make the model actively fit to smoother deformation fields during the training.

Different Reg weight for TransMorph on LPBA
Reg | DSC | Jacob
0.02(default) | 0.678 | 0.438
1.0 | 0.678| 0.418

Different Reg weight in Eq(4) for OFG:
Reg |  DSC   |   Jacob.
0.1 |  0.654  |   0.896
0.5 |  0.672  |   0.385
1.0 |  0.684  |   0.150
2.0 |  0.685  |   0.028

As shown, the smoothness regularization from smoother direct supervision provides better Jacob, along with DSC improvement.

3.[R13] OFG vs Direct Instance Opt
We compare OFG results with the results of âdirect instance optimization applied after learning-based registrationâ(R1), with 10 and 20 extra opt steps. As shown in the table, OFG could achieve similar DSC improvement but better Jacob. However, our method doesnât require instance optimization in inference, which is faster (R1) and close to the upper bound (R3).

TransMorph on LPBA
Config  |  DSC   |  Jacob.
base  |  0.678  |  0.438
base+10 opt |  0.684  |   0.518
base+20 opt |  0.686  |   0.631
OFG  |  0.684  |  0.150

4.[R12] Extra Evaluation
Besides brain MRI, we also provided evaluation results on Abdomen CT as shown in Tab 2 of main submission. The DSC of affine registration on abdomen is 0.236, and OFG could improve TransMorph 6.5% on DSC and 93.9% on Jacob.

5.[R3] Optimization Stability
(1) As in response 3, extra optimization could refine the prediction from trained unsupervised methods. 
(2) We provide optimization process for deformation from (a) random initialized model and (b)random generation as in Fig 2 of Supp, which demonstrates optimization is able to refine registration even from random initialization. Therefore, it could perform well on the starting stage of training. The warm starting is an interesting topic and we will explore more in this direction.

6.[R3] OFG loss is applied on the whole image as in baseline methods, i.e. TransMorph, etc.

7.[R3] Network-based Optimizer: Apply cascaded VoxelMorph to improve registration results iteratively. The model prediction of n-th step will be the input for the n+1-th step.

8.[R3] Optimized Self-training: Optimize predicted deformation field from pre-trained registration model only once.The optimized field is used as final pseudo ground truth in supervision, with no optimization involved in training.

9.[R3] Opt weight: Considering the Training loss (a * NCC + b * OFG), opt weight is b/a to explore loss combination.

10.[R3] Threshold: The horizontal axis shows the percentage of training images using OFG loss, while the remaining will use unsupervised loss.

11.[R3] Optimization Steps: As shown in Fig. 1 of Supp, we recommend 5~10 Opt steps."
https://papers.miccai.org/miccai-2024/578-Paper0244.html,"Q1: Clarity of contribution (R1)
We focus on: 1) Learnable prototype to denote features of seen classes, which mitigates feature conflicts between inliers and outliers; 2) Multi-binary discriminator for outlier identification. Our method takes advantage of unlabeled data mixed with unseen classes guided by limited labeled data. It could be utilized effectively in open clinical settings, where the uncontrol and unseen class images may be collected into the unlabeled set.

Q2: Implementation details (R1, R4)
We use ViT-B/16 in CLIP from https://github.com/OpenAI/CLIP as backbone for all methods with the same settings. Total parameters are 88M, trainable parameters are 1M. The server is RTX 3090 with 24GB memory. Training time of all methods are 3~4 minutes per epoch. Training epoch is set to 50 to ensure convergence.

Q3: More details of our framework (R1, R4)
Fine-grained classification often shares significant similarities and it is crucial to discern distinctive features during learning. Thus, we propose learnable prototypes to learn seen-class center. We mentioned it in the first paragraph of section 2.2. This is different from other classification task in CV. If we simply adopt these methods in CV for medical domain, it leads to poor results, as shown in Table 1. 
Equation (5) is only applied for seen-class after the outlier filter. Closed-set classifier is used to obtain class scores to assist multi-binary discriminator to recognize whether the image belongs to inliers or not. After training on labeled data, they all have acquired discriminative features, and can be used jointly as outlier filter. Post-processing OOD detection do not sufficiently learn features of seen and unseen classes, resulting in poor detection results.

Q4: Comparing with SOTA baselines (R1)
The aim of this paper is to improve classification performance for open-set SSL. However, SOTA baseline models are trained with only seen samples in supervised manner. The closed-set results in Table 1 are obtained under open-set SSL training, and comparing with the SOTA baseline models is unfair due to different experiments settings.

Q5: Simple baselines (R1)
We give the closed-set results follow the format in Table 1:
Fully-supervised:   (63.34,48.74, 62.11,46.94, 71.85,58.96, 70.25,57.22)
plain Mean Teacher: (64.11,49.54, 62.58,47.29, 72.13,59.45, 70.87,57.98)

Q6: Per-class result (R1)
Due to page limitation, we only report the average results, as same as [9,14]. We will add per-class result in the supplementary.

Q7: Better settings to evaluate open-set SSL (R1)
Collecting datasets of DR as seen classes and other related diseases as unseen classes is more realistic and can better evaluate the effectiveness of our method. This is a promising experimental setup and we will explore it in our future work.

Q8: Class balance in the dataset (R3)
The datasets in the experiments have class imbalance, the ratio between outliers and inliers for DDR (575 outliers, 6320 inliers) and ISIC2018 (6847 outliers, 3168 inliers) is 0.09 and 2.16.

Q9: Which side does the proposed modules contribute to inliers or outliers? (R3)
The proposed modules contribute to both inliers and outliers, learnable prototype learns the class center of each seen class, and it can help the backbone learn robust features, improving the performance of closed-set setting. Multi-binary discriminator decides whether the current sample belongs to inliers or outlier, and improve the performance of inliers and outliers, and we validate each component in Table 2.

Q10: Outlier detection accuracy (R4)
The performance is evaluated on both seen and unseen classes. We regard all unseen classes as a new class and report the Closed-Set and Open-Set performance in Table I following IOMatch [9].

Q11: The basis for dividing ISIC dataset (R4)
We can simulate open-set scenario by randomly designating classes as inliers or outliers, tailored to the specific needs of the clinical situation."
https://papers.miccai.org/miccai-2024/579-Paper1442.html,"#1:

Q:The RL method is typical.
A:This work focuses on modeling optimization of interaction strategies. RL is employed not to introduce novel methods but as a powerful tool to enhance and optimize the decision-making process for SAM usage.

Responses for detailed comments:
Figures enlarged.
âRâ is the reward function, ârâ is the specific reward at a given moment. 
We have fixed abbreviations/typos.

#3

Q:Cost in testing
A:Manual interaction is inevitable in interactive medical image segmentation. We aim to reduce it in future work.

Q:Lack HD
A:Higher Dice scores generally correlate with lower HD values, as same as in our experiment. We didnât include HD95 results due to space limits but will provide code for detailed results.

Q:Logits brings unstable
A:Previous logit helps determine the optimal action. Errors affect performance, but training with incorrect logits reduces impact. Further exploration needed.

Q:Correct prompt is hard
A:We introduced random perturbations(10 pixels) to enhance robustness. We use âcenter pointâ to reduce confusion between FN and FP.

Q:Inconsistency following
A:The random policy, akin to users with no preference, simulated this situation, and it was worse than our method. To address this, we suggest using post-processing methods.

Q:Split âCenterâ
A:Worse results were found. It may due to a larger action space complicating RL exploration.

Q:Doubts about learned strategy
A:Using âcenterâ before âboxâ yields better results. Deep learning allows RL agents to adapt quickly in new scenarios, outperforming manual exploration.

Q:Fig problem
A:Fig 1 has been redrawn for clarity. We made an error in Fig 2. We split it into two charts: Average Length vs. Penalty and Average Length vs. Dice. 
A grid search of penalty hyperparameters yields various penalty levels.
Experimental results show the random policy performs poorly, which represents it is a conservative strategy, leading to fewer misunderstandings.
The red rectangle indicates actual interaction; counterfactual results stem from not following the algorithm with the center instead. Future revisions will use bold formatting for maximum Dice values.

#4

Q:About first contribution and distinctionsA:[1] focuses on optimizing interactive medical image segmentation interactions but ignores that fixed-length interactions may lead to suboptimal solutions.. Our work enhances prompt strategies by considering adaptive interaction rounds and early termination strategies with SAM, optimizing segmentation and reducing computation while maintaining accuracy.

Q:The related work is light. 
A:The added related work about [1] is shown below; Others have been added in the revised version!
To address SAMâs sensitivity to interaction forms, a framework was developed to adaptively offer suitable prompts for users [1]. This method optimizes temporally extended prompts but overlooks fixed-length interactions.
Our work improves prompt strategies by considering interaction forms and adaptive termination, introducing early termination to optimize SAM segmentation for efficiency and accuracy while reducing computational efforts.

Q:Comparison with [1]
A:Our AIES is a more generalized formulation. When Î» is 0 and the penalty is removed, it is equivalent to [1]. AIES(6) in Table 1 reproduces the TEPO algorithm. For other steps, since our interaction steps are averaged and not integers, a rigorous comparison with TEPO is not feasible, hence it was not included in the table.

Q:Detailed configurations
A:We will release our code soon. We found that the number of environments sampled, training epochs, and buffer size are the key factors.

Responses for detailed comments:
Experiments were conducted using four NVIDIA A100 40GB GPUs. Cost 8h to 9h.
We have fixed grammar/typos.

Thank you for your review and suggestions. We have addressed all concerns and improved the manuscript. We appreciate your feedback!"
https://papers.miccai.org/miccai-2024/580-Paper0311.html,"We thank all the reviewers for their insightful and valuable comments. We are pleased that the reviewers recognized the novelty and strengths of our work: the novel experimental setting and research direction (R1), the introduction of multimodal knowledge guidance (R3), the innovative application of large vision-language models (R5), and better results than the previous state-of-the-art, with good performances on downstream tasks (R1).

Regarding the zero-shot performance of existing LVLMâs in our setting (R1), we want to point out that both the operating room domain, as well as the scene graph generation tasks are uncommon for these models, and our initial experiments showed very poor results in the zero-shot context. Furthermore, these models are also incapable of accepting multi-view input. Therefore, adapting these models, both in terms of modifying the architecture, and in terms of fine tuning them, was crucial in our work.

Regarding the integration of the temporal prompt (R1), all details will be included in the code, which will be available at the time of the publishing of this work at MICCAI 2024. Nonetheless, we want to provide a short example here: If at timepoint T the triplet <head surgeon, sawing, patient> is predicted, we add this to our memory. For timepoint T+1, the model prompt now looks like this (simplified). âMemory: <head surgeon, sawing, patient> Entitiesâ¦ Predicates â¦ â Now the model would predict new triplets for timepoint T+1, and we would add these to the memory as well.

Regarding the data / code that will be released (R3), we will indeed release everything, including the entire code, any data we have generated in addition to the previous work, but also the relevant model checkpoints.

Regarding the computational complexity (R5), we agree that LVLM can have high costs. While this initial works focus was not on this, we want to briefly address some concerns. Unlike previous works, ORacle is a fully end-to-end approach, making it more efficient in terms of compute time. Previous methods relied on three or more steps, which usually consist of different neural networks. Additionally, we see a great effort in the community to reduce the runtime complexity of these LVLMâs, including quantization techniques, as well as custom designed hardware, which we believe will make it possible to run these models in the operating rooms in real time, in the future.

Overall, we are grateful for the constructive feedback of all the reviewers and will incorporate these improvements in the final version. As acknowledged by the reviewers, our approach presents a significant advancement in holistic OR modeling using large vision-language models, achieving state-of-the-art results and demonstrating practical adaptability."
https://papers.miccai.org/miccai-2024/581-Paper2860.html,"We thank the reviewers for appreciating our workâs novelty and superior performance on an understudied new clinical task and the constructive comments.
Q1: Reproduction (R3&R4&R5)
The code will be available upon acceptance.
Q2: Imbalanced distribution (R3)
We use weighted cross-entropy, assigning 0.65 weight to STAS, to address class imbalance. Contrastive loss pulls positive pairs closer and pushes negative pairs apart in the feature space but doesnât address imbalance. We will update it.
Q3: Other comments (R3)
We will add the specificity metric and color bar in Fig. 4 in final version.
Q4: Feasibility in current clinical settings (R4)
Our work can be integrated into clinical setting to (1) intraoperatively identify STAS, enabling immediate and complete resection and thus reducing additional surgery and potentially improving patient prognosis; and (2) STAS initial screen, reducing pathologistsâ workload and improving diagnostic consistency. We will add this discussion.
Q5: Motivation of framework design (R5)"
https://papers.miccai.org/miccai-2024/582-Paper1554.html,"We greatly appreciate the reviewers for the effort and insightful comments regarding our submission. We are encouraged by the reviewersâ positive feedback. We will further correct errors and clarify all the concerns of the reviewers in the final version. Below are our detailed responses to the comments:

Reviewer 1.
Q1. Dataset characteristics.
We will include a more detailed introduction about the datasets used in our study in the revised manuscript.

Q2. Introduction.
We will add a more discussion about the related work for the final version.

Q3. Further exploration.
Thank you for the insightful suggestion. In future work, we will explore refining the model architecture and integrating multi-modality features to further enhance the risk prediction capability of our model. We plan to investigate the integration of additional imaging modalities, as well as non-imaging data like clinical history.

Reviewer 3.
Q1. Cross-validation and OOD performance:
Thank you for the suggestion. In future work, we will explore the robustness and generalizability of our risk models through cross-validation and external validation. These validations are important for assessing the applicability of our models in clinical settings.

Q2. Figure 3.
We apologize for the misunderstanding. In Figure 3, we plot the distribution of the expected time to cancer for detected high-risk patients (true positive cases). Both grey and colored dots indicate true high-risk patients. The colored dots represent patients identified by the risk model with an estimated time to cancer within 5 years, while grey dots represent cases where the estimated time exceeds five years. Due to the higher performance of our proposed model, it detects more high-risk patients, leading to a difference in the sum of colored and grey points between the baseline and our method. We will clarify this explanation in the final version, ensuring that the figure is accurately interpreted.

Q3. The statement âthe time-to-event estimation task could â¦â in the introduction.
Thank you for the suggestion. We will revise the statement and add relevant references to support this claim.

Q4. The definition of STP and MTP:
STP refers to single-time point methods, and MTP refers to multi-time point methods.

Reviewer 4.
Q1. Readability of method:
We will add more detailed explanations in the method section to improve clarity in the final manuscript.

Q2. Fig 1.
Thank you for your comments. We will clarify the descriptions for Figure 1. Our proposed method is based on two-time point risk modeling. We will ensure that the multi-time point concept is clearly defined as involving two-time points.

Q3. Table.
We will update the tables to a standard three-line style."
https://papers.miccai.org/miccai-2024/583-Paper0661.html,N/A
https://papers.miccai.org/miccai-2024/584-Paper3324.html,"Overall, reviewers found our work to be âsimple, elegant, and effective (R1)â & a âgreat paper, couldnât be much happier (R4)â. The main criticism was from R3 around motivation and novelty. We hope to clarify several points below.

R3-Motivation+Novelty
Our method is specifically designed to improve predictive modeling based on connectomes, a widely used biomarker for investigating brain-behavior associations.  FLECHAâs novelty lies in overcoming regulatory constraints on sharing raw imaging data, enabling the aggregation of predictive information from distributed heterogeneous connectome data. Refs. [3]+[12] introduced the general mapping methods but did not integrate advanced machine learning, like federated learning. Our goal is to apply federated learning in connectomics while accounting for heterogeneity caused by different atlases.

R3-Choice of Tasks, Labels, & Evaluation Metrics 
Age and working memory were chosen as the primary predicted variables as they are easy to measure, are common benchmarks in brain-behavior models, and have significant clinical relevance. Structural connectomes have been shown to be strongly correlated with age, making them suitable for predicting age. In contrast, functional connectomes strongly correlate with complex higher-order cognitive functions, like working memory. Age (in months) and working memory scores are treated as continuous variables. Predictive performance is assessed using Pearson correlation or mean square error (MSE) between observed and predicted values. Pearson correlation was chosen for its standardization and comparability across measures and studies. Although not shown due to space constraints, results were similar with MSE. Future work will include more tasks, encompassing classification and other labels.

R1-Does Parcellation Granularity Mismatch Degrade Performance 
The atlases used in our study vary significantly. For example, the Dosenbach atlas is the smallest (by >25%), with 160 nodes, and does not cover the whole brain. Despite these differences, our results show robust performance across federated learning scenarios for that atlas. Further, common atlases are within the 200-400 node range used here, suggesting similar performance for other popular atlases not tested. Still, we expect FLECHAâs performance to degrade at some point. Ablation studies with a broader array of atlases, varying in node number and coverage, will be needed to understand which atlases are too coarse for FLECHA.

R1-Privacy Concerns
We want to clarify that FLECHA does not transmit the target dataset. Only parameters need to be transmitted. These parameters include the between-atlas mappings (pre-trained on external data and centralized) and site-specific models. Still, we agree with the reviewer that the privacy-preserving aspect could be enhanced. Future research could explore machine unlearning to remove the impact of a silo from the centralized model.

R3+4-Realism of Data Distribution
All participants came from one dataset. Still, no individual was in two silos (i.e., no data leakage). This choice was deliberate for experimental control. It allows us to specifically correct the data heterogeneity problem caused by using different atlases in preprocessing. If different scanners and sites were used, we could not separate which domain shift affects prediction. FLECHA is flexible and can be added to existing methods for scanner-induced domain shifts. Future work is needed to benchmark FLECHA in real-world cases.

R4-Absence of Coef_avg in Fig 4
We excluded Coef_avg from our DNN model analyses as it is equivalent to FedAvg with only one communication round, leading to underfitting. This exclusion was intended to provide a clearer comparison of more robust configurations.

R1+3-Limitations and Reproducibility 
We have room to include a few lines about limitations, reflecting the points above. Also, we did not release code to maintain blinding but will upon the paperâs acceptance."
https://papers.miccai.org/miccai-2024/585-Paper0481.html,"We appreciate the feedback from reviewers R1, R3, and R4.

@R1(motivation of missing data): We would like to address a possible misunderstanding. Masking is a straightforward method to show whether the trained model has learned the overall shape. Specifically, the model trained by our method is not limited to pixel-by-pixel classification but can perceive that missing part violate the overall shape of the LA as shown in supplementary materials. Prediction on the mask part is similar like MAE, which is more of an interpretation experiment than a comparison. Thank you for your suggestion on âthe anatomy is not clearâ, which inspired us to discuss this method in ambiguous situations in later extensions.

@R1(Q3: lack of novelty, Q8: application-level novelty): Thank you for your comment. The innovation of this paper lies not on the combination of augmentation and the student-teacher model, which is a common approach in previous works such as Mean-Teacher, UA-MT, BCP, etc. Firstly, we introduce an Overlay strategy. It is a new angle on data augmentation for addressing edge uncertainty and an easy-to-use scheme, but effectively. Secondly, we innovate by removing EMA between student and teacher compared with other related works. Instead, we directly pre-train to fix the teacher parameters and design a Mantle-Free distillation. In this way, it is beneficial to generate pseudo-labels and learn the difference between background and overlay images.

@R1(Q8: augmentation noveltyï¼@R3(Q6.3:marginal improvement): Due to continuous advancements, the performance of semi-supervised methods on LA is close to fully supervised, making marginal improvements challenging. However, We think the point is the Overlay data augmentation method and the Mantle-Free differentiated inputs for the teacher and student models are worth discussing. Notably, âS1â in Table 2 shows that V-net using 10% labeled data with our Overlay data augmentation methods achieves a 4.35% Dice improvement compared to using original images and labels, and 1.06% greater improvement than the regular method even with 20% labeled data (Compare âS1â in Table 2 with the second row of Table 1). We will highlight the unique advantages of the proposed methods in the revised version.

@R4(Q6:binary classification segmentation task weakness): Thank you for your constructive advice. We have to admit that this manuscript has the limitation that it only discusses binary segmentation tasks, but the idea of Overlay is inspired by the âClassMixâ is originally a multi-class data augmentation. For multi-class, such as the ACDC, the solution is to randomly select one class label of three to make a mantle and paste it on the background image. Inspired by your feedback, We plan to explore multi-class research.

@R3:(the performance of BCP dropped): Thank you for your careful analysis and We apologize for any confusion. We attempted to reproduce the result but did not achieve the same result as the original paper. To avoid lowering the baseline, we used the result provided by the original paper for the 10%. The original paper did not provide data for the 20%, so we obtained the 20% result using the same settings as the 10%. Our work also uses the same parameters for 10% and 20% settings in code.

@R4(post-processing): Thank you for bringing this to our attention, We should add an ablation study for NMS to ensure rigor.

@R1(evaluation lack)@R3(Q6.1, Q12:experimental validation) @R4(Q6:robustness of this method, Q10: more datasets): LA is commonly used in previous work, is able to clearly explain how Overlay mask works. Even though we observed excellent results, robustness needs further testing. If it is allowed, future work will include additional datasets such as ACDC on multi-class segmentation. We also plan to explore to apply OMF on diï¬erent modalities (CT, Xray,WSI).

Finally, we would like to thank all of the reviewers for their careful reading and suggestions."
https://papers.miccai.org/miccai-2024/586-Paper0942.html,"Thanks for the comments from all the reviewers. Sorry that additional experiments cannot be included in the response following the rebuttal guidelines.

To Reviewer1:

To Reviewer3:

To Reviewer4:"
https://papers.miccai.org/miccai-2024/587-Paper1022.html,"Thank you for your thorough review and valuable feedback on our manuscript. Below, we provide a detailed response to address the concerns raised.

Comments about Interpretability: The reviewers mentioned the lack of interpretability comparison.
Response: Compared to conventional attention-based MIL methods, our PAMIL offers both attention score and prototype score as explanations. The interpretive process is analogous to that of a pathologist, who diagnoses by comparing key areas of the slide to typical patterns, as shown in section 4.2.

Comments about Probabilistic Prediction and KL Divergence: The reviewers have doubts about the Y_proto and Y_inst, as well as the use of KL divergence for regularization loss.
Response: In our paper, Y_proto and Y_inst represent probability values from two prediction branches, with the final model prediction being their average. KL divergence measures the difference between probability distributions (not the original logits) and is used to maintain consistency between predictions from two branches. Ablation experiments validate this approach. For multi-label classification, KL divergence is applied to each labelâs probability predictions independently.

Comments about Necessity and Effectiveness of Two-step Clustering Process.
Response: In our paper, we utilize a two-stage clustering method to initialize prototypes, ensuring alignment with the distribution of slide patches and thereby enhancing training stability. Since prototypes are updated during training, any initial randomness does not significantly impact the overall process. Moreover, this approach is more efficient than using a single k-means, as evidence by Yu et al. (2023) [17].

Comments about Optimization of Prototypes and Meaning of w_c Matrix: The reviewers doubt why not updating prototypes with k-means clustering, how prototypes relate to categories and actual patches and how does the matrix w_c work.
Response: Prototypesâ optimization is in three stages: backpropagation, projection operation to replace prototypes with the most similar instances (link to actual patches) and fine-tuning other parameters.
The w_c matrix (dimensions n_classÃn_proto) captures the relationship between prototypes and categories (section 3.2).
This category relationship cannot be learned by using k-means clustering optimization prototypes.

Comments about Differentiation Between the Instance Branch and Prototype Branch
Response: We designed the instance and prototype branches separately to aggregate features from both prototypes and instances as shown in section 3.2. This enhances model performance and provides dual aspects of explanation. Since the number of prototypes is fixed while the number of instances varies per slice, we design different aggregation methods for each branch.

Comments about Effectiveness in Multi-Class Scenarios: The reviewers pointed out that the method did not achieve SOTA on the multi-class classification task.
Response: There are two reasons for not achieving SOTA on the multi-class task. First, current SOTA methods have excellent performance (More than 90%), making further improvements difficult. Our t-test revealed no significant difference between our PAMIL and SOTA methods. Second, this paper prioritizes model interpretability and demonstrates significant improvement in more challenging scenarios (multi-label).

Comments about the potential to focus on Features of Non-cancerous Regions.
Response: The attention score makes the model pay more attention to the category related features (cancer area). In addition, the visualization of the model also shows that the model will not pay too much attention to the normal area (section 4.2).

Finally, regarding other details, our pre-trained encoders and patch cropping follow established practices. We will rectify any typographical errors and clarify expressions in the final paper. Additionally, we plan to publicly release the code in a future update."
https://papers.miccai.org/miccai-2024/588-Paper0928.html,"We thank the reviewers for their detailed feedback and positive comments regarding motivation (R4&R6), formulation (R4) and results (R4, R6&R7). We address the major concerns point-by-point."
https://papers.miccai.org/miccai-2024/589-Paper0560.html,"We thank all the reviewers for their thoughtful feedback & constructive criticisms. We address all the comments below:

R1) Tuning methods specific to medical imaging
Thank you for your feedback. We realized it was crucial to first empirically validate the efficacy of the existing PEFT to multi-scanner as rigorously as possible via exhaustive experiments involving numerous En-De and PEFT combinations. As suggested, we plan to develop PEFT methods tailored for medical imaging.

R1) How to get low-count PET?
Each ADNI PET is a sequence of six 5-minute frame scans (i.e., 0-5, â¦, 25-30). The low-count (i.e., short-time) is the first 5-minute scan (0-5) only. We will clarify this and release the data preprocessing code.

R1) Sample split details
We apologize for any confusion caused by the brief explanation. 
The sample split is Pretrain (Source): Train 30 / Valid 15 and PEFT (Target): Train 10 / Valid 15. Data setup details (e.g., split, 3-fold CV, etc.) will be clarified.

R1) Details on multi-scanner and data splits 
We see the need for details which we clarify below.

R1) Abbreviation
PETITE refers to the best PEFT setup for PET recon among various Mix-PEFTs for En-De. We will consider a simpler alternative.

R1) Full-FT in Table 1 is not similar to Fig. 4
We appreciate your point. Fig. 4 shows an example from a specific slice, sample, and scanner, which may not best align with Table 1, which shows the average of all scanners. We will include a better-aligned sample. Due to the sheer number of PEFT combinations, we regretfully could not include all visual results. Our submitted code will be updated to provide all error map generations.

R1&R3) Availability of PET-specific models
While more models would always be better, we hope the reviewers see the rationale behind our choice. To gain experimentally sound insights on this first work, we prioritized exhaustive and rigorous experiments on two models that best met the following criteria:
1) robustness & high performance 
2) reproducibility (open-source)
3) compatibility with ViT-based PEFT 
Based on these criteria, we chose (A) 3D CVT-GAN, the only SOTA PET recon model with reproducible code, and (B) UNETR, one of the most widely used 3D transformer-based models with UNET-like robustness. As the first step, we believe these two models effectively span the range of possible PET recon solutions, although we agree more models could better fill the gap. As suggested, we plan to explore various PET-specific models in future work.

R3) âOnly one dataset cannot prove the universality of methodâ
We do agree that universality is indeed important, but proving it is a long-term pursuit, especially in a field with rapid methodological advances. Therefore, like other initial attempts, we aimed to validate the method in a controlled environment, providing insights through exhaustive experiments within the PET modality. Nonetheless, we fully agree that the universality suggested is a crucial direction for future work, and we will pursue it.

R4) Details on PEFT suitability
We apologize for the brief explanation due to space limitations. Based on the tables (See Table 1; Suppl. Tables 5 and 6), VPT and LoRA are most suitable for ViT in both En-De. Using them together creates a synergistic effect, as both operate within the critical attention component of ViT. Additionally, SSF is best for CNNs. Tuning SSF in ViT with other PEFT methods results in a performance drop, as it hinders ViTâs complex attention mechanism. We will clarify this further."
https://papers.miccai.org/miccai-2024/590-Paper3722.html,"Dear Reviewers,

We are grateful for the acceptance of our paper and for the insightful comments and suggestions you provided. We have carefully considered all feedback and have made revisions to enhance the quality and clarity of our manuscript. Below is our detailed point-by-point response to your comments.

Reviewer 1 
Thank you for your remarks. We refer the reviewer to page 6, paragraph âAvailability,â where we provided an anonymized link to the data. For the camera-ready version, this will be replaced with the location of the dataset for public use. We anonymized the links to preserve the anonymity of the submissions. Code and dataset will be available after the camera-ready submission on July 8th.
Additionally, we plan to implement an access-controlled system where users must provide specific information before accessing the dataset. This measure is necessary since the dataset includes medical images of children. The access-controlled system will also enable us to notify users if patients decide to withdraw their consent and request the removal of their data from the dataset.
As specified in the paper, the case collection is based on teledermatology settings, resulting in several different settings (camera, resolution, compression, etc.). We therefore neglected this information as it was seen to contain irrelevant and noisy data. If present, the information is available in the EXIF data of the images (72.6% of all images).

Reviewer 2
Thank you for your comments. We purposely refrained from evaluating state-of-the-art (SOTA) models as they are either closed source or not capable of handling the tropical diseases present in our dataset. Instead, we evaluated SOTA encoder architectures to showcase the performance achievable with standard techniques. To enhance our contribution, we have included a more detailed ablation study on generalization across different phototypes, as suggested by Reviewer 3. This addition should strengthen the methodological contribution, even though the novelty of our paper primarily originates from the dataset itself.

Reviewer 3,
Thank you for your detailed feedback and suggestions. We appreciate the recommendation for additional experiments focusing on the pediatric population using the existing dataset. We are currently setting up these experiments and will endeavor to include them in the camera-ready version if space permits. Regarding the use of existing models, please refer to our response to Reviewer 2.
While a quantitative comparison between our dataset and existing models is indeed interesting, we are likely to face difficulties including it in the paper due to size constraints."
https://papers.miccai.org/miccai-2024/591-Paper0501.html,"We thank all reviewers for your time and effort in reviewing our manuscript. We are grateful for your insightful comments and constructive feedback, which not only confirm the quality of our work, but also help us to refine our arguments. We are truly happy that all reviewers acknowledged the novelty and the thorough analysis in our paper. Below, we would like to address some open remarks.

Descriptions for âCycle Exchange Consistencyâ (R3)
To clarify R3âs confusion, our Cycle Exchange Consistency has a unique cycling process, where the roles of the conditioner and denoiser arms are swapped in the two stages of the cycle (MRIâPET and PETâMRI). The two networks x_Î¸ and Ï_Ï have the same architecture (UNet) but they operate independently. In Fig.3âs left side,x_Î¸ serves as a conditioner in the G_m stage, but as a denoiser in G_p. Ï_Ï functions oppositely. This cycling approach requires only two trainable networks, x_Î¸ and Ï_Ï. We will also release our code on GitHub for these implementation details.

Notations in Fig. 2 (R3)
R3 noted that the ânotations such as h, t, c, are not explainedâ in Fig. 2. We apologise that due to the page limit we only explained them in Sec. 2, in which h is the feature maps from the denoiser arm, t is the timestep, c is the clinical data, and h_m is the task-specific representation from the conditioner arm.

Requirement of paired image data (R4)
R4 concerns that âa potential weakness of the method is that it requires paired image dataâ. While we agree that unpaired data can increase the training dataset, PASTA is specifically designed for applications where paired data is essential. Our goal is to generate PET images from MRI scans tailored to individual patients. For this reason, accurate and reliable translations are crucial, necessitating paired data to learn precise mappings between the modalities.

We also thank R1âs constructive suggestions on including discussions of limitations and experiments on additional datasets and computational efficiency, and R4âs comment on Fig. 2. Unfortunately due to the page limit we couldnât elaborate too much on these aspects, but with brief illustrations in the conclusion and supplementary materials. We will try our best to cover them in our final version and release our code on GitHub for implementation details. We once again express our gratitude to all reviewers and eagerly await the application of our proposed methodology."
https://papers.miccai.org/miccai-2024/592-Paper0962.html,Thank you for your constructive review and recommendations to improve the quality of work.
https://papers.miccai.org/miccai-2024/593-Paper3991.html,"We sincerely appreciate the constructive feedback from all the reviewers. Our responses to the raised concerns are as follows:

For our problem formulation and contribution: Our method addresses a realistic scenario where only limited captions are available. Therefore, we employ the multi-tasking setting during training, where images are used as the input, with captions serving as ground truth. During the inference in this scenario, images alone are used as inputs (R5). Additionally, we only input both images and captions (textual embeddings) when all of them are available, which is the upper bound of our method (R3, R5).

Regarding the shortcomings of previous methods, as stated in Section 1, reliable WSI diagnostic captions require specialized pathologists and face privacy concerns, leading to a limit of captions for training models. Therefore, previous methods gather data from textbooks and the internet, which only yield patch-text pairs. We would like to clarify that in Section 2, when we state âOur work is distinct from these works as it seeks alignment at the bag levelâ, it means that our proposed approach differs from instance-level image-text alignment methods like CITE and MI-Zero, as our method achieves image-text alignment at bag level.

For our methodology, the input query is learnable vectors, which are initialized with the pretrain weight from Blip2 (R4). We employ a self-attention mechanism to establish correlations, as demonstrated by TransMIL (R5). âAlphaâ is a hyperparameter utilized for loss balancing (R4, R5) and we experimented with values ranging from 0.2 to 0.8 to find the best one. In Equation 2 (R5), âKâ and âQâ follow standard attention mechanisms, and we use Nystrom attention approximation instead of regular attention. The correct notation in Equation 4 should be âLG,â and we apologize for the typos and will correct them in the final version (R3, R5).

For the experiments: Regarding the significant improvement of multi-task and correlation components (R5), we propose that correlation could learn spatial redundancies and contextual relationships, which identify the key diagnostic areas. The multi-task setting further leverages subtype labels to locate ROI. Together, these enhancements enable significant progress in captioning tasks by focusing on diagnostically relevant areas. Regarding the image and text contributions to the results, Table 3 in our ablation studies indicates that using both image and text data during training improves accuracy by at least 4.81% compared to using only text input (R5). The AUC of our method is 85.22 (0.24), and for the detail subtype accuracy: âmoderately differentiated tubular adenocarcinomaâ is 67.93 (3.27) âpoorly differentiated adenocarcinomaâ is 63.22 (6.53), and âwell differentiated tubular adenocarcinomaâ is 79.06(3.35) (R4)."
https://papers.miccai.org/miccai-2024/594-Paper1354.html,"We sincerely thank all reviewers for their valuable feedback that help us greatly to improve the quality of our manuscript.
Response to Reviewer 1: 
Thanks for your questions. For question 1: Due to image-level labels instead of pixel-level labels, the supervisory ability of Multi-Instance Multi-Label Learning (MIML) is weaker than fully supervised methods. Therefore, to utilize the advantages of image-level labels in the training process, we combine MIML with deep contrast supervision to enhance the networkâs learning ability. This is done by using the side outputs in the network to predict the probability maps of each side output layer and training the network by minimizing the loss between the output probability maps and the image-level ground truth. 
For question 2: Our proposed PathMamba is a weakly supervised segmentation method, and to highlight its superiority, we compare it to UNet, a fully supervised algorithm that uses pixel-level labeling. Although UNet is an old method, it is also a generalized segmentation method.
We will refine the above description in the manuscript.

Response to Reviewer 3: 
Thanks for your suggestion. Weakly supervised methods require a high feature capture capability of the network, and previous methods are usually based on Vision Transformer (ViT) or CNN for modeling. However, CNN cannot capture long-range dependencies in images, and ViT is highly complex. Mamba can scale linearly while capturing long-range dependencies, and the memory footprint is smaller than that of ViT with quadratic complexity. We will add the above description of Mambaâs relevance to weakly supervised segmentation tasks to the manuscript to clarify how the proposed Mamba addresses the challenges inherent in weakly supervised segmentation.

Response to Reviewer 4: 
Thanks for your questions. For question 1, Mamba uses the S4 modeling approach, which is a purely sequence-to-sequence network. Mamba is characterized by modeling long sequence dependency information, which stems from the way it is parametrically run. It is an autoregressive model that is usually unidirectional; e.g., it has good temporal properties and can model causal sequences. However, it is not capable of modeling relationships between sequence elements, e.g., between non-causal sequences.
For questions 2-7 regarding the details of the article, we will refine the description of the method section in the revised manuscript.
For question 8, UNet is trained from scratch. Our proposed PathMamba combines Mamba and Multi-Instance Multi-Label Learning to extract long sequence-dependent information and pixel-level features, respectively, and Deep Contrast Supervised Loss to supervise the feature representation of both. UNet is a fully supervised segmentation method, and the Deep Contrast Supervised Loss cannot be used for it. This is because the UNet fitting process has only a single convolutional feature and does not contain two types of information (pixel-level and patch-level information) similar to those in our method."
https://papers.miccai.org/miccai-2024/595-Paper2078.html,"We thank reviewers for constructive comments and we elaborate on comments below.
R#1
Q1: We will clarify Fig.2 refer to R1,Q3 & R3,Q1 in revised version.
Q2: We fully agree tissue loss is an inevitable issue. But it is highly likely that the corresponding patches in two stains share the same diagnostic label[7]. And we have involved a 20% tolerance of tissue loss when calculating protein aware loss in Eq.3.
Q3: Clarifying PCLS: We pretrain a segmentation UNet on real IHC image and freeze it to extract tumor semantic, aiming to enhance the authenticity of generated tumor in PCLS. Initially, input IHC image is processed by UNet, yielding a seg probability map as output. Feature maps are then derived from the layer preceding the output layer. Using these feature maps and probability maps, we aggregate class-wise prototypes representing pixel-wise features of tumor and background (Eq.7). For semantic interaction, cosine similarity is computed leveraging image features with prototypes from another image (e.g. generated image features and ground truth prototypes) (Eq.8), referred to as cross-prototype. Finally, the similarity map applies softmax (Eq.9) to get probability map of tumor class for calculating loss (Eq.10). 
Q4: We have included pix2pix in results, and cyclegan performs worse than pix2pix in [8].
Q5: The real/fake classification would be a valuable additional test for our method. But our metrics have showed our method has strong diagnostic consistency on two datasets, which is core of virtual staining.
Q6: Please refer to R4,Q1.
Q7: Discussion on metric: We include PSNR and SSIM for the popularity in previous works but these have shortcoming: PSNR is calculated based on MSE, and SSIM, by default, involves sliding a 7x7 window for computation. Hence, they can be affected by spatial misalignment. Specifically, the blurry IHC images with more averaged pixel value may cause them to be inflated (Zhu et al, âBreast Cancer Immunohistochemical Image Generation: a Benchmark Dataset and Challenge Reviewâ). In ablation, PALS and PCLS respectively amplify changes at tumor and pixel level. Such intense changes lead to PSNR and SSIM decrease. We want to emphasize our goal is to highlight tumor area precisely. To supplement this, we involve mIOD, IOD[12,15] to evaluate positive signal and Peason-R[9] to evaluate pathological correlation. These shows our method exhibits stronger pathological consistency in H&E-IHC virtual staining.
R#3
Q1: Clarifying FOD estimation: We use traditional color deconvolution (Ruifrok et al, âQuantification of histochemical staining by color deconvolutionâ) for stain separation. Initially, we apply a logarithm to the IHC to obtain the OD image. Then, color deconvolution involves multiplying the OD image by the inverse of the OD matrix, yielding OD values of H, E, and DAB stains. We specifically select DAB stainâs OD values to generate the RGB image(IHC DAB). For focal OD, we simulate Eq.1 by converting IHC DAB to grayscale and using the focal calibrated map to assign gray values to positive signal(FOD). Limitation of method refer to R3,Q3. We will clarify these in revised version.
Discussion of segmentation model: The seg model is UNet. Our PCLS drives tumor and background features to be orthogonal, so the model which feature has better inter-class orthogonality is beneficial. Considering IHC image highlights tumor area, we want to emphasize UNet has effectively acted as feature extractor refer to R1,Q3.
Q2: Please refer to R1,Q7.
Q3: Limitation of method: Our method is appropriate for all the DAB stained IHC image but not for AEC or other IHC stained image. In [11], many proteins such as ER,PR can be revealed by DAB. Due to test only on Her, we will mention it explicitly in revised version.
R#4
Q1: Correction: We will clarify our method refer to R3,Q1 in revised version. Thanks again for constructive suggestions on expression and figures.
Q2: Reproducibility: We will release our code and pretrained model."
https://papers.miccai.org/miccai-2024/596-Paper1648.html,"Thanks for the valuable comments.

R1, R5: Contribution of PathoTune. Current research on pathological models primarily focuses on the pre-training phase. PathoTune introduces a multi-modal prompt strategy to adapt either the visual foundation or the pathological foundation model to downstream pathology tasks, which is one of the few approaches focusing on pathological PEFT and the first to address domain gaps using varied types of prompts. The two defined foundation gaps are not just renamed, but a new concept proposed in this paper for the pretraining-finetuning era for pathology modeling, where such domain gaps are more significant than natural images. To mitigate these domain gaps, the paper proposes corresponding prompts based on the easy but effective principle, while outperforming current PEFT-based SOTA methods by a large margin, providing a new paradigm for downstream pathological applications.

R3, R5: Comparisons with Non-PEFT SOTA Methods. Due to page limitations, we only present the comparisons with the PEFT-based SOTA methods in the main paper. Actually, we have compared PathoTune with other Non-PEFT SOTA methods. The results of the proposed PathoTune  outperform the current SOTA pathology expertise models, achieving 97.5% vs 95.2% on RJ-Prost, 99.8% vs. 99.2% on NCT, and 97.6% vs 97.1% on SICAPv2. We will include these results in the supplementary material.

R1: Efficiency of PathoTune. It is true that the reduction in the number of parameters does not directly equate to faster convergence or lower training times. However, this is a concern with all prompt-based PEFT methods. Admittedly, compared to adapter, LoRA, and other PEFTs, we believe that prompt is more extensible. As a token on the input side, it allows the design of three prompts with different modals for two domain gaps. This approach also enables the inject of external domain information (e.g., textual descriptions and encoded visual features) into the network. As for the consideration of efficiency, we will add a figure to the supplementary material to show precision-parameter comparison between PathoTune and other methods to verify the balance of these two factors.

R1: Implementation Details. Some implementation details had to be streamlined due to page limitations. BERT is used as the encoder for TTP since experiments revealed that existing medical-specific BERT encoders (e.g., BioLinkBERT, BlueBERT) perform even worse than BERT. The followed projection layer is a single trainable Linear layer. The theta_{VRM} for IVP is trainable and initialized from the first 4 layers of ResNet-18 pretrained on ImageNet. For each dataset, we follow the same data evaluation criteria as other papers. The optimal number of TVP, TTP, and IVP combinations varies, but near-optimal performance is generally achieved with 10, 2, 2. In the final manuscript, we will provide a more detailed implementation description.

R5: Intuition for the IVP tokens. IVP tokens are designed to mitigate Task-Instance Gap. The ideal approach to describe global staining and glandular features is employing a simple visual encoder. Compared to TTP or TVP, IVP based on the original image better portrays coarse-grained instance-level features, complementing the fine-grained flattened patches.

R3: Risk of Overfitting to Prompts. It is challenging to assess whether the model is overfitting the prompts. However, to avoid overfitting to the test data, we have employed 4-fold cross-validation for all datasets except BCI.

R1: Results using Single TVP and IVP. The results using a single TVP and IVP can be seen in Fig. 4.

R1: Reference to Other Pathological Foundation Models. We will add the references to these vision-language foundation models in the final version.

R1, R3: Other Additional Experiments. We recognize that additional experiments would enhance our experimentation. However, due to page limitations and MICCAI guidelines, we had to present the most important ones in our main paper."
https://papers.miccai.org/miccai-2024/597-Paper1788.html,"We thank the reviewers for their insightful comments. Reviewers found our work âpertinentâ and ânewâ (R1), âinnovativeâ and âniceâ (R3), as well as âinnovative,â ârelevant,â and âinterestingâ (R4). We are confident that minor edits can address most of the expressed concerns.

CLINICAL RELEVANCE (R1)
Reviewer 1 mentioned, âThe author must emphasize the clinical relevance of the described pipelineâ and asked for clarification on âhow these methods will advance current clinical practice.â We have elaborated on this in the revised manuscript, highlighting the potential impact of our framework on improving surgical accuracy and patient outcomes.

CLARIFICATION IN EVALUATION (R3)
Reviewer 3 found it difficult to understand the evaluation, which is performed on 2D ultrasound frames but relies on segmentation performed in 3D pre-operative MRI with various annotation protocols. We have clarified this point in the revised version. First, we describe how 3D pre-operative segmentations with different protocols were obtained using 3D pre-operative MR scans. Then, we explain how ground truth annotations were derived for the 2D ultrasound images, via image registration.

We also would like to respectfully refute the claim that âit should be possible to eventually train a real-time iUS segmentation model that works accurately in a non-patient-specific manner and is possibly even safer.â The definition of the surgical target varies across neurosurgeons. Thus, we believe models should be patient-specific (and thus surgeon-specific). Moreover, ultrasound images are inherently ambiguous, and tissue boundaries are not always clearly visible. Our experiments demonstrate that general models and trained neurosurgeons familiar with iUS images struggle to identify tumor boundaries accurately. Therefore, in this work, we propose disambiguating ultrasound data by leveraging pre-operative data. As tumor boundaries in synthetic ultrasounds are not always clearly visible, our framework also relies on the geometry of the patientâs brain to better identify tumors.

ASSOCIATION WITH NEURONAVIGATION (R4)
One of the motivations of this work is the limited integration of ultrasound imaging in current neuronavigation systems. This work aims to assist surgeons in better interpreting ultrasound images by automatically segmenting surgical targets based on pre-operative planning, without relying on neuronavigation systems. We agree that our framework could complement neuronavigation systems that support ultrasound imaging and have added this to the Future Work section:
âFrom an application perspective, we will explore whether our framework can automatically detect significant misalignments between pre-operative and intra-operative data. This could alert surgeons to inaccuracies in the navigation system.â

COMPUTATION COST (R4)
Our revised manuscript acknowledges the limitation regarding the computation cost of training a patient-specific model. However, as Reviewer 3 mentioned, âmodel training can be launched once a pre-operative scan is available.â We also respectfully refute the claim that it ârequires too much input from the surgeon.â Firstly, our method does not require a âplanned craniotomyâ but simulates plausible craniotomies. Moreover, our method is fully automated and does not require any surgeon input if the BraTS protocol is used to identify the surgical target. If the surgeon defines the surgical target, input is needed. At our institution, this annotation process is already part of the routine clinical workflow, where surgeons annotate the surgical target the day before surgery for use in the neuronavigation system."
https://papers.miccai.org/miccai-2024/598-Paper3528.html,"We would like to thank the reviewers for their valuable feedback. We are committed to ensuring the full reproducibility of our work - the code repository will be made available and the dataset used in our study is publicly accessible at https://hecktor.grand-challenge.org/Data/. All typographical errors identified will be corrected in the final camera-ready version.[R1.1] Our work addresses a specific challenge in cancer diagnosis and prognosis - integrating two primary imaging modalities namely, CT and PET scans. The need for publicly available registered multimodal data makes the HECKTOR dataset particularly suitable for our study.[R1.2] According to [1], PET/CT fusion is a rapidly growing technique in medical imaging, underscoring the relevance of our focus on these modalities. The potential of our work could even be extended to the integration of a different type of medical imaging modalities, such as PET scans captured with different tracers https://autopet-iii.grand-challenge.org.[R1.3] We will revise the PEMMA framework description to clarify the changes in the network architecture.
[R1.4] As indicated in the title, our goal is to improve parameter efficiency, i.e., reduce the number of parameters to be modified. The main real-world benefit of PEFT is the reduced cost of storing model parameters. Since only a small fraction of parameters in a large model are modified, it is possible to store all the fine tuned models, thereby providing more flexibility at inference time to pick the most appropriate model.[R1.5] Most existing cross-modal fine-tuning methods including ORCA are de- signed for a single static fine-tuning task. In our work, we consider a scenario where additional batches of data from diverse medical centers need to be incorporated dynamically. Since this new data could be from one or more modalities, we need to avoid cross-entanglement so that both modalities can be updated independently.[R1.6] Since early fusion requires both modalities at the input level, all the model parameters need to be modified to handle this change. For late fusion, a completely separate model is required to handle the new modality and the existing model (for first modality) also needs to be finetuned based on new data. Thus, freezing the model parameters will lead to significant performance degradation in both early and late fusion. Therefore, the comparison between the three fusion approaches is fair.[R1.7] It is difficult to find many large multi-modal and multi-center datasets, but in future, we plan to test on other modalities and datasets too.We thank the [R3] concern for pointing out where exactly to focus in Table 2, where we consolidate all findings to demonstrate continuous performance across different testing scenariosâinitially training and testing on both modalities, then on each independently. We highlight these key outcomes: There is a significant reduction in trainable parameters, with PEMMA showing a 92% reduction compared to the 92.5 million parameters of the standard U-NETR architecture. (highlighted in red in Table 2). The efficacy of LoRA in preserving information across modalities is evidenced by comparative results between early fusion and PEMMA approaches on new datasets from HGJ and HMR centers. (Here, we compare the results of the last rows of early and PEMMA fusion in column P. New dataset 1 and 2 correspond to HGJ and HMR centers, respectively.)
[1] Fahim-Ul-Hassan, Cook GJ. PET/CT in oncology. Clin Med (Lond). 2012 Aug;12(4):368-72. doi: 10.7861/clinmedicine.12-4-368. PMID: 22930885; PMCID: PMC4952129."
https://papers.miccai.org/miccai-2024/599-Paper0253.html,"We appreciate all reviewersâ valuable comments and suggestions. All reviewers (R1/3/4) agree that (1) âthe problem of contrast-agnostic learning with pathologies is interesting, important, and impactfulâ; (2) âthe idea of data generation from labels and pathology encoding is novel, interesting and well-motivatedâ. R1/3 appreciate that âextensive experiments support PEPSIâs superiorityâ on various public datasets and pathologies. R4 values that PEPSI has âgreat reproducibilityâ, âstrong promises in clinical applicationâ, and can âintrigue valuable discussion at MICCAIâ.

First, we would like to address two comments from R1:

Modeling of partial volume effects: the âcorruption pipeline [15]â referenced in the last paragraph of Sec. 2.1 includes a model of partial voluming from [2]. We will clarify this in the camera-ready version. Partial voluming is further simulated by the continuous mixing of pathology and healthy tissue proposed in our work.

Results:
â¬ âNo-Segâ outperforms âDir-Segâ?: While the extra segmentation loss in Dir-Seg enables better representations overall, it penalizes metrics in Table 1 which only evaluate image synthesis (this is consistent with Table 3 in [21]).
â¬ General performance of PEPSI: We agree that although PEPSI consistently outperforms competing methods in Table 1, it does not achieve large improvements in every setup. We will soften our claims in Sec. 3.1 in the camera-ready version.

We would also like to clarify other points made by the reviewers:

Rationale for pathology probability generation (R1): We use a heuristic that was shown in [18] to improve performance in modeling white matter lesions, where pathology is typically darker in T1w and brighter in T2w/FLAIR. In the camera-ready version, we will rephrase the description above Eq. (1) (and cite [18]) to clarify this.

Segmentation network in Eq. (4) (R3/4): We want the segmentation model to work wellonlyif the inputs (both healthy and pathological tissue) are realistic and of good quality; if the segmentation network provides good labels for any image, it would be uninformative for the synthesis. Thus, we train a segmentation network using data with minimal corruption. We will add a footnote under Eq. (4) to clarify this.

Appearance model of pathology class (R4): The intensities of the voxels of this class are modeled not only by Gaussian (which has been proven successful in prior work [18]) but also by a voxel-wise probability map (Eq. (1)) that modulates the mix of pathological and healthy tissue. This allows us to simulate a wider range of lesion appearances, including the transition from healthy tissue to lesions, while retaining full control of the generative model - which is crucial for domain randomization.

Model generalizability (R4): while our model takes advantage of domain randomization techniques to yield models with state-of-the-art generalizability [2,15,18], we acknowledge that it may falter on images with lesion patterns that are very different from those seen in training. We are actually working on a lesion generator that helps models generalize better. We will add a brief discussion of future work in the camera-ready version.

Finally, there were some comments for clarity:

Figure caption on Page 5 (R1/3): We apologize for this. We will add the caption: âLeft: an axial slice of a FLAIR scan from ISLES dataset, with WMH marked in red. Right: corresponding gold-standard segmentation of abnormalities, which only includes stroke lesions (no WMH).â

Notations (R4): We will change $I^{anat}$ & $I^{pathol}$ to $I^{T1}$ & $I^{T2/FLAIR}$; We will add definition for $\tilde{S}$: âthe predicted pathologyâ.

Fig. 3 caption (R3): We will rephrase to: âQualitative comparisons on T1w and FLAIR synthesis (â highlights pathologies). Rows (columns) refer to datasets (compared methods).â

Training time (R3):  We will add a short sentence to Implementation Details: âTraining took ~5 days on an NVIDIA RTX8000 GPU.â"
https://papers.miccai.org/miccai-2024/600-Paper3008.html,"(R1&R3&R4) We thank all the reviewers for their careful considerations and beneficial suggestions. 
(R1&R4) About nnUNet. Our method exhibits a performance decrease on the Synapse dataset compared to nnUNet. Conversely, on the ACDC dataset, our approach demonstrate an improvement over nnUNet, with a HD reduction of 6.1% and a DSC increase of 1.0%. These contrasting performances highlight the distinct advantages of our method and nnUNet across different datasets. Furthermore, it is important to note that our method adopt the MISSFormer framework and follow a similar data augmentation paradigm as previous works, while nnUNet employed a stronger augmentation scheme, we refrain from direct comparison with nnUNet in the paper to ensure a fair evaluation.
(R1) Fair comparison. Our method follows the framework of MissFormer, and the division of training and testing sets is identical to previous work. We believe there is no unfair comparison. Supplementary materials include the source code and list for dataset division utilized in this study. Additionally, we commit to making all code and pth file publicly available to ensure the reproducibility and fairness of our results if our paper is accepted. 
(R1) Comparison between 2D and 3D methodologies. In the comparison between 2D and 3D methodologies, our analysis employs a slice-wise evaluation approach as previous works. 
(R1) Performance of our method on the liver and spleen. These organs possess larger volumes and smoother boundaries, which generally reduces the complexity of segmentation. This, indeed, contributes to our method achieving DSC accuracies of up to 0.96 for these organs. It is common for larger organs with smoother boundaries to score higher in segmentation tasks, a trend that can be observed in other segmentation studies as well.
(R3) Latency. Our method can perform segmentation on one 512x512 medical image frame in less than one second. The computational time of our approach is comparable to small model such as Swin-Unet, thereby satisfying the real-time inference requirement for clinical applications of medical image analysis.
(R3) Ablation Study. Due to the page limit, we can only show four of the eight ablation settings. The ablation study consistently highlights the efficacy of the BPRB and SCSI modules in enhancing performance. Full model incorporating all three modules achieves the highest performance across both evaluation metrics. The configuration with only BPRB and SCSI modules ranks second, outperforming the individual use of either BPRB or SCSI alone. Additionally, configurations incorporating BPRB module exhibited relatively higher segmentation accuracy, with an approximate 0.1% improvement in DSC, and lower boundary errors, with a reduction of around 21.3% in HD, compared to those without BRPB. 
(R4) Theoretical analysis of ENLTB. The main idea of ENLTB is to employ linear approximations for reducing the computational complexity of non-local attention modules. Under nonlinear transformations, salient regions in feature space exhibit robustness to minor perturbations, retaining their saliency in the approximate representation. Although linear transformations cannot fully replace the original attention mechanism, they are sufficient in distinguishing critical differences between background and salient objects. This approach enables emphasizing feature points with the most substantial impact on the final task while minimizing the effect on segmentation quality. Besides, mapping high-dimensional features to a low-dimensional space facilitates rapid identification of significant features in the high-dimensional space.
(R4) About nnFormer. On the Synapse dataset, the nnFormer surpass our modelâs performance. However, on the ACDC dataset, our approach demonstrate superior results, exhibiting a marginally higher DSC by approximately 0.5% and a smaller HD by approximately 6.1%."
https://papers.miccai.org/miccai-2024/601-Paper3664.html,"We sincerely thank all the reviewers for providing insightful comments. We extracted the major concerns and consolidated the opinions from different reviewers. Below, we provided our responses to each point.

Reviewer #1âs fifth concern and Reviewer #4âs first concern: Both reviewers mentioned the limited novelty of our work. Reviewer 1 noted that âthis is not the first use of 3d networks within DDPM for medical images: Z. Dorjsembe, et al. âThree-Dimensional Medical Image Synthesis with Denoising Diffusion Probabilistic Modelsâ.â Reviewer 4 mentioned that âthis work maybe a nature extendation from the previous Gongâs work (ref [8]), that is from 2D DDPM to 3D.â
Response: Dorjsembe et al.âs work utilizes an unconditional 3D DDPM for brain MRI synthesis, while our work adapts the 3D DDPM to conditional image generation. The low-dose PET images were incorporated as additional network inputs of the score function in the training stage and guided the generation of corresponding high-quality PET images in the sampling stage. This conditioning process ensured that the DDPMâs outputs were specifically tailored for particular applications with clinical significance.
Regarding Reviewer 4âs comment, Gongâs previous work only focused on 2D brain images; our research targets 3D whole-body PET images, which have a more complex data distribution due to the varying levels of radiotracer uptake across different organs, making the denoising task more demanding.

Reviewer #1âs first and second concerns and Reviewer #4âs second concern: Both reviewers mentioned the limitations of using 3D UNet as a comparison model and suggested including 3D GAN as an additional comparison.
Response: We will incorporate the 3D GAN comparison results in the camera-ready file. As for the 3D UNet model, given that it served as the backbone of our approach, comparing its performance with our method is particularly insightful, which helps illustrate the advantages of the diffusion model over traditional convolutional network-based methods.

Reviewer #1âs second concern: Reviewer 1 mentioned: âQualitative assessment appears limited. Were other images visually graded or compared in the test set, or was only this single image looked at?â
Response: We conducted the qualitative evaluation on all test set data, and our method consistently demonstrated superior performance. Due to space constraints, we only presented the results of a randomly selected test image in the manuscript.

Reviewer #3âs first concern: Reviewer 3 mentioned that the paper does not present any metric to evaluate whether the images are overly smoothed. This is evaluated only by visual analysis.
Response: We used PSNR and SSIM for quantitative evaluation. SSIM considers luminance, contrast, and structure to assess the structural similarity between the denoised image and the ground truth image. If the denoising process overly smooths the image, it will reduce local contrast and edges, leading to a decrease in SSIM, which indicates potential oversmoothing.

Reviewer #3âs second concern: Reviewer 3 suggested, âThe comparison with other methods could also be extended to adaptive filters, such as Total Variation Minimization, to demonstrate the benefit of training a CNN in comparison with more direct methods.â
Response: For PET imaging, the image noise is very different from MR and CT imaging due to the limited photon count received. As a result, total variation (TV) minimization-based methods are not widely used in PET imaging, due to the patchy and cartoon shapes in the final images.

Additionally, we appreciate all the reviewersâ constructive suggestions, such as including more training details, descriptions of the uncertainty map, and relative error maps. We will address the reviewersâ concerns as much as possible within the camera-ready fileâs length constraints. We are also planning to open-source our code and trained models soon."
https://papers.miccai.org/miccai-2024/602-Paper0563.html,"Thank you to the three reviewers for their constructive feedback. We will revise the final version of the paper according to your requirements.

We thank Reviewer R3, R4, and R6 for the in-depth discussion, constructive suggestions, and endorsements for our contributions: (1) a personalized framework named pFLFE for medical image segmentation, (2) an alternative fast-converging framework, and (3) extensive experiments on 3 segmentation tasks. We are encouraged that our framework is ânovelâ (R3, R4), âeffectiveâ (R4, R6), and âreasonableâ (R4), the experiments arecomprehensive"" (R3, R4, R6), and the paper iswell-writtenâ (R3, R4, R6). We will release the code upon paper acceptance.

[R3Q1, R4Q2,R6Q2]. Two-stage training process efficiency.
A training phase in pFLFE includes two communication rounds, whereas other methods include only one communication round per training phase. To ensure fairness, we evaluated our method over 50 training phases (ours, 100 communication rounds) against other methods over 100 training phases (theirs, 100 communication rounds) in experiments. Besides, in our two-stage training, the supervised learning stage uses Unet for fully supervised training which is similar to other methods. In the local enhancement stage, we replace the decoder with a smaller linear layer, which will reduce the computational resources and complexity. The results in Fig. 4 show that we achieve better performance with fewer communication rounds, which means our approach converges faster and more efficiently in training.

[R3Q2]. Generalizability concerns of the personalized framework.
Similar to the previous methods, in our design, the encoder of the network is shared during inference, with the personalized part being only the decoder. Although in the contrastive learning stage, we perform feature enhancement for each clientâs data, the goal of this stage is to train the encoder, whose parameters will ultimately be aggregated and shared among all clients during inference. Therefore, our design does not introduce additional generalizability concerns compared to previous methods. We demonstrate the generalization in three different segmentation tasks, as shown in Tab. 7, Sup Tab. 1, and Sup Tab. 2. The experimental results indicate that our method improves generalization compared to previous methods.

We appreciate your constructive ideas on the extensiveness of our experiments, additional representation learning methods, communication rounds reduction, and multi-modal design. Your suggestions have greatly inspired our future work.

[R4Q3]. Why only show the learning curves of the three compared methods?
FedAvg, FedRep, and LG-FedAvg are the most widely used frameworks in this area. Comparison with those 3 frameworks would help us better understand the effectiveness of our method.

We thank R4 for the constructive and insightful comments on the training resource analysis and experiment details!

[R6Q1]. Data augmentation.
The data augmentation we use is served as a standard approach in contrastive learning. As contrastive learning is a part of our newly designed framework, comparison with previous methods using data augmentation would not bring unfairness issues. Moreover, for the supervised learning part, our approach does not involve any form of data augmentation, which is the same as previous approaches. This would further ensure fairness in comparison."
https://papers.miccai.org/miccai-2024/603-Paper1221.html,"We would like to thank all the reviewers for their constructive comments.

Q1:Reproducibility of the paper
A1:We will release the code and models on GitHub if paper is accepted.

Q2:Add detailed experiments of OWC
A2:We implemented OWC using adaptive dynamic weight adjustment. This method focuses on individual model predictions and adjusts weights based on the input dataâs contribution. Hereâs the process: 1) Feature Integration: Concatenate the feature vectors Z_i from all models into a combined vector  Z=[Z_p,Z_g,Z_h]. 2) Optimal Weight Learning: Use a small neural network to learn optimal weights W=[w_p,w_g,w_h] are finally determined to maximize the performance of the combined model. 3) Concatenation with Weights: Once the optimal weights are determined, apply these weights to the corresponding feature vectors. The final combined feature vector is Z_h^â=[w_p Z_p,w_g Z_g,w_h Z_h].

Reviewer#3:
Q3:Several details of the proposed method are unclear
A3:In Section 2.1, our inputs consider extracting pathological images and genomic information separately. Pathological images were scaled to 512 Ã 512 pixels and trained on a 20x magnification region using a CNN. Genomic data was obtained from cBioPortal and a normalization layer in a self-normalizing network in Klambaeur et al. was applied to reduce overfitting.

Reviewer#4:
Q4: Clarify whether OWC significantly contributes to improving survival prediction
A4:1) Performance Improvement: Though the gains may seem modest when comparing LMF only and LMF + OWC in Tables 2 and 3, given the C-value of 0.891, significant enhancements are challenging to achieve. 2) Statistical Significance: Despite the minor improvements, The p-values in Tables 2 and 3 confirm the significance of these improvements. 3) Robustness: While concerns about variability from different random seeds are valid, repeating experiments with different seeds yielded consistent results. Additionally, employing fifteen-fold cross-validation enhances model stability.

Q5:The results of simply using a concatenation operation after MAM
A5:Sorry for not showing the results of simply using a concatenation operation. But we have done the related experiments. In the GBMLGG and KIRC datasets, direct concatenation after MAM gave C-values of 0.876 and 0.709, while the OWC method achieved 0.881 and 0.718. These results show OWCâs superiority over simple concatenation.

Reviewer#5:
Q6:Advantages over Transformer-based methods
A6:1) Transformers require large pre-training data, but our method handles multimodal data effectively without extensive pre-training, suitable for small medical samples. 2) Transformersâ computational complexity is a challenge, but our fusion technique efficiently manages complex medical data feasibly. Further work will explore transformers.

Q7:Features dimensions reduced from exponential to linear level
A7:In simpler terms, TFN is a tensor outer product and then fully connected process, whereas we no longer do the same as the TFN network, but instead perform a linear transformation for each mode separately. This is then followed by a multidimensional dot product, which essentially combines the results of multiple low-order vectors. This approach greatly reduces the number of parameters in the model while maintaining the validity of the model.

Q8:Some technical details are not described in detail.
A8:Due to word limit, only part of key descriptions have been demonstrated.

Q9:Why is a ResNet-50 used for feature extraction?
A9:In order to be consistent with the pathology feature extraction in the comparison experiments.

Q10:Which embedding vectors are chosen? At which layer?
A10:We extract the embedding vector Z_p,Z_gâR^(32Ã1) from the last hidden layer of the training network, using it as input for MLIF.

Q11:Please specify what ID number you are referring to?
A11:The ID number here is a unique identifier for each patient in the public dataset."
https://papers.miccai.org/miccai-2024/604-Paper1948.html,"We thank all the reviewers for their thoughtful comments and constructive remarks.
Reviewer #1:
1-We apologize for the size of the figures; we will make them bigger in the revised version and will also highlight the regions where the phenotypic variations are visible. Indeed In Fig1 some clusters of cells do disappear, this is expected as the treatment is toxic and kills the cancer cells. Our model thus has faithfully replicated the effect of the treatment on the image.
2-The aim of the method is not to reproduce the treated image shown in the figures, as mentioned. The goal is to simultaneously see what the same cell would look like with and without treatment by image synthesis, as this can not be done experimentally. Therefore we expect that the generated images of treated cells will not show the same cells as a real image but instead artificially display the same treatedphenotypeon the real untreated cells. Fig2a shows a clear phenotype: the fragmentation of the Golgi apparatus (green channel). We can see that in both the generated and real treated images, the apparatus is splitted whereas it appears aggregated in the untreated images (one big spot). Similarly as shown in Fig3 and quantified in Fig4 the phenotypes related to different treatments are also recapitulated in synthetic images.
3-We respectfully disagree with the reviewer about the statistical significance of the boxplots. The distribution of treated versus untreated cells is significantly dissimilar as validated by the p-values of t-tests (p=1.0110^-28 between conditions using real images and p=1.0910^-14 between conditions using generated images) so the conclusion of the experiment would be the same whether using generated or real images. We will add these p-values to the plot. Furthermore, Fig4 shows a strong correlation between the features extracted by CellProfiler in the real and generated treated images, quantitatively confirming that our method faithfully reconstructs biological features.
5-We will move the table and add more information about imaging type, relevance, patient demographics as well as clinical feasibility in the revised version.
6-For CycleGAN, we did not perform any adaptation as it can translate real images. For Phenexplain (which is based on StyleGAN), we adapted it for real images by implementing the state-of-the-art technique for StyleGAN inversion (iterative refinement of latent codes) and using both W and W+ spaces to balance image fidelity and editability.
7-The datasets used in the paper are the ones used by Phenexplain, which is the reference method. 
8-We also want to emphasize that the source code of our method is publicly available, as is the BBBC021 dataset, ensuring the reproducibility of our paper.
Reviewer #3:
We thank the reviewer for the positive feedback. We will add the parameters used in training our model in the revised version.
Reviewer #4:
We thank the reviewer for the positive feedback on the analyses we performed in our paper. 
1- PhenDiff represents a new application of diffusion models to spot subtle phenotypes in microscopy images via image-to-image translation which is very useful in biology. Importantly, we do not claim that PhenDiff is a new architecture of diffusion models. We will clarify it in our revision.
2- The generation process in Phendiff allows the generation of a new image starting from a random noise and a selected class. The inversion is the reverse process that enables the manipulation of real images. This allows us to see the effect of a treatment on a real untreated image, unlike Phenexplain which is based on GANs and cannot invert a real image. The inversion starts from x_0 to x_T and the generation starts from x_T to x_0, we will correct this in Fig1.
3- We have actually addressed this in Fig3. In addition, Fig4 compares hundreds of quantitative features measured on both real and generated images of treated cells to demonstrate their strong similarity. We will better emphasize it."
https://papers.miccai.org/miccai-2024/605-Paper2374.html,"We would thank all the reviewers for their constructive comments.

R1Q1/R3Q1: Test dataset.
We trained our model based on 2D dMRI from the HCP S1200 dataset. Each subject contains 135 b-vectors with 110 slices at each b-vector. Therefore, we have included 1351109 slices altogether. Due to the long training time, we were only able to include nine subjects, in the same settings as the previous works (Ren et al., 2021). We will include more datasets in our future works.

R2Q1: Lack of description of the comparative experiments.
We would thank the reviewer for this comment. Due to the page limit, we were unable to include these details. In the accepted version, we will provide a more detailed description.

R2Q2: More detailed ablation experiments.
Our model contains three main modules, therefore we performed three ablation studies. We would thank the reviewer for the insightful comments and will consider adding more ablation experiments in the future.

R3Q2: Experimental setup.
We would thank the reviewer for this comment. In the training phase, we set the maximum epochs of 80 and used the early stopping techniques to avoid overfitting. For the objective functions, we included these in section 2. In the testing phase, we tested a subject using b-values of 1000, 2000 and 3000 individually, with their combined set. We will add these details in section 3 in the accepted version.

R3Q3: Table 1 reference. Specific values for arbitrary bn.
We will reference Table 1 in section 3.3, and provide more detailed clarification.
Arbitrary bn means we used all b-values in {1000, 2000, 3000} when training and testing, instead of using specific b-values. The selection of these 3 values is due to the setting of the dataset, we plan to test more datasets with more b-values in the future.

R3Q5: Explanation for PSNR.
PSNR calculates the pixel-squared error, and high values may blur important edges and texture details, while SSIM considers the structure of the image, similar to human vision. In our experiments, we observed that images with high PSNR and low SSIM appeared overly smooth. We inferred that if the SSIM significantly decreases after processing (e.g. using L2 optimisation, which is relevant to PSNR), it usually indicates that the structural information has been somewhat damaged. This is likely due to excessive smoothing, resulting in the loss of details and textures. In clinical diagnosis, using images losing important anatomical details is problematic. We will further explain this in the accepted version.

R3Q6: Error map.
We would like to thank the reviewer for this comment. In the accepted version, we will further improve this figure and add a color bar."
https://papers.miccai.org/miccai-2024/606-Paper0345.html,"Thanks for the valuable comments and suggestions. Below we respond to the comments.
Code available: According to MICCAIâs requirements, our code will be provided in the final version of the paper."
https://papers.miccai.org/miccai-2024/607-Paper0605.html,"We would like to thank the reviewers for their thoughtful evaluation of the manuscript. We will incorporate small requested clarifications into the final submission. Additionally, the signal modeling code will be made open source and available on GitHub."
https://papers.miccai.org/miccai-2024/608-Paper1914.html,"We thank the reviewers for their constructive comments and positive assessment (ânicely leverages the multiple contrastsâ - R1, âwell-motivated and clever wayâ, âquite novel, and seems to work wellâ - R3, âproposed physics loss novel and effectiveâ - R4).

We would like to address their main comments as follows:
[R1] More comparison methods: We would like to highlight that our choice of comparison methods was based on the need to correct for motion-induced B0 inhomogeneity changes for T2*w GRE MRI. Most other motion correction methods, e.g. SAMER, only correct rotations and translations and are thus not applicable to our problem.
[R3, R4] Validation: 1) We thank R3 for recommending to include perceptual losses. To the best of our knowledge, such losses have not been comprehensively validated for MR motion artifacts. Since we wanted to report commonly known metrics and avoid âmetric-pickingâ we refrained from including perceptual and feature losses. We will include such losses in a future extension if further studies show a correlation with radiological evaluation. 2) We agree with R3 that evaluation with simulated data would avoid registration problems. However, realistic motion simulation - especially in regard to secondary effects like B0 inhomogeneity changes - is challenging. The inclusion of simulation results with a fair discussion and setting it into context with real motion results was not possible due to space constraints, but will greatly add to an extension of this work. 3) We agree with R4 that real motion patterns are indeed very diverse, which is why we instructed 6/7 test subjects to move randomly throughout the acquisition without any guidance from our side. As any methodological work, this should be seen as a proof of concept and a more comprehensive evaluation needs to be performed before clinical translation.
[R4] Ablation of physics loss: We would like to clarify that we have provided ablation results for the physics loss in the manuscript by comparing PHIMO to OR-BA, which aggregates the reconstructions of random masks without leveraging multi-echo information. A pure varnet-like approach (without excluding specific k-space lines) would apply data-consistency checks with the motion-corrupted k-space data and thus counteract the denoising step.
[R4] Only applicable to multi-echo MRI: We would like to emphasize that quantitative MRI (Relaxometry, QSM, Fingerprinting) is of growing interest in the community and reconstructing multi-echo data is more challenging than standard 2D images, offering many potential applications for PHIMO.
[R1, R3] Practical aspects: PHIMO addresses the critical issue of reducing acquisition time which stands in contrast to most MR reconstruction works focusing on reducing the reconstruction time. Further, the computational burden is a limitation of most self-supervised MR reconstruction methods. We are certain that further efficiency enhancements will also reduce the reconstruction time.

Minor points:
[R1] Motion-correction in title: We would like to clarify that by excluding motion-corrupted k-space lines from the data-consistent reconstruction, we are performing implicit motion correction in line with other MoCo works (Oh et al. TMI 2021, Oksuz et al. MICCAI 2019).
[R1] Correlation as physics loss: In our experiments, correlation of fit and signal has shown to be more effective than e.g. MAE between fit and signal. However, we agree with R1 that other loss functions might be worth investigating in the future.
[R1] Acceleration factor. None, we have acquired fully sampled images.
[R1] Main results in supplementary material: Visual examples for 2 subjects are provided in the main article and only additional examples in the supplementary material.
[R1] Structure not very clear: The comment seems contradictory to the other reviewerâs judgments: âwritten very clearlyâ - R3, âclearly presentedâ - R4.
Remaining minor issues: we will incorporate the helpful suggestions space permitting."
https://papers.miccai.org/miccai-2024/609-Paper3403.html,N/A
https://papers.miccai.org/miccai-2024/610-Paper4115.html,"We are grateful to the reviewers for taking the time to review our paper and providing constructive feedback. We have grouped the comments into four categories and later addressed some of the individual questions:

Ground truth mesh: To obtain a high-resolution volume, the sagittal and coronal sequences were fused using multi-planar fusion as described in [Castro-Mateos et. al, 2014]. This was done only to train and validate the model and is unavailable in clinical routine. The discs in the resulting high-resolution volume were annotated by clinical experts

Novelty: We appreciate the concern about the novelty of differentiable rendering (DR), but our work makes a contribution by applying DR to perform 3D shape reconstruction from scans, which has not been done before. This specific application, along with our novel hybrid rendering approach utilizing depth maps and silhouette images, has not been explored before in the medical domain. Additionally, we have shown that the reconstruction of meshes from rendered images can be beneficial for 3D reconstruction from sparse annotations.

Reproducibility: We acknowledge that reproducibility is an important aspect, and we will address this by sharing the dataset, the DL model, and FEM scripts upon acceptance of the manuscript. Moreover, a web-based cloud platform to perform simulations directly from MRIs will be made available.

Anonymization: We apologize if some statements were unclear due to the anonymity. We hope that the following explanations regarding certain aspects of our previous works will be useful for your evaluation.

R1Q1: Similarity % and morphology mm?: âSimilarity %â refers to a similarity score based on Hausdorff distance (defined in our previous publication). We will include the definition in the caption of Tables 2 and 3. âMorphology mmâ refers to the height of the disc. We will change it to âHeight mmâ to make it clearer. Table 2 compares the similarity score of our morphed models with those obtained by DL.

R1Q2: Segmentation and interpolation: We apologize for the lack of segmentation methods in the results section due to limited space, and our focus was on integrating a DL pipeline with FEM results for direct clinical practice. Segmentation approaches such as [Turella et. al, MICCAI 2021; Isensee et al, 2021], processed by marching cubes are available in [1], and we found that segmentation excels at achieving high Dice scores but struggles to capture unique geometric variations in each disc, which is crucial for our simulation requirements.
R2Q1: Interpretation of Table 3: Table 3 shows the similarity score, disc heights, and the principal stresses in the three defined volumes (PTZ, CNP, and ATZ). These results correlate with those observed in our previous publication, in which there was no linear relationship between stresses and mid-height, as other publications suggested (Urquhart et al., 2014; Tavana et al., 2024), highlighting the need for a holistic analysis of the problem. This is mentioned in the discussion, where the reference to Table 3 will be included.
R2Q2: PTZ, CPN, ATZ manually obtained?: The Transition Zone (TZ) in our FE mesh structure emerged from a need for computational stability. This TZ exists, as revealed by quantitative MRI, synchrotron imaging, cell phenotypes, and composition measurements through the IVD (Marchand and Ahmed, 1990; Bruehlmann et al., 2002). Defining this region resulted in a more realistic description of the IVD, unlike the abrupt change of material properties from the NP to the AF used in many FEMs. Nonetheless, the TZ is interesting, representing a volume of increased radial compression at the periphery of the inner IVD, where the nucleus pulposus material is pressed against the confining annulus material due to the lateral expansion of the nucleus under mechanical loads. This mechanical particularity coincides with the emergence of early signs of tissue disorganization in IVD (Smith et al., 2011), which is relevant."
https://papers.miccai.org/miccai-2024/611-Paper0193.html,"To R1: 
We appreciate R1âs overall positive comments, in particular, several insightful suggestions for improving our work, including further validation on larger data sets, more ablation studies and practical benefits by combining Gleason groups. We would also like to clarify that the comparison with human radiologist performance does not only demonstrate the diagnostic value of the proposed approach alone, but providing an interesting evidence to support combining ML models and radiologists, e.g. acting as a second reader.

To R3: 
We thank R3 for her/his positive recommendation, with a number of suggested references to include. We would like to clarify that the proposed method assumes a functional form on the distribution of the change of ordinal categories. No further assumption is made between classes, therefore these categories are not necessarily positively correlated with or monotonic to input variables.

To R4: 
We thank R4 for valuable comments and address the issues below. We first clarify the difference from the prior Poisson-based study[1]. The key differences are the label encoding, the loss function and the incorporation of a contrastive learning strategy. First, [1] proposes one-hot encoding where all classes are encoded equally and orthogonally, while we introduce Poisson label encoding, which incorporates the order of Gleason group into the encoding. Second, [1] employs cross-entropy loss, which focuses solely on the numerical ground truth and penalizes mispredictions without considering the class order. As grading prostate MRIs is a challenging task with limited data, we propose Poisson Focal Loss, which encourages the model to adapt to the label distribution and focus on hard samples. Additionally, considering the diverse appearances of prostate lesions in MRIs within the same group, we introduce contrastive learning. In the ablation study (Sec 3.2), we denote [1] as Poisson-based Prediction, and the results in Tab 3 demonstrate the effectiveness of our approach over [1].

We agree that summarising the key differences between [2,3] and our method would highlight the novelty. Our approach diverges from them in two key aspects. The first one is the task and clinical relevance with different data and ground truth. [2] and [3] segments clinically significant lesions based on Gleason grade and PI-RADS respectively, while in our work we use the estimation of the Gleason groups from MRI scans. We formulate our problem as a classification task to reduce the otherwise required subject-level biopsies. As the different tasks and evaluation methods, the performance cannot be compared directly. The second one is the motivation and method.  Method [2] focus on instance detection and segmentation evaluation while [3] evaluates the multi-modal shift and data augmentation. Our work focuses on ordinal classification and class dependency, unknown inter- and intra-class distributions. In addition, our study was validated on different types of ground truth based on saturated biopsy, for the first time, and is arguably more representative of true disease status.

We agree with the reviewer that any potential dependency between these transitions may not be modelled by our approach, but its significance remains unknown and unobserved from existing clinical data. We model the prediction distribution using a Poisson distribution to encourage the model to learn the underlying order of classes, which seemed to provide a superior alternative to the existing i.i.d. classes assumption.

We would also like to thank R4 for other suggestions including testing other ordinal classification approaches, practical memory bank size (which is currently configured empirically to the size of available training samples) and validation on further data sets such as ProstateX."
https://papers.miccai.org/miccai-2024/612-Paper0697.html,"Parameters and Run Time (r1,r4): Polyp-Mamba, with 16.4M parameters, achieves approximately 90 FPS real-time segmentation efficiency, outperforming PraNet (32M parameters, 50 FPS) and PolypPVT (25.08M parameters, 48 FPS) on a single RTX 4090. Additionally, Polyp-Mamba has a lower computational complexity, with 7.99G FLOPs at an input size of 352Ã352, compared to PraNetâs 13.01G and PolypPVTâs 10.0G. In terms of training time cost, Polyp-Mamba takes 20 minutes, while PraNet requires 2 hours, and PolypPVT takes 45 minutes. These results indicate that Polyp-Mamba maintains high segmentation performance while being more efficient for real-time applications.

Fair Comparison (r1): To ensure a fair comparison, the combined loss functions are consistent with those used in PolypPVT and PraNet. With an input size of 352Ã352, Polyp-Mamba achieved a Dice score of 0.941 on the Kvasir dataset, 0.946 on the ClinicDB dataset, 0.833 on the ColonDB dataset, 0.820 on the ETIS dataset, and 0.923 on the EndoScene dataset. These results demonstrate that Polyp-Mambaâs performance remains stable across different input sizes and consistently outperforms other models in all cases.

Loss Functions (r1): The combined loss functions used in our experiments are consistent with those employed in PolypPVT and PraNet. The Binary Cross-Entropy Loss, widely used for binary segmentation tasks, effectively measures the pixel-wise error between predictions and ground truths. Additionally, the Weighted IoU Loss addresses class imbalance issues, ensuring that regions with fewer pixels, such as polyps, receive appropriate attention during training. This combination allows for accurate and balanced segmentation performance.

Major (r3): (1) The semantic inconsistency between different layers is primarily due to differences in feature abstraction levels, receptive field sizes, information processing, and fusion methods. Cross-scale dependency refers to a modelâs ability to process features at different scales. Addressing cross-scale dependency means that the model can effectively combine and utilize information from various scales to capture features of an object at different resolutions. (2) The SAS module processes input features through multiple VSS blocks (from VMamba), which are designed to handle a wide range of contextual information, thereby capturing semantic information at different scales. By exchanging information and integrating semantics across multiple scales, the SAS module enhances the modelâs understanding of multi-scale semantic information. (3) The GSI module uses a cross-attention mechanism to inject global semantic information into local features, allowing the model to combine global and local features effectively. This process resolves semantic gaps before feature fusion, helping to maintain consistent feature representation across different scales.

Minor (r3): Thank you for your suggestions. We will: (1) Simplify the module structure and add annotations and explanations; (2) Rewrite Section 2.3 to emphasize the main structure and provide a clearer description; (3) Add feature visualization diagrams to illustrate semantic gaps and their solutions; (4) Include visualizations and analyses of hidden layer features in the experimental section to demonstrate the improvements from cross-scale feature fusion.

Methods for Handling Variations in Imaging Conditions During Colonoscopy (r4): The approach includes multi-scale feature fusion and data augmentation. Polyp-Mamba uses the SAS and GSI modules for multi-scale feature fusion, effectively handling variations in image quality or blurriness. The SAS module captures semantic information at different scales, enhancing the modelâs robustness to features under various imaging conditions. Additionally, we use data augmentation techniques such as rotation, translation, and blurring during training to simulate different imaging conditions, improving the modelâs adaptability."
https://papers.miccai.org/miccai-2024/613-Paper3555.html,"We thank the reviewers for their valuable comments and suggestions on our paper. Here are the responses:

R1R3R4 - âAbout the clarification of âsupervisedâ and âunsupervisedâ methods used in Table 1â: We apologise for the confusion. In Table 1, âsupervisedâ and âunsupervisedâ are not referring to supervised and unsupervised learning for model training. âsupervisedâ in Table 1 refers to the approaches using an external motion sensor as the supervision signal for pose estimation, and âunsupervisedâ refers to sensor-free approaches. Sensor-free approach is usually preferred as it is more applicable in a clinical environment without the need of an external sensor to record the 3D motion. To make it clear, the terminology used in the result Table will be changed to âsensor-free methodsâ and âsensor-based methodsâ.

R1 - â..lacks detailed descriptions of the encoder structures (PoseE and SemanticE).â: The pose encoder PoseE employs ResNeXt50 [1] to learn the pose embedding and SemanticE employs SonoNet [2] to learn the semantics of fetal US. This will be clarified in the main text. - âWhether the method would be effective in a typical motion scenario beyond TVP and TCPâ: The method can be readily extended to other cases where a target plane is defined. However, if the search is not for a neurosonography plane, a 3D atlas specific to the anatomy under examination would be required.

R3 - âHardware information should be reportedâ: The experiments were run with PyTorch 1.10.1 on a 32GB NVIDIA Tesla V100 GPU. - âFig3 could be improved by removing ââ¦ââ Thanks for suggesting this and we will remove the ellipses. - suggestion âit would be interesting to see applying to other brain images..â: Since the method is designed for US images, directly applying it to other types of brain images (e.g. MRI, CT) may face cross-modality challenges. However, the method could be adapted to analysis of fetal brain MRI scans given that a MRI-based 3D atlas (e.g. [3]) is available.

R4 - âEvaluation on KLD rather than pose distanceâ: The method performance is measured based on the probeâs orientation. However, since the probe and the 3D fetal head positioning are operated under different coordinate systems, their angular or positional distances are not directly comparable. Despite this, their motion dynamics during scan are related: a significant transformation in 3D fetal head will correspond to a relatively large probe movement. We thus evaluate on the distribution of the motion dynamics during scan using the statistical metric (i.e., KLD), rather than the absolute pose or angular distance. We will clarify this in the manuscript. -âmissing details about atlasâ: The 3D fetal head atlas used is open-sourced from [4]. 1059 US brain volumes collected from 899 fetuses across 8 countries were used to generate the 3D fetal head atlases for different gestational ages. In this paper, we use the atlas at 20 week pregnancy as the median description of the fetal head in the second trimester.

[1] Aggregated residual transformations for deep neural networks, CVPR 2017 [2] SonoNet: real-time detection and localisation of fetal standard scan planes in freehand ultrasound, TMI 2017 [3] A normative spatiotemporal MRI atlas of the fetal brain for automatic segmentation and analysis of early brain growth, Scientific reports 2017 [4] Normative spatiotemporal fetal brain maturation with satisfactory development at 2 years, Nature 2023"
https://papers.miccai.org/miccai-2024/614-Paper0578.html,"We sincerely appreciate the reviewers for providing highly insightful comments. We greatly appreciate the feedback, as it will undoubtedly help us improve the quality of our paper. We would like to address several issues raised by the three reviewers. According to the âRebuttal Guide,â new experimental results in the rebuttal are not allowed. We will show additional results for the suggested experiments when releasing the source code on GitHub.

Q1: Compared with diffusion-based methods and additional ablation study on another dataset. (R3)
We appreciate the suggestion to compare our results with diffusion-based methods. We will conduct these comparisons and make the results available on GitHub. In addition, we have already performed ablation studies on another dataset, which yielded conclusions consistent with those in the paper.

Q2: Explanation of learnable prompts. (R3, R4)
Prompt learning is a proven method for fine-tuning pretrained models. In our approach, we concatenate data embeddings with learnable prompts to create a new input, thus adapting task-specific data to the pretraining data.

Q3: How is the position-guided prompt reflected in the text prompt? (R4)
It is reflected through the position prompt embedding (E_t^{pos}). Details are provided in sec. 2.1.

Q4: Noise due to concatenation operation. (R4)
The use of learnable prompts and embedding concatenation is a common practice, as seen in methods such as CoOp [25] and MaPLe [10]. Additionally, concatenation of different modality embeddings is widely applied in large vision-language models like Flamingo, PaLM-E, and LLaVA, known as the fully autoregressive architecture. Based on these precedents, we think that embedding concatenation does not suffer significantly from noise issues.

Q5: What does the âMâ in formula (2) represent? (R1)
We have explained it in sec. 2.1 line 6 as âa binary mask.â A clearer explanation will be added near formula (2) in the final version.

Q6: Are the results the same in Tables 3 and 4 because SAS was used by default in Table 3? (R1)
Yes, Table 3 only analyzes position-guided prompts with SAS as the default anomaly synthesis method.

Q7: How are the results from different positions aggregated during inference? (R1)
The inference process is detailed in sec. 2.3. We will provide a clearer version in the revised paper."
https://papers.miccai.org/miccai-2024/615-Paper4076.html,"We appreciate the reviewersâ detailed feedback and insights. Below, we address the primary concerns raised.
(R5) Dataset Efficacy is doubted due to datasetâs cross-sectional nature. 
A. While longitudinal data would ideally validate our models, we would like R5 to understand that building a longitudinal 3D database of the hip, particularly over extended periods from âhealthyâ to âsevere OAâ stages, would be a fairly large and challenging project in itself. Thus, we consider that it is beyond the criteria of a MICCAI publication to make evaluation with longitudinal datasets an absolute requirement. As R3 and R4 recognized, our paperâs strength lies in its methodological innovations and proof-of-concept experiments, and recent literature supports using cross-sectional datasets for longitudinal predictions [1]. To prove modelâs robustness, an ablation study showed that our model tend to converge to homogeneous shapes for each grade when given only the normal femur and its/targetâs OA grade. The model generates distinctive shapes when provided the grading difference between normal and diseased femurs. We will include these in the final version and extend our experiments with a longitudinal MRI dataset in future research.
(R5) Model reliability is questioned, as direct comparisons sometimes yield the best results while some comparison models generate unreliable shapes. 
A. The first issue primarily occurs with small OA grading differences, indicating minimal disease progression and making subtle geometry predictions challenging. As stated in S.3, our focus is on significant shape changes, which are more clinically relevant, particularly for surgical candidates. Regarding the comparison model issue, the best default sets of hyperparameters and templates provided officially, validated by the same metrics, were used. The failure to generate âreliableâ shapes from comparison models reflects the complex nature of geometric learning and limited trainable shapes. If a prediction model generates shapes resembling the input, it functions as an autoencoder, shifting from being predictive to descriptive. We appreciate your pointing this out and will add related descriptions.
(R3, R4) Details of GE and CCA embeddings? 
A. We use sinusoidal encodings following [16] to convert each of three coordinates into a 20-D vector. These are then appended to the original coordinates, making it 63-D. This aims to amplify subtle changes of input, differing from the goals of RoPE and ALiBi. Positional and index embeddings are concatenated, not added, to convey distinct information. Before being sent to a 2-layer dense network, CCA includes multiple 32-D demography and pathology vectors. The description will be added.
(R3) Why these PCMs were chosen over SSMs for comparison? Reconstruction details? 
A. These PCMs are SOTA models for 3D point clouds, suitable for comparing with our DL-based method. FlowSSM and MeshSSM focus on descriptively building non-linear SSMs to capture subtle variances, while our method integrates geometric info with clinical context for shape prediction. Further description and additional mesh reconstruction details will be provided.
(R4) Dataset diversity and practical application.
A. Our dataset includes CT scans from patients aged 17 to 87 with unilateral hip OA, covering common KL and Crowe grading combinations. More details about the model sensitivity will be provided in the revised paper. Future longitudinal work will also include case studies with treatment planning.
(R3, R5) Bias from upstream models. 
A. The accuracy of the segmentation model is DC: 0.991Â±0.005 and ASD: 0.152Â±0.384 mm. The accuracy of the OA grading model (one-neighbor) is 0.955Â±0.021. The training data and predictions were verified by experts.
(R3, R4, R5) Minor concerns 
A. We appreciate the pointing out and will revise accordingly.
[1] Campello, et al. âCardiac aging synthesis from cross-sectional data with conditional generative adversarial networks.â"
https://papers.miccai.org/miccai-2024/616-Paper1307.html,"We sincerely thank all reviewers and ACs for their detailed and constructive comments. 
Reviewer: #1

Reviewer: #3

Reviewer: #4"
https://papers.miccai.org/miccai-2024/617-Paper0293.html,"We thank the reviewers for their positive reviews that led to an early accept decision. We would like to take the opportunity to address a few minor issues that were raised and clarify misunderstandings. The manuscript text will be revised accordingly, and all typos etc. raised by reviewers will also be fixed.

R1: Comparison fairness. This seems to be a misunderstanding: All the interactive seg-mentation models access the labels for prompt sampling during inference, so the com-parisons are fair. Furthermore, for fairness re: prompt types such as bounding box, we included PRISM-plain, which uses the same prompt settings as the compared methods.

R1: Hybrid encoder. We directly adopted the CATS network without modifications.

R1: Retain the gradient. This is a typo, we retain the gradient for continuous maps, not the dense prompt. Indeed, y.detach() is passed in the subsequent iteration.

R1: MLP. This is another typo,  we have two separate MLPs. We fixed this by adding  superscripts.

R3: Propagation from 2D. For some 3D images with large slice thickness, propagating methods may fail as they rely on prediction information from adjacent slices. Such methods may not be robust for tumor segmentation, where large variations and ambiguous edges are present.

R3: Prompt types. This seems to be a misunderstanding: our results indeed do include points, bounding boxes, and scribbles. PRISM-plain is a simplified ablation version that only uses point prompts (no BB or scribble) for a fair comparison to other point-only prompt methods. Indeed, seed refers to the model random generator seeds.

R4: 2D vs 3D. Our network is built in 3D and takes 3D images as input, with the prompts also generated in a 3D manner. Figure 2 is merely a 2D illustration of the prompts used in our study.

R5: Incorrect claim. We apologize for the poorly worded claim: we were referring to the recent wave of SAM-based methods rather than generic interactive methods. As the re-viewer notes, our method and experiments are entirely focused on SAM-style interac-tions (see also next point). We will carefully clarify this in the main manuscript.

R5. Why SAM-based only. We focus on SAM due to its wide generalizability on various tasks and prompt-efficient encoding.

R5. SAM notorious for medical images. We note that the compared SAM-based meth-ods are in fact all designed for tumor segmentation (except SAM itself) with publicly available code repositories. We chose these methods because they can be classified as model adaptations from SAM and medical image foundation models, allowing us to compare different aspects.

R5. Non-SAM comparisons. Tumor segmentation is a challenging task that lacks robust solutions. Our primary goal is to provide a feasible solution for clinical routines. We acknowledge that older methods could be compared against. However, further compari-sons are not essential for demonstrating the effectiveness of our approach, as our pro-posed methods have already achieved human-level performance, as evidenced in our submission. This underscores the main point of interactive segmentation rather than fo-cusing on exhaustive comparisons."
https://papers.miccai.org/miccai-2024/618-Paper3076.html,"We would like to express our gratitude to the reviewers for their detailed feedback on our paper.

The reviewers have recognized several key strengths of our work, including its ânovel approachâ (R5) and its technical advances (R4: âsignificant advancement over existing techniquesâ; R1: âa considerable effort to prevent hallucination while maintaining the relevance of the de-identified MRI examinationâ) to MRI de-identification. Additionally, they noted a âconsiderableâ/ârobustâ experimental study (R1/R4) and CP-MAEâs relevance to privacy concerns (R4: âthe approach is timely and highly relevantâ). The clarity, organization, and detailed description of our algorithm have also been acknowledged by all reviewers.

We kindly ask you to re-evaluate our paper in light of this rebuttal and encourage you to consider increasing your scores if you find that we have addressed your concerns."
https://papers.miccai.org/miccai-2024/619-Paper3431.html,"We appreciate the feedback from both reviewers. We are delighted Reviewer 3 recognized both the novelty of the method and the importance of the clinical application, and found the experiments and results to be convincing. We thank Reviewer 3 for providing interesting avenues for future work. Given that Reviewer 3 stated that there are no major weaknesses in the paper, we focus on addressing the major points brought up by Reviewer 1.
[The technical novelty is unclear. The NSDE method is already established and this study was merely applied for medical image data]: 
Reviewer 1 questioned the technical novelty of the paper, specifically focusing on the novelty of the NSDE method. We wish to reiterate the novelty of the entire framework developed in this paper: We built thefirstcausal, spatio-temporal framework that predicts future disability trajectories (and their associated uncertainties) and future individual treatment effects of MS patients from high dimensional images (3D MRIs) and tabular information. The NSDE is only a part of the overall architecture. That being said, although NSDEs have been used in theoretical machine learning research (we provided a citation), we are the first to use it in the context of a complex causal framework for predicting disease progression, particularly for neurological diseases. This is also thefirstlongitudinal model built to predict future treatment effects in continuous time for a neurological disease which is chronic, complex, and heterogeneous. An additional novelty of the work lies in the fact that predicting future disability evolutions has never been successfully achieved in the context of MS.[The experiments are relatively weak. They only compared the model performance with two methods in Table 1.]:
Reviewer 1 focused on the comparison of the NSDE model and implied that additional validation of this component of the framework was missing. For the factual predictions, we did compare the NSDE method to two popular baseline models, but this was only one component of our extensive set of experiments. In fact, we performed an extensive number of experiments in order to validate all the components of the entire causal framework on a large-scale, real world clinical trial dataset for practical clinical outcomes, as was noted by Reviewer 3. We evaluated the quality of the factual predictions, with and without the addition of uncertainty estimates. From our factual and counterfactual trajectory predictions, the model found subgroups of responders to different treatments for a disease with no cure, leading to huge potential clinical impact. The individual treatment effect predictions were further augmented with uncertainty estimates, narrowing in on those predicted to respond with high confidence. There is no other framework in the medical imaging literature to be compared against. The reviewer did not provide references to support their concerns regarding the lack of comparisons against other methods. 
[Data heterogeneity among different datasets]:
Reviewer 1 questioned issues resulting from potential data heterogeneity among the different clinical trials and wondered what pre-processing steps were taken to address this issue. For the merged clinical trial dataset, pre-processing was performed by another group, such that all images were N3 normalized and registered to a common template. We can provide these preprocessing details in the camera ready version."
https://papers.miccai.org/miccai-2024/620-Paper2008.html,"We would like to thank reviewers R1, R4, and R5 for their positive assessment and thoughtful comments. The motivation for resource-efficient model training is strongly supported [R1,R4,R5]. We apply curriculum learning for resource efficiency, which is an âunderresearched domainâ [R5]. The significantly increased efficiency and segmentation performance of our method compared to vanilla nnUNet training is acknowledged [R1,R4,R5], underlining the âgreat potential and relevanceâ [R5]. The simplicity, task and model-architecture agnosticism are also appreciated [R1,R5]. We are pleased that the âresults are convincingâ [R5] and thank R1 for appreciating our well-done evaluation of the approach on all 10 MSD tasks. Below we address the reviewerâs main concerns.

[R1,R4,R5] Missing method details: We want to thank the reviewers for underlining the missing clarity of the method description. In Subsection 2.1 we explained the schedule briefly. As the curriculum can be applied to different architectures (model agnostic), we kept the formulation general, as there is no one-fits-all equation for possible patch sizes. In the case of nnUNet, the minimal patch size and increases of it depend on the number of pooling layers. To keep the task similarity maximal, we increase the patch size in minimal steps. We will add this to the main paper for clarity.

[R5] Contradictory assumption: We agree with R5 that, in reference to the maximization of achievable performance, larger patch sizes, including more global context, correspond to a lower difficulty. However, this is not the case in terms of optimization complexity. A trivial example would be a patch of one foreground and one background pixel, for which the model would only need to learn a single threshold (low complexity). Following this intuition, we perform small-to-large patch size training. Another optimization complexity factor is added by the class balance. Smaller patches result in better balance and can lead to improved performance, as shown by Tappeiner et al. in âTackling the class imbalance problem of deep learning-based head and neck organ segmentationâ (2022).
In the approach âCurriculum Consistency Learning and Multi-Scale Contrastive Constraint in Semi-Supervised Medical Image Segmentation.â by Ding and Li (2024), the authors use patch size variation in a fundamentally different context. They perform consistency learning for unlabeled data on pairs of full images and image patches. Hence, their difficulty refers to the level of data augmentation. Stronger data augmentation refers to smaller patch sizes, as achieving consistency is more difficult. Therefore, they also establish a simple-to-hard curriculum for their consistency loss optimization. Fine-tuning on patches missing global context is less close to our modelâs target domain (i.e. full images) and therefore not the most fitting assumption in our scenario. We like to thank R5 for addressing this important ambiguity, and we will clarify that in the manuscript.

[R4] nnUNet as baseline: We did not consider comparing with âA New Three-Stage Curriculum Learning Approach for Deep Network Based Liver Tumor Segmentationâ by Li et al. (2020), as their goal is to improve performance and not efficiency. Under the same training settings, their approach is less efficient, as it iterates over more voxels. Furthermore, their curriculum can not be applied in a general fashion, as the largest foreground component bounding box is used as patch size in their second stage, which easily can exceed GPU limits for larger components (organs, etcâ¦). Additionally, it can only be applied to binary segmentation tasks. We choose the most widely applied patch-based segmentation approach as our baseline which is nnUNet, which uses constant patch size training.

We thank all reviewers for helpful comments on formal and grammatical aspects. We believe that all review comments will greatly contribute to an improved final manuscript."
https://papers.miccai.org/miccai-2024/621-Paper1885.html,"Thank you for the valuable feedback on our manuscript. We will first address common questions, then respond to individual comments in detail.

Are the results statistically significant? (R3  &R4)
We acknowledge the reviewersâ observation regarding the perceived minimal improvement of our PKD framework over existing methods. We emphasize that we have rigorously quantified these improvements. We performed a paired t-test to statistically validate the differences. This analysis confirmed that the improvements, while subtle, are statistically significant. We will add the statistical results in the final version. Beyond improvements, our PKD framework offers unique clinical advantages. The use of multiple trained teacher models provides flexibility to address diverse resolutions based on specific demands, enhancing clinical adaptability and effectiveness.

Lack of discussion of limitations and future work. (R3 & R5)
Due to space constraints, we could not initially include a detailed discussion of limitations. We acknowledge the extended training time and resources due to the requirement for teacher models. Despite this, the clinical applicability and inference time remain unaffected, staying within a few seconds. We will add these points and suggest further optimization of training efficiency and scalability as future work.

Model implementation detail? (R3 & R5)
Upon acceptance, the complete code, detailed training processes, data preprocessing, and teacher modelâs selection criteria will be available in the associated repository.

Clarification of loss. (R3)
To clarify, Equation (5) presents a simplified version of the knowledge distillation loss, L_KD = L_feature + L_soft target. L_feature corresponds to the first term of right-hand side of Equation (5), while L_soft target corresponds to the remaining terms.

Experimental setting: train on single dataset and test on both? (R3)
While we tested using a single dataset for training and application across internal and external datasets, it was not fully pursued due to the limited data in this specific setting: 510 cases (3615 CT scans).

Why did you stop at 1/16? (R3)
We identified 1/16 as the lower bound where results significantly deteriorate without our proposed framework. 1/16 already uses only two frames to compute the PPMs.

Lack of a comprehensive comparative analysis with existing state-of-the-art PPM generation models. (R4)
Most standard methods require AIF data, while our model generates PPM without needing AIF data, representing a significant contribution. Thus, our comparisons focus on existing AIF-free models, which are scarce. Including AIF-dependent models would not provide a fair assessment due to differing data requirements. Thus we additionally compared our model with various knowledge distillation frameworks to fully showcase its capabilities.

Unclear motivation for PPM generation. (R5)
As highlighted in our abstract and introduction, PPMs are instrumental because they âdeliver detailed measurements of cerebral blood flow and volume.â In the introduction, we mention that deriving PPMs from CTP aims to address clinical issues with CTP. We will further elaborate the details in the final version.

Data preprocessing details? (R5)
To standardize the original temporal resolutions of the scans from the ISLES2018 and our in-house dataset, we interpolated frames to achieve a uniform 40 frames per scan. The extraction of random volumetric patches of size 128x128x32 was used to enhance data augmentation and improve model learning by capturing diverse spatial brain areas. For temporal alignment, the initial frames were selected to capture the inflow, peak, and outflow of the contrast agent, as confirmed by manual annotations from medical experts. For the repeated frames, this method was used to enhance model performance by maintaining consistent input data representation. We acknowledge that this approach could be further explored in future work through an ablation study."
https://papers.miccai.org/miccai-2024/622-Paper2751.html,"We thank the reviewers for recognizing our work as novel(R1,R4), well-organized(R1,R3,R4), and effective(R1,R3,R4). We will address the main issues:
1) Missing comparison with the latest Work(R1-Q1,R3-Q3)
Our main contributions are as follows:
a) New problem. We unveil an unexplored issue in KD methods: teacherâs wrong predictions lead to misguide the studentâs learning. By analyzing these wrong predictions, we innovatively propose progressively correcting mechanism and decompose the problem by devising two assistant networks: one for global false-pixel correction and the other for local boundary refinement. We believe our findings advance appropriate teacher ensemble distillation.
b) Significant improvement. Experimental results show that our PLC-KD outperforms the previous best methods by an average of 1.96%/2.47% Dice/IoU across three challenging medical segmentation tasks.
c) Compatibility. The core of our PLC-KD is the progressively correcting mechanism, thus can be compatible with various existing KD methods and makes better performance.

Compared to the latest methods [1,2] suited for natural images or multi-class problems, our KD approach is specifically designed for more challenging medical image segmentation tasks, which requires accurate knowledge of blurred lesion/tissue boundaries and insignificant texture differences between lesion/tissue and background. Moreover, the lack of source code release hinders us from sufficient comparison[3,4]. Updated comparative methods will be included in the final version.

2) The effectiveness of each component(R3-Q1,R3-Q4)
a) Simple and highly effective. Firstly, we introduce two architecturally identical assistant networks to correct teacher errors from two perspectives: global corrections and boundary refinement. As shown in Table 2, the TA_f and TA_b assistant networks significantly boost performance across three datasets, with average Dice/IoU improvements of 2.91%/3.04% and 3.2%/3.6%, respectively. Remarkably, on the EndoScene dataset, TA_f/TA_b achieve Dice improvements of 5.87%/6.24%. Additionally, the visualizations in Figures 2 and 4 further demonstrate the effectiveness of each component and support our argument.
b) Equal Parameters, Better Performance. In [5], the average output of two teacher networks serves as soft labels for guiding the student. Each of their teacher networks has identical functions. By contrast, each component of our teacher team is specially designed for specific functions(basic prediction and corrections). Actually, we previously conducted similar experiments. By maintaining the original teacher team structure and following the KD strategy in [5] led to only a slight improvement over a single teacher network, but performs worse than our method, indicating the superiority of our smart design.
Finally, we hope our groundbreaking approach will inspire future correction-based KD research.

3) Boundary-aware performance evaluation
(R3-Q2,R3-comment1) We appreciate your suggestion. Metrics like Hausdorff distance and boundary-IoU will be added to Table 1 in the final version for better quantitative analysis. (R3-Q4) Additionally, we will also compare with simple baseline with boundary-IoU loss.

4) Writing issues
a) Missing Details
(R1-Q2) We apologize for the lack of clarity in the details. Actually, the progressive refinement through pixel-wise correction is explained in the italicized note at the bottom of Figure 1, while the similarity-based correction is detailed in the figure caption. (R4-Q2, R3-comments2) We will split the figure and incorporate relevant text into the paragraph for better readability.
b) Typo in Fig.1(R4-Q1)
The blue arrow should be âSelect False Positive Pixelsâ, we will revise it in the final version.

Finally, we hope to address all concerns and would be delighted if our paper is considered for acceptance at MICCAI 2024.

Ref:
[1]SSTKD
[2]DPED
[3]BPKD
[4]MTED
[5]Knowledge distillation:A good teacher is patient and consistent"
https://papers.miccai.org/miccai-2024/623-Paper2127.html,"We appreciate the insightful feedback from the reviewers and are encouraged by the positive comments: ânovel frameworkâ with âclear motivationâ (R1), âimprovement is significantâ with âinsights providedâ (R3), âremarkable parameter efficiency and superior performanceâ (R5).

Q1 Preprocessing details (R1). We adhered to the preprocessing pipeline of the pretrained fMRI model [15]. Data was preprocessed using fMRIPrep (20.2.3) with the default settings, followed by spatial smoothing, detrending, high-pass filtering, and regression to remove confounders. The data were parcellated using Nilearn (0.10.4) with the function ânilearn.datasets.fetch_atlas_difumoâ.

Q2 Selection of the datasets and tasks (R1&R3). The brain-behavior phenotypes established include three domains: cognition, personality, and social emotion. We hypothesized that ScaPT would perform well in tasks relevant to these domains. We first test if our model can perform NC/MCI and amyloid-/+ classification (related to cognition), most challenging tasks related to early diagnosis and prognosis of AD. We selected ADNI as it is the most widely used fMRI dataset for AD. To assess the model in both personality and social emotion, we selected Neuroticism as our third task and chose UKB, the largest public fMRI dataset. Regarding the amount of training data, we follow a similar setting in [15], using limited data to showcase the adaptation performance and varying the size to demonstrate how performance scales.

Q3 Clarification on language model and prompts (R1&R5). We call the fMRI model a âlanguage modelâ because it is a GPT-2, initially for NLP. It learns to understand brain dynamics by modeling sequences of activity, similar to how text is processed. A soft prompt is a trainable embedding that directs the modelâs responses without changing its architecture. We developed a hierarchical prompt structure with three levels: 1) Modular Prompts (MoP), encode abstract information and fundamental skills required for fMRI understanding. 2) Phenotype Prompts (Phep), combine MoP, each tailored to a specific phenotype. 3) Vertex Prompts, direct the model for downstream tasks by integrating Phep with a target prompt.

Q4 Typo and citation (R1&R3). We will revise âadaptionâ in title to âadaptationâ (R1), and add the citation of Nilearn to the revised version (R3).

Q5 Prompt interpretations (R3&R5). We are the first to explore the prompt space in fMRI analysis, where we observed phenotype-based clustering of prompts (Fig.2. (1)). Furthermore, ScaPT introduces âsemanticâ interpretability to fMRI prediction, through the input-prompt attention. It intuitively highlights important aspects for different tasks (Fig.2. (2)). While ScaPT is not a model designed for interpretation, it mitigates the black-box nature of prompt.

Q6 Implementation of other methods (R5). We implement baselines using their open-source codebases in the same environment as ours. Batch size settings are the same for classification as ours. For other key hyperparameters (batch size for regression (chosen from 8/16/32 for the best performance), learning rate and dropout): SPoT: (8, 1e-4, 0.3); MP2: (16, 1e-4 for ST and 1e-3 for TT, 0.2 for both ST and TT); ATTEMPT: (16, 1e-4 for both ST and TT, 0.1 for ST and 0.2 for TT).

Q7 Clarification on motivation and clinical contribution (R5). Our paper presents âa clear contribution and motivationâ (R1) by âproposing a novel framework for adapting fMRI language modelsâ (R3). We focus on fMRI as we model its time series akin to text representation. We have demonstrated the efficacy of ScaPT by focusing on two most challenging tasks in AD, i.e., early diagnosis and prognosis with limited training data and computing resources. Moreover, our work enhances our understanding of brain-behavior mapping for cognition and social&emotional functions, crucial for clinical practice. ScaPT could pave the way for disease prognosis and treatment planning in neuropsychiatric disorders."
https://papers.miccai.org/miccai-2024/624-Paper0930.html,"Q1: The paper stated the challenge of SAM is it relies on location clues (point, boxes, and masks), which resulting limitation of full automation of the segmentation process. Which is very confusing, isnât the point clicks, boxes or masks helpful to provide local prompts to assist segmentation, why it unable to achieve automation? (R1)
A1: In CAD systems, we aim for fully automated processes. Achieving fully automated segmentation results by SAM or MedSAM would necessitate the development of a corresponding keypoint detection or bounding box localization algorithm. On the other hand, our method can complete the segmentation process simply by providing a descriptive prompt of the segmentation target.
Q2: I would be interested in to see the CLIP model used as the encoder for prompt information. (R1, R2)
A2: This is a great idea! Once this paper is accepted, we will add some results comparisons when using different prompt descriptions (like âlower limbs arteriesâ or âa picture of artery vessels of lower limbsâ). 
Q3: Figure 2, the author should consider modify it a bit to make a difference. (R1)
A3: Thanks for the insightful suggestion. Figure 2 is an illustration of our training dataset, we will make a difference to better show our dataset.
Q4: The Automatic pathway routing part needs more clarification. Some parts of discussion can be improved, such as the introduction, background and challenge description. (R1)
A4: Thanks for the comment. We will enhance these sections by providing a more detailed explanation of the routing mechanism, and providing a clearer overview of the problem we are addressing, the significance of our work, and the main contributions. 
Q5: The authors do not provide detailed insights into several key aspects: the results of the path selection for each task, the dynamic process of path selection during the training process, the unique advantages of using CLIP embeddings, and the impact of varying medical prompts fed into the CLIP text encoder. (R2)
A5: Thanks for the comment. For the results and dynamic process of the path selection, we will provide them for some tasks once this paper is accepted. The use of CLIP embeddings in our task offers significant advantages for multi-task learning. In traditional multi-task models, each task is given a fixed encoding or output position, losing the correlations between tasks. For example, the relationships between various vascular tasks or organs and their corresponding lesions. These correlations can be effectively captured and expressed through CLIPâs text embeddings. At the same time, we will add some results comparisons when using different prompt descriptions.
Q6: The paper lacks ablation studies. (R1, R2)
A6: Thank you for the comment. We have shown the performance of âSole-pathâ model in Table 2. The model share the same structure with our proposed method without the CLIP embedding input and AP modules. We could see that it performance worse than the final framework.
Q7: The paper does not include comparisons with other multi-task joint training techniques. The central idea of using CLIP encodings to steer the training and operation of a multi-task model is very reminiscient of the âCLIP-Driven Universal Model â (Liu et al., 2023). (R2, R3)
A7: Thank you for the comment. It is indeed important to compare our method with publicly available approaches. We have thoroughly reviewed the âCLIP-Driven Universal Model â paper. Since the MSD challenge is already closed, it is difficult to compare our modelâs performance on the MSD test set. If our paper is accepted, we will consider comparing their modelâs performance on our test set or on some public datasets, given that they have open-sourced their algorithm code and model.
Q8: Pairwise statistical tests would have been helpful to verify which differences between architectures are statistically significant. (R3)
A8: Thanks! We will add the pairwise statistical tests once this paper is accepted."
https://papers.miccai.org/miccai-2024/625-Paper0929.html,"We thank the reviewers for the appreciation of novelty and effectiveness: significant problem (R1); novel, well-written, significant performance improvements, thorough validation (R3); extensive experiments (R4). Detailed responses are given below. We will release the source codes.

1, Why memory bank helps domain migration [R1, R4]
We use the knowledge stored in the memory bank learned from the source domain to generate prompts for the target domain features. We project all target domain knowledge into the latent space and use source domain knowledge of the memory bank to represent them. As shown in the t-SNE visualization of prototypes in the supplementary materials. This helps to align the source and target domain, thereby enhancing performance on the target domain. On the other hand, similar to few-shot learning, features in the memory bank serve as guiding support features when encountering target query features, helping to generalize to target domain.

2, Why use low-level features? [R4]
Low-level features contain information such as contours, which are crucial for medical segmentation, and have shown useful in the skip-connection of UNet. Additionally, thanks to the inherent generic segmentation capability of SAM, the low-level features of SAM without task/domain-specifically fine-tuning also contain general information for segmentation. Therefore, we make use of such general low-level features by introducing a selective attention mechanism to filter out features that are not conductive to generalization, retaining robust features across domains. We will make it clear in the revised paper.

3, Details of the memory bank [R1, R4]
As described in Abstract, the memory bank is learnable. We randomly initialized the memory bank. During the training, the memory bank in the prompt generation module is automatically updated through backpropagation. During inference, the memory bank is frozen. We will clarify this in the revised paper.

4, Overfitting concerns [R3]
As depicted in Tab. 4 of paper, using a large memory bank size indeed leads to overfitting. Using a relatively small memory bank size is beneficial.

5, Clarification on benchmarks and baseline [R1]
As described at the end of Sec. 3.1 and at the beginning of Sec.2, our baseline is based on SAMâs original encoder and decoder, using two adapters per layer following [30] and changing the original prediction of SAM to semantic segmentation output.

6, Parameters of models [R1]
The total number of parameters and Dice performance in Tab. 1 for CCSDG [14], SAMed [33], Baseline, and our DAPSAM are 43.80M (67.58%), 90.85M (78.51%), 98.26M (78.87%), 98.94M (81.31%), respectively. We achieve 2.44% improvement with slight increase of parameters with our baseline.

7, Preprocessing, visualization of data and examples of prompts [R3, R1, R4]We adopt the same pre-processing as CCSDG [14]. Indeed, the noisy visualization in supplementary material suffers from an issue with image converting, which will be corrected. We have provided the t-SNE visualization of prototypes for prompts in the supplementary.

8, Details of operations [R4]
We use âexpand_asâ function, which repeats tensor p_i to e_i. The adapterâs MLP has two linear layers, which reduces the channel dimension and restores the dimension, respectively. The rank is defined by the ratio between the input and reduced dimension, and is set to 4 to balance the number of parameters and tuning capability.

9, Experimental settings [R4]
The results are either directly sourced from existing papers or reproduced with the same way as DAPSAM. Each column in Tab. 1 represents leave-one-out results for the model trained on the corresponding domain (in first row) while testing on the other domains. We use a single NVIDIA RTX 3090 GPU with batch size set to 8.

10, Paper structure [R3]
Thanks for the suggestion. We will carefully revise abstract and conclusion, and include a comparative discussion of related work, limitations."
https://papers.miccai.org/miccai-2024/626-Paper2968.html,"We thank all reviewers for the constructive comments and recognizing the novelty and effectiveness of our method. In this rebuttal, we mainly address the raised concerns on the preference for VLM models over pure vision models such as YOLO, the stability analysis of our method, and the comparisons with dental specially designed models showing our method is preferable to address fine-grained dental abnormality detection (ours 66.3% vs best baseline 46.4% in AP50). We thank all reviewers for the valuable advice and will include more details in the revised version. For reproducibility, code and re-annotated dataset will be released upon acceptance.

â¤ R1

why VLMs over pure vision models?
We thank the reviewer for insightful comments. We would like to clarify that, although text prompts serve as classification labels in both GLIP and G-DINO, in our method, we further use the dental notation system to empower language interpretation capability on accurate tooth locations. As shown in Table 4, our prompt design is effective for improved tooth enumeration. In addition, our method also adopts language-guided query selection based on G-DINO. Finally, the overall performance of our method (AP50 66.3%) drops when all VLMs are replaced with pure vision models, e.g., Faster RCNN (51.6%), YOLO (33.9%) and DINO (34.2%), also validating the necessity of VLM.

Result fluctuations with limited data size (645 images) 
We report the mAP with a standard deviation of 35.8Â±1.1%, indicating the stability of our method. For the limited data, due to the data scarcity and expensive labeling in the field of dentistry, data size is typically small, e.g., DENTEX, the dental enumeration and diagnosis challenge, only provides 645 tooth labelled panoramic X-ray images, which we further enriched by annotating six fine-grained abnormalities.

Fine-tuning of other approaches
All baseline models are evaluated with fine tuning using the same data split as our proposed approach.

Limited application beyond the dental area
Although in this work we focus on dental abnormality detection, we would like to clarify that our work may be extended to other areas where notation or numbering systems can be leveraged, e.g., vertebrae and ribs.

â¤ R3

Task setting: prompt modification before detection? 
We appreciate the reviewer for the enlightening questions. We would like to clarify that all prompts used in our framework are fixed during the inference, not requiring determination or modification before detection.

Influence of tooth recognition error
Our framework can achieve precise tooth detection, with AP50 97.1% and mAP 67.4%. To further minimize the error accumulation in the next stage, we also crop each tooth with more pixels on the crown, root, left, and right sides to guarantee tooth recognition accuracy.

Validate on multiple datasets
We appreciate the reviewerâs valuable suggestions. Due to the current limitation of few open-access datasets that align with our task formulation, we will address this in future work by collecting an in-house dataset.

Weaker performance on impacted tooth detection
Our results on impacted tooth detection (mAP 71.2%) are only slightly lower than those of G-DINO (mAP 71.7%). Since the design of our framework focuses on detecting more fine-grained local-level abnormalities, the improvement in detecting global-level issues such as impacted teeth are not as significant as local abnormalities.

Comparison with dental models
In Table 1, we have compared our method (AP50 66.3%) with the dental model SegAndDet (37.7%). Comparing with other dental models PDCNN (4.0%) and HierarchicalDet (46.4%) also indicates the superiority of our proposed method.

â¤ R4
We appreciate the reviewer for acknowledging the novelty of our methodology and the organized presentation. We would like to clarify that our method achieves better performance compared with the first-place winner of DENTEX (MICCAI 2023 challenge)."
https://papers.miccai.org/miccai-2024/627-Paper0622.html,"Rebuttal:
We thank all reviewers and the AC for constructive feedback. First, we do not simply integrate image and text or apply Transformer to integrate multi-tokens. Our main contributions are (1) a coarse-grained pathological instance selection strategy, (2) a prompt-guided fine-grained pathological component (PC) grouping strategy, and (3) hierarchical contextual interaction of intra- and inter-PCs mining strategy, which enhances tumor microenvironment (TME) modeling. We will release the code once accepted. Next, we will address the major concerns one by one.

1)@R1 @ R3: the novelty
We argue our method is novel. We propose a coarse-to-fine approach to mine biomarker information, introduce text prompts to group instances and focus on modeling TME hierarchically.
1.1)Compared to [1], we do not simply integrate image and text. [1] guides aggregation of instances considering similarity between image and text features as pooling weights, but we group instances into different PCs based on similarity. We compared ours with [1] in 8th row of Table 1 in paper, showing ours is superior. Compared to [2], our motivation in integrating image and text differs. [2] aims to align image and text features, but we introduce aligned text features to group instances into PCs in TME.
1.2)Compared to [3], (a) grouping objects differ: [3] groups genes, but we group instances of WSIs. (b) Grouping criteria differ: [3] groups genes based on similar biological functions, but we group instances guided by text prompts related to genetic biomarkers. (c) Grouping purposes differ: [3] groups gene to reweight WSI embeddings, but we group instances to further learn the interactions of intra- and inter-PCs in TME.
1.3)We do not simply merge multi-tokens by Transformer [a,b], we focus on modeling TME hierarchically instead. The Transformer layers are designed to learn features of TME hierarchically including interaction of intra- (first Transformer layer aggregates tokens within each PC) and inter-PCs (second Transformer layer aggregates all pooled tokens of PCs).

2)@R3: the selected patches meet the requirement.
1.1) From medical knowledge: stroma has been proven to be related to gene biomarkers. 
1.2) From pathologists: pathologists have validated selected patches in Fig 2 indeed belong to stroma and the selected patches contain inflammatory response and lymphatic infiltration related to gene biomarkers. 
1.3) From results: in Table 2, the ablation in 2nd and last rows shows selecting stroma patches can enhance our method.

3)@R3: without cross-validation
Experiments are widely conducted via Hard-split of dataset without cross-validation in WSI tasks including biomarker prediction (Chen R, UNI, Nature Medicine 2024; Lin T, IBMIL, CVPR 2023; Jin T, GiMP, MICCAI 2023; Wang T X, PALHI, ISBI 2020). We follow their settings. Further with a 4-fold cross-validation on TCGA(MSI) dataset, our method still outperforms 2nd best TOP and 3rd best DSMIL by 5.63% and 5.66% in AUC.

4)@R4: ablation
4.1) number of text descriptions
We adjust seed of inference parameters in GPT-4 and regenerate 5 descriptions. Result difference on TCGA(MSI) dataset is 0.06%, indicating varying number of pathology text descriptions has little impact on result.
4.2) explanation of performance drop using Selection+PGG combination 
The reasons for performance drop were explained in Sec. 3.3 in paper (1st line on Page 8). W/o PCIM (simply aggregate multi-tokens), interaction of intra- and inter-PCs is not mined.
4.3) exploration of Selection+PCIM
We explored Selection+PCIM in paper (see âK-meansâ in the 4th row of Table 2). W/o PGG, instances are not grouped into different PCs and PCIM cannot be applied. We replaced PGG with Kmeans results in a performance drop, showing the importance of PGG. We further replace PGG by random grouping. Results reveal an approximate 4.1% drop on TCGA(MSI) dataset. These both indicate PGG can better model fine-grained PCs related to genetic biomarkers in TME."
https://papers.miccai.org/miccai-2024/628-Paper3451.html,"We thank reviewers (R1, R3, R4) for positive feedback: well-written and easy to follow paper (R1, R3, R4), computationally efficient  approach (R1,R4), SOTA results (R1,R3,R4), and extensive experiments in a highly practical setup (R4). We will make our code publicly available.

[Global Comment] Certified Robustness as a Defense Method Against Adversarial Attacks (R3, R4)
The work of Cohen et al. [4] theoretically proves that âany classifier that performs well under Gaussian noise can be transformed into a new classifier that is certifiably robust to adversarial perturbations under the L2 normâ. This guarantee holds regardless of the adversarial attack type, provided the perturbation is within the certified radius. While various approaches exist to improve a classifierâs robustness to Gaussian noise, such as retraining with noise augmentation or prepending a denoiser, our work introduces a novel prompt-learning-based method to achieve this goal for large medical VLMs-based classifiers, providing robustness against any norm-based adversarial attack.

R1
On Trivial extension: While fine-tuning VLMs is often achieved by learning prompts, our application of prompting to achieve certified robustness for MedVLMs via randomized smoothing is novel and shows significant gains over existing baselines.
On Fig. 1: We will add the fire symbol to the few-shot block in Fig. 1 for clarity.

R3
1-On Definition of Robustness and Why Gaussian Noise?: Following [4], we define robustness as resilience to norm-constrained adversarial attacks.
Although there are various types of noise, adapting the classifier to Gaussian noise is an intermediate step in achieving certified robustness against any L2-norm adversarial attack, based on the randomized smoothing theory by [4] (see global comment on top).
2-Motivation of Prompt-learning and Zero-shot/Few-shot setting: We use prompt learning because it allows efficient adaptation of large MedVLMs against distribution shifts like Gaussian noise [31]. By leveraging prompt learning, we can effectively adapt MedVLMs to Gaussian noise, thereby enhancing their certified robustness.
In the context of VLMs, zero-shot generalization refers to the setting where a pretrained VLM is turned into a classifier without supervised learning [18], while few-shot learning involves fine-tuning with a few labeled samples. Importantly, zero/few-shot learning does not depend on how the VLM is pretrained; the VLM may have encountered samples from a specific category during pre-training, albeit without labels [18,31]. In this work,  users can utilize Zero-Shot PromptSmooth if they have only a single test sample. If a few labeled samples are available, Few-Shot PromptSmooth adapts the classifier to Gaussian noise using these samples. To further improve test-time robustness, Zero-Shot PromptSmooth can be used in conjunction with Few-Shot PromptSmooth, as discussed in the Method section.
3-Focus on Classification and Experiments: We highlighted the widespread adoption of MedVLMs across various medical imaging tasks in the first line of the introduction. However, as clarified in the Method section, our focus is on classification tasks and we will clarify it further. For classification, we conduct extensive experiments against baselines on 6 datasets from 2 medical domains using 3 MedVLMs.
4-Typo and On Robustness: We will fix the typo. Regarding robustness, see R3 (1).

R4
1-Types of Adversarial Attacks: Unlike empirical defenses, a certifiably robust adversarial defense is agnostic to norm-based adversarial attacks (see Global Comment).
2-On Fair Comparison. The baselines Denoised Smoothing and Diffusion Smoothing are model-agnostic and can be prepended to any classifier, including VLMs. In contrast, PromptSmooth achieves certified robustness through learnable prompt tokens, an inherent characteristic of VLMs. We ensured a fair comparison by applying the baseline methods to the same MedVLM backbone as PromptSmooth in our experiments."
https://papers.miccai.org/miccai-2024/629-Paper2644.html,N/A
https://papers.miccai.org/miccai-2024/630-Paper1433.html,"We thank the reviewers for their valuable feedback. We are pleased that they found the paper well-written [R3, R4], our multi-scale probabilistic registration approach novel [R3, R4], and the qualitative and quantitative evaluation strong [R3].

We note that R1âs recommendation may stem from misunderstandings of our method, which we will clarify below and in the paper. We further stress that the two main contributions of our work are: 1) demonstrating that Diffeomorphic VoxelMorph (DIF-VM), likely the most widely used learning-based registration technique, produces highly uncalibrated and pathological uncertainty estimates; and 2) introducing a novel method (PULPo) that provides significantly better uncertainty quantification while maintaining similar registration performance.

The main contribution lies in the uncertainty quantification not the registration performance. We are convinced that improved uncertainty quantification for DL-based registration is a crucial research avenue.

[R1]Flaws in prior modeling and network designThe latent variables z are not velocity fields as stated in the review. They are transformed by several network layers to obtain a velocity field, as shown in Fig. 2 and described in Sec. 2.1. Using a diagonal covariance matrix for the latents is a common simplification in VAE-based techniques. We did explore a non-diagonal covariance matrix as in DIF-VM, but it led to worse registration and calibration performance. Instead, we regularize our deformation fields using an external diffusion regularizer.

[R1]Unclear calculation of NCC_{VX} and NCC_{LM}â
The calculation is described in the second paragraph of Sec. 3.2. We do not calculate the NCC of the uncertainty with the image intensities, which would not make sense. We calculate the NCC between the errors and the uncertainties. We will clarify a potentially ambiguous sentence at the end of Sec. 3.2.

[R1, R4]Incomplete related work sectionWe acknowledge our related work section was incomplete and thank R4 for suggesting additional papers. We will discuss these works in the final version of the paper if accepted. Many of these papers build on DIF-VM, underscoring the need to address its flaws. Interestingly, Grzech et al. confirm our findings that the uncertainty magnitudes of DIF-VM are very small and underline âthe need for further research into calibration of uncertainty estimates for image registration methods based on deep learningâ.

[R1, R4]Performance vs uncertainty, OASIS results worse than Learn2Reg leaderboardOur registration results are not directly comparable to the L2R leaderboard due to different data splits. This work primarily explores our hierarchical approach for uncertainty quantification, where we show substantial improvements. Optimizing for performance will be the focus of future work. However, our results show comparable performance to an established baseline technique, DIF-VM.

[R3]Experimental comparison to Krebs et al.We agree that more comprehensive empirical validation is important and will explore this in future work.

[R4]Standard deviation of NCC_{VX} and NCC_{LM}âThe standard deviations of our NCC scores will be reported in the final version.

[R4]Uncertainty ranges for Fig. 3The uncertainty ranges (min, max) for DIF-VM/PULPo are:
var(f): (0, 3.975eâ6)/(0, 0.042)
var(Ï): (0, 3.371eâ5)/(0, 1.407)
The low max values for DIF-VM underscore the pathological nature of its uncertainty estimates. We will add a legend to this figure in the final version.

[R4]Performance of the model when uncertainty is lowOur experiments on OASIS and BraTS show that our technique provides better-calibrated uncertainty scores in both low and high uncertainty settings. An evaluation on completely healthy images with lower uncertainty is an interesting addition for future work.

[R4]Epistemic vs aleatoric uncertaintyOur method estimates aleatoric uncertainty. We will clarify this in the text."
https://papers.miccai.org/miccai-2024/631-Paper1637.html,"We appreciate the efforts of the AC and all reviewers in handling our manuscript. Below, we provide a general response and individual responses to each reviewerâs commentsã

General Response: (1) We will release the code, implementation details on GitHub. (2) We appreciate reviewersâ detailed comments and apologize for the grammatical errors, which will be corrected in revised version.

Response to Reviewer #1
Q1.2: Technical details of PFM.
PFM integrates 3D features with 2D features to enhance the accuracy of tooth tip(Fig.3B). Using multi-head cross-attention, PFMâs inputs are Query (Q) from TGNet via W_q and Key (K) and Value (V) from PXSegNet via W_k and W_v. After cross-attention, the fused outputs feed into subsequent TGNet layers. Detailed descriptions and settings will be in the revised version and code.
Q1.3 The novelty of this work and old techniques.
Our contribution lies in a simple, effective framework for 3D CBCT reconstruction from 2D images, validated by a large-scale clinical dataset. Additionally, the proposed PFM integrates 2D information into 3D features, while MB, UB, and RT Losses enhance quality (Fig.3B, Table2). We have replaced the backbone with various networks. TransUnetâs 2D and 3D IoU decreased by 4.75% and 1.43%, respectively, and simplified Unetâs by 10.51% and 5.16%, showing our method effective. We will seek better alternatives and discuss their applicability to dental images in the revised version.
Q1.4: Registration Details.
We will add the registration details and manual check efforts in the revised version.
Q1.5: Mesh surfaces v.s. point clouds.
We agree that smooth surface models are important for dental applications, hence we use mesh illustrations. We used the Ball-Pivoting Algorithm for converting point clouds to meshes. Mesh transformation is not our focus; all computations are based on images and point clouds. We will discuss implicit representation and mesh evaluations in the revised version.

Response to Reviewer #3
Q3.1: The impact of segmentation on generated parts.
We replaced Unet with TransUnet to simulate inaccurate segmentation. TransUnetâs 4.57% drop in segmentation IoU led to only a 1.43% drop in 3D IoU, showing our modelâs robustness. Missing teeth segmentation can be managed as we generate teeth individually (up to 32). Additionally, our method still needs adjustments to be compatible with supernumerary teeth. Addressing these limitations is part of our future work.
Q3.2: Limitation on clinical validation.
Current methods focus on teeth, while CBCT includes broader details like the jawbone and nerve canals, which our method has yet to validate. The clinical utility of 2D X-rays for 3D CBCT and generalization for complex cases need validation. These limitations will be discussed in the revised version.
Q3.3: Overclaiming & Code release
In revised version, we will replace any inappropriate words, present the paper neutrally to avoid overclaiming and release the code on GitHub.
Q3.4: GPU cost
Our method is cost-effective, training on a single RTX 3090 for 56.23 hours with 0.692 GB VRAM and 103.94 million parameters. It infers 32 teeth in 6.03 seconds, making it highly efficient for medical center deployment.
Q3.5: Compare with the latest technologies
The question might address the baselines or accuracy of the latest CBCT machines. Table1 and Fig.3A compare the latest methods, including X2Teeth and Occudent. Our CBCT machines have a 0.125 to 0.47 mm resolution, which is an acceptable distance error for many clinical applications.

Response to Reviewer #4
Q4.1: About Fig.2 simplified and grammar mistakes.
We deeply appreciate your comments. Fig.2 will be appropriately simplified in the revised manuscript and will correct all writing mistakes in revised version.
Q4.3: Description of Fig.4
Fig.4 visualizes tooth-level analysis, aiding future reconstruction improvements. Teeth No.5, No.7 show high IoU (Table3, Fig.4B), while No.6 shows lower IoU with less root detail (Fig.4A)."
https://papers.miccai.org/miccai-2024/632-Paper2558.html,"We thank all reviewers for their comments, and appreciate the overall enthusiasm on our work. We respond to major comments here and will address all comments in final paper.
Code: We will make our well-documented code and pre-trained model publicly available. 
More experimental results: We will provide explanations for the additional experiments suggested by the reviewers and will include these enhancements in the journal version.
R#1
Q1: Related work
A1: The purpose of MFMM is to deal with the fuzzy information in brain networks. We did not provide detailed descriptions of MFMM and competitor methods due to space limit. We will clarify this in the revision.
Q2: MFMM+QAE 
A2: NoQAE removes the QAE module, making it unable to assess view quality. Therefore, the performance of NoQAE is inferior to that of TMC and MMD, which both take view quality into account. However, we believe that the results of MFMM+QAE are better than QAE combined with other methods because most other methods use softmax or directly fuse the outputs of networks. During the experiments, we have validated that MFMM performs better than softmax, which is why we designed MFMM.
Q3: Positioning
A3: We improved FMM into a multi-view method, which is unprecedented, and we have fully considered the quality of each view. The experimental results also fully demonstrate the effectiveness of the model.
Q4: simple baselines to aggregating predictions over views
A4: We validated simple baselines like concatenation (Acc: 81.99) or addition (Acc: 81.35) for combining predictions from multiple views.  Additionally, we tested the performance using single views (Acc: 76.52). Unfortunately, we did not present these results due to space limit.
Q5: ablation study for QAE
A5: This is a constructive suggestion. QAE can be directly applied to other competitor methods. And we encourage further exploration using our code, specifically by replacing the softmax with QAE and using the loss function in section 2.4.
R#4
Q1: Objective
A1: Our model is applied to dynamic brain network analysis, particularly in the diagnosis of brain disorders. We validated the performance using two schizophrenia datasets. We will explicitly state our objectives in the revision.
Q2: font sizes of figures
A2: We will increase the font size of the figures in the revision.
R#5
Q1: Motivations
A1: We explain how fuzzy information and uncertainty arise [1, 2, 9] in the introduction. Fuzzy information refers to ambiguous data from unreliable sources. Uncertainty arises from the different confidence level in multiple views. We will further clarify this in the revision.
Q2: The order of three filters
A2: According to equations 1-3, it can be inferred that E2E, E2N, and N2G are performed sequentially, enabling hierarchical extraction of brain network features, thereby reducing the dimensionality of feature mapping.
Q3: c_k^v and e_k^v
A3: c_k^v can be understood as the probability for each class, which can be directly used for classification in each view. Figure 2 illustrates this result. e_k^v is the input to MFMM, while c_k^v is the output. We ensure the credibility of e_k^v through the loss function elaborated in section 2.4.
Q4: Multi-class classification
A4: Our model is suitable for multi-class classification tasks. Due to space limit and considerations for the overall experimental setup, we were unable to present the results for multi-class classification. We have made the code publicly available for validation of applicability to multi-class tasks.
Q5: views and windows
A5: We follow literature guidelines for sliding window division. Exploring the impact of different combinations and quantities of views, as well as ensuring performance in the absence of views, is our main focus in the future.
Q6: ablation study
A6: We completed extensive ablation study, such as sequentially removing three filters (Acc: 78.57, 76.19, 71.43). We present the most important results (NoQAE and NoMFMM) in Table 1."
https://papers.miccai.org/miccai-2024/633-Paper0306.html,N/A
https://papers.miccai.org/miccai-2024/634-Paper1037.html,"We thank area chairs and reviewers for their valuable time and constructive comments. We address key concerns as follows:

Common concern:
Reproducibility and additional experiments: We will release github link in final paper. And more results with new methods and results on new datasets will also be posted on github.

To Reviewer#1:
1.Experimental settings: For fairness, we havenât made any parameter changes compared to baseline. L means Q learns L times from features, if L=6, Q will interact with F1,F2,F3,F1,F2,F3 in turn. We apologize for lacking parameter analysis due to paper length limits. More results and analysis will be released on github.
2.Baseline analysis: In our ablation experiments, No.1 shows the Mask2Formerâs performance. The baseline is powerful, but other experiments proved the effectiveness of our novel modification.

To Reviewer#3:
1.Multiple loss function: Dice,bce,giou,L1 losses are commonly used in previous PS and PD works. Dice loss produce smooth boundaries, bce loss improve binary classification accuracy, giou loss emphasizes shape matching, L1 loss is robust to outliers and box estimation. These are all needed in PS and PD, so we utilize a multiple loss func.
2.Low and high level features: Low-level features capture fine details, while high-level features grasp broader context. By continuously looping between low and high level features, queries could learn a more balanced understanding, aiding in accurate and contextually relevant generation.
3.SE layer: G aims at fusing multi-scale receptive fields to generate the refine mask. However, successive interpolations in backbone may shift some key feature corners, leading to a poor fusion. Hence, we utilize SE to align features channelly and spatially. Therefore, if model doesnt use SE, result will slightly decrease.
4.Details of CA: In CA, q=Q(bnq), k=v=F_i(bqhw). sim=qk (bnq,bqhw->bnhw), att=simv^T (bnhw,bqhw->bnq). So different feature sizes donât affect the process and parameters.

To Reviewer#4:
1.Transformer decoder: It consists of L basic operations, each consists of CA, SA and FFN. Q and feature F_i interact in CA. Attention masks M is also used to accelerate the convergence in CA. M is obtained by multiplying Q and F4. However, this would cause discontinuities in M during convergence. Thatâs why we proposed G to generate the refinement masks.
2.Query: We can treat queries as instance samples. Each contains multiple infomation for one instance. This is why we can apply seg and det supervision to it simultaneously.
3.Quantitative analysis: Each query represents a single polyp, so all labels of bboxes are 1. The query number, i.e. bboxes number, is fixed, means TP+FN is consist. So when our TP is low, our recall will be significantly lower; Itâs interesting in Tab.3-row3. As explained before, TP has a significant impact on recall. Seg may cause the model to focus on particular texture features, which may cause bboxes to shift, resulting in lower TP rate.

To Reviewer#5:
1.Potential and limitation: Unified framework is currently mainstream, our work proves the unified framework is also feasible in polyp field. Furthermore, we will attempt to use bboxes to identify potential lesions not just polyps, which is more clinically meaningful; As shown in paper, our model failed on small, concealed polyps. This is also an urgent problem in polyp field. We would conduct further research on this problem.
2.Fairness: We implemented MMDetection to reproduce det methods. Itâs a pitty that there is no DETRâs seg version in MMDet. We will reproduce segDETRâs codes and release the entire results on github.

The above considerations will be addressed in the manuscript. In addition, we would add as many details as possible to the experiments given the space constraints. We thank the reviewers for the constructive feedback and the opportunity to clarify some of the misunderstandings and improve our paper."
https://papers.miccai.org/miccai-2024/635-Paper0297.html,"We thank the reviewers(R) for their comments(C). They find our work novel, sound, & interesting(R1,R3,R4), clear & informative(R3,R4), evaluated extensively(R3,R4), outperforms SoTA(R1,R3,R4). We address their comments:

R1C1:Difference with other TTA: 
Source-driven TTA uses spatial alignment or feature-level consistency, ineffective for objects with varied shape/position(e.g. tumor) or large distribution shift. They fail to maintain structured predictions, as they lack explicit constraint on target shape representation. We address this with a novel latent optimization strategy to find a source-like clone for each target image, eliminating need for target feature mining. This ensures good performance across varied shapes & domains(Tab1,2 of paper & Fig1,2 supp)

R1C2:Correlation & label shift
Spurious correlation is unexpected association between irrelevant input features & predictions, shifting with domain change & causing poor performance on target [Geirhos et al.,â20]. We avoid this by not using target features at all. Starting from a random z in source latent space, we limit latent optimization to this space, computing SSIM loss between reconstructed source-clone & actual target image(Fig1B). This confines our workflow to source domain, avoiding correlation shift. Label shift is planned for future work

R1C3:Suitable for medical domain
(A)We mitigate domain shifts without target supervision, crucial when annotated medical data acquisition is difficult.(B)Variational sampling & latent optimization capture intricate patterns, reconstructing a proxy image in source latent space, preserving critical anatomical & pathological features(Fig2 supp).(C)Our approach outperforms existing DA methods on benchmark datasets(Tab1,2), showing efficacy in capturing diverse pathological features across modalities

R1C4:Influence of radius p
We clarify the confusion:(A)While thereâs a tradeoff between sphere radius & point density, any sphere can theoretically contain infinite points.(B)Even if radius is small, probability of a non-empty sphere being zero is negligible because infinite sampling ensures a closest clone exists. Conversely, a large radius trivially satisfies the condition, although increasing the search space but not diminishing the proofâs validity.(C)To address the challenge of large search space, we propose using SSIM loss as distance metric. Minimizing L^SS gradually reduces search space, making the optimization more efficient

R2:Qualitative result; simplify text; demo
Weâll follow the suggestions in the revision. Demo, code will be released after acceptance

R3C1:Source & target distribution
We experiment on challenging DA benchmark datasets with multiple modalities(MRI & CT in MMWHS;{T1,T2,T1CE,FLAIR} in BraTS) with large distribution variations. Our method still produces SoTA performance. Though currently tested on modality shifts only, we will extend this paradigm across datasets with different organs & anatomical variation

R3C2:Visual difference in reconstructed image
We clarify the confusion: Each column in Fig2 of supp shows original target image in 1st row, translated into its source-like clone in last row. This translated image retains targetâs structural attributes but adopts source domain style. E.g. 1st column of Fig2, CT(src)->MRI(trg) setting shows a target MRI image(1st row) translated to CT-like clone(last row) using latent optimization. This CT-like clone resembles style of CT images(e.g.1st row of col.2), where source model was trained, making segmentation using source model feasible

R3C3:Training VAE
Training VAE(takes 1hr) is necessary to learn source features used to translate target image to its source-clone. This step, though additional, is justified by our 5% DSC gain in Tab1,2 over TTA baselines. Inference is realtime & doesnât need VAE(Fig1D)

R3C4:Adversarial & disentanglement learning
These require target data during training, which is infeasible in TTA that donât use target data for training"
https://papers.miccai.org/miccai-2024/636-Paper1807.html,"We sincerely thank the reviewers for dedicating their time to review our paper and for providing constructive feedback. We are encouraged by the reviewersâ recognition of the novelty, significance, and clinical relevance of our work. To improve the quality of this paper, we will do our best to address all of the reviewersâ concerns in the final version. Issues that are difficult to resolve within this paper will be improved in future work. Please see the clarifications below."
https://papers.miccai.org/miccai-2024/637-Paper3922.html,"We thank the reviewers for their insightful comments. We will address all the concerns.
Q1: Grammatical errors and the reproducibility.(R1)
We have asked a native English speaker to go through the paper thoroughly. The code will be released once the paper is accepted.
Q2: The contribution and differences between [Ref.-13, 17] and proposed method.(R2)
Ref.[17] proposed the Implicit augmentation method, utilizing moments information. It swaps the moments of the learned features of training images. Ref.[13] proposed an Explicit augmentation to perturb the style of the source domain data by randomly sampling from a uniform distribution. The domain-invariant representations are learned with the feature styles by randomly mixing the augmented and original statistics.
We proposed a randomized joint data-feature augmentation and deep-shallow feature fusion network for automated diagnosis of glaucoma. It consists of three main components: DFA, EI, and DS. DFA randomly selects Data/Feature-level Augmentation statistics from a uniform distribution. EI involves both Explicit augmentation, perturbing the style of the source domain data, and Implicit augmentation, utilizing moments information. The randomized selection of different augmentation strategies broadens the diversity of feature styles. DS integrates deep-shallow features within the backbone. Extensive experiments have shown that RDD-Net achieves the SOTA effectiveness and generalization ability.
Q3: Clarification : 1)the meaning of x_iâR^(BÃC); 2)the relationship between R(x) in Section 2.1 and Section 2.2.(R2)
1) x_iâR^(BÃC) represents the input features of the i-th batch in ResNeSt.
2) In Section 2.1, Eq. (1) indicates random sampling of the data/feature-level augmentation statistics from a uniform distribution. In Section2.2, Eq. (2) is revised as R(y)= ð2ð¸ð¥ðððððð¡ (y)+ (1 â ð2)ð¼ððððððð¡ (y), y=x_i, when ð1=1; y=S1(x_i), when ð1=0. And ð2~U(0,1).
Q4: In Fig.1, the meaning of R~ U(0,1), DFA and DS are all unclear. There are double S1 and S2. Need to discuss and draw a clear diagram.(R2)
We apologize for the ambiguous information in Fig.1. We will draw a clearer diagram in the revised paper and provide more details. R~U(0,1) is a mistake. It is revised as ð1~ð (0,1). Si(i=1,2,3,4) indicate the four layers in ResNeSt. DFA is a module that indicates the random selection of the data/feature as the input feature in EI, depending on ð1. S1 and S2, with different colors, represent the first two layers in ResNeSt used for data/feature-level augmentation, respectively. DS integrates deep-shallow features from S1 to S4.
Q5: Clarification on the way of concatenation in DS.(R5)
We use the âconcatâ operation to concatenate different layers along the 2nd dimension.
Q6: The comparison of the params and FLOPs with ResNeSt. Why not apply average pooling for feature aggregation?(R5)
We chose Refuge2 as the target domain and calculated the paras and FLOPS of the two methods. The paras of RDD-Net and ResNeSt used in our paper are 32.98M and 32.96M, respectively. The FLOPS for both methods are 0.44G. We compared the performance of Flatten versus Avgpool on the four benchmarks. The ACC(%) for Flatten and Avgpool are 92.8vs.91.65(Refuge2), 82.25vs.78.37(Harvard), 75.85vs.74.15(ORIGA), 84.54vs.79.18(RIMONE), 83.86vs.80.84(Avg.). Flatten yielded better results.
Q7: Explanation of DeepAll and Fig.3.(R5)
DeepAll is the implementation setup where models are trained on all source domain data and tested on unseen domains.
Fig.3 is the scatter plot of the statistics produced by our data augmentation strategy using 2D t-SNE on four datasets. Different colors denote the image statistic features from different datasets. From Fig.3, Refuge2 and Harvard produce better points separation. Meanwhile, as is shown in Tab.2, the ACC of Refuge2 and Harvard are improved significantly compared with the SOTAs. It indicates that broadening the feature search space is vital to improving generalization."
https://papers.miccai.org/miccai-2024/638-Paper1448.html,"We thank the reviewers for their insightful comments. Bellow, we summarized their comments into three main points.

1.The method assumes a static background (no tool-to-tissue interaction or deformation)Two key aspects of the generation of synthetic surgical images are the relevant positioning of the tools and realistic background deformations. Our work focuses on the tool representation for the generation of synthetic images adding, removing and altering the tools on realistic backgrounds without modifying them. The goal is to pre-train neural networks gaining a strong realistic prior before real data is used.While works like Endonerf and EndoGaussians(arXiv paper without peer-review) consider deformations, they focus on replicating and reprojecting deformable scenes that have already occurred (while removing occluding objects) and are unable to synthesize new surgical states or unseen deformations. We agree with the reviewers that this is an interesting direction for the work and wish to move in this direction, but it is beyond the scope of the novel-pose synthesis method we present here. For this study, generating synthetic images of surgical tools in novel poses to train neural networks is the goal, and static scenes are sufficient to achieve this objective.

The comparison scope is limited to NeRF-based methods, and cross-fold validation should be performed to better evaluate the method. 
One major challenge in image generation is obtaining ground truth (GT) images for direct comparison. We addressed this using Gaussian editing and tool pose tracking, enabling us to acquire precise GT images for comparison with synthesized ones. This capability is uncommon in other generative methods, which typically lack precise GT data. Thus, we compared our results with GT images for a more accurate evaluation of our methodâs effectiveness. 
As 3D Gaussian Splatting (3DGS) is a novel view synthesis technique, we primarily compared it with NeRF, the current SOTA in this field. We selected NeRFacto and InstantNGP for their comparable training times, ensuring a rigorous experiment. 
For the selected downstream task, tool detection, we trained a YOLO model.  Although we did not perform multi-fold validation, our test dataset, as mentioned in the manuscript, includes backgrounds and tool poses entirely independent of the training and validation sets. This hold-out set  effectively demonstrates the modelâs generalization ability and robustness beyond the training data distribution. Thanks to the reviewer comments we recognize the value of multi-fold validation and plan to incorporate it in our research. If allowed, we would include such analysis in a camera-ready version.

Technical contributions are considered limited, as the method mainly combines existing techniques and depends on complex setup. Descriptions in the paper could be improved. 
Our method is based on 3DGS, but we have made significant advancements to make it suitable for laparoscopic scenarios. The original 3DGS method could not achieve scene fusion, editing, or automatic annotation generation. Our method incorporates these capabilities, all while reducing the need for recording and annotating surgical videos and addressing the paucity of available data. Our lighting-accurate tool scans may be inserted to any trained preexisting surgical scene. 
In MIS, lighting conditions and tool geometries remain consistent, but tool poses are constrained by trocar placement. Our method can synthetically project the tool in any pose at any entry position, increasing the potential for available data by utilizing existing scenes. This innovation is crucial for enhancing the flexibility and applicability of our approach. 
Regarding the reviewersâ comments on the lack of detailed descriptions, we will supplement and improve these in our paper. We thank all the reviewers once again for their comments."
https://papers.miccai.org/miccai-2024/639-Paper0730.html,"Dear editor and reviewers,
Thanks very much for taking time to review this manuscript.  We will clarify confusing expression in our paper and fix syntax problems.
Response to Reviewerâs Comments
1.Clinical validation is only done by novice sonographers and not by expert sonographers.
A comparison between these groups two would have been beneficial. (#5)
Reply: Thank you for your suggestion, we will try to include this part of experiments in future studies. Positioning the cardiac views recommended by the guidelines is a basic skill for expert sonographers, who can quickly accomplish this task. Considering the additional time required when using navigation systems, the efficiency gains for expert sonographers may be relatively small. However, expert-level sonographers are scarce resources. Our approach is focused on assisting novice sonographers, especially in regions with limited medical resources, enabling more sonographers to complete the acquisition of cardiac ultrasound views for diagnostic purposes by senior physicians."
https://papers.miccai.org/miccai-2024/640-Paper1844.html,"We sincerely thank all reviewers for their recognition of ReCoâs novelty. Here are responses to their invaluable suggestions.

Reviewer #1
Q1: Enhance the paper by offering more elaborate descriptions of the datasets and the methods employed for comparison. Additionally, discuss the potential clinical implications and limitations of ReCo, along with supplementary validation.
A: We plan to supplement our journal version with a more comprehensive analysis of ReCo and extend its application to additional datasets to further establish its utility.

Reviewer #3
Q1: The weighting of the loss functions.
A: For simplicity, we assigned equal weight to all loss functions in the experiments.

Reviewer #4
Q1: More comparisons with SsCL, CCSSL and CoMatch.
A: We have compared with CoMatch in this paper, and we will compare with SsCL and CCSSL in our journal version.
Q2: Why using the âsecond-highestâ prediction score? rather than âthird-highestâ and so on?
A: The goal of ReCo is to identify the category most susceptible to confusion, where the class with the âsecond-highestâ prediction score naturally exhibits associated characteristics. Hence, we employed the âsecond-highestâ prediction in our experiments. We will discuss the impact of other predictions in our journal version."
https://papers.miccai.org/miccai-2024/641-Paper3024.html,"We sincerely thank the reviewers for providing constructive feedback.

Code (R1&R3&R6)
We promise to make our code publicly available.

Reviewer #1
Q1 Novelty
Our work explores the underexplored area of FEW-SHOT medical video object segmentation. The novelty lies in leveraging existing abundant annotated images and few video frames through a few-shot learning paradigm, along with the proposed spatiotemporal consistency relearning method to bridge the gap between image and video domains. Our approach achieves SOTA results on multiple benchmarks.

Q2 Misleading statement
We reword it as follows:

We explore few-shot medical video object segmentation using image datasets, without reliance on full video annotations.

Q3 Explanation of comparison methods
This will be added in Sec 3.2.

Q4 Comparisons are less intuitive.
Due to page limit, we did not show visualization results of competing methods. These will be included in the final version of the paper.

Q5 Experiments on the same dataset?
Yes. The image and video datasets have no overlap in data or categories. All models used the same train/test sets.

Reviewer #3
Q1 Why few-shot learning set up?
For the target (video) domain, our use case involves few-shot segmentation, where the model segments subsequent frames based solely on annotations for the first frame. Consequently, in the source (image) domain, we have to employ the same few-shot learning strategy.

Q2 Domain mismatch was not considered.
With full respect, we have taken this into account. The proposed spatiotemporal consistency relearning addresses this issue by leveraging the temporal continuity prior. In addition, the problem of category mismatch between images and videos is solved by the few-shot learning paradigm.

Q3 Model collapse
No model collapse observed in our experiments. However, ablation studies (cf. Tab. 3) show that the loss in Eq. (5) is crucial. Removing it causes model collapse.

Q4 Teacher-student learning setup
EMA is commonly used in teacher-student learning for updating model weights, but it requires the two models to have the same architecture. Our approach introduces new modules in the second phase, resulting in a different architecture. Thus, it cannot be applied as a baseline in our case.

Q5 Look into papers attempting to reduce the domain gap.
Our literature review found no prior work aligning with our concept. F. Perazzi, et al., CVPRâ17 cannot handle scenarios with mismatched training and test data categories. The domain agent network (DAN) requires both image and video data for training, unlike our model.

Q6 Cross-resolution feature fusion vs. self/cross attention
This technique is widely adopted and is not our novel contribution. Hence, no ablation study was conducted. We chose it for its simplicity and effectiveness, as attention modules tend to have more parameters with limited gains for few-shot segmentation (ASGNet, CVPRâ21).

Q7 Experiments on CT and MRI
Thanks for the advice. While our current project focuses on medical video segmentation, we acknowledge that extending our approach to 3D medical images is a promising direction worth exploring in future work.

Reviewer #6
Q1 Complexity
Our model achieves ~78 fps, comparable to baselines. We will analyze and discuss the complexity of models.

Q2 Theoretical justification
Sorry for missing this part.

The shift from medical images to videos leads to performance degradation when deploying image models on videos. The proposed relearning method enables these models to adapt to test video characteristics, mitigating this domain shift. By fine-tuning a subset of parameters, coupled with newly introduced ones, on a few labeled frames from the test data, the models can better capture the underlying distribution of the test data, improving performance.

Q3 Enhance paper writing style and structure.
Thanks for the suggestions. To this end, we will include analysis regarding model complexity and provide additional theoretical justification."
https://papers.miccai.org/miccai-2024/642-Paper2483.html,"Thank you for the valuable reviews and suggestions. Here are our responses:

[1] Isotropic reconstruction of 3D fluorescence microscopy images using convolutional neural networks. MICCAI 2017
[2] DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images. MICCAI 2023

Not superior results, especially in terms of LPIPS [R1]
Although our method falls behind TPDM in LPIPS scores, we believe that our reconstruction result is more reliable and closer to the ground truth due to higher PSNR/SSIM. Even though LPIPS scores are better, we observed severe misalignment artifacts in the ZY view of the TPDM result, which is reconstructed using XY and ZX planes.
Moreover, all the methods including TPDM, which directly uses the diffusion model, require 1000 reverse diffusion steps, while ours requires a much smaller number of iterations, between 200 to 500. Moreover, our method does not require backpropagating the diffusion model during reconstruction(compared to TPDM), which eventually leads to up to 10 times speed up depending on the choice of INR.

Ablation study on R(x) and weight lambda [R3, R4]
Although the ablation study is not included in the main manuscript, we observed that the proposed diffusion regularizer is effective compared to other regularizers, such as total variation.
The weight parameter lambda is chosen empirically. A small lambda  (= 0.01) is similar to âno regularizationâ.  A large lambda introduces strong diffusion prior but conflicts with the consistency loss, consequently failing to reconstruct. The best parameter we found is between 0.1 to 0.25.

FFE+SIREN [R4]
Fourier feature embedding and periodic activation functions (SIREN) are both well-known for capturing high-frequency details. Following other INR-based medical image reconstruction methods [3], [4], we adopt both techniques to capture fine details.
We compared random Gaussian FFE with others such as linear, or exponential embeddings; however, they cannot fit properly. Moreover, small std(2,4,8) of the Gaussian FFE(lower frequency) fails to capture detail and generates blurry results.
We empirically found that SIREN is more capable of learning texture-rich data(FIB) and is easy to train compared to vanilla(ReLU activation) INR. Although the vanilla setting works, it shows more blurry results.

[3] NeRP: implicit neural representation learning with prior embedding for sparsely sampled image reconstruction. IEEE TNNLS 2022
[4] Multi-contrast MRI Super-resolution via Implicit Neural Representations. MICCAI 2023

FIB: Synaptic circuits and their variations within different columns in the visual system of drosophila. PNAS 2015
CREMI: https://cremi.org
Zebra-Fish Retina: Content-aware image restoration: pushing the limits of fluorescence microscopy. Nature methods 2018"
https://papers.miccai.org/miccai-2024/643-Paper3136.html,"To R1, R3, R5
Q1: Reproducibility.
A1: Codes will be released upon acceptance.
Q2: Metrics.
A2: For IOL prediction, we adopt MAE and MedAE following [1, 2]. We evaluate accuracy for two reasons: 1) Predictions with AE < 0.5 D are deemed clinically acceptable [1, 2], and accuracy reflects the proportion of clinically useful predictions. 2) The ground truth is accurate to 0.5 D increments.
Q3: Dataset.
A3: It includes 16 views of 2D OCT scans acquired from CASIA2 device, and biometric data detailed in Sec. 3.1 and Fig. 3. With 174 eyes from 117 patients, we split 139 eyes for training and 35 for testing. The test sets of 5-fold cross-validation are: Fold 1 (Eyes 1-35); Fold 2 (Eyes 36-70), â¦, Fold 5 (Eyes 140-174).
To R1, R3
Q4: Novelty.
A4: We address the challenge of representation learning from OCT images by employing RepLKNet and CLA to emphasize shape information and informative regions. The feature fusion using ECA facilitates efficient integration of multimodal data. Our contribution lies in identifying key challenges and proposing a viable solution. 
To R1
Q5: Segmentations as input.
A5: Pixel-level annotations required for segmentation are challenging to obtain. Heat maps in the supplementary file, show that our model mainly focuses on informative regions. 
Q6: Dataset size concerns.
A6: 5-fold cross-validation ensures reliable results. Our method outperforms transformer-based models prone to overfitting, showing its effectiveness on limited data. 
To R3:
Q7: CLA details. 
A7: A detailed diagram of CLA is present in the supplementary file. It utilizes features from last and current layers for spatial attention.
To R5
Q8: Low information density and dumb windows.
A8: Low information density refers to extensive meaningless zero-value pixels. These may lead dumb windows for convolution, where ReLU activation often results in zero responses and affect feature aggregation when pooling. While RepLKNet reduces dumb windows, the extent of this reduction is constrained, as indicated by statistical analysis. Therefore, we introduce CLA for the emphasis on informative regions through attention mechanisms. ResNet-50 employs smaller kernels compared to RepLKNet. While RepLKNet has more parameters and FLOPs than ResNet-50, it shows better results on the collected small-scale dataset, showcasing its superiority to avoid underfitting and overfitting. Additionally, the highlighted regions in CAMs may be informative in other views rather than view 0 and view 15.
Q9: The motivation of utilizing multi-modal data and model design.
A9: AS-OCT allows detailed analysis of ocular structures, introducing imaging data benefits IOL prediction [3]. Simplistic model designs may struggle to extract effective features, potentially leading to underfitting. Results in Table 1 support this concern: traditional formulas achieve only 60% accuracy, while easy AutoML attain less than 70%. Thus, effective model design is crucial, especially with limited samples, to extract pertinent features while avoiding underfitting and overfitting. 
Q10: How to count the percentage of dumb windows.
A10: It is determined by sliding a window across the image and checking if the sum of pixel values in each window is less than the total number of pixels. (np.sum(window) < w_size * w_size).
Q11: Statistical analysis.
A11: According to the rebuttal guideline, adding analysis or experiments are not allowed.
[1] Carmona Gonz Ìalez, D., Palomino Bautista, C.: Accuracy of a new intraocular lens power calculation method based on artificial intelligence. Eye 35(2), 517â522(2021)
[2] Stopyra, W., Langenbucher, A., Grzybowski, A.: Intraocular lens power calculation formulasâa systematic review. Ophthalmology and Therapy 12(6), 2881â2902(2023)
[3] An, Y., Kang, E.K., Kim, H., Kang, M.J., Byun, Y.S., Joo, C.K.: Accuracy of swept-source optical coherence tomography based biometry for intraocular lens power calculation: a retrospective crossâsectional study. BMC ophthalmology 19, 1â7(2019)"
https://papers.miccai.org/miccai-2024/644-Paper0515.html,"We sincerely appreciate the reviewers for acknowledging our methodological contribution and providing constructive comments for further clarification. Our feedback is as follows.

Q1(R4): Insufficient motivation. 
A1: The interference issue from irrelevant regions in attention calculation (where a token is fused with many other irrelevant tokens in attention operation, resulting in interference) is also highlighted by sparse attention [1,2] and deformable attention [3]. Both methods aggregate similar tokens for attention calculation to avoid interference. As stated in [1], standard self-attention may assign high scores to irrelevant tokens. Moreover, data is often insufficient in medical tasks to properly train standard self-attention. In such cases, more concentrated attention is needed to avoid interference. We will add these references in the final submission.
[1] Zhao G, et al. Explicit sparse transformer: Concentrated attention through explicit selection. 
[2] Mei Y, et al. Image Super-Resolution with Non-Local Sparse Attention. 
[3] Xia Z, et al. Vision transformer with deformable attention.

Q2(R1&R4): Lack comparison with SOTA Transformer-based methods.
A2: In our paper, we have compared RAT with Transformer-based methods such as CTformer (2023) and SwinIR (2021). In fact, RAT also outperforms Eformer (2021), Restormer (2022), and Spach Transformer (2023). We will include these conclusions in the future.

Q3(R1) Computational time.
A3: The average inference time of RAT on AAPM CT image is 3.73s, consisting of 3.58s for SAM branch and 0.15s for restoration branch. In the future we will use Efficient-SAM to improve efficiency.

Q4(R1): Add dataset bias. 
A4: The pathological images in TMA dataset exhibits diversity with bias, as they contain six different stains. RAT demonstrates the best performance and robustness.

Q5(R3): Failure of SAM.
A5: SAM is robust to diverse degradations. Minor failures generally exist in our study and it does not affect the overall results very much.

Q6(R3): Only applying R-MSA.
A6: R-SAM is responsible for inter-region attention and W-MSA is utilized for cross-region connection. There will be performance drop without cross-region connection.

Q7(R4): Inaccurate attention range.
A7: This is common in segmentation tasks, where the segmentation network increases the receptive field of each token by feature extraction but does not change its semantic label. The final output segmentation map still corresponds to the input image at a token-to-token level. In our study, although tokens from the target spatial feature are enriched with neighboring information by the CNN encoder, the semantic label of each token generally remains consistent with the interpolated segmentation mask. When computational resources permit, we will try to conduct R-MSA at the original resolution.

Q8(R4): Small CT dataset.
A8: We just follow EDCNN and CTformer to use the AAPM dataset. The one testing patient consists of 421 images, each sized 512Ã512. We will implement our method on a larger CT dataset in further studies.

Q9(R4): Limited improvement. 
A9: As shown in the paper of SwinIR, EDCNN and reference [2], an improvement around 0.05 dB in PSNR can be regarded as significant improvement in image restoration. RAT obtains 0.37/0.19/0.1 dB over the second-best methods across 3 tasks. We conduct significance tests and our improvements over all comparison methods are statistically significant (p<0.05).

Q10(R4): Imprecise focal region loss.
A10: We acknowledge that only using MAE loss to assess the difficulty of each region is too simple. However, the incorporation of dynamic weighting can still represent a modest advancement compared to the commonly employed L1 or MSE loss. In the future, we will take the texture and structure into consideration for difficulty identification.

Other minor issues will be addressed in the final submission.

Thank you once again to all the reviewers for your constructive comments!"
https://papers.miccai.org/miccai-2024/645-Paper2219.html,"We thank the AC and reviewers for their time. We are encouraged that reviewers recognize the novelty(R4,6,7), rigorousness in validation(R4,6), structure(R4,7) and improvements(R4,6,7) of our work.

1) R4,R7âIs there a test set leakage? Why do results match GT? Are results cherry-picked?

We categorically confirm no test leakage. From Section 2, RS-RAG uses only the training split, excluding validation/testing images. The splits faithfully follow from EKAID[8] with no contamination across. RAG contains no extra information beyond training data, making test leakage impossible. Performance improvement is purely from RS-RAG.

The matching outcome is expected. From Section 3, the Language Model(LM) performs well at learning and reproducing general response syntax, also seen in EKAID but with incorrect pathology changes. The key value of RegioMix lies in accurately identifying pathology and severity changes. Thus it is expected for generated responses to match GT, when predicted pathology changes are also correct. We also observe occasional ordering differences, further supporting this matching is positively not from overfit or leakage.

Results are definitively not cherry-picked. Out of 70070 testing samples(same as EKAID), RegioMix generated 36719 exact matches. For difference questions, RegioMix produced 1512 exact matches vs EKAIDâs 874(+73%). Samples are not a special case and were randomly chosen from such to highlight methodâs validity. We have added this into paper.

2) R4âTask too easy to model realistic clinical setting?

We acknowledge R4âs concern but disagree the task is easy. Although LM does well on response syntax, identifying and describing pathological differences is very challenging, as seen in EKAIDâs errors. The dataset has 31 pathologies; resulting in 3^31(approx 10^14) response combinations in difference questions. While multi-label classification with text template might seem sufficient, it lacks flexibility for other question types. Adapting multiple prediction heads for various questions is not scalable. In contrast, a VQA model with generative output can adapt to any question.

3) R4âMore focus on accuracy metrics

Use of NLP metrics is standard for assessing generative VQA models and follows from EKAID. We agree accuracy is crucial for evaluating clinical correctness and is the reason for Table 3, examining accuracy(RegioMix+7.7%) of non-difference questions including abnormality(pathology). CIDEr(RegioMix+77.7) also evaluates clinical correctness by giving higher weights to pathology words.

4) R4,R6,R7âLess successful examples, structuring of figures and paragraphs

We have included two less successful examples in Fig 3, improved Fig 1,2 clarity, addressed use of JPG format. Long paragraphs were broken down and rephased.

5) R4âEvaluation on another dataset.

MIMIC-Diff-VQA is the only available dataset for Medical longitudinal VQA to our knowledge. Also see 6). Additional experiments are not allowed by rule but as future work we aim to test it on neonatal X-ray and is collecting a dataset.

6) R6âConfidence Intervals(CI), fractures comparison.

We conducted experiments with same seed and hyperparameters as EKAID, with sole addition of our modules, ensuring improvements are from such, not random variations. From experiments with 3 seeds, the 95% CI in CIDEr [234.0-237.0], BLEU-4 [48.6-51.1] is significantly higher than EKAIDâs 189.3, 42.2. RegioMix shows slight advantage with 45.6 vs EKAIDâs 44.9 in BLEU-4 for fracture questions. However fractures account for only 1.5% of test set, so result should be interpreted with caution. We have added this.

7) R7âReliance to EKAID and dataset

We acknowledge RegioMix builds on EKAID and uses the same dataset. Our novelty is in integrating RS-RAG, DA, and PairNCE, showing significant improvements over EKAID alone, as noted by R4 and 6.

8) R6âFaster-RCNN Image size

Original size imposes high computation; we follow EKAID and downsample to 1K keeping aspect ratio."
https://papers.miccai.org/miccai-2024/646-Paper2346.html,"We would like to thank the reviewers for their insightful comments. Reviewers agreed that the concepts introduced in the paper are innovative and address the fundamental problem of noisy inputs, in contrast to prior art that mainly focuses on noisy labels. We will clarify this in the final version of the paper.

R1 stated that there lacks a comprehensive description of the âoverall network architectureâ. As R2 correctly points out, âRT4U is a model-independent method that doesnât require hyperparameters and can be seamlessly integrated into existing approaches for aortic stenosis (AS) classification with minimal additional complexity.â We would like to emphasize that RT4U is network architecture agnostic. We have demonstrated its efficacy with three network architectures (ResNet-18, R(2+1)D, and ProtoASNet). Our only requirement is that the model produces prediction confidences over a set number of training epochs, which is the case for almost all deep learning approaches. If accepted, we will further emphasize the architecture-agnostic nature of our proposed contribution.

Regarding disease severity and demographic information of the AS Private dataset: There are 1088/575/909 studies of normal/mild/significant AS cases, respectively. For TMED-2, there are 126/171/301 studies of normal/mild/significant AS, respectively. During training, we used weighted random sampling based on the inverse of the class proportion to account for class imbalance.

A description of the categories in the private data is included in Section 4.2, but to reiterate, the dataset was initially categorized into four classes: normal/mild/moderate/severe using American College of Cardiology (ACC) guidelines. To make our labels consistent with TMED-2, we combined moderate and severe together into the âsignificantâ category. The ACC guidelines determine severity based on Doppler measurements obtained as part of the echo exam.

Regarding the inter- and intra- observer variability of the AS severity label: the severity label depends purely on the Doppler measurements, as such, there is no inter-observer variability in the traditional sense, since Doppler measurements are quantitative. However, there is potentially significant inter-observer variability in the acquisition of Doppler ultrasound data1. Running an independent study to verify the observations of1is outside the scope of our paper.1reports coefficients of variation as high as 28% for the aortic valve area, one of the Doppler measurements.

Regarding patient demographics: due to the data deidentification process as part of our study protocol, most of the demographic information (such as sex and race) are not available to our group. However, we downloaded data with no selection bias in terms of demographic information, therefore the patient population should follow the general patient admission at our local hospital.

R6 requested clarifications for eqn. 2 and parts of Section 4, as well as the addition of standard deviation for conformal prediction results. We will make these improvements in the final submission."
https://papers.miccai.org/miccai-2024/647-Paper2349.html,"We appreciate the identification of some writing and grammar errors by the reviewers and have addressed them accordingly.

The images we generate adopt the style from the source domain, while their spatial structure is provided by the edges of target data. Therefore, we believe this approach does not pose a risk of leaking private information from the source domain.

âBatch-based fine-tuningâ means that all test data are inputted in batches to update the model and simultaneously output final predictions. Each batch can only be used once. âCentralized fine-tuningâ means that all data is used to update the model over many epochs before making the final prediction.

There are significant differences between MR images from different sequences, which limit the effectiveness of entropy minimization and pseudo-label methods. We found that a significant performance improvement can only be achieved through source approximation."
https://papers.miccai.org/miccai-2024/648-Paper3269.html,"We thank reviewers for their feedback, highlighting several strengths: novelty (R1), extensive validation (R1), clinical importance (R2), studying an understudied problem (R2), simple effective/extendable (R1, R3), interpretability (R3).

R1 MLP baseline: For smaller anatomically complex regions (hilar structures, costophrenic angles, etc), MLP consistently showed poor performance, since it does not explicitly account for spatial relationships and contextual details. The integration of attention has led to noticeable improvements in classifying these difficult regions. Attention exhibits more balanced performance across different regions and a reduction in variance.

R1, R3, R4 Code/data availability: Our data is already public. Weâll open-source all code.

R3 Anatomical regions partition: Anatomical regions used are those defined and provided in the ChestImageGenome dataset, explained in publications [12,13]. We did not alter these.

R3 highlightâdiffâ inconsistency: Typo, apologies, will be fixed.

R4 Direct comparisons with AnaXNet baselines: Our main contributions are introducing thelocalizedprogress monitoring task and demonstrating that the proposed DETR region representations work well for various downstream tasks such as localized progress monitoring. The addition of the localized disease detection problem here was to show the potential of the approach in producing features that can be used in a variety of tasks. Since we donât use the exact same dataset, it is fair to say that we are close to state-of-the-art with a representation learning method that was not optimized for this task.

R4 Comparison with original ImageGenome paper on localized disease progression: ChestImageGenome dataset paper [13] reported a simple version of the disease monitoring problem only in lungs, with 2 diseases (heart failure and hazy opacity), and a binary classification problem (Improved/Worsened). The addition of âNo Changeâ label significantly increases task complexity which motivated the authors in [5,8] to work on this task. However, these works donât tackle localized disease detection, and as shown in Fig.3 fail to predict different progression labels for different anatomical locations of a given instance. Also similar to [5,8] we include 9 different findings which means our data is different from [13]. And we classify 12 anatomical regions which is a distinction from both [13] and [5,8]. Nevertheless, we now trained a CNN Siamese network equivalent to the one in [13] with 3 classes, on 9 diseases and 12 anatomical regions, maintaining our original train/test splits. This simple model delivered an weighted ave accuracy of only ~34% and f1 score of ~32% on our dataset.

R4 Limited novelty in localized disease detection: Although our method performs comparably well with more sophisticated end-to-end baselines on localized disease detection, this task is not central to our claim of novelty. Also, there are distinctions such as our use of DETR as a source of features and not just a localization mechanism.

R4 CheXRelFormer disease progression model not presented in Table 4: As the reviewer pointed out, we do mention the closest match (work in [8]) in our paper. We didnât include this number in Table 4 to avoid the impression of a direct comparison since previous work in this area doesnât tackle localized disease progression.

R4 Table 4 + more details about attention variants: In the region-based self-attention method, we use the output row associated with a specific ROI from the self-attention mechanism. In global attention (our main method), we average all output rows from self-attention to create a single âglobalâ vector representing all ROIs in the CXR image. For each specific ROI, the global vector is concatenated with the ROIâs difference vector. Global attention performs better in detecting progression.

R4 IoU threshold for mAP + BioViL-T Citation: Threshold is 0.5. Weâll add this detail, and also the missing reference"
https://papers.miccai.org/miccai-2024/649-Paper1549.html,"R#3 scored ârejectâ and provided conflict comments such as âlack noveltyâ (weakness #3) and âseems novelâ (strongness #3). Concerns by R#3 are summarized below:

1.1 Limited comparison against SOTA GNNs.
We proposed a new SC-FC coupling approach (DC) and how its embedding can benefit a graph model (SFDN) for brain network recognition. We focus on the model interpretability (such as coupling performance indicated by our t-test of the functional community detection) and the clinical value (such as improvement on downstream applications by the embedding of DC). Since our primary interest is an explainable deep model for human connectomes data, SOTA performance on general graph data is not the major focus of this work. 
GNN is the framework of baseline for our DC embedding to plug in, but the page limitation makes it difficult to add other SOTA GNNs as another focus in the current work. 
In our experimental design, a comprehensive comparison against some baselines without our DC embedding is delivered to show not only the effectiveness of DC for downstream applications but also the robustness under different preprocessing and datasets.
On the other hand, we already have additional results of SOTA brain models solely using FC that further show a superior performance by our SFDN based on the SC-FC detour, but we will put them to future works due to page limitation.

1.2 Overlooked literature of advanced topology-inspired GNN.
We have covered most related works of SC-FC coupling in the second paragraph of the Introduction section. Compared to these most relevant works, advanced topology-inspired or high-order GNNs are not relevant to the SC-FC coupling topic. Considering the page limit, we didnât include them.

2.1 The node detour idea exists. 
First of all, R#3 comments that the node detour exists without any reference. But, to our best knowledge, our work first proposes the term of node detour to do the topological measurement, and the searching results of ânode detourâ on Google Scholar can support us.

2.2 The detour connectivity (DC) uses the same conceptual-level idea of other brain models, and hence it is not novel. 
No work in the literature mentioned by R#3 focused on SC-FC coupling. As our title highlighted our focus on âdecipher structure-function couplingâ, a new SC-FC coupling approach named detour connectivity is delivered by our work, and it is novel in the field. This is also agreed upon by other reviewers.

2.3 Any other topological measure could have been used to replace the DC.
Our idea of detour connectivity is inspired by the neuroscience discoveries on SC-FC coupling, supported by literature mentioned in the 3rd paragraph of the Introduction section. Furthermore, we validated the idea in our t-test experiments. We think other topological measurements, such as the clustering coefficient mentioned by R#3, are interesting. However, it is arguable to replace DC with classic graph theory measurements in SC-FC coupling since the latter is not designed to model the relationship between SC and FC.

R#1 and R#4 are concerned about lacking SC preprocessing details, explanation of some t-test results, and limitations of our approach facing directed SC. Also, code release is a concern by all three reviewers.
We will add those details, explanations, and limitations to the revised version. The code is already organized and will be released in the revised version."
https://papers.miccai.org/miccai-2024/650-Paper0912.html,"We sincerely thank all reviewers for the valuable comments and the recognition of our work, which we take seriously to improve the manuscript. The detailed concerns are addressed as follows: 
R1 (Accept):
1ï¼Comparison detailing. For these KD methods, we freeze the teacher model and directly use its output as supervision for the student model without adaptation, as the foundation model contains general medical knowledge that can guide the student model.
2ï¼Model differentiation. In RD, the teacher model refers to a large model pre-trained on broad datasets. Its role is to provide guidance for the student model. The student model is a lightweight model whose role is to learn downstream tasks better under guidance.
3ï¼Method Difference. Conceptually, the difference between RD and KD is that RD considers the task inconsistency, while the difference between RD and PEFT is that RD considers model lightweighting and cross-model knowledge transfer. Technically, RD additionally introduce co-training reprogramming and CKA distillation.
4ï¼Mechanism discussion. Co-training reprogramming encourages teacher and student models to simultaneously adapt to downstream tasks and learn more easily transferable features under the constraint of a shared classifier. CKA distillation focuses on the transfer of core high-level information rather than low-level information that is easily disrupted by noise.

R3 (Weak Accept):
1ï¼We will add the more detailed implementation description including hyperparameter settings, GPU setup, and the training strategy w.r.t. optimizer, learning rate and batch size, and promise to release the code upon acceptance.
2ï¼Regarding experimental results, Table 2 is implemented on dataset BUSI, and Figure 2 is generated following the tool from [1]. We will follow the reviewerâs advice to enrich the necessary details for clarity.
3ï¼Explanation about noise. Noise is inevitably present in KD because the supervision provided by the teacher model cannot be completely accurate, which may mislead the training of the student model, leading to negative transfer. More related discussion can be found in [2]. We will add the relevant explanation and papers for support this statement.

[1] Can Neural Nets Learn the Same Model Twice? Investigating Reproducibility and Double Descent from the Decision Boundary Perspective
[2] Discrepancy and Uncertainty Aware Denoising Knowledge Distillation for Zero-Shot Cross-Lingual Named Entity Recognition

R4 (Weak Reject):
1ï¼Presentation. We will take your suggestions to comprehensively improve our presentation, including results and the frozen sign.
2ï¼Performance explanation. The main factors contributing to performance improvement are co-training reprogramming, and CKA distillation. The former encourages teacher and student models to simultaneously adapt to downstream tasks and learn more easily transferable features under the constraint of a shared classifier. The latter focuses on the transfer of core high-level information rather than low-level information that is easily disrupted by noise. 
3ï¼More elaborate ablation. We will add more elaborate ablation studies for CKA and Co. reprog under differet datasets to make it more convincing.
4ï¼We will add the more detailed implementation description including hyperparameter settings, GPU setup, and the training strategy w.r.t. optimizer, learning rate and batch size, and promise to release the code upon acceptance.
5ï¼Terminology Definition. The foundation model is defined as a model pre-trained on board datasets, thereby possessing general knowledge. We will carefully check unclear terminology definitions in our paper.
6ï¼Presentation of Figure 2. Thank you for your suggestions. The purpose of Figure 2 is to explain the improvement of performance from the perspective of decision boundaries. We will provide more  descriptions for Figure 2 and highlight the core information, and explore some numerically auxiliary ways to convey information."
https://papers.miccai.org/miccai-2024/651-Paper3437.html,"We thank the reviewers for their constructive feedback. They appreciate that our paper is solving a common clinical problem (R1&R5&R6), is âclear and logical, verified though experimentsâ (R1) and âshows promising resultsâ âwith a well-designed approachâ (R6).
We address the main criticism below:
1.Comparison methods (R1&R5): We would like to highlight that our method does something that very few other methods can do: estimate variable motion based on unsorted CT slabs at all timepoints during 4DCT acquisition. Most other methods focus on improving sorted 4DCT images and estimate images/motion that represent one arbitrary breathing cycle. This makes meaningful comparison with other methods difficult. We are only aware of surrogate-driven methods that aim to model variable motion and make for meaningful comparisons. We also compared to the simple groupwise registration method despite it not modelling variable motion, as this or similar methods are in clinical use at some centers.
2.Reproducibility (R1&R5&R6): We will open source the software, phantom data and relevant scripts to reproduce our results prior to publication.

For more specific comments of each reviewer, 
To R1:
1.Fig3 explanation: Fig3 shows our method can estimate variable motion as the mid and right images show the end-inhalation phase from two different breaths, and you can see that the diaphragm height differs between the images.
2.Dataset details: The phantom dataset consisted of images at 182 timepoints, each with size of 355x280x115 voxels and resolution of 1x1x3mm.

To R5:
1.Details of groupwise registration: It was performed as described in ref23. NiftyReg was used for registration. Based on experience default parameters were used except for velocity field transformation and SSD similarity metric.
2.Simple method: We think a simple method is preferable to an overly complex method if the simple method can produce good results as ours does. The reviewer suggests estimating lung volume/airway inflation is better, but only a few CT slices are available at each timepoint, which is insufficient to estimate lung volume or airway inflation.
3.Only visual results on real data: For real data only a few slices are acquired at each timepoint and the ground-truth volumes are unknown. This makes quantitative assessment using segmentations or landmarks very challenging. This is why we also evaluated with phantom data, where the ground truth volumes are known facilitating quantitative evaluation. Visual results of real data also clearly show that artefacts have been removed, and the supplementary videos show the estimated motion is plausible.

To R6:
1.Clarify hypergradient: We further clarify discussion on page 8 of our paper as below. Simultaneous optimization can be unstable due to the interplay between S and C (Eq 3). Alternative direction methods are commonly used to make the optimization more stable, but these require the gradient to be calculated separately for S and C. The hypergradient method allows us to reuse most of the gradient calculation, i.e. gradient up to M, for both S and C.
2.Initializing surrogates: The surrogate free method uses cos and sin to initialize the surrogates for both XCAT and real data. The surrogate optimized method uses the measured chest signal to initialize the surrogates. The surrogate optimized method has the best results (table 1), showing the method is sensitive to the initialization, but the surrogate free method can still achieve good results when a measured signal is not available.
3.Motion curve accuracy: Comparison between GT and solved motion curves is easy and will be done in the updated version. We agree that the motion estimates will be less accurate for CT slices with limited motion, and this is why the surrogate free method has higher RMSE than the surrogate driven/optimized methods (table 1). These slices can be detected and the motion estimates for these timepoints can be discarded prior to downstream applications."
https://papers.miccai.org/miccai-2024/652-Paper1812.html,"Novelty clarification (Reviewer#3, #4):
Thanks for the valuable comments. We will rearrange our contributions and revise them accordingly. Our contributions can be summarized as follows:
1) Clinical diagnostic reports are utilized as the textual information for CLIP, which we believe is the first attempt in retinal foundation models. The benefits of employing diagnostic reports includes: First, the textual information in diagnostic report is richer than classification label, which can improve the performance of foundation models. Second, the diagnostic report is patient specific, so the image and text correspondence is stronger, which is more appropriate for the training objective of CLIP. Last, diagnostic reports are widely available in ophthalmology, which makes the proposed method easily applied in clinics.
2) A novel strategy is proposed to decouple the information of left and right eyes in diagnostic reports, which is a simple yet effective paradigm to build a retinal foundation model. In practical scenarios, diagnostic reports are usually in patient-level, mixing information from both eyes, which brings a big challenge for directly using CLIP to build foundation models. The proposed monocular and patient level contrastive learning approach can handle this challenge in ophthalmology domain.
3) Previous CFP foundation models either did not incorporate textual information (RETFound), or employed simple labels and fixed descriptions constructed in advance as textual information (FLAIR), or utilized images and diagnostic reports generated by AI, which lacks clinical reliability (VisionCLIP). To our best knowledge, our paper presents the first work to directly integrate large-scale clinical data to build a well-established CFP foundation model.
 Various downstream tasks demonstrate that our model exhibits superior performance. We hope the release of our model can benefit the society of retinal image processing so that the foundation model can facilitate practical applications.

Reproducibility: 
The source code is already attached, and the pre-trained model will be released after acceptance.

Reviewer#1
Dataset split: 
The division ratio for training/validation/test sets is 0.56:0.14:0.3 for all downstream datasets, which is consistent with RETFound. To ensure the distribution consistency of different categories, we divide the data of each category accordingly, and combine them into a complete dataset.
Minor questions:
1) The clinical diagnostic reports are all in Chinese, but the proposed method is also applicable to English.
2) Different random seeds are only used in downstream tasks, determining the shuffling of training data.

Reviewer#3
Text preprocessing:
 The âtext standardizationâ here only involves correcting typos and consecutive punctuation errors caused by human input, restoring abbreviations to their full expressions, and unifying mixed Chinese and English expressions into Chinese. We will explain this in the final version.
Model and performance comparisons:
 Following the RETFound published in Nature, which is a retinal foundation model pretrained with CFPs based on the traditional MAE, we propose the first (to the best of our knowledge) retinal foundation model pretrained with CFPs as well as diagnostic reports. 
 For model comparisons, CN-CLIP and DINOv2 are two foundation models pre-trained on natural images, PMC-CLIP is pre-trained on general medical images, while RETFound and FLAIR are both pre-trained on CFPs (in-domain). We follow the similar comparison study in these works. The above methods havenât considered the mixed information of left and right eyes in diagnostic reports, thus inappropriate to be trained on our dataset. The domains of compared pre-trained models will be clearly noted in comparison tables in the final version.

Reviewer#4
Highlighting novelty and qualitative analysis:
Thanks for the valuable suggestions. We will follow the suggestions and revise accordingly in the final version."
https://papers.miccai.org/miccai-2024/653-Paper1633.html,Sincerely thanks to all reviewers and meta-review for their positive and constructive comments. We believe that the constructive feedback will help us improve the quality of the paper and promote further studies on this robustness and generalization segmentation topic.
https://papers.miccai.org/miccai-2024/654-Paper0616.html,"[Proof-Proposition 1]
Q1:(R4) Why there are constrains on W&b in Eq4-5?  A1: Here we assume an identical mapping in the AEâs bottleneck and then find out the conditions to be satisfied in this case. As a result, Eq4-5 are the determined conditions for identical mapping.
Q2:(R3&4) Why the solution of Z0=Z0W1W2+(b1W2 + b2) is W1W2=I, b1=b2=0? A2: Thank R3 for pointing out the typo. In identical mapping, Z0=Z0W1W2+(b1W2+b2) should always hold for any Z0, rather than hold for specific Z0. To help understand, we move all terms into one side of the equation and get Z0(W1W2-I)+(b1W2+b2)=0 for any Z0. Clearly, we must have W1W2=I, and then b1=b2=0, to make this equation hold for any Z0. This deduction is also verified by a previous paper [1]-Sec3.1
Q3:(R3&4) Explain the statement âTo ensure that Eq.4 has at least one solution, the number of independent scalar equations should not exceed the number of learnable parametersâ. A3: Based on the definition of Matrix product [2], W1W2=I_(DxD) is equivalent to D^2 scalar equations. The number of elements in these equations is the sum of the number of elements in W1 and W2, i.e., Dd+dD=2Dd. Based on the consistency of System of linear equations [3], D^2 should be <= 2Dd to make the solution exist, i.e, d>=D/2.
Q4:(R3) Relationship between âat least one solution exists â> d>=D/2â and âd<D/2 â> no solutionâ. A4: They are contrapositions [4], thus equivalent.
[Proof-Proposition 2]
Q5:(R3&4) Does H(Xa) always contain H(Xn), and if so, why? A5: We argue that for regional anomalies, H(Xa) always contains H(Xn). The reason is that information entropy measures variation of random variables [5]. Thus, H(Xn) represents variation of normal regions. H(Xa) represents variation of normal and abnormal regions, since all normal patterns can appear in abnormal images. Note that H(Xa) is a measurement regarding the distribution of abnormal images, thus, specific samples with bigger anomalies mentioned by R3 donât affect the conclusion.
Q6:(R3) How to get âI(Xn;Z)<=H(Xn)â from âI(Xn;Z)=H(Xn)+H(Z)-H(Xn,Z)â? A6: Based on the property of joint entropy[6], H(Xn,Z)>=max[H(Xn),H(Z)]>=H(Z). Thus, H(Z)-H(Xn,Z)<=0 â> I(Xn;Z)=H(Xn)+H(Z)-H(Xn,Z)<=H(Xn)
[Experiment]
Q7:(R4) No results at d=512, and Tab.1 doesnât support the theoretical claim. A7: There exist misunderstandings. We kindly request reviewer to read Sec4.2. We believe that a careful reading will address the concerns. As described in the 1st paragraph of Sec4.2, we present reconstruction errors w.r.t. d (from 1 to 1024) in Fig.3, instead of in Tab.1. Fig.3 shows that when d is small, an increase in d results in a decrease of rec errors. When d>D/2=512, an increase in d doesnât lead to smaller errors. This supports Proposition 1 that AE with small d doesnât encounter identical shortcut, and d>D/2 makes the bottleneck saturated.
As shown in 2nd-3rd paragraphs of Sec4.2, Tab.1 presents results to support Proposition 2. First, reducing d from 128 to 1 initially improves the performance and then leads to deterioration. This aligns with proposition 2 that H(Z) should be minimized to H(Xn). Second, the optimal d for 2D scans is smaller than 3D scans (MRI). The reason is that MRIs offer richer information than 2D scan, resulting in larger H(Xn) -> larger optimal d.
(R1&3: More SOTAs, input size, open source) We aim to build a theoretical foundation for AE-based AD. Thus, introducing extra designs to improve metrics is out of the scope of this study. The size 64 follows previous works. Larger size doesnât bring improvement. Code&data will be public.
[1]You et al. A unified model for multi-class anomaly detection.NeurIPS2022
[2]https://en.wikipedia.org/wiki/Matrix_multiplication#Matrix_times_matrix
[3]https://en.wikipedia.org/wiki/System_of_linear_equations#Consistency
[4]https://en.wikipedia.org/wiki/Contraposition
[5]Shannon C E. A mathematical theory of communication.1948.
[6]https://en.wikipedia.org/wiki/Joint_entropy#Greater_than_individual_entropies"
https://papers.miccai.org/miccai-2024/655-Paper2995.html,"We appreciate the reviewers for their constructive comments.

Code (R1&R3&R4)
We promise to make our code publicly available.

Data description (R3&R4)
Thanks for the suggestions. We will describe the datasets in more detail in a table in Section 3.1.

Reviewer #1
Q1 Figures and text descriptions are inconsistent.
Sorry for the typo. We will change VGG-16 to VGG-19 in Fig. 1.

Reviewer #3
Q1 Motivation is not clear.
Traditional cell counting algorithms rely on density maps to simultaneously learn counting and localization, which can be susceptible to errors when cells are connected. The proposed method decouples these two tasks and leverages global feature representations for counting, thereby circumventing this issue. Furthermore, we aim to counteract the trend of increasing complexity in recent state-of-the-art counting algorithms by designing a simple yet effective model.

We will make our motivation clearer in the final version of the paper.

Q2 Crowding counting and cell counting
Yes, it is an apples-to-apples comparison. Both tasks involve counting, albeit different objects: one counts people, while the other counts cells (M. Marsden, et al., CVPRâ18).

Q3 Cell counting versus cell segmentation
Cell counting and cell segmentation are distinct tasks. The former requires only point-level annotations, while the latter necessitates pixel-wise annotations. Consequently, a direct comparison between the two would not be equitable.

Q4 Global message passing module versus pooling
The proposed module differs from average pooling. Although average pooling can serve the purpose of information aggregation, it is local, non-adaptive, and alters the size of feature maps. In contrast, our module performs global information aggregation, is adaptive, and preserves the feature map size. Furthermore, V(p) represents a set of sampled positions.

Q5 SAM
This section explores using cell localization results as prompts for SAM to segment cells. Please note that the core objective of this work is cell counting, not segmentation using SAM. This content is solely for additional discussion. Our results show SAMâs suboptimal performance on cell segmentation in pathology images, even with point prompts. Following Reviewer 4âs suggestion, we will remove this section.

Q6 The counter module doesnât see any location labels.
Thanks for the comment. Our phrasing in that sentence is imprecise, and we will remove it from the final version of the paper.

Reviewer #4
Q1 Decoupling part
In our model, the input to the localizer includes features learned by the counter, which are directly relevant to counting. Therefore, they are not disjoint.

Q2 The sample sizes of each data set are very small.
Sorry for the confusion. Our data split ratio is train:test:val=10:9:1, not train:val:test=10:9:1. We observed insignificant result variance (e.g., 0.16 on ADI), which will be discussed.

Q3 Centroid map validation
Although a modelâs cell count predictions can be close to actual values (e.g., prediction: 100, GT: 102), establishing a one-to-one correspondence between predicted and actual cell locations is challenging, preventing a direct assessment of positional accuracy. Furthermore, individual point coordinate errors are more volatile compared to common bounding box errors. Currently, there is a lack of effective metrics in the community to quantify such errors. Therefore, in line with conventional practices (M. Marsden, et al., CVPRâ18; SANet, ECCVâ18; DM-Count, NeurIPSâ20; OrdinalEntropy, ICLRâ23; CLIP-EBC, arXiv, 2024), our study focuses solely on quantitative count comparisons.

Q4 Section3.4
Thanks for the suggestion. We will remove this section from the final version of the paper.

Q5 IHC images
We appreciate the advice. Following prior work (Count-ception, ICCVWâ17; Z. Wang, et al., ICLRWâ23), experiments were conducted on the four public datasets. As suggested, we will evaluate our model on IHC images for future work."
https://papers.miccai.org/miccai-2024/656-Paper2279.html,N/A
https://papers.miccai.org/miccai-2024/657-Paper1723.html,"Thanks to all reviewers for the valuable comments. Below are our point-to-point responses.
To Reviewer #1:
Thank you very much for your feedback. Regarding the issue you raised about the dataset, we acknowledge that our description was not very specific, and we will make corrections in the final version of the paper. For the Camelyon dataset, we divided the entire Camelyon 16 dataset into training and validation sets using four-fold cross-validation. Each time, the model that performed best on the validation set was tested on the fixed test set (Camelyon 17). We did this to evaluate our modelâs performance more effectively using a larger and more comprehensive dataset. The same approach was applied to the Lung dataset.
For the BRACS dataset, we followed the official dataset division, so the training, validation, and test sets are fixed. To thoroughly validate the modelâs performance, we conducted four repeated experiments.
Additionally, we have added a detailed description of the model in the caption of Figure 1.

To Reviewer #3:
Thank you very much for your suggestions. The additional research you provided greatly enhances the completeness and impact of the paper. These studies innovatively use graph structures and hierarchical structures to build models, as well as the recently widely discussed Mamba structure. We will include a discussion of these works in the introduction section of the final version of the paper. Unfortunately, additional experimental results in the rebuttal are not allowed by MICCAI official, so we cannot provide detailed comparative experiments. We appreciate your understanding. Your suggestions have greatly improved our paper. Once again, thank you for your revision comments.

To Reviewer #4:
Thank you for your suggestions. Your suggestions to compare with the Chunkwise Recurrent representation method and to explore the S_{q+1} sequence is very valuable. We will design experiments to verify these in future work. However, unfortunately, additional experimental results in the rebuttal are not allowed by MICCAI official, so we cannot provide detailed comparative experiments at this time. We appreciate your understanding. Additionally, your idea of comparing our work with Graph Transformer Networks and extending the work to 2-D is indeed very innovative. Thank you for these ideas, and we will continue to explore them in future research."
https://papers.miccai.org/miccai-2024/658-Paper1356.html,"We appreciate all the reviewersâ valuable comments and acknowledgment of the contributions of our work. We will address the reviewersâ comments below.

R1Q1: The compared methods are rather old.Our experiment introduces an ensemble framework based on reconstruction Autoencoders. Therefore, to validate the effectiveness of our framework, we chose AE, MemAE, and AEU, which are classic and widely used backbones in the field of anomaly detection. Uncertainty-based anomaly detection is a relatively rare approach. We compared all relevant approaches (Student-Teacher method) in CVPR from 2021-2023 and the uncertainty estimate approach in MICCAI2022.

R1Q2: Cite related papers.In the 4th paragraph of the Introduction, we cited the paper [1]:

âTo address simplicity bias, previous methods attempted to induce repulsion among learners in either the output space [20] or weight space [8].â

Both [1] and [2] restrain the similarity in learnersâ output space, which culminates in the underfitting of individual learners. In contrast, our method sets the similarity constraint in learnerâs feature space to avoid underfitting.

Additionally, our motivations for inducing diversity differ. Paper [1] aims to improve the modelâs transferability, and paper [2] seeks to enhance the modelâs out-of-distribution robustness. However, our objective is to amplify the ensembleâs uncertainty in anomalies.

[1] Agree to Disagree: Diversity through Disagreement for Better Transferability[2] Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement

R1Q3: Many repetitions.We will carefully check and reduce repetitions within the same section.

R1Q4: Number of learners.The frameworkâs performance improves as the number of learners increases. However, there exists a bottleneck in performance improvement. We chose 3 to balance performance and computational efficiency.

R2Q1: Segmentation task.Due to the lack of pixel-level labeled medical anomaly detection datasets, we have not currently included a segmentation task. However, to demonstrate our methodâs anomaly localization capability, we provide examples based on bounding boxes in Fig. 3.

R2Q2: Inference process.In the subsection âDual-Space Uncertainty,â we explain how to detect anomalies during inference. The sample is input to all learners, and the pixel-level anomaly score map is generated according to Eq. (6).

R2Q3: Improve writing on Page 5.We have revised the example explaining orthogonal transformation invariance to facilitate better understanding.

R3Q1: Acronyms are difficult to follow.Acronyms in the Abstract will be elaborated further in the Introduction.

R3Q2: Improve Fig.1 and Fig.2.We have enlarged the text in Fig.1 and included Fig. 2(C) in Fig. 1.

R3Q3: No distinction of experiments.We will make the Experiment section more concise.

R3Q4: Table 3 is difficult to follow.We have added an explanation in Table 3âs caption. A checkmark indicates the presence of the given mathematical property, while a fork indicates the opposite.

R3Q5: Why lambda 1?In our approach, we set lambda to 1 only for simplicity. We do not want to tune too many hyperparameters to validate our methodâs effectiveness.

R3Q6: Explain Fig. 1 in the appendix.Our model optimizes a multi-objective task during training. If Î» tends to positive infinity, each learner only focuses on behaving differently and thus fails to learn normal samples. If Î» tends to zero, the framework degrades to a randomly initialized one. What we want to illustrate through Fig.1 is that our method can consistently outperform the backbone under a wide range of Î»s.

R3Q7: Add standard deviation.We will add more detailed evaluations to future work, including more ablation studies and statistical analysis.

R3Q8: AUC and AP are never defined.We have added the definition of AUC and AP in the Experiments section."
https://papers.miccai.org/miccai-2024/659-Paper1620.html,"We thank the reviewers for their valuable comments and recognition of the effective method (R1, R3, R4), extensive experiments (R1, R3), innovative (R3, R4), practical implementation (R3), and clear presentation (R4). Major concerns are addressed as follows and modifications will be reflected in the final version:
Q: Unique problems encountered by ViT in medical scenarios. (R1)
A: Analysis of ViT in medical scenarios is presented in the third paragraph on Page 2 and Fig 1. Overall, compared to natural scenes, medical datasets are of small scales, similar backgrounds, and low signal-to-noise ratios, resulting in ViTâs global dependencies and attention maps being sub-optimal and collapsed. The visualization results in Fig 4 (c) indicate that the proposed DMA can solve the problem of uniform dependency in medical scenarios and generate richer attention matrices.
Q: Comparative methods based on linear transformers should be considered. (R1)
A: Linear attention is a type of sparse attention. As summarized in Table 1, we have compared the proposed method with linear attention, e.g., kNN [7], axial attention [16], BRA [18], STA [19], and PaCa [20]. Since linear transformers are typically based on linear attention, the effectiveness of our method can be demonstrated through comparison with linear attention.
Q: Compare with token pruning methods. (R4)
A: Thanks for your suggestion. Considering that most token pruning methods are designed for classification tasks and often come with performance degradation, we did not compare DMA with token pruning methods. We will compare DMA with token pruning methods in the future.
Q: Differences between DMA and existing methods. (R1).
A: The differences between DMA and existing methods are presented in Fig 2. Existing prototype-based methods (i.e., Fig 2 (g)) generate each prototype by merging all feature tokens, which ignores the differences between prototypes and the heterogeneity across feature tokens. Comparatively, DMA (i.e., Fig 2 (h)) first divides feature tokens into different groups based on their feature distributions and only feature tokens located within the same group are merged to form a new prototype. As illustrated in Fig 2 of the supplementary material, our merging region is adjusted according to the context, the merging strategy is more flexible, and the merged dependencies have a higher signal-to-noise ratio.
Q: Dependency on prototype quality. (R3).
A: Yes, the quality of prototypes will affect the token merging results, and further affect the model performance. Therefore, we introduced the prototype loss to help the model generate high-quality prototypes. The visualization results in Fig 2 of the supplementary material demonstrate that DMA can generate high-quality prototypes for reasonable dependency merging.
Q: Computational cost of prototype generation. (R3).
A: Comparison results on GPU memory cost and computational complexity (FLOPs) are summarized in Fig 4 (b). Although the generation and updating of prototypes will increase the computational cost, it can greatly reduce the redundant calculations in global dependency modeling, finally resulting in a significant reduction in GPU memory and FLOPs.
Q: Additional implementation details. (R4).
A: The deployment of DMA in ViTs is illustrated in Fig 3 of the supplementary material. All efficient-attention methods are used to replace the vanilla self-attention in TransUNet for comparison. All models are implemented based on PyTorch and trained for 400 epochs by an Adam optimizer with a batch size of 8 and a learning rate of 0.0001 under the same experimental environment. Random rotation, contrast adjustment, and gamma augmentation are adopted for data augmentation.
Q: Scalability with larger datasets (R3)
A: Thanks for your suggestion. We will evaluate the performance of DMA on larger datasets in the future."
https://papers.miccai.org/miccai-2024/660-Paper1711.html,"We thank the reviewers for their thoughtful comments and the appreciation of remarkable performance (R3) of our model, assessed as âfirstâ (R1) and âinterestingâ (R4) of RIP. Below, we address the main concerns raised:"
https://papers.miccai.org/miccai-2024/661-Paper3051.html,"We thank all the reviewers for their useful and positive comments, which overall commend the clarity of the paper and the novelty of the ideas. We acknowledge all suggestions to improve the final version.

We agree with R1: despite promising experimental results reported in the paper, more experiments are required to confirm the interest of the approach, in different contexts and when applied to largely different datasets. Our code is available to the community for possible feedback. The link will appear in the final text.

As pointed out by R1, the improvement of the coverage can lead to a too-large estimated width. This is a limitation of the technique that can only be verified using simulations. In the conformal calibration step, only the bounds of the intervals are modified,  while the estimated volume is the same for the standard and weighted conformal step. The only way to correct the under coverage caused by the covariate shift is thus to enlarge the intervals. Moreover, the final width may depend on several factors: the classifier used and its calibration, leading to a more or less precise approximation of the weights; the score function, and the presence of outliers. Note that since the introduction of WCP by Tibishirani et al 2019, some works have investigated improvements in the best way to handle non exchangeability, see for instance Farinhas et al 2024.

Availability of calibration data (R6): the current guideline for conformal prediction is to rely on a set-aside calibration dataset of N=1000 samples in order to obtain precise intervals (see Angelopoulos et al. 2023). The BraTS 2023 dataset, which is one of the largest and most popular open-source medical image segmentation dataset, only contains around 1100 subjects which are used in this study for training, calibration, and testing. For other tasks such as Multiple Sclerosis lesions segmentation, the number of open-access cases is much smaller (a few hundred), making the calibration procedure noisier. We thank (R1) for pointing out new datasets for brain tumors.

Concerning the generation process of synthetic data (R7), we start by sampling uniformly a random target SNR from the range [1, 20]. The next step is to convert the binary mask into an intensity image matching the predefined SNR. This is achieved by setting the background intensity to 0, the sphere intensity to 1, and then injecting an additive random Gaussian noise to the image following N(0, 1/SNR). As a result, the generated image has an SNR that matches the target one. We agree that this process generates a very specific type of covariate shift, and it is used only to demonstrate our approach on a dataset with a controlled shift.

Concerning the influence of shift amplitude (R1) and Out-of-distribution samples (R7): importantly, the covariate shift can only be tackled if it is not too important. For example, if the proportion of glioblastoma / meningioma was 100:0 in the calibration dataset and 0:100 in the test data, then the density ratio would be undefined and there would be no hope of accounting for this radical shift (see DockÃ¨s et al 2021). This is also true for an OOD input (e.g. an image without a tumor).

Section 2.3 (R6): even if final binary classification is not a high computational task per se, standard CP procedure requires, each time a new test data is considered, to reweight the calibration dataset, which can be computer intensive and not feasible in practice (no access to the calibration data set, no computer resources available at the center where the system is deployed, etc.). Moreover, the density ratio estimation using a classifier heavily relies on the  calibration of the predicted probabilities, which is known to be a pitfall of modern deep CNNs ( Guo et al. 2017)

Eq. 5 seems incorrect (R6): thanks for your careful reading, the sum is over i=1 to n. This is corrected in the final version."
https://papers.miccai.org/miccai-2024/662-Paper2001.html,"We thank the area chair and reviewers for their time and supportive comments highlighting our ânovelâ method with âwell-supported contributionsâ to tackle the âdata scarcity challengeâ in multi-modal medical image segmentation. We are also glad to see that reviewers recognize the âwell-alignedâ and ârobust fusionâ of our work, which addressesÂ the emerging topic of semi-supervised multi-modal learning with high Dice scoresÂ demonstrated in âextensive experiments on three datasetsâ.

As we include extensiveÂ experiments, our paper had to omit some implementation details due to limited space, which might have caused minor confusion among reviewers. Although the issues raised are minor or stem from misunderstandings, we will release all our code in the final version to ensure clarity and reproducibility.

Q: Unfair comparison due to heavy-weight model
A: To maintain the integrity of the original structural network, CML[13] is a 2D structural network, while our model is a 3D structural network. For a fair comparison, we note that although we use a heavy-weight model, comparison methods like mmFor, EFCD, and UMML also rely on heavy-weight models.

Q: Explanation for single-modality training 
A: First, single-modality images are input into two encoders for feature extraction. Then, a cross-modality collaboration ensures channel-wise consistency using the CSC loss. Finally, a contrastive consistency learning module aligns prediction maps from unlabeled data with the CAC loss.

Q: Generalization to more modalities
A: Our method, evaluatedÂ on three datasets encompassing six modalities, exhibits good generalizability, as evidenced by both qualitative and quantitative results, as shown in Fig. 1 & 3, and Table 1. Moreover, it can also be extended to other modalities.

Q: Limited noveltyA: Our methodâs novelty lies in addressing the data scarcity challenge in multi-modal  segmentation by leveraging the fine-tuned SAM-Med3D model, which is effective in scenarios with limited labeled data and misaligned modalities. We also introduce a CSC loss to align channel-wise features and a CAC loss to regularize predictions on unlabeled data.

Q: Difference between the CSC lossÂ and the supervised lossÂ 
A: Our CSC loss uses cosine similarity to align channel-wise features, ensuring channel consistency without relying on ground truth (GT). In contrast, our supervised loss is applied only to labeled data and requires GT to minimize prediction errors. Thus, they have fundamental differences.

Q: Results for a subset of categories in the AMOS and TAO datasets
A: Our study focuses on paired modality data. For the AMOS dataset, we use 40 CT/MRI pairs for training and 20 pairs for testing, despite it containing 200 CT and 40 MRI images for training, and 100 CT and 20 MRI images for testing. For the TAO dataset, we use all 100 T1/T1c pairs for both training and testing.

Q: Why only using one modality for evaluation 
A: Following the studyÂ of ref.[13], our study aims to develop a more clinically applicable model that excelsÂ under the practical constraints of real-world medical imaging, where multi-modal data may not always be available.

Q: Experimental support for âexceptional robustnessâ 
A: Extensive experiments on three datasets demonstrate our modelâs robustness. Visual comparisons with CML show that our model produces more robust predictions with fewer semantic errors and better alignment with ground truth, as shown in Fig. 3. Quantitative results with 10% labeled data reveal performance comparable to the CML method (The SOTA), as presented in Table 1, further highlighting our modelâs robustness.

Q: Reported result is still on par with fully-supervised methods
A: Our method significantly outperforms the fully-supervised method[8] on three datasets with 10% labeled data, as shown in Fig. 2 and Table 1. Our model achieves Dice score improvements of 15.0% and 8.1% for CT/MRI (#Liver), and 19.7% and 18.8% for T1/T1c (#SOM), respectively."
https://papers.miccai.org/miccai-2024/663-Paper3325.html,"We thank all reviewers for their thoughtful review and constructive comments. The following are our responses to the major critiques.
1) Expression and marking(R3Q1,2,3&7): We will revise Abstract to eliminate ambiguous expressions. The caption of Alg. 1 will be updated to âFeature Decoupling and Adaptive Adversarial Training Optimization Process.â The best and worst results in Table 1 will be marked differently.
2) Hyperparameters(R3Q5&6): The hyperparameter values in Eq. 1-4 are: Î»1=0.2, Î»2=0.4, Î»3=1, Îµ=2/255, Î²=1. Additional hyperparameters will be detailed in the experimental section.
3) SENet+ICFDNet accuracy(R3Q8): A key challenge in fatty liver classification is the varying difficulty of recognizing different classes. This can make the normal training process more likely to optimize towards simpler classes, resulting in unbalanced performance. SENet shows this issue, while ICFDNet mitigates it, ensuring balanced performance across classes. The final result improves both balance and average accuracy.
4) Data collection and partitioning(R3Q5, R4Q4): Data were collected from patients aged 65+ diagnosed with fatty liver. Due to the extended collection period, ultrasound machine parameters varied based on the patient. While we cannot specify the exact settings, we ensured that two physicians confirmed the diagnostic results for each sample. For the experiment, the dataset was split into training, validation, and testing sets in a 7:1:2 ratio.
5) Table 2 missing ViT(R4): Due to memory requirements, our device couldnât support ViT integration into our method. When testing ViT, we split the decoupling and the adversarial training(AT) into two sequential processes. ViT achieved 69.99% accuracy. Although it has a significant improvement, it was not tested under the same conditions. So, it is not listed in Table 2.
6) Decoupling process(R4Q1): The decoupling process involves two parallel branches: one extracting specific fatty liver features and the other extracting common ultrasound features using ICFDNet with unshared parameters. Special features are supervised with BCELoss after MLP, while common features are supervised using image reconstruction loss after a convolutional layer.
7) Algorithm selection in Table 1(R4Q3): Our requirement for comparative methods is that they are highly generalizable. Most existing works use transformers and CNNs as their main structures, then integrate residual structures and attention mechanisms for specific optimizations. Following your comment, we tested the FairAdaBN(MICCAI 2023), which achieved 78.41% accuracy, which is lower than the best result of 82.95%.
8) Related work(R4Q2, R5Q5): We will add more previous related work in Introduction.
9) Figures and visualizations(R4Q1, R5Q6): At this stage, we can only provide text and canât offer visual results. If space permits, we will include visualizations in the manuscript or release them as open-source material. We will add more detailed legends to the figures.
10) Further explanation for AT and its effectiveness(R1Q6, R5Q2&3): Images are processed by ICFDNet to get two decoupled features. These features are concatenated with the Y-channel and sent to the classifier for further training. Adversarial loss affects the entire training process by maximizing the KL divergence between adversarial and clean samples. AT identifies and optimizes classifier weaknesses by causing errors and adjusting its adversarial strength based on the accuracy of each class. Experimental results show all classifiers improved, with the best achieving a 3.4% increase, demonstrating our methodâs generalizability. Additionally, our method only increases training consumption without adding inference complexity.
11) Inference process(R5Q4): Images are fed to two ICFDNet models in parallel. The decoupled features are concatenated with the Y-channel and fed into the classifier for inference results.
12) Reproducibility(R3, R4, R5): The code and data will be shared via Github."
https://papers.miccai.org/miccai-2024/664-Paper2031.html,"We greatly appreciate positive feedback from reviewers regarding the novelty (R1 R3 R4), âconvincing experimentsâ (R1), âelegant wayâ (R3), clear descriptions (R1 R3), and all the constructive comments (R1 R3 R4). We summarize comments and our responses below.

R1,R3: We thank R1&R3 for their careful review and encouraging suggestions. The minor concerns will be carefully revised in final paper.

R1,R3,R4: Reproducibility. We will release the code for reproducibility.

R3: 1) Interpolation. In Fig.1, different US scan views introduce varying distortions. We train a separate SDF for each view to learn its unique shape, and fuse them using set operation. For a naive 2D example, the column-scan elongates a rectangleâs height, whereas row-scan elongates the width but maintains the original height. The union of these scans reflects all the distortions: including elongated height and width, while the intersection keeps the original height and width. The union and interpolation can be used for future shape completion[18].
2) Eq.2. We will use â.â instead of âxâ to void misleading.
3) The last paragraph refers to fitting the MLP.

R3,R4: Loss. In addition to newly introduced regularizers, Lscc and Ladl are two constraints in UNSR[6]: sign consistency constraint and on-surface adversarial learning, to improve Lsdf in [17]. Lsdf is an L2-distance loss to optimize the projected xâ² in Eq.2 reaching its nearest x^hat in Eq.6. The released code can help follow the details.

R4: Method Clarifications
1) Eq.1. SDF is a concept in computer vision to describe the objectâs shape and geometry. In Euclidean space, for a 3D point x and a watertight object surface S, d(x,S) is the orthogonal distance from x to S. The sign is positive (negative) when x reaches the surface from outside (inside) of object.

2) LevelSet. The value of d(x,S) decreases to 0 when x is on the surface. Thus, the surface is defined by zero-level-set of SDF, formulated by all points where SDF(.)=0.

3) Neural SDF. The use of MLP (fÎ¸) to represent SDF has gained great attention [11-18]. The zero-level-set of neural SDF is denoted as fÎ¸(.)=0. Here, we build a novel framework to address the challenges of view-dependent and discrete pixel connectivity in multi-view US reconstruction by directly operating shapes in a continuous neural SDF field.

4) Eq.2. This equation, defined following Eq.1, describes how a point x can reach its nearest surface point xâ along or against gradient based on the signed distance. Here, fÎ¸(x) is the predicted signed distance, and âfÎ¸(x) is the gradient derived by back-propagation. Eq.2 is part of Eq.6 for self-supervised learning of SDF.

5) Hyperparameters. We empirically set them based on the findings in [21].

R4: Data
1) Phantom. For one model scanned by one UT, each row-column scan corresponds to 1 shape and 2 scans. Two UTs obtain 12 shapes and 24 scans from 6 models. An average of 540Â±159 frames from UT1 and 686Â±244 frames from UT2 are collected. We learn the SDF directly from each individual input without splitting dataset for implicit neural representation (INR)[6,17].

2) In-vivo. In this paper, our primary goal is building a framework to accurately restore the shape geometry from multi-view US scans. To validate our method, we use the phantom data since it has clean images and ground-truth CAD model. As noted in the Introduction, for hard tissue imaging, US signal struggles to penetrate target surface and is susceptible to interference from soft tissue in in-vivo data, which hinders the evaluations. Thus, the phantom or simulated data becomes a practical choice to validate INR-US in recent studies [6,9,10]. In future work, other modules can be added to the framework to consider interference, e.g., denoising module.

R4: T-test. Statistical significance is established with p-value<0.01 against UNSR across all metrics.

R4: Tracking resolution. We report it using accuracy-position-1.4mm-orientation-0.5^o from specifications of NDI-driveBAY."
https://papers.miccai.org/miccai-2024/665-Paper2092.html,"First of all, we would like to sincerely thank all reviewers for their constructive comments and all chairs for their great efforts. We express our heartfelt gratitude to the reviewersâ recognition on our work, for example, ânovel methodâ (R1), âchallenging application, functional model, and new toolkitâ (R3), âgood ideaâ (R4), and so on. In the following, we will respond to all questions and concerns point by point.

Results on more datasets (R1,R3):
Although we are very willing to include more datasets in our revised manuscript, we would like to give some key reasons why we think the SUN-SEG only is also sufficient in this study: 
(1) SUN-SEG is the well-known largest dataset for video polyp segmentation, and it provides 49,136 annotated frames. The other two comparable datasets, i.e., ASU-Mayo and PolypGen, only provide 3,856 and 3,788 annotated frames, respectively. SUN-SEG is 10+ times larger than them.
(2) SUN-SEG is from the most comprehensive and realistic clinical scenes, such as fast camera motion, complex lighting, surgical instruments, diverse polyps, and so on. In comparison, the other existing datasets have relatively limited scenes.
(3) Based on the above two consensuses, there are lots of top-notch works (PNS+ and MAST) also benchmarked on SUN-SEG only.
Having said that, we agree that a public dataset has a certain limitation. In fact, we are already putting efforts on collecting a large-scale multi-center private dataset with local hospitals. In our future work, we will include and release the private dataset to make more contributions to the field.

Working condition (R1):
Although our method achieves a new SOTA performance, it still has some limitations, for example, it makes false positive predictions on a few highly polyp-like sites such as ileocecal valve. Having said that, these limitations are challenging and shared by all polyp segmentation methods. In the future work, we will take them into consideration, and push the performance to a new extreme.

Clinically-practical values (R3):
The clinically-practical contributions of our method include, but not limited to, (1) provide accurate edges to assist surgical resection; (2) provide accurate morphological information to assist polyp typing diagnosis. We agree that it is valuable to experimentally assess these contributions, but it involves lots of new efforts, which are hard to be squeezed into the MICCAI paper. In our further work, we will assess them comprehensively.

Backbone fairness (R4):
Actually, our method uses the consistent backbone (PVTv2) with the two BEST comparison methods, i.e., MAST and SLT-Net. Thus, using PVTv2 in our method is still convincing for comparison. Having said that, we are willing to give the variant of using Res2Net-50 as backbone, which is consistent with the other four comparison methods, and list the corresponding MACs of all methods (see the code link).

Stability and reliability of the features (R4):
Stability: The upper case of Fig. 3 indicates two adjacent frames with large spatial variations. As can be seen, the results of SALI are not only more accurate, but also more temporally stable (more similar across frames), which we believe can verify the stability of the learned features. Besides, we are going to include a specific metric of Temporal Stability [1], to quantitatively verify the stability.
Reliability: The bottom case of Fig. 3 indicates consecutive low-quality frames, and the segmentation results on them are strongly correlated with the reliability of the learned feature. As can be seen, SALIâs results are closest to the GT masks. Besides, we are going to construct a specific sub-set of SUN-SEG containing all the consecutive low-quality sequences, and separately report the comparison results on the sub-set to quantitatively verify the reliability.

[1] A benchmark dataset and evaluation methodology for video object segmentation. ICCV. 2016."
https://papers.miccai.org/miccai-2024/666-Paper3533.html,"We thank the reviewers for taking their valuable time in reviewing our work and providing with insightful comments. We have arranged the replies to the reviewers based on their concerns.

Typos and Grammatical Errors (Reviewer 1, 3): We have carefully reviewed and corrected all typographical and grammatical errors throughout the manuscript.

Selection of SAM Encoder (Reviewer 1, 4): We opted for the base model checkpoint as it contained more ambiguous global information required for nuclei segmentation. We only use the output from the image encoder of SAM. 
Initially, we intended to use medical domain based vision models like MED-SAM or I-MedSAM but it was not viable options due to the unavailability of the saved checked points at the time, and the authors had indicated that these models were not adequately trained on pathology images for ensuring high performance. Consequently, we opted for SAM despite its limitations.  We plan to explore these medical-domain specific models in future work, although we believe that the enhancements offered by our proposed methods will remain beneficial even then.

Insights from Feature Visualizations (Reviewer 1): To provide clear insights, we have revised the description of Figure 3 clearly explaining each of the sub-figures. We show that the encoding after the application of the proposed X-GFB produces a better and detailed representation at the bottleneck of the network. The bottleneck compresses all learned features into a dense encoding. It is widely recognized that a more detailed and rich representation at the bottleneck enhances the modelâs performance. Figure 3 clearly shows that our proposed method of fusing SAM encoding with our task-specific eU-Net3+ encoding using X-GFB provides a better representation leading to better performance which we show through our experiment results.

Ablation Study (Reviewer 1, 3): A detailed ablation study is indeed needed for the validation of the proposed methodology but we are unable to do so due to the constraints of the rebuttal submission guidelines. But we do think that the significant performance improvements and achievement of SOTA results by incorporating our proposed methods demonstrate the effectiveness of our approach. Additionally, we have attempted to mitigate the absence of a full ablation study by providing justifications through intuitions from Figures 2 and 3, which illustrate the rationale behind our method.

Description of Novelty (Reviewer 2):  We agree that the novelty of our method could be highlighted more prominently and we have done so in Section 2.

Experimental Details and Source Code (Reviewer 2, 3): We appreciate for pointing out the need for detailed and some missing experimental settings and made changes as below -

The training process spans 50 epochs, with an initial learning rate set to 0.0001 and batch size of 16. We use Adam optimization along with drop out of 0.3. For the loss function, we use a combination of weighted Dice and focal loss with equal weights, aiming to balance the training focus between prevalent and rare segmentation targets. We plan on making the source code publicly available upon publication ensuring the reproducibility of our results."
https://papers.miccai.org/miccai-2024/667-Paper3979.html,"To Reviewer 1

To Reviewer 3

To Reviewer 4"
https://papers.miccai.org/miccai-2024/668-Paper3565.html,"Reply to R1 - Anonymity requirement:
We would like to clarify that we used the Lecture Notes in Computer Science template. To adhere to the space restriction, we did not make any changes to the default templateâs title. The affiliations that appear on the manuscript are not ours but the ones in the template.

Reply to R1 â Model without SAFA size:
For the ablation study without the SAFA module, we upsampled the encoded features and concatenated them with the series of features from the higher resolution image after the transpose convolutional layers. The concatenated features were then sent to a convolutional layer before interacting with the high-resolution image. The proposed network without SAFA has 25.868M parameters, while the full model has 25.874M parameters.

Reply to R1 and R4 â Novelty of SAFA:
Thank you for raising this concern. We would like to clarify that the inspiration for SAFA stems from our observation that the first layer of the transformer encoder contains the most information about boundaries and texture. To effectively utilize this local information, we introduced the SAFA module, which multiplies features from shallow to deep layers, progressively refining the local context by leveraging features from the shallower layers of the encoder. 
The g^n Conv module proposed in HorNet has a recursive design with large kernel gated convolutions. The g^n Conv module is used to achieve higher-order interactions, primarily aimed at capturing long-term interactions. In contrast, SAFA focuses on multi-scale feature fusion and featuresâ local information enhancement. These differences in both methodology and purpose distinguish our approach from that of the g^n Conv module.

Reply to R4 and R5 - Lack of validation and analysis:
Thank you for your feedback regarding the need for additional validation @R4 and a thorough analysis of suboptimal performance @R5. We recognize the importance of these aspects in strengthening our research. We will incorporate comprehensive validation and detailed performance analysis in our future work.

Reply to R4 and R5 - Comparison to other SOTA methods:
In Table 2, the proposed model is compared with state-of-the-art (SOTA) models that have a similar size of parameters and FLOPs. We greatly appreciate the reviewersâ suggestions regarding the inclusion of MERIT-CASCADE, Parallel-CASCADE, and MERIT-GCASCADE as other SOTA methods on the ACDC dataset. However, it is important to note that these models have significantly higher parameter sizes and FLOPs compared to our proposed network. For example, Parallel-MERIT has 148M parameters and 33G FLOPs, achieving a dice score of 92.32 on the ACDC dataset. Similarly, MERIT-GCASCADE has 132M parameters and 25G FLOPs, with a dice score of 92.23 on the ACDC dataset. Due to these substantial differences in parameter sizes and FLOPs, we did not include their performance in the model comparison. Moreover, our proposed network achieves a dice coefficient of 92.29, which is comparable to the aforementioned SOTA methods, while utilizing 82.52% and 80.40% fewer parameters than Parallel-MERIT and MERIT-GCASCADE, respectively. Additionally, our model requires 83.39% and 78.08% less FLOPs than Parallel-MERIT and MERIT-GCASCADE, respectively.

Reply to R4 and R5 - Data access and data selection:
The data used in the paper is publicly available. The cardiac dataset is the well-known ACDC dataset [20] that has been used for a previous MICCAI challenge, while the speech dataset was recently published and is the only one to include both speech MRI and ground truth segmentation [18,19]. 
Those data sets were selected as:  1) Our extended group has an interest in speech imaging; 2) We wanted to validate on different types of dynamic MRI and cardiac MRI segmentation is also clinically relevant. We appreciate R5âs recommendation of additional datasets and will explore them in our future work.

Reproducibility:
We will make the code available in the final version."
https://papers.miccai.org/miccai-2024/669-Paper1664.html,"We thank all the reviewers for their valuable suggestions. We were encouraged by the positive comments on the novelty and extensive experiments in real-world settings by all reviewers. We address their concerns below. 
#R3 [Dependence on Synthetic Data & Potential Overfitting] As pointed out correctly, in supervised learning, the quality of synthetic data is crucial, like SyntheticTumors (CVPR2023); DiffTumor (CVPR2024). We agree that using synthetic data for supervised training may cause overfitting. However, we did not use the âsynthetic imagesâ for supervision on segmentation. Instead, we aimed to prompt the network to focus on key âtumor semanticsâ in an unlabeled manner and facilitate pre-training, similar to SemMAE (NIPS2022); EfficientSAM (CVPR2024). Our downstream training was based completely on real clinical data. The highest performance on the real clinical data achieved by our method shown in Table 1 demonstrate that the facilitated pre-training could mitigate overfitting in the downstream tasks. This finding is consistent with arXiv:1906.12340 (NIPS2019); arXiv:2206.04664 (CVPR2023).
[Extended Comparison] We compared with various methods, including MedNeXt, but did not include it because of a DSC of 57.8 lower than ours 64.7. Instead, we rather focused on comparisons with a broad range of SOTA self-supervised methods.
#R3, R4, R5 [Ablation studies] As suggested, we conducted 5-fold cross-validation ablation studies to investigate the effects of Synthetic Masking (Sec 2.1) and Frequency Branch (Sec 2.2). Compared with the baseline, the Baseline + Sec 2.1 achieved +5.0 DSC and -19 HD; adding Sec 2.2 further improved by +1.8 DSC and -16 HD.

#R4 [Justification of Frequency Branch] As suggested, we did ablation studies to investigate that (see the results mentioned above). As seen, the effectiveness of this branch was significant.
[Structural Operations] As suggested, we analyzed the initialization of the radius for the expansion and erosion based on the number of voxel points (1-5). Fewer voxel points (1,2) resulted in sparse bowel wall semantics, while too many (4,5) caused excessive adhesion. Hence, we configured the radius to 3. This will be mentioned in the final version.
[Minor Mistakes and Other Suggestions] As suggested, we will fix all the errors you pointed out, re-organize Experiments and Related Works, add the standard setup of Swin-UNTER, and professionally proofread the final version.

#R5 [Abbreviations] As suggested, we will define all abbreviations in their first appearance in the final version.
[Diverse Validation Dataset] In addition to the novel technological development, this studyâs clinical motivation is to tackle the segmentation of colon tumors in non-contrast (NC)CT, a task few have explored. Current datasets are based on contrast CT, so we aim to establish a benchmark for tumor segmentation in NCCT. Our dataset of 110 cases aligns with mainstream CT validation benchmarks (e.g. BTCV with 30, MSD-liver with 131, MSD-colon with 126). This unique NCCT validation benchmark is a valuable clinical contribution, and we plan to expand it further.
[Clarification of Weight Transfer] Swin-UNETR includes a Swin-Transformer feature encoder and a U-net backbone. During pretraining, an extra frequency branch was added to the backbone. For finetuning, only the Swin-Transformer weights were transferred, and the backbone was reinitialized. This clarification will be included in the final version.
[Transparency of Dataset Source] We had to anonymize the dataset source as âXXXâ because of the MICCAI anonymization policy. This dataset was specially collected for our study from our collaborating hospital because there was no public NCCT dataset for colon tumors. We will be able to reveal it in the final version. 
[Extended Comparisons] As suggested, we validated on SegResNet, in addition to our comparisons with a broad range of SOTA self-supervised methods. SegResNet achieved a DSC of 57.4, lower than ours 64.7."
https://papers.miccai.org/miccai-2024/670-Paper3047.html,"We appreciate the reviewersâ feedback (R#1, R#3, R#4), and address these concerns below.
-Figures (R#1): we will improve figures about the architectures of SFP and ASR and include figures to show figures of high and low consistency samples. Then we will discuss the advantages and limitations of the proposed system in camera-ready paper.
-Figures (R#1, R#4): due to the page limitation, we only showed segmentation results quantitatively in tables. We will show output segmentation results and plots with the AL curves in camera-ready paper.
-Novelty notion (R#3): we believe using segmentation results to understand input samples in SBC-AL is novel. It has not been adopted to evaluate uncertainty for querying in AL by considering the consistency in both anatomical structures and boundaries.
-Ablation study (R#3): We showed the results for the overall uncertainty, but in early AL stage, we evaluated the correlation of SCS and BCS by calculating them separately for unlabelled data. It can help to understand their differences and be used as ablation study. We will discuss it in camera-ready paper.
-Novelty (R#4): We believe our SBC-AL is novel in motivations and designs. Our boundary consistency score (BCS) is motivated by Hausdorff distance, but it has not been adopted for querying in AL before. It is used for evaluating segmentation accuracy for labeled data, but BCS is used for evaluating uncertainty in unlabeled data. The calculation of BCS is also different from Hausdorff distance. Our SFP and ASR are proposed as novel and efficient modules. They have not been published before, and their motivation is to facilitate evaluating uncertainty and calculating consistency scores during querying. Overall, we have proposed this well-designed and well-motivated AL methods. The novelty has been highlighted by other reviewers.
-Experimental details (R#4): the initial labeled set was generated randomly and was the same for all AL methods in each experiment. We repeated experiments 5 times with different initial sets. In our paper we showed the average values of these experiments, and we will include standard deviations in camera-ready paper.
-Experimental results (R#4): The same model was used on all AL methods with same initial set. When 100% train data, the results are the upper limit for the model performance and used for comparison. In 6.6% data, different AL methods have the same dice since this segmentation model is initialized by the same initial set. VAAL has a higher dice, since it has a UNet encoder with self-encoding training. We will further describe plausibility of our results in camera-ready paper.
-Comparison methods (R#4): In Table 4, we aim to show that our querying is superior in different segmentation backbones. Thus, we showed several superior AL baselines, and compared results of our querying with them due to the page limitation on MICCAI paper. Similarly, we did not show results of VAAL and Mean STD in Table 2. They are inferior to other baselines, so we only showed results of several superior methods for better comparison. We will improve our descriptions in camera-ready paper.
-Random sampling (R#4): the suggestion of random sampling is helpful. Our solution was to evaluate the effectiveness of our query strategy and segmentation networks separately. We used the same networks for our querying strategy and others, demonstrating the superiority of ours (Tables 1 and 2). We also evaluated the effectiveness of our modules by end-to-end training of these modules without AL querying (Table 3).
-Individual classes (R#4): In KiTS19, we used kidney labels and avoided tumor labels to avoid the effect of unbalanced data. In ACDC, samples are queried based on the uncertainty of individual samples by calculating consistency scores for all foreground regions, not any specific class, so we only showed average results over all classes.
-Typos and Equations (R#4): we will correct typos for SFP and update our equations in camera-ready."
https://papers.miccai.org/miccai-2024/671-Paper2991.html,"We thank the reviewers for your constructive and valuable feedback. We appreciate your recognition of the novelty and interpretability of our method. Especially, thanks to R1 and R4 for accepting the paper directly.

To R1 and R4
The hyperparameters of our model were determined by pre-experiments on the LUAD dataset and used on all datasets. Due to the length of the paper, we have not listed all of them in the submitted paper. Thanks for suggestions of ablation experiments, which we will consider including in the journal paper.

To R1
1.SoftFilter and SCSA are interdependent modules.With numerous patches as input, using SCSA alone leads to overfitfing on noise patches. Adding the SoftFilter guides the SCSA to learn interactions among key areas, as evidenced by the ablation experiment in Section 3.2. A description of the relationship between each module will be added to the first paragraph of Section 2 to help understanding.
2.By pairwise t-test, do you mean Kaplan-Meier (KM) analysis? Class-based KM analysis is not specific to the patient, but rather an entire population. We adopt a more suitable time-dependent evaluation metric, TDC, to evaluate the modelâs capacity to predict individual survival probability distributions.[1]

To R3
1.In employing K-Means, the distance is defined as dist=w_1(1-feat)+w_2pos. Where feat denotes the consine similarity between patch features and pos represents the normalized Euclidean distance between patches.
2.L_i is a collection of features of the i-th cluster. Â Attempts to âaggregate features â¦â, resulted in information loss and degraded modelâs performance. Moreover, the primary computational overhead and memory usage are not within the AMIL and these cost are justified.
3.Sparse self-attention optimizes the self-attention mechanism by confining interaction to local positions. Different sparse self-attention methods mainly focus on how to choose the locations for interaction. The SCSA we proposed determines the interaction locations through clustering. [1] has similar idea and also is considered as a form of sparse self-attention.
4.w_i(Featâ) refers to the weight of the i-th component in MDN. To reduce confusion, we will rename it with a new variable a(Featâ).
5.The optimization of importance scores is described in section 2.1:âthe features of each patch are element-wise â¦ without requiring patch-level supervision.â, resembling Gated MLP.
6.âCSSAâ is a typographical error of âSCSAâ. Thanks for pointing it out.
7.Table 1 shows the âthe task relevant features are fed directly to AMILâ method. We also have attempted NystromAttention in TransMIL, but it performed worse than out proposed method.
8.P_m and P_v represent the mean vector and standard deviation vector input to the Gaussian mixture model and are learnable parameters independent of the individual data. Considering the potential for misunderstanding, we have renamed them to âregistersâ, following [3]. This implies that P_m and P_v register the information related to cancer patient cohort.
9.A distinct Gaussian mixture model is used to model each patientâs survival probability distribution. Across different patients, they share the mean vector and standard deviation vector, while weight vectors are derived from patientsâ pathological images. Consequently, P_m and P_v contain the global information of the group.

To R4
Thank you for your suggestions regarding our Figure 1. We will carefully consider them and make the necessary enhancements.

[1]Haider H,et al. Effective ways to build and evaluate individual survival distributions[J]. Journal of Machine Learning Research,2020,21(85):1-63.
[2]Wang S,et al. Cluster-former: Clustering-based sparse transformer for question answering[C]//Findings of the Association for Computational Linguistics:ACL-IJCNLP 2021.2021:3958-3968.
[3]Darcet,et al. âVision transformers need registers.âÂ arXiv preprint arXiv:2309.16588Â (2023)."
https://papers.miccai.org/miccai-2024/672-Paper0821.html,"Thank meta reviewer for the early accept decision. We are deeply thankful to the reviewers for their valuable feedback and constructive criticism. Their positive assessments of our paper as reasonable (R1), well motivated (R3), and Very Good (R4) are truly encouraging. We are pleased that our approach is recognized as very promising (R1), experiments are very comprehensive (R3), valuable (R4). We now present detailed responses addressing the main concerns raised. Q1: Should include the complexity metrics of different methods. (R1) A1: Due to space limitations, we could not discuss the complexity metrics in detail. We will evaluate the model from a more comprehensive perspective in future work. Q2: Ablation studies with different alpha and beta values. Using the same structures for the two students for experiments and discuss diversity. (R1) A2: These two parameters are thoroughly discussed in BCP, so we did not repeat the study. Furthermore, regarding model diversity, current semi-supervised methods typically rely on consistency learning under various perturbations. Network perturbations usually involve different architectures or initializations to build the network. Here, we used different model structures, essentially representing different perturbations. Our approach focuses on correction learning based on prediction differences. Q3: Discuss the performance on other segmentation tasks such as multiple small target segmentation tasks (R1) A3: Evaluating our framework on tasks such as brain tumor segmentation and other small target segmentation tasks is highly valuable. However, due to space limitations, we chose mainstream semi-supervised task datasets for evaluation. We will extend our framework to more diverse datasets in future work. Q4: Review related work, such as [1], and conduct ablation studies without BCP. (R3) A4: We will add the discussion of [1] in the final version. [1] uses differences in learning paradigms of different network structures to achieve perturbation effects, followed by consistency learning through cross teaching. Our approach is based on correction learning from students prediction differences. Due to space and conference limitations, we will improve our framework in future work, demonstrating its effectiveness under the Mean Teacher framework and in combination with other methods. Q5: Reasons for outperforming fully supervised methods and issues regarding Figure 1. (R3) A5: We believe that reducing empirical distribution gap is crucial for semi-supervised learning. However, the inherent bias in BCP can severely affect performance, so we chose BCP to demonstrate our frameworkâs performance. By effectively utilizing unlabeled data combined with correction learning, we achieve superior performance. When drawing the figure, we structured it to help readers better understand. Increasing the size of the formula text might significantly alter Figure 1 and increase difficulty in understanding. Q6: About the bias issue addressed by BCP and improvements with SDCL. (R4) A6: The advantage of BCP lies in reducing the empirical distribution gap, where the knowledge learned from labeled data is well-preserved. However, there are issues with learning local attributes and low-contrast target segmentation, which are potential bias areasâthis is a drawback of BCP. SDCL efficiently solves these bias problems through correction learning based on prediction differences. We sincerely appreciate the reviewersâ valuable feedback and will carefully integrate their suggestions to improve our work. We highly value and are grateful for their time and effort."
https://papers.miccai.org/miccai-2024/673-Paper2098.html,"Thanks to all reviewers for taking the time to review and provide feedback for our submission. Please see below for our response to major weaknesses:
1.Main Contributions(R3, R4)
Our paper addresses the challenge of balancing accurate geometric reconstruction with fast training speeds in internal environments. Our framework combines Lerplaneâs six-plane structure for acceleration with NeuSâs SDF rendering for precise geometry. As noted by reviewers, this simple yet effective design allows us to achieve accurate 3D structures without extended training times. Furthermore, we found limitations in Lerplaneâs sampling strategy. To address this, we developed a more versatile Error-Guided Importance Sampling strategy. This approach significantly improving efficiency and achieving superior results. It also broadens applicability beyond single-viewpoint scenarios, marking a key difference from Lerplane.
2.Training Time Accelerate(R3)
The training speed improvement in SDFPlane is primarily due to encoding spatial dynamic scenes with a multi-resolution six-plane structure. Essentially, it replaces implicit structures with explicit ones, significantly reducing the number of parameters the model needs to optimize. Besides, the six-plane structure uses trilinear interpolation from eight neighboring points for faster feature queries than MLP.
3.Convergence Criterion (R3)
As mentioned in Sec 3.2, we standardized the batch=2048 and epoch=9600 for different methods to test model performance. We did not compare models based on the same training duration for several reasons. First, for models like EndoNeRF and Endosurf, which require hours of training, a few minutes of training is insufficient to demonstrate their capabilities. Second, maintaining consistent training times in code implementation is challenging and may lead to experimental inaccuracies if not precisely managed.
4.Render Time(R4)
The rendering time for SDFPlane is 8 min, while Endosurf takes over 30 min. This is a huge improve in rendering speed for SDFs. Fast high-quality reconstruction is very important, and our reconstruction quality and training time meet practical requirements. 
5.NeRF vs 3DGS(R4)
Compared to NeRF, 3DGS has the advantage of faster training speeds and can produce high-quality, high-resolution images from new viewpoints. However, in internal environments, NeRF holds an advantage. Firstly, internal datasets often lack enough viewpoints. Since 3DGS uses direction-sensitive SH coefficients to calculate color, limited viewpoint information can easily cause training failures. Secondly, the 3D ellipsoids in 3DGS make it difficult to define surface normals and depth, resulting in poor performance with SDF-based methods.
6.Retrained?(R4)
Our experimental results are derived from models we trained ourselves by modifying the SOTA to use batch=2048 and epoch=9600. Apart from that, we didnât modify the baseline methods in any other way.
7.Training Time(R4, R5)
The training time for w/o samples is similar to that with the sample strategy.
8.Sample(R5)
Firstly, Lerplaneâs sampling strategy uses a prior mask from input images to focus learning on masked areas, but it lacks the ability to adjust based on the modelâs learning progress. In contrast, our method updates in real-time based on previous loss data, allowing the sampling weights to be adjusted according to the learning progress. This enables our method to adapt and refine the sampling process throughout the training period. Secondly, the implementation of this sampling involves averaging all images and measuring the differences between each image and the average to find areas with big deformation. This approach is only suitable for single-viewpoint datasets, as average image loses its meaning when the camera moves. Our proposed method isnât restricted by camera movement, making it more broadly applicable.
9.Reconstruct Quality(R5)
The average RMSE: 0.0247(endonerf), 0.0191(endosurf), 0.0479(lerplane), 0.0157(sdfplane)."
https://papers.miccai.org/miccai-2024/674-Paper2373.html,"Thanks for the positive feedback! We are encouraged that the reviewers found the new task of surgical planning interesting (R4), valuable (R4), important, and novel (R5). The idea is considered straightforward (R5), highly significant (R1), clear, easy to follow (R5), and interesting (R3). We are pleased that our method using diffusion is recognized as the first attempt (R1), a new line of research (R3), and the first baseline and benchmark (R4) in the field. The work is also seen as interesting and beneficial for the community (R4). In the final version, we will provide further clarity on the novelty and contributions and expand the discussion section to include more potential improvements and clinical insights related to our approach. We address specific reviewer concerns as follows:

Reproducibility (R3, R4): We will make the source code, data, and pre-trained models publicly available by including a link in the final version.

Contribution Clarification (R1, R4): Our focus is to introduce a novel task aimed at predicting target-conditioned actions in surgical videos to achieve desired visual goals. This represents a significant shift from existing research that primarily focuses on recognizing or predicting the current or future states in surgical videos. In addition, we construct a dataset and build a strong baseline using the proposed diffusion method to establish a new benchmark for evaluation. Automating the planning of surgical procedures can enhance robotic surgical systems. By predicting the sequence of actions needed to achieve a surgical goal, the system can avoid unnecessary movements and minimize errors, improving patient safety. Our benchmark for procedure planning in surgical videos allows systematic evaluation of different approaches, contributing to future research.

Figure Visualization & GT (R3): We will add annotations for surgical images and include âGround Truthâ in Fig.2 to avoid misleading in the final version.

Clarity and Organization (R4): Thanks for pointing out the issue. We will provide further clarity on: (1) In Fig.1, the inputs are only the initial and goal observations (Oi and Og), while the outputs are action sequences and the optimal scale selection given the input pair. We will change âstartâ to âinitialâ for more clarity. (2) Besides multi-scale diffusion, we propose to employ ViT rather than MLP in PDPP for phase classification (at the end of Sec 2.1), since we found that the phases are very similar, which makes it difficult for MLP to classify accurately. In this regard, our results (s=1) are better than PDPP. (3) We will adjust Fig.3 and add more captions for clarity.

Missing Selection Details (R5-1): We clarify that the scale selection branch adjusts input scales as conditions and cascades diffusion models at different steps for training, optimized indirectly through the procedure planning objective to avoid additional losses and hyperparameters. (as mentioned in the third paragraph of Sec 2.2). The outputs include both predicted action sequences and the optimal scale (see Fig.1), which is then used to update the inputs (Oi and Og) for the next denoising process. The scale selection branch randomly selects the scale during the initial 50 steps of each 200-step epoch. When the scale changes, the initial and goal observations in Fig. 1 are updated, resulting in changes to the action sequence prediction, which in turn affects the loss in Eq. (4). This guides the scale selection branch to achieve more accurate action sequence predictions. We will enrich the descriptions in the final version.

Limited Ablations (R5): Thanks for your suggestion. In our future work, we plan to explore features extracted from pre-trained models on surgical videos, e.g., GSViT (arXiv:2403.05949, Mar 2024) for ablation analysis.

Empty Spaces & Dws (R5): We will revise Fig.3. âDwsâ means diffusion branch for window scale selection, and we will add the descriptions in the final version."
https://papers.miccai.org/miccai-2024/675-Paper3643.html,"We thank the reviewers for the valuable comments.
R#1 Q6 (rejected cases):
Bias: This procedure might introduce bias, but evidence suggests otherwise. Registering contrast and non-contrast CT images is challenging due to their differences, with success varying by registration and case specifics. We hypothesize that poor registrations are not linked to the anatomical properties of the aortic root. By rejecting cases with incorrectly registered ostia, we believe our dataset is improved; Scalability: Rejecting poorly registered cases is efficient, taking <1 min/case, much faster than delineation or manual 3D modeling, making it scalable; Performance: The modelâs precise reconstruction of the aortic root suggests no significant bias. We compared two meshes with anatomical information from contrast and non-contrast CTs. Rigid registration accurately identified corresponding regions, supporting our approachâs validity.
R#1 Q6+Q10 (other metrics):
Indeed, providing more metrics could be beneficial. However, we believe our measure is appropriate for the following reasons: Suitability of Dice: The aortic valve (AV) in our modeling is a 2D surface object, and Dice is best for comparing volumes. Analyzing a section of the aortic root with the AV would result in varying outcomes based on the section, so we opted for surface-based measures; Analysis of Distance Distribution: We analyzed the distance between the non-contrast AV mesh and the registered contrast AV mesh. The std. dev. averaged around 0.6mm. By estimating Hausdorff-95 as the mean distance plus 2 std. dev., we get approx. 2mm. Fig. 3 shows this distance with a colormap on the registered mesh (range: [-2mm, 2mm]); Advantages of Mean Distance: The mean distance is less sensitive to insignificant AV details not fully resolved in non-contrast CT. Significant deviations were found in the middle of the AV leaflets, parts of the ostia, and leaflet positions (relevant to this study). Thus, we used the mean distance, which is less affected by outliers. The non-contrast 1.5mm z-spacing provides an absolute scale for our findings.
R#3 Q6.1 & R#1 Q6 (novelty):
Our novelty lies in combining SOTA, open-source codes to address novel problems, ensuring high reproducibility and focusing on real-world applications. By tackling unresolved & challenging problems in a non-obvious manner while leveraging SOTA, we believe our contribution is valid and impactful.
R#3 Q6.2 (dataset):
While AV segmentation in non-contrast CT is typically independent from AV calcification detection, it is crucial for identifying AV calcium, as in [7], which drives our work. Our models were also trained on scans with AV calcifications. Although we showed results on an open dataset, we evaluated the method on scans with AV calcifications and found no influence on AV segmentation.
R#3 Q6.3 (evaluation & limited AV visibility):
A key result of our work is the quantitative evaluation method for AV reconstructions in our ML model. Using our ICP-based procedure, we identified local transformations of the AV region by utilizing the visible part of the aortic root. This approach allows for an exact comparison of the AV, which is not directly visible in CT images, as shown in Fig 3. It is one of the most comprehensive approaches available, as direct segmentation of the AV in non-contrast CT is not feasible.
R#3 Q6.4 (comparison with SOTA):
Directly comparing with SOTA methods is challenging due to their limited reproducibility. Based on visual inspection, our approach demonstrates superiority by accurately segmenting the AV, unlike other methods that yield coarse segmentations of the aortic root.
R#3 Q8 & R#4 Q8 (source code):
As stated on page 7, we provide access to the code for the ICP Method for Accuracy Estimation. The other methods are based on open components.
R#3 Q10 (errors):
Thank you for noticing the error in Fig. 3, weâll correct it.
R#4 Q6 (anomalies):
Thanks for this comment. We will include it in future papers."
https://papers.miccai.org/miccai-2024/676-Paper0663.html,N/A
https://papers.miccai.org/miccai-2024/677-Paper0518.html,N/A
https://papers.miccai.org/miccai-2024/678-Paper0869.html,"Response to Common Questions:
[Q1] There are minor mistakes in writing such as spelling, grammar and notations.
[R1] Thanks for the comment. We will correct these mistakes in the camera-ready version of this paper.

[Q2] More discussion about related work is needed.Â 
[R2] We will try to include more discussion of the related works and add relevant references in the camera-ready version of this paper.

Response to Reviewer 1:
[Q1] No comparison with GAT.
[R1] We included the comparison with GAT in Table 1. And our method can outperform the baseline with GAT.Â

[Q2] The discussion about the computational cost and scalability of the method is needed.
[R2] The computational cost of the method grows quadratically with the number of nodes within the graph data and grows linearly with the size of uncurated domain knowledge. When the size of graph data and domain knowledge becomes huge, it may cause scalability issues. We will try to address this challenge in future work.

[Q3] Further explanation about fusion graph and fusion GNN is required.
[R3] The fusion graph consists of one graph node with graph embedding and N knowledge nodes with knowledge embeddings. The graph embedding is derived from the graph data. And each knowledge embedding is derived from the corresponding domain knowledge in text modality. In the fusion graph, the graph node is connected to all the knowledge nodes. Therefore, the fusion graph has N + 1 nodes and N edges, where the adjacency matrix can be simplified to a 1 X N vector. Moreover, fusion GNN is applied to the fusion graph to get the final prediction of the given task.Â

Response to Reviewer 3:
[Q1] More clarification of pretrained model h is required.
[R1] The pre-trained model h can be an arbitrary pretrained language model. In our experiments, we use Bert for evaluation. Please refer to our supplementary materials for the details of Bert.

[Q2] More explanation about the evaluated graph dataset.
[R2] DTI and fMRI are two methods of brain imaging to get the brain images from a given set of subjects (e.g., OASIS). The obtained brain images are used to derive the brain networks, serving as the graph input to the multi-modal GNN. Moreover, fMRI can generate bold signals that are used to build the functional brain network, serving as graph data.

[Q3] More explanation about Fig. 3.
[R3] Fig. 3 shows the top 10 salient regions of interest (ROIs) found by our methods. The colors are only used to distinguish different highlighted ROIs.

Response to Reviewer 4:
[Q1] The ablation study to show the impact of the importance retrieval and graph augmentation stages is missing.
[R1] As shown in Table 1, the performance after the two stages outperforms that before the two stages, which can indicate the importance of the two stages. We will try to provide more fine-grained ablation studies in our future work.

[Q2] The authors do not provide evidence for the explainability of the method.
[R2] Fig. 3 shows the important ROIs found by our method. And the distribution in Fig. 4 shows that only a few of the domain knowledge is important and can be used to explain the prediction. These two figures can be regarded as evidence of the explainability of the method"
https://papers.miccai.org/miccai-2024/679-Paper3156.html,"We thank all the reviewers for their constructive comments. They found our work is novel (R1, R5), well-organized (R1, R5) and effective (R1, R4, R5).

Problem formulation and contributions (R1, R4, R5)
The state-of-the-art BSS approach, i.e., PLN, builds upon the registration-segmentation paradigm, which includes a registration module and a teacher-student segmentation model:
For a volumetric image with a single-labeled slice,
1) the registration module transfers the label of the annotated slice to its two adjacent slices iteratively to predict the registration pseudo label Y_r;
2) the teacher model predicts a pseudo segmentation label Y_s;
3) the student model is trained by the weighted pseudo label Y = w * Y_s + (1 â w) * Y_r with w gradually increasing to 1 by a time-dependent Gaussian function.
1 New Problem. We pinpoint the issue of the registration-segmentation paradigm in existing BSS methods: the noisy pseudo labels caused by registration seriously degrading the training of the segmentation model.
2 New Method. We explore a new registration-segmentation-selection paradigm for BSS. Based on the weighted pseudo labels, we propose Self-Paced Sample Screening (SPSS) mechanism for selecting pixels with high-quality pseudo labels in both the image and feature spaces. Additionally, we introduce a new contrastive learning sample construction strategy based on strong-weak augmentation to increase the diversity of positive-negative sample pairs, thereby enhancing the discriminative ability of the model.
3 New Findings. We validate the impact of pseudo label quality for BSS, and our self-paced selection strategy alleviates the noise issue caused by registration. Moreover, the results indicate that only 25% of to the training of the segmentation model (cf. Fig. 1(a)).

Compared to ComWin (R1)
Our BSS setting, where each labeled image has only one labeled slice, is more challenging than the BSS setting in ComWin, which is, in fact, a semi-supervised regime where all slices of each labeled image are annotated.

Self-paced Uncertainty sample selection (SU) (R4)
SU require two parts of hyperparameters: 1) the age parameter has been investigated in the supplementary, and 2) the self-paced course includes temperature coefficient, warm-up function. We set the temperature coefficient to 0.8; more detailed experiments will be provided in the appendix. Following PLN, we adopt the gaussian function as the warm-up function. The self-paced weight v is not a hyperparameter and is generated by v = (1 â L_u/Î»).

Self-paced bidirectional feature Contrastive learning (SC) (R1, R4, R5)
Weak augmentations include random image geometry and pixel transformations. We leverage CutMix as the strong augmentation due to its efficacy in image segmentation (CPS, CVPR 2021). We will explore the impact of different strong augmentations, e.g. MixUp and Cutout, in the appendix.
Positive samples are defined as pixels with the same pseudo labels at corresponding positions in feature maps of two weakly-perturbed feature maps; negative samples are defined as pixels with different pseudo labels from the positive samples in the strongly perturbed feature map.

No. labeled slices (R1, R4)
We have conducted experiments on LA and KiTS datasets with various No. barely-labeled 3D images (single-labeled slice) [5%, 10%, 20%, 50%, and 100%]. The performance increases with No. labeled images, and our method consistently outperforms the compared methods. Due to limited space, only the results for 20% barely-labeled data are reported in the manuscript. Furthermore, we will investigate different numbers of labeled slices per barely-labeled image in the appendix.

Details of the compared methods (R5)
The compared methods were implemented using their official codes and following the optimal hyperparameters posited in their papers. We will specify the details in the final version.

Age parameter (R5)
Ablation study on the age parameter has been reported in the appendix."
https://papers.miccai.org/miccai-2024/680-Paper0712.html,"Reviewer #3: Reviewer 3 raises questions about the generalizability. While suggesting evaluation on Mamba models, they also question the performance drop for the spleen organ in our tests.

Our method is designed for most mainstream CNN/ViT-based Unet models, such as SwinUNet, which have an encoder-decoder architecture with skip-connections. We validate the proposed method using standard CNN-UNet and SwinUNet due to their popularity and success. This simple yet effective method enhances the performance of UNets with different configurations. Our method is compatible with all UNet family excluding unusual and rare architectures like HiFormer, which comprises an encoder and a simple segmentation head with no decoder and skip connections. We will discuss and further explore this matter in the final manuscript.

Comparison with Mamba: We recognize Mamba-based UNet as an important emerging alternative architecture to CNN/ViT-based Unet. The proposed method is a generic solution that can be easily extended to any U-shaped network including mamba-based UNet (we will include this in our final version). For the demonstration of the methodology, we have compared with the recently established and peer-reviewed baselines  [8,17,23] . 
In addition, as of our submission deadline for MICCAI, none of  mamba-based vision models have been accepted by prestigious journals or conferences.

Performance drop for the spleen organ: we conjecture that the primary reason is due to the inadequate localization of the spleen organ (see supplementary). This may also be attributed to the heterogeneous appearances of organs.

Feature recalibration: We believe the feature recalibration, which further  enhances the segmentation performance of existing models  across multiple datasets can potentially benefit the monitoring of disease progression, streamlined image-driven analysis.

Reviewer #4:
The main questions include the extension of the method from 2D domain to 3D, the overparameterization of DL methods (like CNN/ViT), and inappropriate usage of the term âSOTAâ.

Limited Discussion on the 3D Scenario: Our work aligns with Hiformer and Uctransnet, which purely focuses on the study of 2D-based medical image segmentation. We do not envision any obstacles in extending the proposed method to 3D models if they satisfy similar conditions (see reviewer#3 generalizability). However, overemphasizing 3D seems to overshadow the significant contributions made in the 2D context.

Overparameterization and Feature Redundancy: Our proposed method applies these baseline methods due to their success and popularity in medical image analysis. Using deep learning methods with this level of complexity is a common practice accepted by the community and and our method only introduces negligible extra compute cost; therefore, â inefficiencies inâ is not the issue caused by our method. (please refer to proposed two losses details)

âSOTAâ: we will use the most accurate term of ârepresentative methodsâ in our final version as suggested.

Reviewer #5
Positive feedback on our main contributions followed by requests for further clarifications of on ablation analysis and Fig. 1.

Please refer to Fig 5 (c and d) in the manuscript, where we have performed an ablation study on the presentence/absence of these two losses.

The main issue is that semantic information from skip connections is harmful. Features from the encoder(e.g. E2,E4,B), when passed through skip connections to the decoder, cause semantic damage (as detailed in our supplementary material and observed in UCTransNet).

As mentioned in our response to Reviewer #3, the proposed method is applicable to all standard UNet architectures.

The similarity matrix in Fig 1 (c) shows the similarity between features in deep and shallow channels, not all features. Thus, it can only conclude that shallow features are more diverse than deeper features.

We will correct typo errors in final versions."
https://papers.miccai.org/miccai-2024/681-Paper0862.html,"We thank all reviewers for insightful feedback and comments raised, which we believe will help improve the work. In the following we provide detailed responses, and will incorporate them in our revisions.

Rev.#1-Q1: Sorry for the typos. We will fix them in revision.

Rev.#1-Q2: We use the 3D Frangi filter with scales ranging from 1 to 5 and a scale step of 0.5 to extract vessel masks. The hyperparameters in the Frangi filter are set as \alpha=0.5, \beta=0.5, and \gamma=15.0. The rest parameters are default values in the implementation of scikit-image library. We will add the details to revision.

Rev.#1-Q3 & Rev.#2-Q1: Mainstream segmentation models generally exhibit strong performance across various regions but can encounter challenges in handling significant noise, resulting in incomplete predictions. The observed performance of nnUNet, particularly in addressing breaks with high precision but extremely low recall, highlights the importance of break completion. Our method leverages connectivity-related features learned from previous methods in low-noise environments and subsequently corrects breaks in severe-noise regions. Compared with previous arts, our method achieves a better F1 score and enhances the vascular connectivity in real scenarios. We will clarify this in revision.

Rev.#1-Q4: Skeleton transformations are randomly applied during training. We will release the codes with implementation details for reference.

Rev.#1-Q5: Sure, the real clinical cases could be far more complicated. We set \tau as 10 for both datasets, as suggested by experts.

Rev.#2-Q2: In real scenarios, the degradation of skeleton tips due to artifacts can disrupt curvature similarity, leading to diminished performance. For further clarification and illustrative examples, please refer to Fig. 1 in the Supplementary. We will clarify this in revision.

Rev.#3-Q1: No manual annotations are required during training; they are solely utilized for evaluation purposes. Throughout training, we randomly sample patches with long skeletons for break synthesis. Itâs important to note that there is no overlap between the patches used for training and those used for testing. Additional details regarding the dataset and annotation can be found in Fig. 3 and Tab. 3 of the Supplementary. We will address and clarify this in revision."
https://papers.miccai.org/miccai-2024/682-Paper2059.html,"We deeply appreciate all constructive comments for improving our work. We have to response main concerns. The minor issue will be corrected in the revised manuscript.
Reviewer 1#ï¼
Q1: â..explain the specific novel in terms of methodologyâ
A1:We are aware that we fail to clearly explain our major contribution in the paper and somehow overclaim our methodâs novelty. We agree with the reviewer`s comments that almost all the major components of the proposed method have appeared individually in previous works[1,2]. To avoid overclaiming the novelty in methodology, we will rename FlyCGV to FlyGCL, make clear citations and revise our claims in the manuscript accordingly.
It should be noted that our main contribution is the introduction of the graph contrastive learning method to neuron classification and neuron connection prediction at neuron level, both of which have rarely been studied from a graph learning perspective so far. We believe that integrating existing contrastive learning components into a new system to address critical neuroscience problems is non-trivial and falls within the scope of MICCAI topics. Acquiring connectome data heavily relies on human annotation [3]. We believe that graph contrastive learning is simple enough and user-friendly, requiring fewer training labels. 
Q2: â..they releasing these datasets?â and â..clarify their contributions to constructing the datasets. 
A2: Thank you for your comments. The raw data have been released on the site https://neuprint.janelia.org/, which includes neuropil, neuron morphology, neuron connections, and typing results. Unfortunately, these data cannot be directly used for deep learning tasks. we have made the following efforts:"
https://papers.miccai.org/miccai-2024/683-Paper0736.html,"Reviewer #3:
(1) Writing. Thanks. We will fix writing issues.
(2) Theoretical support. The distribution of noise in OCTA is very complex without a closed-form mathematical model. Therefore, we are not able to rigorously prove (like in [2]) the equivalence between using the center B-scan for supervision and using its clean version.
(3) Other comments:
(a) Typo: Yes, it should be b_iâ. 
(b) Weight: This description is about w_iâ. We use the raw image to calculate the weight term for training (the line below Eq.2).
(c) Clinical setting: We follow settings in related work to show the potential of the self-supervised method. In clinical settings, we can quickly finetune a pretrained model on a new noisy volume for better performance.
(d) Small vessels: Most of them are capillaries, and some are small arterioles and venules. No major vein or artery.
(e) Segmentation result: In rare cases, there can be consecutive corrupted B-scans. NL aggregates the neighbor BMA information for the inference, and the BMA in the center B-scan may not be perfectly removed without WL in training.

Reviewer #4:
(1) Network novelty. To our best knowledge, we are the first to apply a non-local block in CNN for 3D self-supervised denoising.
(2) Expansion in 3D. It is hard to design a 3D-to-3D mapping that is effective in blinding the 2D correlation of noise in OCTA volume for self-supervised learning. This is why 3D Noise2Self fails. In contrast, our 3D-to-2D mapping utilizes both 3D information and inter-B-scan independence of noise.
(3) Comparison with 2D methods. Comparing with the 3D methods is for fair comparison. It is redundant to show the results of 2D methods since the two tested 3D self-supervised methods are extensions of 2D methods [2,14]. The failures of both (compared with our SOAD) show that ignoring the 2D correlation of the noise may cause overfitting to noise.
(4) Other datasets. We focus on awake brain OCTA volume denoising, but there is no such public dataset. Both OCTA-500 and ROSE are retinal datasets and not for denoising. We will clarify this in the revision.
(5) Other comments:
(a) New method: Thanks for the suggestion. Unfortunately, the conference policy does not allow adding new results. We will discuss it in the background introduction.
(b) Reference of statement: Thanks. The basis of the statement is that BMA appears in 3.6% B-scans in the data.

Reviewer #5:
(1) Baselines:
(a) We did not find any self-supervised volume denoising method published in major venues in the last two years. We compared our method with [29] because it aggregates 3D information for denoising.
(b) As for BMA, please note that our task is for BOTH denoising and BMA removal, and both are critical in processing awake brain OCTA. So, it is fair to make a comparison of BMA removal with other denoising methods. Besides, no related work about brain OCTA BMA removal on the B-scan level is found.
(c) For saturation artifacts, studies are mainly about artifacts spanning only several A-scans (columns) in OCT, while BMA may appear in a much longer range in awake brain OCTA. It is unclear and nontrivial to apply those methods to OCTA; moreover, those studies do not work for denoising.
(2) Dataset size: More data are desirable but nontrivial to collect from awake mice. Previous works [10,18] use only 6 volumes (5 training and 1 testing). We use 7 volumes captured at different dates and angles from 2 awake mice.
(3) Study scope and 3D data: Note that our task is for BOTH denoising and BMA removal, and 3D information is helpful for both. First, comparing BM3D with BM4D, 3D information helps BM4D to recover small vessels. Second, [25] also uses neighbor B-scan information (MIP) for BMA removal.
(4) Metrics: We follow previous work [8] using ROI-based metrics. Moreover, the results of downstream segmentation tasks can compensate for the ROI-based metrics, especially for small vessels.
(5) Highlighting the importance of BMA removal: Thanks. We will do it."
https://papers.miccai.org/miccai-2024/684-Paper3260.html,"We thank the reviewers for the positive feedback on our manuscript and their recognition of our work as novel (R1), effective (R3) and sound (R4). We are grateful for their recognition of the flexibility of the method and the âwell executed study designâ (R4) using simulations and in-vivo data.

Our manuscript introduces the novel concept of PISCO, which leverages self-consistency in k-space by solving multiple subsets for weight estimates. As pointed out by reviewers, the effectiveness of the method would be further supported by analysis of the actual weight estimates (R1) as well as ablations (R1/R4). We agree with these important points and analyzed the estimated weights throughout the learning process, observing convergence of the weights from multiple subsets. Also, as stated in the manuscript, we included Thikonov regularization and overdetermination of the weight subsets to enhance robustness against outliers and prevent overfitting. Yet, in this work, our main focus was to provide a thorough technical explanation to ensure a clear understanding of the method. Unfortunately, due to space constraints, these additional findings and ablations could not be included in the final manuscript but will be valuable inclusions for future extensions of this work.

With regards to the improved performance of PISCO (R1), we demonstrate significant quantitative improvements. In in-vivo data, we show noise reduction in our results, aligning with our goal to enhance NIKâs representation capability in lower-sampled outer k-space. We observe smoother vessel structures and anatomically more plausible dynamic motion using PISCO, consistent with the anatomical expectations set by XD-GRASP50 (high temporal resolution but strong undersampling artefacts). While this was the first introduction of this work, we expect further improvement when exploring further design choices.

Once again, we sincerely thank the reviewers for their insightful feedback and valuable suggestions. We will incorporate these comments, including points regarding formatting (R3/R4), into our final manuscript to the best of our abilities."
https://papers.miccai.org/miccai-2024/685-Paper0719.html,"We thank all reviewers for the insightful comments. Our responses to the major concerns are itemized as follows.
1) Provide design reason for âMulti-state brain network encoderâ (R1): Existing methods directly encode brain networks into a feature vector, which entangles different brain functional characteristics. Thus, we design the encoder to separate distinct functional brain states and capture a high-level representation of brain networks.

2) Why force each row of P close to one-hot (R1)? P is a probability matrix where each column represents the probability corresponding to a specific brain state and each row represents the probability distribution of a one-dimensional feature in distinct brain states. Each feature characterizes the specific nature of the brain and should ideally be clustered into a specific brain state. Thus, we use one-hot to constrain feature space, which has no relationship with brain connections among different brain states.

3) Unclearness in method (e.g., What is the difference between bold signals and fMRI? Explain matrix P and HP) (R4): Our method utilizes original fMRI data as input, which is voxel-based 4D (XYZT) imaging data. After preprocessing, we acquire bold signals, which are ROI-based 2D (ROI_numT) sequence data. We generate a probability embedding matrix HP, which is a hidden representation and utilized for learnable clustering to acquire probability matrix P. Each element in P represents the probability of features that are assigned to distinct brain states.

4) Unclearness in the experiment (e.g., How to perform non-cross-dataset methods? How to define the number of brain states?) (R4): We train from scratch on the target dataset and compare it with our w/o pretext model to ensure fairness. The number of brain states is a hyper-parameter and is obtained through parameter tuning. The detailed parameter analysis refers to Appendix A.

5) Missing any of the components leads to poorer results (R5): Each item of loss is specifically designed for the modules, so they need to work together to be effective. Furthermore, compared to the baseline (i.e., methods without proposed modules), individual ablation on both modules shows improvements, and they can be applied to other methods to improve performance.

Our responses to the reviewersâ specific comments.
R1: 
1) Explain the second challenge: The challenge is how to improve the representation ability of the model to deal with the complex functional characteristics of the brain network.
2) How to obtain W^b: It is a learnable weight obtained by model training.
3) The functional significance of P: P is utilized for computing the representation of each brain state.
4) Explain ThetaL: It is the learnable weight of the l-th GIN layer.

R4:
1) Compare more baselines: We will consider them for future work based on rebuttal guidelines.
2) The method is only for the same neuroimaging and brain network size: Our work focuses on solving the challenges of various brain diseases and complex brain functions. The multi-modality or multi-atlas tasks are out of the scope.
3) Explain adaptive brain network: The structure and representation of brain networks can be continuously optimized during training to adapt to different brain diseases.
4) Discuss limitations: Our method is built on a pre-training and fine-tuning framework, thus relying on a large amount of unlabeled data to improve the performance of the model.

R5: 
1) Differentiation from prior work: We propose a learning-based contrastive framework that differs from traditional handcraft methods.
2) Why not use simple classifiers as baselines? They are traditional methods and have been defeated by SOTA methods (e.g., [7,24]).
3) Can the localization of disease be quantified? Yes, we can utilize the gradients to quantify them.
4) Provide a summary of references [4,13]: Cognitive dysfunction is associated with disrupted global and local brain network connectivity."
https://papers.miccai.org/miccai-2024/686-Paper0740.html,"We sincerely thank all reviewers for their thorough evaluations and valuable feedback. We are encouraged by the positive assessment of our workâs organized structure, clarity, novelty (R3, R4, R5), relevance to the field (R3), and experimental evaluations (R4, R5). We appreciate R4 recognizing the supporting role of our figures and R5 commending our literature review. We have carefully considered all comments and suggestions and hope to satisfactorily address the major concerns below. Additionally, we hope that the release of our source code will help mitigate any reproducibility concerns. We deeply value R4âs positive feedback, recognizing our work as âup to something greatâ and noting its potential to âcontribute to the field.â This affirmation strengthens our belief that MICCAI is an ideal venue to present and engage with the community on this line of research.

Grid artifacts in synthetic images: causes and solutions:
We acknowledge the presence of grid artifacts in our synthetic images (R3, R4, R5). These artifacts arise from our image synthesizer (IS), which generates images by combining patch-wise anatomy features from one image with characteristic features from another via dot-product multiplication of their respective feature matrices. We chose the dot-product approach inspired by the Attention mechanism in Transformers due to its speed and space efficiency compared to a learnable Multi-Layer Perceptron (MLP). However, we agree with R5 that modifying the IS could mitigate the grid artifacts. Specifically, replacing it with a learnable MLP could help preserve overall characteristics better and reduce artifacts. This change would also remove the size constraint on the feature embeddings, as correctly noted by R3, which is currently needed to facilitate the dot-product-wise image synthesis. We will add a brief comment on this matter to the manuscript.

Consequences of grid artifacts for image quality and assessment:
We appreciate R4 for highlighting the theoretic concern regarding a potential hallucination of content within the generated images. Although this concern is justified, we mitigate it through our proposed losses: the feature consistency losses ensure the disentanglement of the two feature classes, while the self-reconstruction loss ensures the encoding of meaningful information for image reconstruction and generation. Thus, our approach preserves the original anatomy during the image synthesis, as shown in Figure 4 for images from the training set (rows 1 and 2), the validation set (row 3), and the unseen test set (row 4). These results further emphasize our encoderâs promising generalization capabilities, as seen by the consistent image quality across these (unseen) samples during training (R4).

Impact of grid artifacts on method deployment:
Furthermore, while grid artifacts impact the visual assessment of the generated images, as observed by R4, they do not negatively affect our methodâs impact. Our method is only used for augmenting the training set, thereby enriching its diversity, but does not affect the test set. Thus, the latter is not subject to hallucinations and downstream performance will not be affected, as shown by our state-of-the-art benchmark performance. Nonetheless, we acknowledge that deployment to clinical practice is a complex process that requires substantial efforts and care including security and regulation that are far beyond the scope of a conference paper.

Performance consistency even under extreme domain shifts:
We thank R3 for raising an important point on how our method can improve downstream performance even for extreme domain shifts without requiring information on the test set. While we donât solve this issue entirely, we provide significant progress with initial evidence (cf. Sec 3.2) showing substantially increased classification robustness. This improvement stems from our methodâs superior diversity, allowing for more invariant representations compared to reference works."
https://papers.miccai.org/miccai-2024/687-Paper1524.html,"We thank all the reviewers for their insightful and generous comments. We are encouraged by the reviewersâ comments that SAG is generic(R1), effective(R3,R4), and clearly motivated(R3).

R1- Show TG-only results and include standard deviation (std).
Although not included in the paper, we have performed experiments with only TG, which yielded results marginally better than current baselines on the melanoma dataset. Qualitative attention visualizations suggest the same conclusion. Meanwhile, we observed no irregular std across experiments. We will report TG-only results and std in our revision.

R1&R3 - Show quantitative measurement and qualitative progression of attention weights.
While our primary focus is to improve diagnosis performance (as shown in Table 1), qualitative visualizations (e.g. Fig. 4 in the paper and Fig.1,2 in the appendix) support the effectiveness of SAG in guiding attention towards relevant regions. We will consider incorporating similarity analysis to offer a more comprehensive assessment of attention guidance.

R3 - More baselines.
We strategically chose the two contrasting baselines: ScAtNet, a generic ViT-based model, and ABMIL, a generic attention-based MIL method. Importantly, these baselines are evaluated on two distinct cancer types. The consistent improvements across these very different datasets and models validate SAGâs generalizability and model-agnostic nature.

R4 - Motivation for using TG instead of filtering background patches.
We agree removing background patches is standard and efficient in MIL-based methods. However, in a ViT-based model, this would lead to variable-length inputs and potential loss of spatial information, which is critical for the modelâs performance. Positional embeddings can help but cannot fully compensate for the missing context. In contrast, TG is a weak supervision method that accounts for potential noise and is not limited to foreground-background differentiation. It can encompass different anatomical maps, allowing transformer heads to learn representations of various tissues, such as epithelium, stroma, blood, and necrosis in breast biopsy WSIs.

R4 - Comparison with Tourniaire et al.
We recognize the valuable context in the referenced work. Despite the similar goals of improving attention learning, SAG is distinctive from MS-CLAM in three key aspects: (1) MS-CLAMâs attention network hinders its applicability to ViT models, which excel at leveraging global information through inherent attention mechanisms. In contrast, SAG seamlessly fits both MIL and ViT architectures. (2) The attention guiding loss in SAG (Eqn. 4,5) offers a more generous form, allowing both binary (tumor vs. normal) and continuous signals. (3) SAG incorporates diverse semantic information beyond those directly related to diagnosis â for example, brain atlases could be incorporated as TG, and cell maps are utilized as HG. Compared to MS-CLAMâs binary representation, this broader approach can lead to a more robust learning process, especially for complex datasets where the binary MIL assumption â all instances from a normal slide are normal â does not hold. In such scenarios, SAG provides a weakly-supervised but more generalized framework that guides the model to focus on diverse signals and handle potentially noisy data.

R4 - Missing references
We thank R4 for bringing to our attention the missing references. We will include these in our revision.

R4 - Clarity
We appreciate the feedback on our manuscript. We will incorporate the suggestions to improve the clarity and organization of our paper. As for dataset details, due to the double-blind review process, we cannot disclose specific dataset information, but we will address this in the final version. Regarding training details, the randomly chosen seeds are used for model initialization, and the SimCLR embedder weights are from DSMILâs official repository which is trained on Camelyon16âs training set without data leakage concerns."
https://papers.miccai.org/miccai-2024/688-Paper3843.html,"We appreciate all reviewersâ constructive comments. They found our approach highly interesting (R5) and novel (R1, R5), generating interpretable features (R1). They regarded it as one of the first applications of the disentanglement method on complex pathology images (R1, R4), achieving significant performance (R1, R4, R5). We elaborate on their comments below. They made suggestions to perform modifications to enhance readability (R4). We appreciate their suggestions for the writing and display; we will address them in the revised version.

Q1. The copy-paste mechanism is a limiting factor (R1). 
A: The copy-paste mechanism optimizes the model for disentanglement by simulating paired background/foreground data. We donât find it limiting; it works as long as we have sample patches of different types, and a reasonable nuclei segmentation model, e.g., Hovernet [8]. Our approach allows easy generalization to other cancers and other tissue types.

Q2. IDGAN as a novelty does not perform as well as Double-Infogan in reconstruction (R1).
A: The primary focus of the paper is on latent space disentanglement rather than reconstruction. Although SS-cVAE using IDGAN does not surpass double Info-GAN in terms of FID score (the difference is marginal), as shown in Tab. 2, it significantly outperforms double Info-GAN in disentanglement quality, as measured in silhouette score [5, 20] in Tab. 1. Qualitative results (Fig. 4) indicate the same: double info-GAN can confuse different conditions (cell vs. no-cell, high TIL vs. low TIL).

Q3. Selection of hyperparameter values (R1).
A: We empirically selected Î»_1 and Î»_2 in the loss (Eqs. 1 and 2 in the supplementary). For each of them, we tested different values ranging from 10^(-3) to 10^3 with steps of multiples of 10 on the BRCA dataset and selected the one with the best validation performance.

Q4. Limited novelty due to existing contrastive disentanglement (CD) models (R4).
A: Applications of existing CD models to our problem are not trivial. Existing methods are restricted to single-object natural images. In our setting, as R5 commended, we have âcomplex pathology images with multiple objects and intricate relationships.â To address the challenge, we propose different contributions, including a cascade architecture (see Q6 below), a copy-paste mechanism (Q1), and IDGAN (Q2). Collectively, these innovations significantly enhance the frameworkâs practical applicability, as demonstrated in Tab. 1 and Fig. 4.

Q5. Details on Eq. (1) (R4).
A: We already provided a derivation of Eq. (1) in supplementary. We will move it to the main paper.

Q6. Ablation studies: two-step disentanglement and reconstruction method (R5).
A: Good idea. Indeed, we have tested the one-step disentanglement with three labels (no cell, low TIL, and high TIL). The result is not satisfactory. Separating the three labels involves different types of factors. Separating no cell from the other two labels is necessary to capture the presence of cells. Separating low and high TIL requires closer attention to cell morphology/spatial arrangement. It is much harder to learn all these in one step. 
We have also compared IDGAN with the VAE decoder. IDGAN has much better reconstruction quality. This ensures that the latent representation is well preserved while being disentangled. 
These ablation studies were actually done before we submitted the paper. We did not include them due to the page limits of the paper (8 pages) and the supplementary (2 pages). We will make sure to include them in the final version.

Q7. Why is the method semi-supervised (R5)?
A: The term semi-supervised was used because the model is trained on synthetic data generated by the copy-paste mechanism. However, we realized this may not be proper because the synthetic data is generated using labeled data. We will remove the word in the final version.

Q8. Reproducibility (R1, R4, R5).
A: We will provide the code and preprocessed data upon acceptance of the paper."
https://papers.miccai.org/miccai-2024/689-Paper3457.html,N/A
https://papers.miccai.org/miccai-2024/690-Paper0793.html,"We thank all reviewers for their comments, especially in noting that our paper is well/clear-written; proposes a novel/well-suited method that solves a challenging/important clinical task; demonstrates the effectiveness in sound and thorough experiments (both ablation and comparison to other work).

Q1: Novelty (R1).
Our main contribution and novelty are 3-folds: 1) Diagnosis of lymph node (LN) metastasis is an essential yet challenging/understudied task in esophageal cancer treatment. We work on a dataset of 1052 patients with 9961 LNs (largest dataset for esophageal LN classification to date), and achieves a substantially improved performance. 2) Our main methodological contribution is the proposed overall framework. Compared to previous work with limited metastasis-labeled LNs, we propose a semi-supervised model with label-sharpening strategy via utilizing LN-stationâs metastasis status in pathology report, which use more unlabeled LNs to improve performance. This is an intuitive way to incorporate clinical information in LN classification that has not been explored before. 3) Our idea of two-stream multi-scale feature fusion comes from clinicians assessing process, where both local (intensity, texture) and global (size, shape) features are important for diagnosis. It can also overcome the drawback that previous single networks are prone to the bias of LN size. This is different from two branches methods of [2] (by R1), which are designed to incorporate different scopes of context for object delineation. Given the complexity of medical imaging task, we view a new framework that provide means to significantly enhance the performance just as valuable as new networks or algorithms.

Q2: Backbone in supervised learning (R1, R3).
Our method is designed independent of the choice of backbone network, hence, we initially choose a lightweight MobileNet for computational efficiency. We replace MobileNetv3 with ResNet-101 as suggested by R1 for both single- and two-stream settings. Results show a ~1% improvement for both settings (single: 0.8788 -> 0.8910; 2-stream: 0.9068 -> 0.9162), which is consistent with our assumption that proposed method is independent of backbone choice. We will add comprehensive experiments using other backbones in the future.

Q3: Statistical analysis and subgroup error analysis (R5)
(1) We employ DeLong test to calculate p-value for results in Table 1 (comparison to others) and Table 2 (ablation). E.g., in Table 2, p-value between mean teacher result (row 6) and our SSL result (row 7) is 0.004, which is statistically significant. We will add statistical analysis in final version.
(2) Subgroup analysis: Using RECIST criteria, we selected the smallest 25% malignant LNs as a subgroup. The global stream network (preserve LN size) has 0.702 accuracy and proposed two-stream method yields 0.808 accuracy. This result demonstrates the effectiveness of two-stream design when classifying LNs that are small yet malignant. We will add more subgroup analysis in final version.

Q4: Other issues of R1.
(1) â-â and â+â in Table 1: There is only â+â in Table 1, which represents the improvement of our method compared to the 2nd-best performing one.
(2) 1st-row in Table 2: We use the global input (preserve LN size) as default single stream comparison.
(3) Data augmentation: We use extensive data augmentation similar to nnUNet for both streams (considering its high performance in medical imaging). Pure data augmentation cannot fully compensate the two-streams because of the large LN scale difference (LN size can differ by 10 times ranging from 2-3mm to 20-30mm).

Q5: Other issues of R3, R5.
(1) Name of label âsharpeningâ: Yes, your understand is correct. 
(2) Two suggested refs: We will include these refs in final version. 
(3) Evaluation metrics: We agree and will omit accuracy in final version.
(4) Unit of evaluation: Metrics are computed on a LN level. For each LN instance, we crop an ROI centered on this LN in CT."
https://papers.miccai.org/miccai-2024/691-Paper1033.html,"Thanks for your comments. Note that our code is open (anonymized link in Abstract). Below we clarify confusions.
R1&R4:Differences (and ablation) with related works [20-21].   We suggest novel bifurcated rival (BR), Victor-guided weighting (VW), and saliency-guided consistency (SC) components compared to [20:DHC,21:MCF]. As explained in Sec 3.1, BR has two different models with a weighting strategy that learns difficulty and imbalanced classes, while MCF has two different models without a weighting strategy. As in Sec 3.2, VW has Victor selection and flipping weight, while DHC did not. We will clarify texts. Due to these differences, BR surpasses MCF for 0.86% and 2.83% in minor and difficult classes (MA and SE) and VW surpasses DHC for 1.22% and 7.43% in major and easy classes (HE and EX) in Table 3 and Supp Table 2.   This clearly shows our components are different from [20-21] and better. Thus, raised âmismatchâed results are expected.R4:Baseline. This was a mistake. The baseline model consists of two different structured models, which differ from CPS[3] which has two models of the same structure. Each incremental element is added to this base model in Table 3. We will make corrections.
R4:Saliency map; alternative mask of [21]. The difference mask from [21] encourages the models to learn the discrepant region that each model doesnât know and involves a lot of contours with high uncertainty. In contrast, our saliency map provides low-uncertainty information from Victor model in lesion-related areas to use reliable information as in UA-MT[24]. In Table 3, our SC surpasses baseline for 5.56% and adding SC surpasses (BR+VW) for 4.29% for the difficult SE class, because our SC distinctly represents broad bright features like SE and also decreases the negative confirmation bias of pseudo mask unrelated to SE.
R1:Details. Optimal threshold for the saliency map was chosen based on inclusion of vessels or other structures. Hyperparameters(\alpha, \beta, \tau, T, \lambda_cor) followed [20-21]. \lambda_u was empirically set to 1. All these values are given in Sec 4 and Supp Table 1 and the way will be clarified. Model sizes differ slightly based on the backbone (MCF, Ours: 26.8MB, Others: 26.3MB). We will add them.
R1:Higher std and poorer scores for MA. Ours had the lowest std for both DSC and PRC at 10% (Table 1).  Still, we admit the poor MA as a limitation due to Victor model dependent on dice. However, we achieved 0.45-1.36% performance gain in MA than [21] thanks to the weighting strategies.
R1:Balanced performance. Other methods showed big gains in one or two classes, but ours improved evenly in all classes. 
R1:Inconsistencies in the results and texts. We acknowledge reporting errors in Table 3 and will revise them.  Our model was not always best in all classes, so we described the result as âcombining them resulted in complementary and balanced advancesâ in Sec 4.
R1:Image Size. Due to variations in sizes provided, we resized images accordingly. 
R1:Visualization. We will update Fig 3 to include results for e-optha dataset.R1:Clinial application. Our model can aid physicians by providing additional information for efficient follow-up.
R1&R2: Inference. Inference step ensembles of two model outputs and takes about 6.1 sec per image similar to [21]. We will update Fig 2 to explicitly show inference path.
R2: 100% supervision. Itâs in the last sentence of page 7.
R4:Notation errors. We will revise them. 1) We admit a missing sum in the denominator for D in eq (2). Subscripts of f (f_A and f_B) were omitted for compactness. 2) In Sec 2, we briefly mention symbols and ideas used in [20] because we adapt their ideas for our purpose. 3) We noted âsupervised lossâ as L_sup and L_X. We will use L_sup for consistency. 4) W_A/B are inverse of W_easy/min and the same as W_diff/dist. This will be clarified.
R4:   SOTA DR lesion segmentation. As they donât utilize unlabeled images, we did not compare them."
https://papers.miccai.org/miccai-2024/692-Paper0651.html,"Q1. How are you able to obtain the predicted DT map and segmentation map simultaneously by using the same decoder. 
A1. Our deep model has two output heads that simultaneously produce a DT map and a segmentation map, both describing the same object that has the same geometry shape. Therefore, we use a same decoder instead of different decoders in our deep model. Numerically, the ReLU function in the distance transform output head converts negative feature values to 0 and positive feature values as distance values. The sigmoid function in the segmentation output head maps negative feature values to the range (0, 0.5) and positive feature values to (0.5, 1).
Q2. Why the âPrecisionâ of our method is not as good as the baselines?
A2. The decrease in our methodâs precision rate is due to increased over-segmentation. We can also observe the over-segmentation in Fig. 3. This issue arises from two main factors: 1) Incomplete Annotations: Our method detected additional unannotated branches, uncovering incomplete annotations in the ground truth, as discussed in [1]. This capability led to more comprehensive segmentation of branches not labeled in the ground truth, owing to the strength of extracting geometry shape features. 2) Boundary Over-Segmentation: Our method over-segmented branch boundaries, resulting in a bit thicker branches and false positives, likely due to errors in approximated differentiable distance transform. Combining cross geometry shape consistency and Hausdorff consistency further extract geometry shape information, compared to only using one of them. Owing to the combination, our method more precisely segmented unannotated airways, but also introduced some false positives. That is why our method underperformed the baseline and previous methods in terms of the precision rate in the evaluation metric. We have to say our method segmented more branches correctly than ground truth, and this deteriorated evaluation metrics. 
[1] Wang, Puyang, et al. âAccurate airway tree segmentation in CT scans via anatomy-aware multi-class segmentation and topology-guided iterative learning.â arXiv preprint arXiv:2306.09116 (2023).
Q3. Why the author did not follow the backbone network they selected, 3D V-Net, but chose 3D U-Net.
A3. In our preliminary experiments, we compared 3D V-Net and 3D U-Net architectures for tubular structure segmentation. The 3D U-Net, with its skip connections, outperformed the 3D V-Net on all metrics, leading us to choose the 3D U-Net as our backbone.
Q4. The metric tree length detected rate should be referred to as TD instead of TL
A4. Thanks for your kind reminder. We will modify the use of words.
Q5. The novelty of this paper is limited.
A5. The main contributions of this paper are the cross geometry and Hausdorff distance consistency, rather than the framework itself. While our frameworkâs structure is similar to DTC, we introduced we introduce several key innovations: 1) we propose a mutual referring technique to leverage unlabeled data not only from the auxiliary task (like DTC), but also from the main segmentation task, presenting a novel application of unlabeled data. 2) Our proposed differentiable distance transform supports our mutual referring technique. 3) Our Hausdorff distance consistency introduces a new semi-supervised learning approach, marking the first use of Hausdorff distance in consistency learning.
Q6. The comparison methods are inadequate. 
A6. Although not compared at MICCAI 2023, our method outperformed another study published in 2023 [2]. Additional comparative experiments are ongoing due to time constraints. For more detailed experiment analysis, please refer to A2. 
[2] Peiris, Himashi, et al. âUncertainty-guided dual-views for semi-supervised volumetric medical image segmentation.â Nature Machine Intelligence 5.7 (2023): 724-738.
Q7. the framework diagram is difficult to understand.
A7. We will modify and make our diagram clear and intuitive."
https://papers.miccai.org/miccai-2024/693-Paper0407.html,"We are sincerely grateful for the insightful and valuable feedback provided by the reviewers. Our responses to the comments are listed below:

To Reviewer#1,
Thank you for the constructive comments.

To Reviewer#3,
Thank you for the constructive comments.

To Reviewer#4,
Thank you for the constructive comments.

Thank you very much for your careful review and helpful comments on this paper."
https://papers.miccai.org/miccai-2024/694-Paper0151.html,"To #R1:

To #R2:

To #R4:"
https://papers.miccai.org/miccai-2024/695-Paper0423.html,"Thank you for your valuable feedback!

Some reviewers are concerned that the shortcuts discussed in this paper are self-evident. As we illustrate next, they are not.

== Our findings are surprising [R3-4] ==
We show that, besides the well-known effect of shortcuts on classification, shortcuts also affect segmentations. This is novel and surprising, which is actually highlighted by the âtank urban legendâ brought forward by R4: This blog claims that models affected by shortcuts would fail in segmentation (point 6 under âCould it happenâ). As pointed out here, you would indeed expect false positive segmentations due to shortcuts to fail drastically at outlining something meaningful. However, false negatives (failure to segment) can most definitely happen as a result of shortcuts â as shown by the examples brought forward in our paper.

This point is missed by the blog, and we have made the same mistake in the past, which makes this important. We thank R4; the blog helped finetune our understanding and we will emphasize this in the final version.

== Our findings are important for medical imaging [R3] ==
We first note that âcenter-croppingâ in our paper refers not to augmentation, but to dataset construction, where the region of interest is centered. We apologize for this confusion. This has been dubbed the âcommon photographer biasâ in computer vision (Kirillov et al., CVPRâ23), but is also prevalent in MICCAI challenges (e.g., SegRap, PENGWING, BONBID-HIE, FH-PS-AOP, Acoustic-AI) and popular medical datasets, see our Fig. 6 as well as Fig. 1 of Ma et al. (Nat. Communâ23) and Fig. 5 of Isensee et al. (Nat. Methodsâ21).

Our illustrations are not artificial: similar effects are evident in published models, see e.g. Fig. 2 in (Wang et al., MICCAIâ23) and Fig. 7 in (Dai et al., MedIAâ22) which show models like U-Net and CA-Net failing at recognizing border-adjacent lesions. Our research explains these failures, offers a mitigation strategy, and suggests that many model performances reported in MICCAI publications and challenges may not reflect performance in the wild, where cropping can be less consistent. This should be a community concern.

Our suggested mitigations are not common in prior works: less than 15% of skin lesion segmentation papers since 2014 used random cropping augmentation (Mirikharaji et al., MedIAâ23), possibly because random cropping risks discarding key objects (Cho et al., MICCAIâ23), making it less favorable for accuracy on center-cropped validation sets. Removing calipers in fetal ultrasound segmentation is also not standard in MICCAI papers, e.g. (Sophia et al., MICCAIâ21; Pu et al., JBHIâ22; Zhou et al., MICCAIâ23).

== Details =="
https://papers.miccai.org/miccai-2024/696-Paper2156.html,"Q1(R#1):Missing related work.
A1:Thanks for pointing out the issue. CWT(Cyclical Weight Transfer) in Ref1 is a serial framework(SF), but it only works on (near) IID scenarios. Ref2 does not propose new SF, but focus on effect of model backbones on federated learning(FL). While Ref4 provides strategies for CWT to handle variability in sample size & label distribution, they may not well handle non-IID scenarios(evidence obtained, but not reported due to rebuttal policy). In contrast, our SF can better handle non-IID scenarios and achieves SOTA performance. Ref3 focuses on domain expansion and irrelvant to SF. Compared to Ref1-4, our SF is novel in (1) whole framework design (i.e., memory module & classifier head), (2) applying a continual learning strategy for FL, (3) utilizing textual guidance to help visual learning in SF. Such discussions will be added.

Q2(R#1):EMA is well-explored in Ref3 and used for serial FL (SFL).
A2:Ref3 is for domain expansion rather than FL and only considers two domains(clients). Also, EMA in Ref3 is only for BN stat update, different from the purpose of long-term model update in our work. So, EMA has not been used for serial FL. Importantly, one novelty here is in applying a dual-model memory for SFL(not EMA).

Q3(R#1):About language model (LM) with FL (Ref5-6).
A3:The purpose of LM is different. Ref5 is parallel-based FL (PFL) of soft prompts built on pretrained LM and only for NLP tasks.Ref6 is PFL of visual adapters built on CLIP. Our framework uses LM to provide textual prior for guiding vision model learning in a serial FL. Also, using LM for model training is not the only novelty (listed in A1).

Q4(R#1):Comparison appears unfair.
A4: We clarify our model during inference is ResNet18 (without LM). BioLinkBERT is only used to help guide ResNet18 training, which is one of our novelties. Ablation study in Fig 4 shows both usage of LM and other components help. Our LM usage can be extended to parallel FL(as future work), supporting generality of our method. Other LM+FL baselines will be compared.

Q5(R#1):More comparisons & discussions on tests. 
A5:HAM10000 is highly class-imbalanced while other datasets are not. Such factor & dataset bias may cause difference in performance b/t IID & non-IID settings. Improvement with Dir0.3 is limited probably due to severe non-IID distributions (i.e., some classes missing at each client) with Dir0.3. Similar positive findings are expected on OrganSMNIST and OrganCMNIST with IID setting(not reported here due to policy). Centrally hosting performance will be added.

Q6(R#3):Incomplete evidence for efficiency.
A6:Our serial FL (SFL) is for scenarios where only a few clients (e.g., hospitals) exist and each client has rich computation resource. In this case, significantly fewer rounds in SFL means less real time compared to parallel FL with many more rounds. Real time infor will be added. Also, reduction in SFL rounds can largely reduce communication cost.

Q7(R#3):Incomplete ablation study.
A7:Thanks! The suggested ablation is expected to degrade performance(Evidence obtained but not reported due to rebuttal policy).

Q8(R#3):About distribution shifts (DS) or label drift (LD).
A8:Dir0.3 setting already causes vast DS(heavily class imbalanced, some classes missing at each client, so not just LD), and ours still converges much faster than others(Table 2, last col). Suggested tests will be done(future work), with positive results expected(since frozen classifier head embeds textual prior that is independent of modalities etc.).

Q9(R#3):Missing related work.
A9:Please see A1 above. Ours is different from related work, so our contributions still hold.

Q10(R#4):To try with other backbones, & discuss t_c.
A10:ResNet18 was adopted following prior work. We expect to obtain positive results on other backbones. t_c is simply class name in tests, & better performance is expected with rich description as suggested(future work).

Other comments are adopted to refine paper."
https://papers.miccai.org/miccai-2024/697-Paper2277.html,"Thanks to all reviewers for their valuable comments.
1.[Reviewers #5, #6]: The computational complexity and computational resources/time complexity of the proposed model"
https://papers.miccai.org/miccai-2024/698-Paper2491.html,"We sincerely thank all of the reviewers for their comments and their acknowledgment of the novelty, methodology of our work. We address the key concerns below and will further improve our paper.
R1-TPNet vs.TCEIP. Different from TCEIP, our TPNet is designed to predict the implant depth. IRD is the first stage of TPNet and is designed to locate the implant region to crop a sub-volume from the CBCT data, which greatly reduces the computation costs. Moreover, IRD is an improved version of TCEIP, which is more suitable for clinical applications in terms of balanced accuracy and speed. 
R1,R3-Encoder+Decoder. Intuitively, implant depth regression only requires global information. However, when we only used encoders+regression head, the network encountered poor performance. Through discussions with dentists and visualization of network features, we found that the texture of neighboring teeth in the implant area has strong reference for determining the implant depth, which means that the local features are also key information. Therefore, we design a decoder to encourage the network to learn fine-grained local features, which greatly improves the accuracy. We will clarify it in the final version.
R3-Canny Operator. Let the network learn a filter is a good idea, however, randomly initialized filter is prone to gradient explosion during initial network training due to their inability to extract good edge features. In contrast, canny operator is simple and effective. We compare multiple operators, canny achieves the best accuracy.
R3-Vocabulary vs.Vector. Using vocabulary to extract text embedding from CLIP has been proven to be more effective than using 1D vector in TCEIP, our results are consistent with this conclusion.
R4-Acc Criterion. The spacing of CBCT is 0.2mm. In our task, the kernel of the implant should not invade the mandibular nerve canal and should maintain a minimum safety distance of 1.5mm. Therefore, as long as the center point of the implant root conforms to this rule, it is a good prediction. In this paper, we consider the timeline of the video as the sagittal axis of CBCT, so we can directly use IOU to measure the accuracy of implant depth prediction while ensuring that the implant roots meet the standards(>1.5mm). We will clarify this definition in the final version. 
R4-Using Sagittal Axis. In the task of predicting implant depth using sagittal axis, the posture of the implant does not need to be considered. As discussed in the above evaluation criterion, we aim to ensure that the center of the implant root maintain a minimum safety distance of 1.5mm with the mandibular nerve canal. 
R4-Performance in Table 2,3. Our annotation sets the maximum value of implant depth in the above rules to ensure the accuracy of network prediction. Therefore, the larger the m, the greater the accuracy of the prediction. When TPL is introduced, it bring large improvement at m=0.8, which is very meaningful for clinical applications, as the accuracy at m=0.6 and 0.7 is far from real clinical requirements. Moreover, we found that powerful backbone will bring large improvement.
R3,R4-More Comparison. The commonly used method to predict implant depth is based on bone measurements. We compare TPNet with measure-based method, e.g.,dental-yolo, in which TPNet has higher accuracy and nearly three times faster inference speed, demonstrating the potential of TPNet in clinical applications.
R4-Visual Comparison. Limited by the paper length, we only selected the most representative visualization results, to validate the effectiveness of TPL loss, from which we can see that the TPL enables the network to make more accurate predictions, both at the start and end slice.R4-Texture Variation. The texture variation refers to the pixel variation of all 2D slices in Fig.2(a), which is obtained by calculating the standard deviation of all 2D slices. The brighter the Fig.2(b), the greater the texture variation. We will clarify it in the final version."
https://papers.miccai.org/miccai-2024/699-Paper0802.html,"Thanks for the support and insightful comments of all reviewers.
To R1:
Q1: Text prompts and fine-tuning data
A1:

Q2: TVHAâs benefits
A2: We tried a U-shape decoder only with self-attn. during initial study, which performed similarly to regular UNet (+<1%) for comparison. Moreover, we emphasize that our aim is to train a light weakly text-driven segmentation model, instead of directly deploying the large model (TVCC+SAM) which is to generate high-quality pseudo masks. It is unnecessary to prove the weakly-supervised model with TVHA gains marginally better results than TVCC+SAM.

Q3: Standard deviations
A3: Standard deviations for mDice and mIOU will be added in the revised paper.

Q4: Ambiguous row 9 of Tab.1
Q4: The U-shape decoder doesnât need text supervision. Weâll edit row 9 in revision.

To R2:
Q1: Generation of text prompts
A1: To avoid prompt design costs and echo the simple yet effective text supervision aim, we use GPT-4 to generate a concise sentence within 20 words. We tried longer texts but did not bring extra increase.

Q2: Model size of different versions
A2: The params of the model with TVHA is 43M, while that without TVHA is 34.91M. We added params (i.e. much deeper) to the baseline with UNet decoder to 40M, but did not bring such increase as TVHA, showing our design is effective.

Q3: Code releasing
A3: Due to the anonymous policy, we are unable to include the code link here, but will add it in revision. Weâll shorten the TVHA description and add more literature review in our paper.

Q4: Visual prompts integration
A4: Our target segmentation network is a light CNN-based framework with ConvNeXt as its backbone, instead of a SAM-based foundation model. We hope to both maintain its effectiveness and simplicity. So, the bounding box info. can only be used as supervision rather than visual prompt inputs for our target decoder.

Q5: Datasets chosen
A5: We chose these two datasets because endoscope and MRI are two very common and representative medical image modalities, and the findings can be easily described in simple texts. We also did experiments on skin image data gaining positive increase.

Q6: Other works with text prompt
A6: Ariadneâs Thread mentioned in the Sec3.2 and Tab.1 in our paper is text-guided fully supervised segmentation network. Our modelâs performance exceeds it.

Q7: The quality of the bounding boxes
A7: The TVCC generates good enough pseudo boxes, achieving around 0.80 mAP for both polyp and Brain MRI data. Weâll add corresponding analysis in the revised paper.

To R3:
Q1: Clarification about baseline
A1:

Q2: Study about component design
A2: We did in-depth studies during initial design. Compared with UNet decoder, with dual-way cross-model atten., the mDice +3.42% in average. With channel atten. the mDice +3.89% in average. The details will be updated in the supp. file due to rebuttal policy."
https://papers.miccai.org/miccai-2024/700-Paper3386.html,"We thank the reviewers for their thoughtful comments. We are encouraged that they found our approach novel and innovative. We are pleased that all reviewers found our explanations clear and that (R4) identified that beyond just performance gains in our example, the proposed pipeline could advance current reconstruction methods.

We note, however, that one of the main concerns shared by reviewers (R1, R4) is the lack of discussion of clinical application:
As stated in our introduction, our work is essentially a proof of concept. To this end, all our experiments were carried out in the 2D setting, which enables smaller-scale testing. Clinical feasibility ultimately depends on the scalability of our method to 3D, which seems achievable without major changes.Indeed, learning a continuous model remains a density estimation problem in low-dimensional spaces (only three coordinates are needed to define an emission source and four for a line of response). Conclusive experiments with this regression step have been carried out in 3D using our same model, but are not
included in this work. In terms of computational resources, the main limitations are likely to be related to the discretization of the continuous model during reconstruction. Another important question concerns the management of the possible bias introduced by the simulator and the regression, as pointed by (R4).

We address other concerns that were formulated :

[@R3] The authors do not compare to normal system modeling in PET (which also models the scatter and randoms components) :
The formulation was perhaps not very clear, but in end of section 4.4, it is stated exactly that we must take scatter into account for the reference factorized model. We then explicitly state how we have done this, by adding to the Poisson model the scatter component that is estimated via simulation from
a first reconstructed image. This method is consistent with what is used in practice. As for random events, they are not considered in our simulator and therefore not included in the models.

[@R3] The paper is limited on detail regarding network training.Information such as the network architecture and loss function are missing:
We do not agree. The description of neural networks, which are classic MLPs (fully connected), is not the main element to remember in our approach, and is therefore rather short. However, we mentioned that the neural nets used in MDN are MLPs with four hidden layers of size 256. We explicitly gave the formula for the loss function in equation (2): âChoosing the best density estimator then becomes an optimization problem on w, with the conditional likelihood (2) over the train set as the objectiveâ (Section 2). We also specify the total number of simulated data (3 millions) used for training and validation.

[@R3] Why not compare to a gold standard of very high count Monte-Carlo results stored in a system matrix?
Even though our 2D experiments are of smaller scale than 3D, the PET matrix still contains 2 billion terms and is not sparse since scatter is taken into account. Therefore it could take up to 10^11 simulations (unless symmetries are imposed) to estimate it directly using Monte Carlo, which is very expensive. In com-
parison we used 3.10^6 simulations in our model to approximate the conditional distribution of observations for any point source (see Figure 1).

Recap : Our goal is to propose a new conceptual approach that allows to efficiently incorporate a realistic generative model of the PET process into an iterative model-based reconstruction. As Monte-Carlo estimation of the system model in PET is still considered to be the most accurate modeling technique,
we believe that this relatively fast and flexible method, which does not impose sparsity or symmetry of the resulting PET matrix, is promising. We would like to end by reiterating that clinical application was not part of the scope of this article, but should possibly be addressed in in future research."
https://papers.miccai.org/miccai-2024/701-Paper1494.html,"We appreciate that the reviewers consider our work âstrong in many aspectsâ [R3], âa foundation for future workâ [R4], âa good ideaâ [R1], to have important clinical benefits [R3], highly reproducible (we open-source synthetic data, real manually annotated data, and code) [R1,R3,R4], to have empirically strong results [R3], clearly presented [R1,R3,R4], and thoroughly evaluated and ablated [R3].

[R1] Lack of any evaluation of synthetic images: Training on synthetic images leads to strong segmentation results (Tab1, Fig5) achieved on real OCTA volumes (test set), empirically demonstrating their efficacy and quality (Fig3). With the sole purpose of image synthesis being segmentation, this represents the primary evaluation measure. Moreover, given that synthetic images were generated from patches of real vasculature embedded in corrosion casts covering the exact same cortical areas (Fig4a), we found morphological properties of vessels (node degrees, radii, length, etc.) to be extremely similar between synthetic and real images. An additional comparison of intensity histograms revealed similar characteristics in the distribution of background and foreground intensity values. Analysis of PSNR values painted a similar picture. We, therefore, argue that our synthetic images resemble real OCTA images very closely, thank R1 for the thoughtful comment, and will provide more details in the adjusted paper.

[R1] Why Gaussian noise & smoothing: It is known from literature that background noise in OCTA images can be approximated by a Gaussian distribution [Fig1 in A]. To mimic the OCTâs point-spread function, we convolve the synthetic image with a Gaussian smoothing kernel.

[R1,R3] Train on both real & synthetic images; high performance is key: Great suggestion! Given that the main motivation of our work is to propose a synthesis pipeline for annotation-free OCTA segmentation, we advise future work to experiment on how to best combine real and synthetic images for potentially increased segmentation performance.

[R3] Details on Frangi filter & Otsu thresholding: Parameters of the Frangi filter were tuned on our validation volume and can be accessed in test.py. Otsu thresholding is a parameter-free approach. Both methods were applied to the same three test volumes.

[R3] Discarding patches limits applicability; more information on patch sampling: We solely discard patches containing vascular structures that will never be observed in real, in vivo OCTA images. In particular, due to constrained OCT depth penetration, rendering whole-structure segmentation impossible, we discard patches that lie out of reach for modern OCT systems. Further, discarding sparsely populated areas with no larger vessels enables us to exclude artifacts of corrosion casts. We, therefore, argue that discarding patches is a necessity to curate a highly representative, synthetic dataset precisely tailored to real-life applications. To provide exact details on patch sampling, we will additionally open-source our volume generation script.

[R3] Synthetic ground truth labels: Synthetic ground truth labels used in our main experiment (Tab1c) are given by unmodified, voxelized patches extracted from corrosion casts (Fig2 left). Modifications were solely made in the experiment on curvature (Tab1e, Fig6 LTAC). We thank R3 for the remark and will stress this in the paper.

[R4] More experiments to demonstrate that the synthesis pipeline can be adjusted to data from different devices: We fully agree that this would be an interesting addition to the paper! However, given that we are the first to open-source large amounts of annotated 3D OCTA data, it is currently impossible to properly quantitatively evaluate our synthesis pipeline on data from different devices. Nevertheless, we demonstrate the diversity of our pipeline in Fig7 and thank R4 for the comment.

We will revise the paper to clarify minor points that could not be addressed in the rebuttal."
https://papers.miccai.org/miccai-2024/702-Paper2617.html,N/A
https://papers.miccai.org/miccai-2024/703-Paper3901.html,"We thank the reviewers and editors for the great efforts in reviewing process.

Reviewer5 (Q1-6)
A1: Our model theoretically can fuse any number of modalities. We focus on tri-modal because: 1) Currently, no studies can achieve tri-modal fusion and super-resolution simultaneously, we take the first step to do this work. 2) Tri-modal is better than the existing two-modality for clinical assistance. Using four modalities simultaneously for diagnosis is uncommon and may lead to information redundancy, affecting clinical judgment.
A2: In R^3HW, 3 is the number of channels, not the super-resolution factor. R^HW is a single-channel image.
A3: We will make Figure 1 clearer. The TMFA Block is similar to the encoder, integrating features from different modalities before diffusion. After the images are encoded by the TMFA, they enter the conditional diffusion process to yield the fused images. The model undergoes a 4000-step diffusion process. We perform inference on the validation set after every 10 epochs. For the loss function L_PSF, the U-Net is used to predict the noise during the diffusion process. So L_PSF is calculated by comparing the actual noise added (noise in ground truth) with the noise predicted by U-Net.
A4: We will correct it and comprehensively check our paper.
A5: We used 84, 10 and 25 images as the training, validation, and test sets, respectively, we will add these information.
A6: TFS-Diff is suitable for 3D images, we may address it in future work.

Reviewer6 (Q1-3)
A1: Our tri-modal work is significant for clinical applications. We will modify the Introduction and carefully introduce the clinical scenarios involving medical image segmentation, prediction, and detection. For more clinical-task-related assessments in medical image fusion, which may involve constructing new datasets and labeling, we will address them in our next work.
A2: The model can fuse two modality images. When dealing with image fusion involving only two modalities, we simply concatenate the images from the two modalities and then train the model.
A3: The quantitative results are all derived from the test set, demonstrating the good generalization of our method. During training, to further enhance the modelâs generalization ability, we adopted dropout, L2 regularization, and data augmentation techniques (such as random rotation and scaling). We will share all the fusion results via GitHub.

Reviewer7 (Q1-5)
A1: Our model is the first to simultaneously realize tri-modal medical image fusion and super-resolution. Although we introduce SR3 as the backbone and propose PSF, which may not be impressive innovations, the proposed simple yet effective model can successfully solve tri-modal fusion and super-resolution simultaneously. Extensive qualitative and quantitative experiments consistently confirm the superiority of our model.
A2: Although Fast MRI and sparse-view reconstruction in CT can improve image quality, they mainly focus on single modalities and cannot address the simultaneous enhancement of multiple low-resolution images. Integrating these tasks into a single end-to-end task is not an easy task, which can: 1) Reduce error propagation: Processing two tasks in steps carries the risk of errors propagating, potentially introducing artifacts into the fused results. 2) Increase efficiency: An end-to-end model allows sharing of parameters for both tasks, reducing the overall computational burden. 3) Enhance data learning: Training both tasks simultaneously, the model can learn more robust and generalized features optimal for both super-resolution and fusion. When tasks are split, this synergy is lost as each model learns features specifically for super-resolution or fusion, possibly missing information beneficial to both.
A3-5: We will meticulously review our paper to ensure no grammatical errors and citation mistakes. We will replace âbimodalâ with âtri-modalâ. In the ablation experiment for PSF loss, we substituted PSF loss with MSE loss."
https://papers.miccai.org/miccai-2024/704-Paper3740.html,N/A
https://papers.miccai.org/miccai-2024/705-Paper2261.html,"We thank all the reviewers for their constructive feedback and appreciate the high-quality reviews! We would like to address the comments in a point-to-point manner below.

Visualization results: We provided the qualitative results in the supplementary materials (see Fig. S2).

Ablation experiments: Ablation studies are illustrated in Fig. 3, where we demonstrated that the combination of SiNG with a margin of 0.5 and Focal-L1 yielded the best results.

Experiments: We plan to add more experiments to the journal version of the paper.

Focal-L1 loss: Thank you for your very insightful observations. We acknowledge that we do not use backpropagation on the sample weighting. This, otherwise can indeed cause unexpected behavior during training. We realize that we can simplify the denominator to only âabs(gt)â to achieve our intended purpose of the loss while avoiding this issue.

Literature review: We acknowledge that our literature review can be improved. We will add these to camera-ready, and will also incorporate relevant work proposed by R3.

UNet3D: We will study this in the journal version of the paper.

GeoLS: We will provide the detailed configurations of this method in our experiments and clarify the potential differences in experimental setup in camera-ready.

We would like to address the concern related to the novelty of our method here.

Firstly, the focus of our paper is on brain tumor segmentation (BTS), which is an established problem in the MICCAI community and of high clinical relevance on its own. None of the papers listed by R2 and R3 solve BTS, and in the BTS literature, we have a novel method and results.

Secondly, our goal in using the geodesic transformation was to produceuncertainty mapsfor the ground truth annotations. The novelty of our method is in predicting uncertainty maps directly, instead of having it as an additional learning objective, as done in other papers (i.e. those listed by the reviewer).

Thirdly, our method is purely a regression approach, which replaces the conventional pixel-wise classification learning. This is difficult because simple normalized distance values of other approaches can be too close to the class boundaries. For example, the difference between 0.01 and -0.01 is minimal, although they belong to 2 different classes. The sign-checking part in the sample weighting of our loss function, as well as the transformation in the soft label generation, is meant to tackle this issue. We demonstrate that this pixel-wise regression approach can achieve better performance than combining pixel-wise and region-aware classification losses such as BCE and Dice loss."
https://papers.miccai.org/miccai-2024/706-Paper1155.html,"We thank all reviewers for their valuable time and feedback, which helped improve this manuscript. We are glad that the reviewers appreciate novelty (R1, R3, R5, R6), superior performance (all reviewers), clinical usefulness (R5, R6), and writing (all reviewers).

We will release the code upon acceptance.

R1
Q1 Since we applied our method to existing frameworks, all models were trained with their original losses (e.g., GAN losses) alongside the proposed structure-preserving losses L_struc and L_sino
Q2 PSNR/SSIM/MAE are widely used image quality metrics in medical imaging ([17,18,19]). The metric mentioned is tailored for vessel-level IQA that requires coronary artery region detection and vessel-level labels, which is hardly applicable to our head and neck data
Q3 Our test set comprises 10,500 CBCT images from 70 patients collected from 5 hospitals spanning Europe and the US
Q4 To the best of our knowledge, the two CBCT data augmentation methods we compared are the latest

R3
Our method simulates CBCT images on the fly during training instead of pre-generating them. Specifically, each model is trained for 200 epochs with 37,500 iterations in each epoch, and thus, there are 7,500,000 simulated CBCT images during the training process. This could well cover the degradation types and prevent overfitting, as demonstrated through quantitative and qualitative evaluation on 10,500 image pairs

R4
Q1 We apologize for a typo - âdGâ should be âdtâ and âx_muâ should be âx_mu(t)â, where t denotes the position along the X-ray beam path within cone beam geometry G. Eq 1 integrates the 3D objectâs attenuation coefficients over t along the X-ray path to obtain the 2D projection data. We used the ASTRA Toolbox and Operator Discretization Library to implement Eq 1
Q2 Metal artifacts result from X-ray polychromaticity [29, 2]. As the polychromatic projection P is linear, following [29], we added the polychromatic projected metal implants to projection data, which amounts to adding metal implants to the CT image followed by P. We experimentally show that our simulation enables high performance on a highly heterogeneous CBCT dataset. We leave more realistic simulations for future work
Q3-4 Tab.3 indicates which kind of artifact is simulated or constraint is applied. The results show that greater diversity in simulated artifacts leads to better performance, and the gains in simulating a particular kind of artifact may reflect its occurrence frequency in the dataset, i.e., the gain is the largest when that artifact is the majority

R5
Q1, Q5, Q8, Q9 We appreciate your suggestions and will revise our manuscript
Q2 As our test data were acquired from 5 European and US hospitals, the protocols could be different. The DICOM header shows different mAs used in scanning (10, 12, 20, and 50). Although CBCT imaging technology has advanced, many hospitals have not upgraded to the latest machine
Q3 Brion et al. adjust brightness by adding a random offset to the CBCT image (sec. 3.3 in [4]), in order to reduce the gap between CT and CBCT intensity distributions
Q4 Dahiya et al. share the same goal as ours, except that they do not explicitly simulate the CBCT artifacts as we do, but directly extract them from other existing CBCT images. They used âpower-law adaptive histogram equalization to extract scatter/noise artifactsâ
Q6 âDomain knowledgeâ refers to 1) CBCT scanning configurations and 2) physical causes of the artifacts, which we translated into algorithms simulating CBCT artifacts (Eqs 1-6)
Q7 We converted HU to linear attenuation coefficients

R6
Q1 Since some CT scans can have metal artifacts, we exclude such affected regions via a mask.
Q2 Yes, it means the Poisson-noise-contaminated sinogram
Q3 Applying the Radon transform to the simulated images allows for inducing device-related effects (Sec.2.2 âScanner effects simulationâ), influencing the final appearance of the simulated artifacts
Sec 10 Thank you for indicating the typos - we will correct them"
https://papers.miccai.org/miccai-2024/707-Paper2762.html,"We are grateful for the time and effort you put into your detailed review. The points you highlighted have been immensely helpful in revising our paper.

Responds to R1:
Comment1. Thanks for your comments. In recent years, 3D systems have indeed become increasingly user-friendly, but most doctors still need the assistance of medical imaging during surgery, especially X-rays. The localization of key electrode points not only aids doctors in determining positions but also plays a crucial role in subsequent registration, reconstruction, and other processes. We hope this method can serve as a cornerstone for the future development.
Comment2. To avoid overfitting, we set a validation set to periodically evaluate model performance during training and employ early stopping method.
Comment3. Thank you so much. We will include the catheter definition in the appendix.

Responds to R3:
Weakness: Thank you for your comments. During our evaluation, we calculate the MRE between the landmarks and GT, where the GT is annotated at the electrode center in the images, and the images are extracted from several sequences. This may ensure a certain level of temporal consistency. Our method in this study indeed does not take temporal information into account, which is a question worthy of further exploration, we will consider this issue in our future work.
The average inference time for a single image using our method is 26 ms.

Responds to R4:
Weakness: In our study, we find that the electrode landmark detection by other methods sometimes deviate significantly from GT, whereas our method mostly yields results that are much closer. Accurate electrode landmark detection is crucial as it forms the foundation for subsequent tasks such as registration, multi-angle reconstruction, and others.
Comment1: Thanks for your comments. In this study, RF and CS catheters are two of the most commonly used catheters in clinical surgery. We will further include the catheter definition in the appendix and discuss the number of landmarks and their potential variations in clinical practice. 
Comment2: Thank you. We will add the relevant descriptions to the introduction according to your suggestions.
Comment3: Yes, itâs MRE. We are sorry for the confusion, we will add descriptions to caption of figure 3."
https://papers.miccai.org/miccai-2024/708-Paper4090.html,"We thank all reviewers for their valuable comments! Here are some clarifications and responses. For convenience, we use W for weaknesses, and C for additional Comments.
Reviewer #1
W1: Yes, we are considering to add those advanced models. But we expect that those models would benefit less from DRAPS as their performance is higher.
W2: Sure, we will add some details of the robustness in the camera-ready version.
W3:Thanks for your suggestion! We have highlighted our good results in bold in the edited version.
Reviewer #3
W1:The method proposed in this work can be generalizable. As specified in the algorithm description, we do not limit the choice of K, ie., the number of possible classes. K does not have to 8. It is 8 only for this dataset. But for other datasets, the K can be adapted accordingly. As this is a supervised method that needs the data for training, for other dermatological diagnostic tasks, when plugging in the data, one just needs to change the parameters of the algorithm properly and it will work.
W2: We acknowledge that the individual responses are noisy. However, the consensus (accuracy after popularity voting) is 61.86% (v.s., chance level of 12.5%), which means that distribution-wise (the focus of this paper), our data captures important information. We will add details of the data analysis into the camera-ready version.
W3: The Naive results (Naive columns in Table 1) are trained without the KL divergence in the loss function. The second sentence in Sec. 4 contains the description of the Naive experiment.
W4: Yes. We can show error bars in the camera-ready version.
W5: We have added the corresponding definition in the text.
W6: Thanks for pointing out! We are focusing on different aspects. We will add clearer illustrations in the camera-ready version and cite the paper.
Reviewer #4
W1: Yes, the regularization is forced by adding the KL divergence loss for training. We are working on to make the illustration clearer in the camera-ready version.
W2: The KL divergence measures how different two distributions are. The larger the KL divergence loss, the more different two distributions are. During training, the model parameters are optimized to minimize the KL divergence loss that forces the learned distribution similar to the empirical distribution. We will add details of KL divergence concept into the text.
C1: First, the medical students in our sample were not significantly worse in overall performance. According to self-reported demographics, medical studentsâ accuracy is 56.20% while non-student participantsâ accuracy is 51.39%. Second, board-certified dermatologists are uncommon so recruiting a large sample of these participants is unfeasible, particularly for large-scale datasets (~940k trials, ~11k participants). The similarly high overall performance of the different groups reassures us that the data are representative of typically trained observers. 
C2: Thanks for your suggestion! We are working on it for the camera-ready version!
C3: Yes, we will release the dataset as well as the baseline model repository along with the camera-ready paper."
https://papers.miccai.org/miccai-2024/709-Paper0531.html,"R1: More pathology cases?
Although not explicitly shown, our superior results are based on the datasets already containing pathological cases: CT-MRI (45% AD patients) and BraTS (tumor cases).

R1/R3: Resulting MRI shows smoothing. Is this due to ISTA?
We agree that the synthetic MRI may appear smoother compared to real MRI. However, on closer look, this level of smoothing is present in any diffusion-based MRI synthesis study we know of. Further, despite our strategic choice to improve structural reliability at the cost of sharpness (aka, perception-distortion tradeoff), we still show the best structural clarity with anatomical fidelity (Fig. 4). Also, we confirmed there is no smoothing effect from ISTA (Supp. D).

R3: Instead of averaged MRI histogram, how about MRI histogram of CTâs nearest neighbors for the style key?
We agree that a detailed study on selecting the style key would be beneficial. As in Sec. 2.2, the histogram is largely independent of anatomy, and since CT and MRI are acquired separately, similar CTs do not necessarily have similar MRI histograms. Instead, using a single MRI histogram as the style key produced satisfactory results reflecting that style (Table 2 for Ours_best & Supp. E).

R3: Why chose 3 slices for ISTA?
As in Sec. 2.3, we chose 3 slices to maintain the same computational load as a pure 2D BBDM, which is the minimum number for overlapping inference. Increasing the number of slices would aid consistency between distant slices, but local shape consistency is sufficient, and SKC already ensures global style consistency. Therefore, increasing the input slices did not significantly change performance. Moreover, as ISTA sampling step is repeated, the guided consistency between adjacent slices spreads to more distant slices, making 3 slices reasonable.

R4: A direct metric for slice consistency is needed (e.g., [1]).
The suggestion to apply [1]âs evaluation method indicates there may be a misunderstanding of our main contribution. R4 seems to refer to consistencybetween synthetic and target volumes. Of course, BBDM+SKC also ensures that consistency, but ISTA and SKC focus on shape and style consistencybetween slices in synthetic volumefrom 2D slice-wise inference. Thus, the synthetic volume mimicking the style key histogram (which R4 seems to think is SKCâs main purpose) is just a byproduct of SKC. Further, the suggested metric [1] (direct mask comparison) can measure the âidentityâ between slice i of volume A and slice i of volume B, but this is not suitable for measuring structural âcontinuityâ between slice i and i+1 of volume A. Instead, our metrics naturally measure the volume-level errors caused by slice inconsistency within the volume. Also, our method clearly shows dramatic improvements in slice consistency (Fig. 1 & Supp. D), demonstrating its effectiveness.

R4: For 2D methods, did you evaluate after stacking each slice into a volume?
Yes, since our focus is on volume level, we naturally evaluated in this manner.

R4: The metric shows a slight increase over the 3D method.
Our 2D method outperformed all 2D and 3D baselines in both visual quality and metrics, showing statistically significant improvements across all metrics on both datasets (paired t-test, P<0.05).

R4: FLAIR-to-T1 performance of [2] is better.
This is not true. Metrics improve as the proportion of image background (value 0) increases. Due to 3D baselinesâ memory footprint, we cropped the background more tightly to 176x176, which is disadvantageous for metrics. When we re-measured Ours_avgâs results only with zero-padding to 256x256 (as in [2]), our performance was clearly better. This can be easily reproduced (code will be released).

R4: Why 100 DDIM steps? Tradeoff between sampling time and quality?
All diffusion baselines used the default time steps in their papers. For our method, 100 steps showed a marginal difference compared to 1000. Since 100 steps already outperformed baselines, we prioritized efficiency."
https://papers.miccai.org/miccai-2024/710-Paper0362.html,"We thank all reviewers for their comments, especially in noting that our paper is well-written (R4), presents a novel lymph node (LN) detector with sensible 2.5D fusion and cross-slice query contrast module (R1, R4), and reports satisfied results on the LN detection task (R1, R3, R4) with dataset contribution (R1). We address the individual comments below.
Q1: LN regions/characteristics of each dataset (R1).
Indeed, we have provided the dataset description in Table 1 of the supplementary (due to space limit in main text), including resolutions and LN regions in each dataset. Meanwhile, we want to clarify that LNs in different body parts have their own context characteristics. E.g., the model trained only with abdominal LNs does not generalize well in the neck region due to the unseen surrounding anatomical structures. Thus, training the model on LNs from various body regions is imperative to enable universal LN detection.
Q2: Results only using public NIH dataset and reproducibility (R1, R4).
When trained and tested only on public NIH dataset, the vanilla Mask DINO only has an average recall of 34.26% across 0.5 to 4 FPs. With the proposed 2.5D fusion and cross-slice query contrastive learning, our performance is significantly improved to 43.94%. Once accepted, we plan to release source codes to ensure reproducibility.
Q3: Issues of method details (R1, R3).
1) Fig.1(d) is merged 3D LN prediction from 2D slice-level predictions in Fig.1(a-c). We train method [20] using our LN training set and generate this prediction in our testing set.
2) Clarity of Fig.2. The framework consists of a 2.5D backbone and a detection transformer (i.e., transformer encoder and decoder) with multiple prediction heads (i.e., mask head, box head, and class head). All these components are shown in Fig.2 from left to right (grey-color feature maps are the output from the 2.5D backbone). The class, box and mask heads are basic modules of CNN or transformer-based detectors to make class, box and mask predictions, while the contrastive head is our new proposed head defined in Sec.2.2.
3) Multi-scale features. First, we extract multi-scale feature maps from output features of res-block2 to res-block5 in ResNet50, where the feature map will be downsampled once after each res-block. Then, the feature map at each scale is processed by a 2.5D fusion layer to get the 3D-context-enhanced feature map. Finally, these feature maps are collected to form multi-scale 3D-context-enhanced feature maps.
4) 2.5D fusion layer. We briefly describe the 2.5D fusion layer in main text due to space limit. The detailed layer structure is shown in Fig.1 of the Supp. We will add more descriptions in final version.
5) Loss weights. We follow the loss weights Î»1-3 as used in original Mask DINO.
6) 0.5 FPs. 0.5 FPs refers to 0.5 false positives per CT scan. E.g., if there are 10 FPs in 20 CT scans, this would equate to 0.5 FPs per scan. Recall at 0.5 FPs is commonly used in lesion or LN detection [2,23,15,20-22].
ï»¿
Q4: Issues of Eq.2 (R3).
We acknowledge the oversight and agree that there is an error in Eq.2 of initial submission. exp() is the exponential function, which should take the inner product of two vectors (we forgot to add inner product). We will correct this in final version.
Q5: Pre-trained weights (R4).
The backbones of the selected baseline models and ours (except 3D nnDet and 3D nnUnet) are all initialized with ImageNet pre-trained weights, while nnDet and nnUNet are randomly initialized. Take our Mask DINOâ  as an example; we find that without pre-trained weights, the model will take more time (~50 epochs vs 30 epochs with pretained) to converge and drop 4~5% in average recall. We will add pretrained information in final version.
Q6: Other medical detection tasks (R4).
We initially intend to also examine on DeepLesion, yet, it only provides masks on one slice for each 3D lesion, making it unsuitable for evaluating our method. We may explore other datasets in the future."
https://papers.miccai.org/miccai-2024/711-Paper2402.html,N/A
https://papers.miccai.org/miccai-2024/712-Paper2479.html,The rebuttal addresses the following four concerns.
https://papers.miccai.org/miccai-2024/713-Paper0405.html,"We thank the reviewers for the positive feedback on the clear writing & organization (R1, R3, R5), novel method (R3, R5), and convincing evaluations (R3, R5). We will update the manuscript according to the responses below: 
Q1 (R1): The contribution is limited to the multi-modal regularization compared to LSOR [10].
Re: As stated in the introduction, we embed domain knowledge via 2 novelties: 1) A longitudinal regularization that enforces one direction of SOM to represent disease abnormality. Beyond giving a neuroscientific meaning, it allows us to align SOMs of different modalities, which eases interpretation across modalities compared to SOMs by LSOR (as they are generally not aligned); 2) A regularization that embeds knowledge on disease progression across modalities. These result in more accurate predictors than LSOR. 
Q2 (R3 & R5): The effect and choice of hyperparameters: 1) size of SOMs, 2) neighboring function, 3) thresholds in losses, 4) loss weighing factors.
Re: Here is our intuition: 
1) Too small SOMs cannot capture enough information while too large SOMs are noisy. We chose 4x8 which gave the best interpretation. For reference, it leads to each cluster on average containing 32 samples (i.e., 1172/(4*8)) and each column representing a 5-yr age range (i.e., (95 yr-55 yr)/8 columns). 
2) Gaussian function is the common choice in SOM. 
3) Too small \alpha_(o) (i.e., threshold in longitudinal regularization) results in disease abnormality not being linked to any directions of the SOM, while a too large value can cause overfitting. A guideline is to set it based on the ratio between the minimum time between visits (i.e., 1 yr) and the age range of each column, leading to 0.2 (i.e., 1 yr / 5 yr). However, as the age range of columns can be less than 5yr, we set  \alpha_(o) to 0.1. Regarding the multi-modal regularization threshold \alpha_(m,p), the larger its value, the stronger the temporal ordering across modalities is enforced. We set \alpha_(m,p)=0.01 as the difference of abnormality across modalities depends on the disease stage, and a too large value can cause overfitting. 
4) Loss weighing factors reweight loss components to similar scales. Too large factors for longitudinal and multi-modal regularizations can lead to less informative representations, while too small values risk the loss of the directionality of SOM and the cross-modal temporal ordering. 
Q3 (R1): Comparisons with other multi-modal longitudinal methods.
Re: Our method aims to get interpretable representations while those fully supervised methods given by the reviewer aim at optimizing predictors. Though we evaluate our method on downstream tasks, our training setting is too different from theirs to make fair comparisons.
Q4 (R1): 1) The method relies on and demonstrates (Fig. 3) âAmyloid PET being more clinically relevant than MRIâ. Thus, experiments on whether multi-modality is beneficial should be included, i.e., 2) using single modality SOMs, and 3) simply stacking modalities.
Re: 1) We are not relying on or demonstrating that amyloid PET is more clinically relevant than MRI. Our model relies on amyloid PET detecting abnormality at an earlier disease stage of AD than MRI. In Fig.3, AD cases are clustered on top in the amyloid PET plot, while the orange lines in the MRI plot reveal a large change, which suggests that both modalities together are better at modeling AD progression than either one alone. 
2) Due to the page limit, we did not include the results as it is not surprising that using multi-modalities does have superior accuracy in predicting MCI converter than single modality SOM. 
3) It is shown in the âNo pretrainâ row.
Q5 (R1): The method uses ROI features instead of images, which is less robust.
Re: We agree it is a limitation of this work and plan to adapt the method to images in future works. We will mention this in the final paper.
Q6 (R3): Ablation study on removing multi-modal regularization.
Re: It was presented in Table S4."
https://papers.miccai.org/miccai-2024/714-Paper0608.html,"Response to Reviewer 2: 
We thank reviewer#2 for their time and recommendations. The reviewer points out the limited diversity of experiments (to multi-class classification and segmentation). Due to limited space, we could only show these results; however, based on our experimentation, models that use fully connected (linear) layers or convolutional layers are applicable. Extension to recurrent layers is of interest and would be a future avenue for exploration. 
Comments: 
1) It would be interesting to have a comparative analysis of the joint Gaussian distribution. In our work, we approached the problem from a standard variational inference perspective, where each parameter is represented by a distribution. 
2) We conducted experiments on the impact of the beta hyperparameter on the loss function and how to balance learning the weight distribution and refining the NLL loss in training step 2 - we can include this additional analysis in the final paper.

Response to Reviewer 3: 
We thank reviewer#3 for their time and recommendations.

1) Regarding computational savings, the current method only improves costs related to training; we can clarify this in the final paper to avoid confusion. Further savings can be made from an inference perspective, but that is work to be completed in the near future.

2) Due to space constraints, we could not add our relative FLOPs computation; we will add it to the supplementary information in the final paper. The FLOPs are related to training, not inference; the number of samples contributes to the total number of FLOPs; this is considered along with the number of trainable parameters in the relative FLOPs count.

3) MC-dropout exhibits overconfidence compared to ensembles, for example, when tested on OOD data, which we briefly mentioned in the introduction.

4) The distribution of weight selected by the TopK first-order gradient method generally concentrates at the edges of the network (towards the input and output), consistent with previous published work.

Response to Reviewer4: 
We thank reviewer#4 for their time and recommendations. The highlighted related work on Laplace/HMC/SWAG will be added to the related works section of the final paper.

2) On the theoretical backing: thereâs a solid theoretical backing to our subnetwork selection criterion based on the Taylor series approximation of introducing small perturbations into the network parameters (g(theta+delta(theta)). We can include more details in the final paper, given that space permits it.

3) We were more interested in the total predictive uncertainty, but we agree that a decomposition of the two uncertainties would be valuable, especially with respect to the multi-rater dataset.

Questions: 
1) The parameters are modeled as univariate Gaussians.
2) The prior is set to an isotropic standard normal. 
3) We accumulate the gradients based on all the batches after the pertaining step - to allow the selection to be sample-independent.4) Lower entropy is desired for in-distribution data and correctly classified data. A higher entropy is needed in uncertain or incorrectly classified data  (see qualitative examples on the ISIC dataset) or out-of-distribution data. For the multi-rater dataset, we would like the model to exhibit low uncertainty where the multiple-raters agree and higher uncertainty where the labels diverge, which we see consistent with our approach (higher entropy with higher disagreement and low entropy in easy-to-classify regions). 
5) Our selection criterion is consistent with this finding; the selection converges to the first/last layers at low percentages. 
6) We need to investigate further to accurately describe the connection between our subnetwork selection and the diagonal Hessian Laplace method by Daxberger (2021) to provide an accurate answer."
https://papers.miccai.org/miccai-2024/715-Paper1676.html,"We would like to thank the meta-reviewer and the reviewers (R1, R3, R6) for their constructive comments. We are encouraged by their praise for the novelty (R3, R6), the effectiveness of our method (R1, R3, R6), the extensive validation with good performance (R1, R3, R6), and the clear organization of this work (R1). We have carefully studied the comments and have then responded to them.

Generalization to other architectures (R1, R6)
Thanks for the constructive comments. Given the limited space and the extensive number of experiments required for multi-backbone evaluation, we have not yet verified the generalization of our SH-PEFT to other architectures. In future work, we will further refine our SH-PEFT method and demonstrate its effectiveness across different Large Vision Models (LVMs), including those based on both Transformer and CNN.

In Fig.3, why intersection rather than union? (R3)
We would like to clarify that Fig. 3 depicts Formula 1, where the tunable weights are jointly determined by the weight importance estimated by both task-specific and task-agnostic methods. Rather than intersection or union, a hyperparameter, lambda, is used to weigh the two importances of each weight and select top-k important weights for tuning. The impact of lambda is shown in Table 3. We will further polish Fig. 3 for clarity, by showing that the selection is not based on intersection but on top-k weight selection.

Differences with solvers (R1)
In comparison with solvers SH-PEFT is interested in adapting LVMs to downstream tasks in a parameter-efficient manner. Specifically, it offers two main advantages: 1) Reduced storage burden: Solvers train the entire model, which requires storing a separate LVM for each downstream task. In contrast, SH-PEFT only records parameter changes at a few key locations. When dealing with multiple downstream tasks, it significantly reduces the storage burden. 2) Greater memory efficiency: Solvers record gradients for each parameter during training. Differently, PEFT solutions, through proper optimization, only record a portion of these gradients during training. This results in the use of a smaller amount of GPU memory, enabling the fine-tuning of large models with limited resources.

Comparison to SPT [11] (R1)
The distinctions between our SH-PEFT and SPT are discussed in the final paragraph on page 5, including: 1) We extend the scope of weight selection for enhanced flexibility. 2) We contribute a new strategy for estimating weight importance. 3) SH-PEFT performs better when applied independently.

Speed of convergence & Number of parameters kept (R1)
Maintaining a small number of trainable weights can accelerate the convergence of the model. Based on our previous experimental results, we found that full model fine-tuning continues to improve results from 20k to 40k iterations, but there is no significant improvement from 40k to 60k iterations. As for PEFT methods, most models, including ours, converge before 20k iterations, and subsequent training does not enhance performance.
Regarding the proportion of selected trainable weights, we primarily consider the comparability with other PEFT methods. As shown in Fig. 4, some existing methods maintain about 1% of the weights. Therefore, in our experimental design, we use 1% to demonstrate that our method is more effective when compared under a similar number of trainable weights.

Typos and formation (R1, R3), Code Release and future work (R1, R6), Model explainability (R6)
Thank you for your constructive comments. In the camera-ready version, we will carefully review our articleâs content, correct typos, and add in-text links according to the MICCAI format. Currently, due to the space limitation of the conference, the modelâs interpretability has not yet been examined. We will further add analyses for model interpretability, improve our method, and make the code publicly available in our future work based on this version."
https://papers.miccai.org/miccai-2024/716-Paper0236.html,"Dear reviewers and editors,

Thank you for very positive (R1), positive and supportive (R3), and constructive (R4) reviews and the opportunity to provide this feedback.

As described in the submitted manuscript, we present a completely novel concept for surgical scene understanding that is the first work to propose the spatial localization of acoustic events in surgery. R1 and R3 considered the provided proof-of-concept to be a significant contribution for our research community that opens up many new research directions.

Regarding the comment raised by reviewer R4 about the lack of algorithmic novelty, we fully agree that our proof-of-concept has its limitations and can be extended and improved in different directions. However, we strongly believe (supported by R1 and R3) that our promising results give important preliminary insights into the localization accuracy and associated challenges which will inspire the research community. In this context, we believe that particularly learning-based methods have great potential for processing and optimizing sound activity heatmaps and fusing them with visual data for improved surgical context understanding. We will add these insights and outlook to the introduction and discussion section of our paper. Moreover, to provide the community a jump start into this new research field, we have decided to open-source the associated data and processing code. In this context, we totally agree with R4 that there are many promising and interesting algorithms and directions, like artifact reduction and the optimization for real-time, to be developed.

Regarding the lack of comparative analysis as mentioned by R4, we want to emphasize that our work is the first to propose the localization of surgical acoustic events in the surgical field. Previous works were not taking location information of acoustic signals into account. As we propose a completely new concept without prior related work, a comparative analysis is not possible.

Regarding generalizability to use cases outside of the orthopedic domain, as mentioned by R4, we have added a statement about potential applications to the discussion section. In this context, an example for a non-orthopedic use case would be the localization of coagulation and suction events in visceral open surgery.

As suggested by R3, we have added a statement to the introduction section to highlight the potential scenarios and use cases. In this context, we believe that sound source localization will be an important component for intelligent systems in surgery, e.g. for surgical robots that perform tasks autonomously or collaborate with the surgical staff. Here, the localization of surgical sound events can help to create a better internal digital representation of the world and enable these systems to better understand their surrounding space.

We fully agree with R1 that the next step after this proof-of-concept is to expand the method into 3-dimensional space which would enable us to fully leverage the potential of SSL for surgical context understanding.

We have furthermore adapted the suggested changes by R3 regarding capitalization and a typo."
https://papers.miccai.org/miccai-2024/717-Paper2613.html,"Thanks for the insightful reviews. Source code will be made public upon acceptance. We will follow the suggestions of R3&R4 to improve the readability. We will include the new results on CoNSeP for cell detection, more generated examples, and repeats with mean and standard deviation in the revised version.

Q1: Performance improvement. (R1, R4)
Our method performs better than TMCCG for both U-Net and MCSpatNet. For U-Net, the means of classification accuracy are 0.666 vs. 0.644. For MCSpatNet, our method improves the performance on all three classes consistently, while TMCCG worsens the performance on Stromal. We explore different density estimators and GMM is best. Besides, our method performs well with both GMM and KDE, indicating that it is robust for different density estimators. The major difficulty of cell detection is classifying detected cells into correct categories. As shown in Tab. 1, our method significantly improves the mean classification f-score from 0.572 to 0.666 for U-Net and from 0.658 to 0.669 for MCSpatNet.

Q2: More helpful for weaker methods. (R1)
Augmentation is indeed more effective for weaker models and this applies to all augmentation methods.

Q3: Analysis of different density estimator performance. (R1)
The cell layout distribution of BRCA-M2C data has distinct subgroups or clusters, and GMM can effectively capture these patterns. The flexibility of KDE leads to noisy density estimation results in our case, preventing our framework from getting better layout generation. GMCM models dependencies between variables using copulas, which can introduce additional complexity. The mismatches between GMCMâs assumption and data distribution lead to inferior layout generations.

Q4: Concerns about spatial-FID. (R1, R4)
Spatial-FID is an adaption of FID on layout generation. We train the autoencode on the training layouts of BRCA-M2C, which has 80 images with size of ~500*500. The dice and cross-entropy losses for training autoencoder decrease from 2.98 and 2.16 to 1.11 and 0.16, respectively. We will provide the convergence curve in the revised version. We will also show that spatial-FID can reflect the increasing disturbance in layout maps to validate that it can indicate the similarity of layouts well.

Q5: Does better layout generation lead to better cell detection performance? (R3)
We introduce the spatial-FID metric to evaluate the quality of the generated layouts and we find that the density model with the best spatial-FID score is also the model with the best scores on the downstream cell classification task. This validates that the improvement is mainly due to the better generated layouts.

Q6: Explain headers in Tab. 1. (R3)
âStro.â, âInfl.â, and âEpi.â refer to the Stromal, inflammatory, and epithelial cells, respectively. âDetâ is the detection of all cells. The values in Tab.1 are the F-score.

Q7: Motivation of our method. (R4)
The previous methods focus on improving image quality for textures, but overlook the importance of cell layouts. To tackle this problem, we propose a diffusion-based framework to generate high-quality cell layouts. Different from GAN-based model (TMCCG), our model adopts the diffusion model as backbone. Benefiting from training stability and high quality generation of diffusion model, our method performs better in layout and pathology image generation.

Q8: Density of various cell types. (R4)
We generate spatial density maps for each cell type independently.

Q9: The cell shape and neighborhood of the generated image. (R4)
As shown in Fig. 3, different cell types have different cell morphology.  Lymphocyte cells are small in dark color, while tumor cells are larger in light color. The shape of stromal cells is more varied than other cells. In the third row of Fig. 3, the lymphocyte cells are gathered in high density to surround/confront tumor cells. This indicates our layout generative framework captures the pattern behind multi-labeled cell neighborhoods very well."
https://papers.miccai.org/miccai-2024/718-Paper2573.html,"We sincerely thank all reviewers for their comments and for acknowledging that our paper is well-organized [R1,R4], illustrates a novel [R1,R3,R4] and commendable [R1,R3] LLM application, and achieves promising [R1] and competitive [R3] zero-shot gene expression prediction performance compared to SOTA supervised learning approaches.

#R1.1: More technical implementation and dataset usage. In addition to implementation details in Sec. 1 (supplementary material) and dataset usage in Sec. 3 (main paper), we will open-source our code to facilitate research reproducibility. 
#R1.2: Interpretable results. Thank you for the suggestion. We will visualize embeddings of seen and unseen gene types.

#R3.1: Generalization, robustness, and experimental validation. We follow [9,22,2,1,23] for standard dataset validations. Moreover, testing on unseen patients and gene types has affirmed our generalization and robustness.
#R3.2: Vulnerability and bias of LLM. Kindly note that the training data of LLM is usually hidden from the public, and the different SOTA LLMs have shown promising PCC@M performance, as seen in Fig. 4. Furthermore, we use the best-performing LLM to directly embed the retrieved reference without any LLM querying. It finds 0.258 PCC@M, which is 0.011 lower than our recommended LLM querying approach.
#R3.3: Evaluation metrics. We use standard metrics for validations [2,1,22,23]. The prediction task is biased in capturing relative variation [22,23], which is the area in which our method excels.
#R3.4: Clinic translation. The gene expression is directly predicted by giving a slide image with windows and gene types of interest. Please see #R3.6 and #R4.1.
#R3.5: Comparison to SOTA. The latest SOTA method we compared was published in January 2024 [23]. The semi-supervised learning and transfer learning approaches have been studied by [22] (unsupervised exemplar retrieval and supervised gene expression prediction) and [9] (supervised pre-training on ImageNet-1K), respectively. Our zero-shot performance is competitive with them in PCC-related evaluation metrics.#R3.6: Ethical and privacy considerations. For training, we use the published datasets [9, 10xProteomic] that have thoroughly addressed the ethical and privacy issues. Our method can be tested locally on a consumer-level GPU with at least 16GB memory under half-precision, addressing these considerations.

#R4.1: Scalability. When using 25%, 50%, and 100% of data from the STNet dataset, the PCC@M is 0.207, 0.240, and 0.269, respectively. Similarly, we test the scalability of our model, which has 0.269 PCC@M and 7.3M parameters, excluding PTExtractor and PTLLM parameters. Halving and doubling the parameters have PCC@M of 0.245 and 0.273, respectively. This investigates our data and model scalability. Benchmarking with an H100 GPU, the overall training takes 28 hours. For inference, we averagely take 1.482 seconds to obtain descriptions of a gene and 0.445 seconds for the remaining computations to infer on a slide image from the STNet dataset. Note that each slide image has approximately 450 windows on average on the STNet dataset. The training and inference can be done by a consumer-level GPU with 16GB memory under half-precision. Thank you, and we will include it in our revised version.
#R4.2: Noise in ST data. We respond to the noise by applying GraphSAGE on windows connected by similar features and nearby positions, as those windows usually share similar gene expression and can be used to mitigate the noise (Fig. 2 of [23]). We will clarify it in our revised version.
#R4.3: Reproducibility. Kindly see #R1.1."
https://papers.miccai.org/miccai-2024/719-Paper1816.html,"We sincerely appreciate the reviewers for providing highly insightful comments. We greatly appreciate the feedback, as it will undoubtedly help us improve the quality of our paper. We have considered suggestions and addressed concerns to the best of our ability. According to the âRebuttal Guide,â new experimental results in the rebuttal are not allowed. We will show additional results for suggested experiments when releasing source codes on GitHub.

Q1: Open source code (R3, R4, R5):
We will provide open access to source code in our camera-ready version paper

Q2: Setting of training data (R3, R5):
There is a key misunderstanding that the training setup is less likely to require normal data. Instead, in addition to normal data, our core idea is to fully utilize unlabeled data through normal data supervision. Therefore, our proposed SAGAN supervises the restoration of unlabeled data by ensuring accurate reconstruction of normal data and precise restoration of pseudo-anomaly data. We acknowledge the practical suggestion of reducing normal annotations and will analyze SAGANâs generalization performance with less normal data in future work.  Additionally, we only control anomaly ratio of unlabeled data and randomly select the appropriate number of anomaly data. We have conducted several experiments and chose the average value as the final result to ensure reproducibility. We will make clarify this.

Q3: Explanation of specific improvements made upon the baseline (R3, R5):
SAGAN addresses limitations in previous works in two key ways. First, previous methods overlook the recurrent anatomical structures present in most radiography images. SAGAN introduces the Anatomical Consistency Module (ACM) to leverage these similarities in structures at the same positions, thereby enhancing the restoration quality. Second, previous methods fail to accurately restore anomalous regions. SAGAN proposes Abnormal Region Restoration Module (ARRM) which incorporates an attention gate mechanism. The attention gate enables the SAGAN decoder to retain normal regions while effectively discarding abnormal regions of interest and generating the corresponding normal structures. Ablation experiments on VinDr-CXR dataset show the quantitative enhancement of ACM and ARRM over the baseline.

Q4: Additional positional encoding techniques (R3):
We have already conducted an ablation study on different positional encoding techniques. Due to limited space, we did not include them in the paper. These results will be released on our GitHub repository.

Q5: More thorough explanation of the anomalous regions (R4):
We detect anomalies by comparing the difference between the restored image and the original one. Our results for detecting anomalous regions are demonstrated in two figures. In Fig. 2, we highlight the anomaly regions of interest using anomaly maps generated by the attention gate. In Fig. 3, we systematically present heatmaps derived from different data reconstructions. The heatmaps are based on the difference maps between the restored images and the original ones.

Q6: Other minor weaknesses (R5):
Thanks for suggesting the valuable article by Tang et al. (2021). We will include it in camera-ready version. The definitions of âDDADâ and âAMAEâ will also be added before using the acronyms. In Eq. (3), We have defined that x1 belongs to xn which  denotes normal data."
https://papers.miccai.org/miccai-2024/720-Paper2205.html,"Thank reviewers for their valuable feedback. Overall, reviewers consider that the proposed method novel (R1, R3, R5) and efficient (R1), appreciate its good performance (R1, R5) and the idea is nice (R1, R5). The major concerns are clinical applicability (Q1-2), method comparison (Q3-4), and elaboration (Q5-10).

[R1] Q1:Imaging geometry robustness: We use cone-beam geometry with magnification controlled by calibration parameters, aligning with real x-ray devices. SdAOFâs robustness to these parameters is validated by experiments in the supplementary materials, which will be moved to the main text in the revision. Moreover, as SdAOF uses accurate geometry instead of learning 2D-3D mapping (current methods), it can enhance robustness by training with varying imaging parameters.

[R3] Q2:Clinical value: Reconstructing bone surfaces from X-rays, rather than CT, can simplify clinical workflows and reduce radiation exposure. Though surface model lacks density distribution, it is useful for orthopedic surgical planning [28] and customized orthodontic appliances [29].

[28]Printed three-dimensional anatomic templates for virtual preoperative planning before reconstruction of old pelvic injuries: initial results. Chin Med J
[29]Application progress of three-dimensional printing technology in orthodontics. Digital Medicine

[R3] Q3:Baseline selection: Compared methods are SOTA on XrayTo3D benchmark [25]. Suggested baselines [30,31] are for CT reconstruction, which differs from our focus on reconstructing mesh surfaces.

[30]X2CT-GAN, CVPRâ19
[31]Neural Beer-Lambert, AAAIâ24

[R3] Q4:More structure validation: SdAOF also applies to other bones. Due to page limit, we report results on pelvis since in XrayTo3D benchmark: 1) It is challenging; 2) Extracted mesh quality from original annotation is the highest, valid for fine-scale reconstruction; 3) Occlusion relationship between left and right ilium is clear, benefiting occlusion analysis.

[R1] Q5:Left and right ilium comparison:  The occlusion of different parts (e.g., left/right ilium) in the AP x-ray is mutual (their information overlaps), so there is no guarantee the right ilium will be reconstructed better than the left. Lower output resolution (128^3) may also smooth out details and affect performance.

[R5] Q6: Occlusion analysis: In AP view, left and right ilium overlap significantly, making models struggle to distinguish different parts, resulting in inaccurate reconstruction. We will include visual  AP-view examples in the revision to illustrate the occlusion. Voxel-based methods indeed suffer from occlusion and perform worse than ours (see more examples in supplementary).

[R5] Q7:Interp operation: \pi(p) = (x_i, y_i) \in R^2 is the projected location. Bilinear interpolation uses features of 4 closet pixels (each of shape Cx1x1) on the feature map F_i (shape CxHxW). The shape of point-feature f_i(p) is Cx1x1. We will release the code later for clarity.

[R5] Q8:Additional extractor: Our method works without additional extractor. This extractor captures global features across subspaces rather than subspace-specific information in the distillation branch, further enhancing reconstruction.

[R5] Q9:View-depth definition: As shown in Fig. 1, the reconstruction space is divided along the view direction (assumed as the z-axis) into subspaces, with feature planes in the middle of each. The view-depth distance of a feature plane is the distance from the X-ray source to the center of this x-y plane. For a 3D point, its view depth is the distance from the X-ray source to the x-y plane where the point is located.

[R5] Q10:Ablation (B): Training the SdAOF end-to-end without the teacher OF network corresponds to setting (A), as it lacks spatial division information and distillation. Without distillation, the model canât learn or use spatial division information. Thus, (B) augments the reconstruction network with pseudo-spatial-division inputs using image-to-image translation nets."
https://papers.miccai.org/miccai-2024/721-Paper0006.html,N/A
https://papers.miccai.org/miccai-2024/722-Paper1068.html,"We would thank all the reviewers for their constructive comments.
R1Q1.  qualitative and quantitative evaluations on spatial integrity
We agree with the reviewer on demonstrating the importance of spatial integrity. Our comparison and ablation experiments show that integrating spatial features with temporal dynamics enhances model performance, suggesting the value of spatial integrity. We appreciate the suggestion on qualitative evaluation. Indeed, the GNN introduces a prior anatomical atlas into the model, enhancing the anatomical interpretation for qualitative evaluation.  However, due to the page limit, we could not present case examples to better demonstrate this. Future work warrants demonstrating the superiority of spatial integrity with more comprehensive evaluations.

R1Q2. Why can the improved perfusion MRI analysis directly improve IDH classification? 
IDH mutation is an important biomarker indicating tumor malignancy, while perfusion MRI reflects tumor vascularity associated with aggressiveness. Therefore, characterizing perfusion MRI could facilitate the classification of IDH mutations (Lu et al., 2021). We will expand the above explanation into the introduction.

R1Q3: digital reference phantom study
We appreciate the reviewerâs insightful comment, which could help validate the model. We will consider adding a phantom study in future work.

R2Q1. The contribution of three main modules.
We appreciate the reviewerâs perspectives. As far as we know, we propose the first study using spatiotemporal GNN modeling perfusion MRI.  We made specific model developments tailored for the tumor-bearing brain. Firstly, we develop a graph structure learning approach, which prioritizes relevant connections based on attention matrix, refining noisy temporal correlations. Secondly, the dual-attention mechanism fused spatial and temporal features at both tumor regional and global brain levels. Additionally, the class-balanced augmentation recombined regional tumor and brain networks, generating more diverse representations reflecting patient brains. We will further clarify our contributions in the future version.

R2Q2. None of the comparison methods are tailored for imbalance datasets. 
We would clarify that in all our comparison experiments, other SOTA models have also applied random resampling for fair comparisons in an imbalanced setting. We will further clarify the comparison setting in the main text.

R2Q3. The proposed method in this paper exhibits a remarkable improvement of nearly 10% in terms of B-ACC compared to other methods, without the augmentation module.
In our ablation studies, we used a random resampling process when removing the augmentation module, ensuring fair and consistent comparison. Thus, the backbone framework itself can handle the unbalanced dataset to some extent but is less capable than our approach, as demonstrated by the experiments.

Reproducibility (R2, R3): we claimed to release the codes in the abstract.

R3Q1. More work can be done to demonstrate how to interpret the latent features learned from the correlation graphs.
Thank you for this insightful idea. The GNN employed in the study introduces a prior atlas into the model, which may enhance the anatomical interpretation. We agree interpreting the latent feature could give more insights. We will extend the study to improve interpretability.

R3Q2. More evaluation on data augmentation and Run time performance comparison between GNN and 3D CNN:
We appreciate this suggestion for creating imbalance in a balanced dataset. However, public datasets are all imbalanced, as most glioblastomas are IDH wildtype. In future, we plan to create a larger dataset for further investigation and evaluation. Our current focus is the modelâs predictive performance. 
We appreciate runtime performance is important. In our future version, we will include a runtime analysis to compare the computation efficiency of our approach with 3D CNNs."
https://papers.miccai.org/miccai-2024/723-Paper1946.html,"We would like to thank the reviewers for relevant and constructive comments on our manuscripts. The main concerns that were raised are addressed in the following bullet points.

Motivation for integrating clinical data 
By integrating clinical data into the generative framework we can explore associations between clinical data and anatomical shape and motion. We consider this an impactful contribution since it allows for investigating how a change in a clinical variable affects the shape and motion and for generating plausible anatomical sequences with characteristics related to the given clinical information. Sequence generation without integration of clinical data would not allow for this kind of controlled generation. 
For sequence completion, we already included the ablation study on the clinical demography that is requested by reviewer #3 (See Table 1 row 2) and demonstrated that clinical information improved the estimation of the functional parameters. A further improvement is expected if more clinical variables were included in the model.

Thorough discussion of the methods and its effectiveness
We recognize that the reviewers would prefer a more in-depth analysis of i.e. the latent spaces and how they are derived. Due to the limited paper length, we have not been able to include thorough discussions on the underlying mechanisms of an auto-decoder but refer the reader to the seminal deepSDF paper [17]. We will however include a short discussion of the differences to a standard auto-encoder.  We appreciate the reviewerâs suggestions for analyzing the covariance of the latent spaces and we aim to work towards a journal paper where the properties of the latent spaces will be further explored.
Despite the simple setup, we argue that the method produces convincing results, where the low chamfer distance suggests an overall good reconstruction accuracy and the meshes vary smoothly in both space and time (See full movies in the supplementary material). The detail level of the meshes can still be improved, but the proposed method outperforms current SOTA by 18-30% across the different metrics on a large test set of 367 unique sequences, which we consider a significant step in the right direction.

Comparison to other methods and datasets
We have evaluated it against the CHeart method [22] since it is considered SOTA within conditional generation of anatomical sequences and their goals are similar to ours. Additional comparisons have not been feasible due to limitations on time, paper length and computational resources, where 48 GB GPUs are not standard at most clinical sites. While a separate appendage evaluation would provide additional evaluation insights, it has not been performed since automatically finding the appendage ostium in a robust way is not a trivial task.

Statistical shape modeling (SSM)
In the introduction, we will add a short paragraph about the great body of SSM work that has been done in this field. Obtaining point correspondence across the complex and diverse left atrial appendage shapes is however an ill-posed problem, where no single correct correspondence can be defined. Integrating clinical data into an SSM is furthermore an unsolved problem. Future extensions of the method might include utilizing the gradients of the neural distance fields to obtain point correspondence over a temporal sequence since this is required in certain applications as correctly pointed out by the reviewers.

Shared code and data
All code will be made publicly available upon acceptance. Due to GDPR, we are however not able to share images or segmentations as the unique shape of the left atrial appendage makes full anonymization impossible.

Additional comments
The typo on page 3 will be corrected and a concatenation will be added to equation 2 and 3 to correspond to the notation in Figure 1.

We hope this rebuttal clarifies the main concerns and that you will consider accepting the paper for MICCAI 2024."
https://papers.miccai.org/miccai-2024/724-Paper1006.html,"We thank the reviewers for their time and constructive reviews. We appreciate that all reviewers see merit in our approaches that model temporal variation in both short- and long-term time series of medical images. In this revision, we address their remaining concerns:

Contributions and novelty (R5, R6)
Our work identifies and addresses fundamental limitations in widely adopted video representation learning methods, that were introduced during their original extension from image-based applications (R6). We strongly believe that it is important to study, report, and address these limitations. While straightforward, our clip-level contrastive strategy is beneficial for preserving large temporal changes in spatiotemporal medical data (R5). Current approaches cannot handle such changes, especially in irregularly sampled long-term sequences, which is why we focus on the performance benefits shown in Table 1 (R5). We have updated the Introduction to reflect these contributions more precisely.

Methodological details (R4, R6, R7)
We now provide more details on methodology. We would like to clarify that the temporal Transformer shares the same architecture as the ViT described in Section 4.1 (R6), which are both randomly initialized before pretraining (R7). Given a sequence of images, we employ a spatial ViT, E_s, to extract a feature vector per image zs_i=E_s(img_i) and a temporal Transformer, E_t, to generate a global representation of the sequence zt = E_t(zs_1â¦zs_i) (R6, R7). While zt is used for the contrastive loss in Eq. 1, Et also predicts a subset of the feature tokens that are masked specifically for the frame-level prediction task. We have included these forward equations in Section 3.2 (R7) and added an overview diagram to Figure 2 (R6). Our two-encoder design efficiently operates on spatiotemporal data by reducing attention token length instead of processing all video patches at once (R4).

Inference and experimental details (R5, R7)
We have added additional details regarding the inference (R7). In sequences with more than eight images, we adapt the common practice in evaluating video models [13,28] and apply a sliding window with 50% overlap to contiguous clips before averaging these predictions. We also clarify that TVRL does not use time embeddings (TE), and have added these details to Section 4.1.
We have repeated our experiments with five random seeds during finetuning and find consistent performance as currently reported in both tables (R5). We have now added the missing augmentation details to Section 4.1, which are standard SimCLR augmentations (R5, R7).

Results discussion (R4, R5, R6)
The reviewers asked for more experimental detail and a more thorough discussion of the results. We present TE as a well-motivated option for incorporating irregular intervals, but hypothesize that the model could overfit on TE during contrastive pretraining (R6), and may benefit from temporal augmentations. Given the inherent difficulty posed by the frame-level prediction task, we found lower mask ratios to be beneficial for TVRL (R6). We also welcome the suggestion of R6 to plot these intermediate reconstruction results using Eq. 2, and have visualized the feature trajectories using their first two PCA components (R6). These findings have been added to Section 4.4.
Regarding Table 2, on video data with regular frame intervals, it is expected that TE does not provide any benefit over positional embeddings (R5, R6). Similarly, TVRL performs comparably to contrastive baselines, as cardiac videos contain limited temporal change within a single cardiac cycle (R5). We have clarified these points in results Section 4.4.
Overall, experiments on two distinct spatiotemporal datasets including eight diverse tasks demonstrate our success in modeling temporal variations for diagnosis and prognosis (R4). Our approach excels on long, irregularly acquired data (Table 1) and maintains competitive performance on short videos (Table 2)."
https://papers.miccai.org/miccai-2024/725-Paper1551.html,"We thank reviewers [R1, R3, R4] for their positive feedback. They appreciate that our approach: provides diagnostic biomarkers for LTBI, a neglected disease condition, using HSI [R1], is a âfundamentally interesting, novel method which opens up new options for medical diagnostics researchâ [R3], and is a ânovel framework for LTBI diagnosisâ [R4]. Below we address the reviewersâ major concerns.

Concerns on limited sample size [R1, R4] â We acknowledge concerns on the limited dataset size. To the best of our knowledge, the dataset in [Gu et al., 2018, DOI:10.1007/978-3-030-01201-4_29] is the only publicly available HSI dataset of skin pathology (skin cancer). However, its limited number of wavelengths (16 vs 204 in ours) and narrow wavelength range (460 â 630 nm vs 450 â 1003 nm in ours) prevent its use as a validation dataset for our framework. Estimation of chromophores such as water and fat require wavelengths beyond 630 nm. The pre-trained DL models used for deep radiomics feature extraction were frozen and not fine-tuned to our HSI data. Our SVM classification-based approach to feature selection and ranking is where a risk of overfitting may exist. We mitigated this risk as well as the risk of result uncertainty raised by R4 by utilising balanced accuracy metric (rather than standard accuracy) over five cross-validation (CV) folds such that the validation set per round is unique. Best features were ranked and selected based on average balanced validation accuracies across the five rounds. Each roundâs SVM model had the same initial condition, and no knowledge transfer occurred between rounds, reducing overfitting risk fivefold compared to a single round of CV.

Scientific Contributions [R1] â While PyRadiomics and pre-trained DL models have been used for spatial feature extraction in other medical imaging modalities, we are the first, to the best of our knowledge, to layer PyRadiomics, and pre-trained models trained on a recent large-scale multi-modality medical imaging dataset (RadImageNet), on top of traditional HSI spectral feature extraction to generate a comprehensive array of features that encapsulate both spectral (chromophore) and spatial characteristics of hypercubes. Showing the potential of multimodal domain knowledge from RadImageNetâs CT, MRI, and US images of 11 anatomical regions in generating promising HSI biomarkers is noteworthy. We pioneer the extraction and analysis of chromophore maps for TST-based LTBI diagnosis.

Comparison with existing methods for biomarker extraction from HSI [R1] â To the best of our knowledge, our framework is the first to generate HSI-based biomarkers that encapsulate both chromophore concentrations and robust quantitative spatial tissue relationship. Current methods focus mainly on chromophore map generation and qualitative analysis of the maps.

Integrating SpeChrOmics into clinical workflows [R1] â This paper focuses on assessing the predictive value and HSI biomarker-generating capabilities of SpeChrOmics. We believe this paper serves as a foundation for later works which can validate the SpeChrOmics on a larger and more diverse dataset, which will generate more evidence of efficacy and safety, in addition to the ones presented in this study, in support of downstream regulatory measures for integration into routine clinical practices.

Suitability of RadImageNet for HSI [R4] â Though RadImageNet images have 3 channels, they are different from conventional/natural RGB images. They are grayscale images captured from 2D slices of CT, MRI, and US, which are repeated into 3 channels per image. We believe that spatial tissue characteristics captured in RadImageNet are significantly similar to the spatial characteristics of chromophore maps. Thus, we believe RadImageNet is a suitable pretraining dataset for spatial feature extraction in HSI."
https://papers.miccai.org/miccai-2024/726-Paper2957.html,"Thank you for your insightful feedback and constructive comments on our paper. Here are our clarifications that address the major concerns raised.

Q1-Preprocessing for Image Alignment (R3, R4)
We appreciate your concern regarding the potential misalignment between the main and reference images. In our current implementation, we assume that the images are approximately aligned, as they are derived from the same patient and typically captured under standardized conditions. However, we acknowledge that minor misalignments can occur. To address this, we will include an image registration step in our preprocessing pipeline to ensure better alignment of the images before feeding them into the Residual Encoder. This enhancement will be described in the future version.

Q2-Text Encoder (R4)
We appreciate your request for more details regarding the textual encoder. In our implementation, we utilize the âget_input_embeddings()â function of GPT-2.

Q3-Decoder (R4)
We will replace GPT-2 with more powerful models in the future.

Q4-Gaps between multi-modal embedding (R4)
This projection module is designed to address these gaps.

Q5-Other comments(R1, R3, R4)
We appreciate your additional comments and will carefully consider them to make the corresponding improvements."
https://papers.miccai.org/miccai-2024/727-Paper0893.html,"Thank all reviewers for the constructive comments.

1.Novelty(R4): The proposed method is different from existing methods [4,8] in the following aspects: 
1) Methods [4,8] directly applied conventional equivariant CNN (ECNN) to improve the modelâs robustness to drifts in scale of the whole objects caused by the variability of patient anatomies, but they lack specialized designs for reconstructing image fine details. To tackle this, we designed a temporal-equivariant convolution module in ECNN framework to exploit rotation symmetry in images, significantly improving the detailed anatomical structure in reconstructed images (see Fig.3).
2) The myocardial edge region (crucial for clinical diagnosis of heart diseases) is evidently better reconstructed by our method, while not by [4,8].
3) Our method is specially designed for dynamic imaging while [4,8] are not suitable for such scenarios.

Method(R3): As ECNN has been well explained in previous works [23,25], we only described the novelty parts in detail due to page limit. We will release our code for reproductivity. 
Equivariant CNN is designed based on the observation that when the relative orientation of an image and a convolution kernel changes, the extracted features will be different. 
1) To ensure the feature robustness to input image rotation, the input equivariant layer creates a set of weight-sharing convolution kernels by copying and rotating a kernel. For instance, in Fig.2b, when the input is rotated 120Â°  clockwise, the features that were originally extracted by red-box convolution kernel are now extracted by green-box kernel for orientational consistency.
2) After multiple convolutional layers, the extracted features cannot be still robust to image rotation as CNNs are not inherently equivariant to rotation. Intermediate equivariant layer is designed to address this, i.e., to maintain the relative orientation with the original input and allow the kernel continuously tracking the matching features through 2D group rotations and channel-wise cyclic shift operations on the convolutional kernels. As shown in Fig.2b, when the input is rotated 120Â°, the channel position of the pink convolution kernel (originally convolved with red-box feature map) needs to be shifted to match the corresponding relative orientation and convolve with green-box feature map.
3) Output equivariant layer is to reduce the additional channels extended by rotation group. The combination of the input, intermediate, and output layers achieves the utilization of the spatial symmetry prior depicted in Fig.1.
4) A temporal-equivariant layer is specially designed for dynamic imaging. Specifically, the 1D convolution kernel is extended using the similar group as that in other equivariant convolution layers, but without rotating the 2D plane. Then, a cyclic shift is performed on the channels of the 1D convolution to ensure the global equivariance of the network. This extended set of 1D convolution kernels jointly extract the temporal symmetry priors (shown in Fig.1) in dynamic images.

Ablation Study(R4): DL-ESPIRiT(R2plus1D) is exactly our backbone and represents an ablation study, which shows that the essential embedding of rotation symmetry prior helps SRE-CNN outperform R2plus1D evidently. Further ablation studies are omitted due to page limit.

Dataset, more experiments (R1, R3): No slices from the same patient were mixed in training/validation/test sets. Data augmentation using rigid transformation-shearing was applied to produce 800-30-118 (training/validation/test) samples. The network is trained with coil-combined images. Complex-valued data are converted into two real-valued channels. We tested our model on OCMR and MICCAI Challenge datasets which has raw k-space data, showing SRE-CNNâs excellent generalization ability. Due to page limit, we cannot provide more visualization of prospective study and left ventricular function assessment, but weâll deeply explore them in our journal work"
https://papers.miccai.org/miccai-2024/728-Paper0669.html,"We thank the reviewers for their valuable comments. 
R1 Q1: We calculated the clinically relevant HD95 metric. S-SAM shows superior performance with lower HD95 values, e.g., 44.5 for Cholec compared to AdaptiveSAMâs 77.2, and 49.4 for GLAS compared to AdaptiveSAMâs 106.2.

R1 Q2, R3 Q6: While S-SAM performs better on average, some methods outperform it for specific classes, suggesting it might be affected by label distribution disparities. It may benefit from methods like re-weighing the loss due to different classes. To motivate this, we presented classwise scores in the tables. We will discuss this potential future improvement in the revised version.

R3 Q1: Directly tuning SVs is akin to changing the original weights. In contrast, we freeze the original SVs and learn the affine params that can learn different transforms for different tasks without harming original model performance. Unlike SVF, this independence of the affine params from the original params opens the possibility of learning them for simpler tasks and then combining them to build a model that can work well across all the tasks, like [1] does with LoRA. Also, unlike SVF, S-SAM doesnât require a support image and mask to guide the segmentation process.
[1] Mixture of LoRA Experts

R3 Q2 Q3 Q4: S-SAM performs full-rank fine-tuning unlike LoRA (low-rank) and AdaptiveSAM (single-rank). At the same time, there are fewer tunable parameters in S-SAM. Hence, it is more robust to overfitting, leading to better results in datasets like GLAS, where there are less images and a single label of interest. We show that using TAL with S-SAM can improve performance over AdaptiveSAM. We ran LoRA with r = {2,4,8,16} and compared the performance on the validation set. r = 4 gave the best DSC with less tunable parameters. r = 8 gave minimal benefit over r = 4, but had much more tunable parameters. Hence, we chose r = 4 for our experiments.

R3 Q5: The SAM decoder identifies âobjectsâ using features from the image and prompt encoders. Keeping the decoder frozen assumes that if the encoders can be transformed to get the expected embeddings for the decoder, for a given task, then the decoder need not be trained, saving memory and time. Comparisons with AdaptiveSAM, which trains the decoder, support this. However, for large datasets with many labels of interest, decoder-training might be required.

R4 Q1 Q2: Full finetuning involves 1 fg point if the ground truth isnât empty and 1 bg point otherwise. These points are sampled from the ground truth masks and hence can be considered as manual point prompts. This is similar to existing SAM-adaptation papers. Using 1 point prompt also ensures a somewhat fair comparison with traditional methods and S-SAM which do not provide such explicit location information about the mask. We also compared our method with MedSAM, which uses box prompts as input. However, using box prompts might not lead to a fair comparison. In Fig 3, the green dot represents the used point prompt. We will update the caption to reflect that the last column shows SAM with a manual point prompt, not in auto-mode.

R4 Q3: Existing adapters require significant compute for training, leading to low-rank methods like LoRA. Our goal was to achieve full-rank adaptation, but with fewer parameters. Hence, we decided to explore tuning singular values (SVs). However, that would have removed the independence between the original weights and the adapted modifications. Hence, we developed tunable affine transforms for changing the SVs. In addition, existing literature [2] has shown the significance of norm layers in learning domain information. Hence, to further aid the adaptation process, we tuned the norm layers that can help learn the domain shift. With these two added changes, we show S-SAMâs effectiveness without an adapter-like structure.
[2] On-the-Fly Test-time Adaptation for Medical Image Segmentation"
https://papers.miccai.org/miccai-2024/729-Paper1426.html,"We thank all reviewers for their insightful comments and provide our responses below.

Reproducibility/technical details: all code/data will be made publicly available and have already requested internal clearance for releasing the codes. We included sample rendering code within the manuscript and will add additional details to facilitate full reproducibility in the final code release. Due to manuscript length limitations, we provided only a brief overview of the rendering in Sec. 3.2 and will add additional details within the code release.

Comparison to state of the art: existing real skin datasets are prone to inaccurate annotations [1], such as labelling errors, near duplicates, and incorrect labels, as well as confounders [2], such as rulers, dark borders, dense hairs and air pockets, both of which may negatively affect AI-based diagnostic tools and any data-driven methods (e.g., generative diffusion models [3,4]) used to generate synthetic training augmentation data. Similar to [2], we take into account known confounders to systematically simulate synthetic samples and analyze performance across subgroups. In contrast to [3,4], our synthetic data generation model is not data-driven and therefore does not propagate confounder behavior. To our knowledge, our model is the first to use knowledge-based models to produce skin images without and with lesions.

Additionally, a known limitation of existing real skin datasets used as original training dataset for data-driven techniques is the low prevalence of examples with dark skin tones. A significant contribution of our work is the ability to generate images across a wide variety of skin tones, thus creating the possibility to supplement existing real skin datasets with currently unrepresented samples.  We are hopeful that such supplementation can help to remove some of the known biases that result from such significant underrepresentation within the existing real data.

Model is Simplistic/Limit to Melanocytic Lesion: we agree that our lesion growth model is to some degree simplistic, particularly compared to real anatomical and pathological skin characteristics. However, our model is also more sophisticated than other models that have produced lesions based on simple geometrical shapes (see Vasudev, Varun, et al. âSimulation pipeline for virtual clinical trials of dermatology imagesâ, Medical Imaging 2019: Physics of Medical Imaging. SPIE, 2019). As correctly pointed out, one area for model improvement is to consider interaction between the lesion and non-lesion areas. In our current model, however, the lesion growth can be affected by material changes that are lesion dependent  and that can vary along the timeline of lesion growth. Moreover,  the level of realism needed depends on the potential utility of the model within the target application (e.g., increasing training dataset size, testing, calibration), and not only on visual comparison to real lesion examples."
https://papers.miccai.org/miccai-2024/730-Paper1007.html,"Thanks for all the constructive comments.

Reviewer #1 Comment 1 âIâm not sure if the third contribution can count as a valid point. The mentioned âTrainable Vision Encoderâ looks not special?

Reply: We want to clarify that  âTrainable Vision Encoderâ is worth a valid contribution point for the following reasons:

Reviewer #1 Comment 2 âMany of the statements in this article are vague (possibly limited by length), such as Concatenate Latent Fusion and Trainable Vision Encoder.

Reply: Concatenate Latent Fusion is to concatenate the noised latent representation with the image feature representation at channel dimension. Trainable Vision Encoder is a feature extractor to learn semantic features from the conditional original image for segmentation. Many statements are indeed limited by length, we open-sourced the code and hoped it can provide technical details.

Reviewer #1 Comment 3 âSome adjustments may need to be made to the structure of the article. The contribution points of the article need to be reorganized.

Reply: For clarification, the 2nd contribution point refers to section 2.1 and 2.2; the 3rd contribution point refers to section 2.3. Thanks for your valuable comment, we will follow this tip in paper writing in future work.

Reviewer #3 Comment 1 âAuthors could provide more motivations of their methods.

Reply: We would like to further explain the motivations of our method:

Reviewer #4 Comment 1 âThe authors claimed to âintroduce the first latent diffusion segmentation modelâ however, there has been already some research on using LDMs for the medical segmentation task [1] (LSegDiff) similar to the proposed architecture.

Reply: Possibly because that work [1] was contemporized with our work, and also due to our negligence, we failed to find that [1] had used LDMs for medical segmentation when we were writing the paper. In addition, the main differences between SDSeg and LSegDiff are:

Reviewer #4 Comment 2 âLack of qualitative comparison.

Reply: Due to page limit, we only showcased the quantitative comparison, which has demonstrated the superiority of our method (Table 2 and 3) and the most important qualitative results such as visualization of latent representation and reverse process (in the supplemental materials) to better explain our work. If allowed, we will add it."
https://papers.miccai.org/miccai-2024/731-Paper0870.html,"We thank reviewers for their feedback.
1.Background and Significance of Research (R1): When a pregnant woman undergoes an ultrasound scan, the sonographer checks for fetal anomalies by scanning through each anatomy of the fetus to find standard frames that contain all anatomical landmarks in correct orientation, position. This process can take up to an hour. To streamline this, we introduce STAN-LOC, where the sonographer can input an image of the desired view, and the model returns the corresponding video clip for analysis. This allows for faster scanning, enabling sonographers to focus on anomaly detection and consult more patients.
Furthermore, STAN-LOC can segment the input video into view-specific clips, facilitating the use of view-specific anomaly detection models. For instance, in our heart sweep project, STAN-LOC can divide input videos into various heart view clips which are fed into a view-specific anomaly detection model to assess fetal heart health.
2.Classifier Details (R1, R3): The classifier is a ConvNext-small CNN that was pretrained (and subsequently frozen) to classify ultrasound views for each dataset. The MLP is part of classifier and follows the classifierâs feature extractor. It is illustrated separately to emphasize extracted features used for distance-based Query Selection.
3.âxâ in âTs(x)â (R4): x here is the binary ground-truth for the video, depicting whether a frame belongs to the ground-truth clip (1) or not (0).
4.Intuition behind Query-Guided Spatial Transformer (R4):  Existing methods concatenate the query (text query) directly to video features and perform self-attention, diminishing the queryâs influence as it is only a single token in the sequence. To counter that, we introduce Query-Guided Spatial Transformer, where cross-attention is performed ensuring each frame is enriched by the visual query information.
5.Averaged image during inference (R4):  In both the training and inference phase, a real image (visual query) is fed to the model. During training that visual query is randomly sampled from VQ database while inference is selected using the VQ selection module.
6.Results in Table 1 (R4): The results presented in Table 1 pertain to the In-Distribution visual query database.
7.The role of using ID and OOD VQ database (R4): Our model is aimed to solve a real-world problem and assist sonographers in ultrasound scanning. In clinical settings, it is highly likely the sonographer might enter a visual query which is captured using a different protocol leading to domain gap.
8.Novelty/application of cross-attention, self-attention and contrastive loss (R4): We respectfully disagree. These operations and learning schemes have been widely utilised in published literature. The contribution of our work (and others using these concepts) lies in their unique design and modification suited to target application. We exploit (a) cross-attention to effectively fuse information between video and visual query to enrich video features and (b) self-attention to further model temporal relationship in resultant visual query aware features. Additionally, our MVAC loss introduces the concept of dual anchor where along with the positive visual query, a negative visual query is introduced during training to utilize the negative query relationship and push the positive/negative samples further away in feature space.
9.Negative View Aware Contrastive Loss (R4): In this loss, a negative visual query is an image from a different class than the visual query. Positive frames are those outside the ground-truth clip, while negative frames are part of the ground-truth clip.
a.Loss aims to close distance between features of frames whose class is different from positive query: Yes.
b.Effect on Performance when class is different: No difference observed.
10.Figure 2 (R1, R3): We will improve Figure 2 according to reviewerâs comments.
11.Writing (R1, R3, R4): We will incorporate the suggested changes to enhance paper clarity."
https://papers.miccai.org/miccai-2024/732-Paper3064.html,"We sincerely thank all reviewers for their constructive reviews. We have carefully studied the comments and will revise the manuscript as suggested.

(R1, R3, R5) Common concerns will be revised in final camera-ready files, including typos and grammar errors, inaccurate expressions.

R1
Q1: The contributions of this paper are very unclearâ¦, so we donât know how this paper compares to the state-of-the-artâ¦
A1: One of the main contributions of this paper is addressing classifier bias problem cased by data heterogeneity from a new perspective, i.e., borrowing linguistic knowledge from pre-trained language models (PLMs) to pre-construct a high-quality classifier. In experiments, we have compared SOTA methods that also focus on classifier bias and data heterogeneity problems, such as FedPROX[10], FedROD[3], FedETF[12], and so on. We will highlight contributions in the final version.

Q2: â¦why not just use the image feature extractor of BiomedCLIP and train local classifiers on that?
A2: The image feature extractor of BiomedCLIP has a large model size. Direct training will incur heavy communication costs and face classifier bias and data heterogeneity problems.

Q3: I donât think the experiments are representative of an actual federated learning scenario, since here âclientsâ are simulated by subdividing the same datasetâ¦
A3: Data heterogeneity can be divided into distribution skew and label skew. This paper mainly focuses on label skew. The experiment settings follow the previous methods, FedPROX[10], FedROD[3], and FedETF[12].

Q4: â¦but here a lot of information is missing such as the task, the datasets, the architecture of the models, etc. â¦ never explicitly says itâs tackling a classification taskâ¦the last table mentions a âbaselineâ, but never says which one it isâ¦
A4: Sorry for the confusion. The pilot experiment is conducted on an OCT-C8 dataset [18]. Experiment details are shown in âImplementation Detailsâ. This paper mainly focuses on medical classification tasks. âbaselineâ defaults to FedAvg in this paper. We will revise these problems in the final version.

R3
Q1: There is a lack of sufficient description to illustrate the distinctions in framework architecture when compared to FedETF [12]. 
A1: FedETF [12] utilizes orthogonal initialization to construct the classifier, which lacks semantic interpretability. The classifiers of different classes are not necessarily strictly orthogonal. In this paper, the classifier constructed by pre-trained language models contains rich semantics and distance relationships and is domain-agnostic.

R5
Q1: The manuscript would benefit from a discussion on how this approach compares with prototype learning in federated settings, as explored in studies like FedProto [1]â¦
A1: FedProto[1] collects and aggregates prototypes from clients to deal with data heterogeneity. However, the prototypes depend on local feature extractors and thus may be heterogeneous, affecting the learning of local feature extractors in turn. Our method directly uses pre-trained language models to construct a high-quality classifier."
https://papers.miccai.org/miccai-2024/733-Paper0240.html,"We would like to thank the reviewers for their constructive comments. All issues will be addressed in the revised paper. About reproducibility, weâll share the code once the paper gets accepted.

R1

Ablation study: In Tables 1&2, we compare the disparity generated by RAFT-stereo with the refined RAFT disparity generated by our StereoDiffusion. This is our ablation study which validates the effectiveness of the diffusion model in refining RAFTâs disparity estimation. We have already tested the model by removing the optical flow component and we found that the accuracy and the temporal consistency of the depth estimation deteriorate. Hence, we did not include these results in our paper.

Monocular/stereo Image: Our method predicts disparity from stereo data. Due to the page limit, only the left RGB images and the corresponding disparity maps are shown which is commonly done in the literature.

Statistical tests: The paired t-test on the EPE and D3 metrics with alpha equal to 0.05, have verified that there is significant difference between SteroDiffusion and each of the baselines.

Writing and results discussion: They will be improved and strengthened in the revised paper.

Limitations:

The limitation is the inference time (~5fps) which could be improved in the future using faster denoising steps.

R3

Conditional GANs (cGANs): The Diffusion model has multiple significant advantages over cGANs, including training stability, higher generated image quality, less often training collapse, higher robustness and generalizability. Our previous experiments showed that cGAN-based models for depth estimation have poor performance on medical dataset, so we didnât include them in our comparison.

Ablation Study: Please refer to R1Q1.

Regression-Based Methods: We have already tested SOTA regression-based methods like Marigold [6]. This method can not generate accurate depth maps on medical datasets like SCARED and STIR. Considering the significant depth error, we didnât include it in our comparison. The low performance is due to the huge domain gap between natural and medical scenes, and the low generalisability of the existing regression-based methods. This is also the motivation and advantage of our proposed integration of the diffusion model into stereo depth estimation.

Temporal Consistency Metrics:

Currently, there are no medical datasets with both GT optical flow and depth information which could be used to quantitatively evaluate temporal consistency. SCARED only provides GT depth for sparse keyframes and STIR provides only sparse feature correspondences.

R4

Novelty: Our contribution are 1. We integrate for the first time a latent diffusion model into a novel stereo depth estimation pipeline for disparity refinement. 2. Our method advances existing depth estimation methods based on diffusion models, as it does not treat the diffusion model as a regression model (using RGB to predict depth as in Marigold [6]) but uses prior knowledge to refine disparity by using RGB and disparity together to predict disparity. 3. A tailored data normalization and denormalization process has been designed to make the disparity prediction invariant to different camera set-ups. Our validation verifies that StereoDiffusion has superior performance on medical data although it is trained on natural scenes only.

R5

Novelty: Please refer to R3Q3 & R4, which explains the difference between our methods with work [6] adapted from [10].

Inference: The use of fewer denoising steps (10) and of an efficient denoising scheduler enables StereoDiffusion to run at faster inference time (5fps) compared to other diffusion models (such as [14] with 0.1fps) while generating high quality images.

Disparity/depth conversion: Given the intrinsic and extrinsic camera parameters, the standard function which relates disparity and depth is:

depth = (baseline*focal length) / disparity)

Ablation Study: Please refer to R1Q1"
https://papers.miccai.org/miccai-2024/734-Paper1091.html,"We thank the reviewers for their efforts in reviewing our paper and their constructive comments. 
@R3: Comparison with SOTA. We have compared with the deep learning-based methods, including VM [4], DAE [14], U-Net [6], and GII [23], as shown in Table 1 and Fig. 4. U-Net_SD [6] is trained using paired defected CBCTs and cleft defect masks of D_s generated by the proposed SAS algorithm. Our approach has shown performance gains of 0.02 (DSC) and 0.01 mm (AHD) over the U-Net_SD. We have compared with the GAN-based method of the GII [24]. The GII is suitable for filling the masked region, but it does not address the morphological variations of irregular cleft defects. Our approach outperforms GII by 0.05 and 0.10 regarding the DSC on D_c and D_s (Table 1).
@R1, R4: Limitations and future research. The proposed SAS algorithm relied on iterative skeleton tracing and dilation, where the progressively generated fat skeleton was used to guide the defected CBCT generation. The proposed SAS algorithm is specific to the 3D craniofacial fusion process involving the primary or secondary palate. The extension of the SAS algorithm to adapt to diversified abnormal developments of other diseases or organs deserves further study. Another limitation is that the proposed restoration model produces missing bony tissues conditioned on the input defected CBCTs. Considering patient-specific bone resorption after the grafting procedure, we would further investigate time-varying restoration learning in a variety of clinical applications.
@R4: Improvements in clinical and simulated data. When given simulated data for supervised learning, the model is feasible to improve restoration performance with a larger margin on D_s than D_c over the model learned from limited clinical defected CBCTs via adversarial restoration learning. We think the reason is that the testing simulated defected data in D_s bear the same distribution as the training data. Ablation study indicated that simulated data enhances the generalization capacity for cleft defect estimation. The SD also improves performances of existing DAE and U-Net (Table 1).
@R4: Motivation of Eq. 1. Confronted with a small cohort of defected CBCT scans in the clinical study, we present the SAS algorithm to simulate the inverse facial fusion process. The SAS algorithm relies on iterative skeleton tracing and dilation for diversified defected CBCT generation under the Tessier system of orofacial clefting. Without loss of generality, we start skeleton-tracing from the bounding plane of the cleft defect. Eq. 1 defines the skeleton-growing vector q_i in the i-th step. We use a randomly perturbed vector a and the vector towards s_u to update the growing vector for diversified skeletons and simulated cleft defects (Sec. 2.1). 
@R1, R3, R4: Code availability. We would make the code available after acceptance.
We clarify the concerns on implemental details as follows:
@R1: Computational time and hardware. We evaluate the proposed model on a PC with an NVIDIA GeForce RTX 3090 GPU. The training and online inference take approximately 140 hours and 0.45 seconds, respectively. As to the metric formulas, DSC (X,Y)=(2|Xâ©Y|)/(|X|+|Y|). |X| and |Y| refer to the cardinality of sets X and Y, respectively. AHD(X,Y)=1/2(1/|X|\sum_{a\in X}\min_{b\in Y} |a-b|+1/|Y|\sum_{b\in Y}\min_{a\in X}|b-a|). MSD(X,Y)=1/2(1/|X|\sum_{a\in X}\min_{b\in Y}|a-b|+1/|Y|\sum_{b\in Y}\min_{a\in X} |b-a|). We would provide a reference to the metrics. 
@R3: \mu_1 and \mu_2 are set to 0.15 and 0.05 (Sec. 3). The skeleton tracing terminates when it reaches a predefined height or the upper bounding plane (Sec. 2.1). As the reviewer has pointed out, the simulated skeleton is not guaranteed to end up in s_u as Eq. 1. 
@R4: We use V_r to represent the restored volume in both supervised learning from simulated data and adversarial restoration learning from clinical data. The clinical data have bounding box annotations of cleft defect (Sec. 2.2)."
https://papers.miccai.org/miccai-2024/735-Paper2948.html,"We sincerely thank the reviewers for and providing detailed and constructive comments.

Code (R1&R3&R4)
We promise to make our code publicly available.

Reviewer #1
Q1 How to perturb feature representations?
We randomly zero out elements in feature maps output by the encoder \mathcal{E} with a probability of 0.3, obtaining perturbed feature representations, which are then fed into the decoder \mathcal{D}_p.

Q2 EMA
In previous experiments, we found that EMA is unsuitable for our architecture. Our model includes a shared encoder \mathcal{E} and two decoders \mathcal{D}_l and \mathcal{D}_p. During training, \mathcal{D}_l and \mathcal{E} are updated with labels, while \mathcal{D}_p and \mathcal{E} are updated with pseudo-labels and DSR. With EMA, \mathcal{D}_l does not actively update, preventing \mathcal{E}.

Reviewer #3
Q1 How can the GAN model identify the location of the mask?
GANâs discriminator is designed to assess the plausibility of shape masks without considering their locations. In our semi-supervised segmentation network, it serves as a shape constraint, while the location constraint is handled by two cross entropy losses (see Fig. 2).

Q2 Can you use the diffusion model to do the same thing?
We cannot do that. The GANâs discriminator scores masks, which we use to guide the segmentation model. Diffusion models, on the other hand, lack this component.

Reviewer #4
Q1 The paper does not sufficiently explain each component of the formulation and the model.
In â2 METHODâ, these variables have been defined. For example, in Eq. (1), we mention x~P_r, \sim{x}~P_g, and \hat{x}~P_x. We state that P_g and P_r represent distributions of generated and true masks, respectively, while P_x is an additional term from [14]. After Eq. (4), we clarify that m_i is a Dropout mask.

Q2 The clear causal link between the general mechanism of GAN and the specific rationale for introducing DSR.
We will polish the expression as follows:
ââ¦ After training, the discriminator gains the ability to assess the plausibility of shape masks. Therefore, we can define DSR as â¦â

Q3 Radar charts.
In the final version, we will replace the two radar charts in Fig. 4 with a table to better present results of ablation studies.

Q4 Algorithmic novelty.
1.Our encoder-twin-decoder network is not widely used in pseudo-labeling-based semi-supervised image segmentation (CPS, CVPRâ21; U2PL, CVPRâ22; R. Yi, et al., TIP, 2022; CU2L, MICCAIâ23; H. Basak, et al., CVPRâ23).
2.We aim to counter the trend of complex architectures in semi-supervised image segmentation and explore a simple yet effective model for ultrasound images.
3.In our experiments, complex models performed poorly. Compared to SOTA methods, our approach achieves better results on two ultrasound datasets.
4.Our DSR is designed to enhance the visual quality of segmentation results. It corrects some errors that do not conform to anatomical principles, as shown in Fig. 1. Although these improvements are not very evident in Dice and IoU (because of the limited number of pixels involved), they significantly enhance the visual quality of segmentations.

Q5 Can the discriminator effectively perform on the \sim{x} generated by the semi-supervised image segmentation?
Although the discriminator is pre-trained, it is trained on the same data distribution as the segmentation network, and thus can effectively work on \sim{x}. Also, the visualization results in Fig. 5 confirm this.

Q6 Can the framework integrate the segmentation model as the generator instead of training the GAN separately?
In fact, we tried to do this, but experimental results were not satisfactory. We observed that this end-to-end training process is very unstable. Therefore, in order to achieve better results, we opt for a pre-training approach to obtain DSR. Of course, this is an interesting problem, and we plan to further explore optimization strategies for end-to-end learning of our semi-supervised segmentation framework in future work."
https://papers.miccai.org/miccai-2024/736-Paper0456.html,N/A
https://papers.miccai.org/miccai-2024/737-Paper1768.html,"We appreciate all the reviewers for valuable comments, and the concerns are addressed in the following.

R1-Q1 and R5: Clarification of the ablation study in Table 2. The ablation study effectively demonstrates the positive effects of similar historical cases (SHC) and indications on model performance. Specifically, (c) and (d) in Table 2 denote SEI-1 without indications and SEI-1 without SHC, respectively. The key difference between (c) and (e) lies in whether they use indications in the report generation module. These clarifications will be added in our final version. We observe that (c) and (d) exhibit improvements across all metrics when compared to (a) and (b), implying that using either SHC or indications independently can enhance performance. While the CX14 metric in (e) shows a decrease compared to (c), the sum of all metrics in (e) exhibits an increase of 4.9%. This indicates that the combined use of SHC and indications, namely SEI-1, leads to further enhancements in performance. Also, the experiments in Table 1 prove the effectiveness of SEI-1.

R4 and R5: Code availability. We use the widely recognized public dataset MIMIC-CXR. Additionally, we have made our code and checkpoints accessible anonymously at anonymous.4open.science/r/SEI-14B8. These ensure that our experiments are reproducible.

R4-Q1: Our modelâs efficiency. AI can quickly generate high-quality draft reports, thereby allowing doctors to focus on analyzing more complex cases and enhancing overall efficiency. According to Reference [1], AI can reduce the diagnostic time for chest X-rays by 10% compared to radiologists. Our model, on average, generates a report in just 7 seconds, providing doctors with timely and high-quality drafts for review, as shown in Fig. 2.

R5: Implementation details of encoders. As our contributions focus on cross-modal alignment and integrating SHC and indications, we use general encoders to demonstrate that performance improvements are due to our design, not domain-specific pre-trained models. This approach allows us to clearly attribute the improvements to our innovative design. Using domain-specific pre-trained models might further improve the generation effect, we will try it in the future.

R5: SEI-n performance. When n>1, the performance of SEI-n declines compared to SEI-1. Our analysis suggests that this reduction comes from the instability in the feature space, which is likely caused by missing inputs, as well as interference between SHC and indications. Interestingly, in the absence of indications, we observe optimal performance at n=5.

R5: Our modelâs limitations. Our model does not incorporate the patientâs temporal and multi-view information. However, it can be fine-tuned to accommodate other 2D medical images with about 1000 cases. We will emphasize these limitations in the final version.

R1-Q2: Adding additional NLG metrics. Due to space constraints, we were unable to include all NLG results. Additionally, the NLG metrics provided in our draft are adequate for assessing the lexical similarity between reports. Itâs important to note that this task places greater emphasis on the CE metrics.

R5: The ratio of missing inputs in Fig. 1. In SEI-1, 42.2% of the testing set lacks indications. Compared to samples without indications, the sum of all metrics for samples with indications increases by 12.9%.

R5: Non-matching color in Fig. 2. The non-matching color in the reference report indicates missing information. Additional examples will be included if supplementary materials are available.

R4-Q2 and R5: Other issues. A detailed description of Eq. (1), the technical components, and paragraph 3 in Section 1 will be provided in our final version. Additionally, we will include explanations of the output rules in Fig. 1 and enhance the clarity of Figs. 1 and 2.

[1] Ahn JS, et al. Association of Artificial IntelligenceâAided Chest Radiograph Interpretation With Reader Performance and Efficiency. JAMA Network Open. 2022."
https://papers.miccai.org/miccai-2024/738-Paper1520.html,"Dear Reviewers, Area Chairs, and Program Chairs:

Thank you for your insightful comments on our paper âStructure-preserving Image Translation for Depth Estimation in Colonoscopyâ. We will take your suggestions into account moving forward. In this rebuttal, we would like to address the following concerns raised by the reviewers:

Concern 1: Lack of open source data and/or code
We plan to publicly release our code and proposed datasets after the paper is accepted, and will update the final version of the paper to include these links everywhere that mentions release upon paper acceptance. The synthetic dataset used (SimCol3D) is already publicly available.

Concern 2: Lack of novelty (Reviewer #4)
In the image translation component, we modify CycleGAN to use an additional mutual information-based loss to enforce depth consistency between the synthetic and generated images. While CycleGAN is an existing deep learning algorithm and mutual information losses have been proposed previously, we claim that the use of a mutual information loss for preserving depth consistency, especially without requiring feature extraction, is novel.
For the downstream depth estimation task, we use the Monodepth2 architecture trained fully supervised for depth estimation. We note that while Monodepth2 is a common architecture for monocular depth estimation, this task is meant to demonstrate the effectiveness of image translation in allowing downstream depth estimation models to generalize well to challenging clinical frames. Thus, we are interested in the effect of changing the training data (and in particular, the effect of using our translated images) on the depth estimation result rather than performance improvements stemming from changes in the deep learning algorithm. Therefore, the use of an existing deep learning algorithm for this task is purposeful and not meant to demonstrate novelty in the depth estimation algorithm.

Concern 3: Data selection method for oblique and en face datasets
We picked continuous sequences of the same (oblique or en face) viewpoint. Note, however, that the distinction between an axial and oblique view, and that between an oblique and en face view, is subjective so the cutoff between viewpoints is similarly subjective.

Concern 4: Ambiguity in Table 3 (Reviewer #5)
Table 3 describes the depth estimation results measured on C3VD after median rescaling. All experiments labelled Baseline or Ours_{â¦} use the Monodepth2 architecture and are trained fully supervised, varying only on the training images. In particular, we use translated versions of the SimCol3D dataset, where the translation is performed using our various ablations and modifications of CycleGAN described in Table 2. The category label in the first column denotes whether the model had used C3VD data in training (where multi-shot models are trained in part or in whole on C3VD and zero-shot models are not trained on it).
With regards to the unexpectedly high performance of Ours_{CG} in Table 3, we attribute this (and the overall similar performance across all models) to the domain gap between C3VD and clinical images. In particular, we find that high performance on this dataset does not require extensive training regimes (note the similar performance of NormDepth which is trained entirely self-supervised), so we rely upon the qualitative evaluation on clinical datasets as a more accurate indicator of generalization performance.

Concern 5: Ambiguity in training procedure for depth estimation (Reviewer #5)
All our depth estimation results using our translation results are trained fully supervised and use the Monodepth2 architecture. However, the modular design of our framework allows for easy substitution of other architectures or training regimes (e.g. semi-supervised training) but we do not include those results in this paper.

Thank you for your time and consideration!"
https://papers.miccai.org/miccai-2024/739-Paper0603.html,"We thank the reviewers for their constructive feedback. We are pleased that they found the work novel [R5], a valuable and interesting research direction [R5], clinically relevant [R3, R4], and well-organized and easy to follow [R4]. Below, we address the main comments.

[R3]Why MR motivation when CT is used?The envisioned MR-guided radiation therapy workflow registers the planning CT volume to the image acquired in the MR-Linac. Dose prediction is performed using the transformed CT image. MR image intensities do not correspond to physical units and lack the necessary information to simulate dose deposition. Will be clarified.

[R3]Clarity and distinction to prior workWe acknowledge a lack of clarity in our submission. If accepted, we will rearrange the introduction for improved readability, defining risk early on. We will distinguish more clearly between the background in Sec. 2.3 and our novel subgroup RCPS algorithm in Sec. 2.4.

[R3]How were patch predictions combined?Inference was performed with overlapping 3D patches, and outputs were aggregated into a full 3D volume. This will be clarified.

[R3, R5]Dose estimation performanceWe validated our system with the gamma pass rate but did not report the figures to our focus on uncertainty quantification. The median gamma pass rate was 98.9% for the 3%/3mm criterion. This will be included in the final version. The dataset currently lacks segmentations, so dose volume histograms could not be computed.

[R3, R5]Inference and training times, and calibration set requirementsDL-based dose prediction reduces prediction times from hours or days (MC simulations) to ~30 seconds. This speedup was addressed by DeepDose [12]. Building on [12], our contribution lies in added uncertainty quantification via RCPS and SG-RCPS. Inference times will be clarified. Training took around 10 days on an Nvidia 2080ti GPU. Calibration took around 15 hours for each entity. Determining the optimal number of calibration samples is difficult. However, each voxel is a calibration sample, providing a large effective calibration set size.

[R4]Actual Gy values for intervals in Fig. 3 and utility of intervalsWe will report actual Gy ranges and add a legend with units. We can be confident in the SG-RCPS foreground intervals being as large as necessary (but no larger) by interpreting Fig. 2 and Tab. 1. The desired risk levels are met in close to 90% (=1-delta) of the segments indicating that the interval size produces our desired risk characteristics. Larger BG intervals are necessary to satisfy BG and FG risks simultaneously. Discussion of results will be improved.

[R4, R5]Code availabilityThe code for SG-RCPS will be made publicly available upon manuscript acceptance.

[R4]Report the lower and upper bounds of the uncertainty intervalsUnfortunately, reporting these bounds as summary statistics is not informative as comparisons between patients are not meaningful.

[R4]Statistical test between the RCPS and SG-RCPSDifferences are statistically significant. Will be emphasized in text.

[R4]Unexpected behavior of different subgroupsWe conclude from the behavior that the requirement for exchangeability between calibration and test set is better satisfied for FG voxels than BG voxels. SG-RCPS, which is dominated by foreground voxels, produces better risk-controlled intervals, while RCPS is more affected by violations in exchangeability.

[R4]Are the uncertainty intervals symmetrical?The intervals are not symmetrical as we estimate separate upper and lower bounds.

[R5]Questions about data splitsWe performed a random train/test/val/calibration split at the patient level (see Table 1 in the supplementary materials). All segments from each set were used for training, evaluation, validation, and calibration. The description of the calibration split will be clarified.

[R3, R4, R5]Minor commentsAll minor comments will be incorporated."
https://papers.miccai.org/miccai-2024/740-Paper0192.html,"We sincerely appreciate the reviewers for their constructive comments, which have helped us refine our work further. Based on comments, we response to them here and will revise some parts of the manuscript.(R1) Advantages over other data enhancement methods that does not rely on physiological signals: As shown in Table 2 (first and third rows), the classification performance significantly degraded when the resting state EEG signals were not used. This indicates the subject-specific features are crucial for classifying EEG signals from unseen subjects. However, Existing EEG data enhancement methods [12, 22] typically augment the training set using random noise, which lacks specific subject information. Thus, the methods perform poorly in classifying EEG signals from new unseen subjects due to the absence of subject-specific data. In contrast, ResTL preserves subject characteristics from RS EEG signals, enabling the model to individual subjects more effectively.
(R1) Stability and consistency of the characteristics in resting and task states: Figure 2 shows the distribution of the task and subject features from the same session of the same subject via tSNE visualization. This visualization demonstrates the task- and subject-dependent features remain stable and consistent, ensuring reliable classification. However, EEG signals from the same subject can exhibit slight variations over time i.e., inter-session variability [a].â
[a] Liu, W., Guo, C., & Gao, C. (2024). A cross-session motor imagery classification method based on Riemannian geometry and deep domain adaptation. Expert Systems with Applications, 237, 121612.
(R3) Some details of the experiment: We conducted a 4-class classification on the BCI IV-2a dataset and a 2-class classification on BCI IV-2b and OpenBMI datasets. Upon careful review, we identified the mistake of MIN2Net on the BCI IV-2a dataset. We reproduced the results and updated Table 1 to reflect a performance of 53.58 +- 6.96%.
(R3) Recent calibration methods: To the best of our knowledge, there is no recent work we further compare with ResTL. Some generative models, such as GAN, might be adapted to compare with ResTL by using RS EEG signals as input instead of random noise. However, these models would require independent training and a novel method to preserve subject characteristics from RS EEG signals.
(R3) CRAM-ResTL: We reproduced CRAM using the official implementation but encountered unstable training issues, which resulted in poor performance.
(R3) Ablation study for each module: The task encoder is essential to classify EEG signal and calibrate resting state (RS) EEG signal. We conducted an ablation study focusing solely on subject-dependent features. As shown in Table 2, omitting these features resulted in degrading classification accuracy. 
(R4) Limitations on real-world application and wider adoption: We respectfully disagree with the limitations raised by the reviewers. ResTL is actually more suitable for real-world applications and diverse tasks compared to existing methods based on task-state (TS) EEG signals, such as domain adaptation and transfer learning, since it only requires resting-state (RS) EEG signals for model adaptation. Collecting TS EEG signals is time-consuming and demands more effort from subjects as the number of classes increases, whereas ResTL circumvents these issues. Moreover, once the calibration and adaptation processes are performed for a target subject, only light computational resources are needed. Therefore, we believe that ResTL is highly appropriate for real-world applications, facilitating its use with new subjects.
Furthermore, ResTL is a model-agnostic method for EEG classification, capable of integrating existing EEG encoders with task- and subject-specific encoders. Additionally, ResTL has the potential to classify other signals characterized by significant inter-subject variability. These features enhance ResTLâs applicability across a wide range of tasks."
https://papers.miccai.org/miccai-2024/741-Paper3995.html,"We thank all reviewers for their constructive feedback and for appreciating how our work supports health equity of underserved populations via developing technology of ultra-Low-Field (uLF, 64mT) MRI. Last week, 3 keynotes on uLF MRI were presented at ISMRM, emphasizing the value of this emerging technology. We are excited reviewers have commended our dual-channel latent diffusion and novel SFNet architecture. Below, we address concerns and indicate responses. Please note our paper was submitted to the âMICCAI for Health Equityâ category.

[R4] Reproducibility (Hyperparameters): 
Our repository [redacted] contains all training and testing code, pretraining and fine-tuning scripts, and hyperparameters for all networks, including the ablation studies. The LDM network hyperparameters are mentioned in section 2.1.

[R1, R3] Comparison to SOTA work; citation inconsistency:
We apologize for our oversight in not explicitly citing SynthSR, 2023, by Inglesias, et al., which will be added. SynthSR was discussed in the introduction and is a compared method in Table 1 under âLF-SynthSRâ. From our citation [21], Inglesias, et al. state âLF-SynthSR, builds on our previous method, SynthSRâ. Both SynthSR and LHResGAN are pretrained and the only known methods affirmed by their authors as usable âout of the boxâ for uLF-HF synthesis; we will correct this in section 3.3 and fix the introductionâs citations.

[R3, R4] Ablation studies; model training: 
Our ablation studies were outlined in paragraph 2, section 2.2, and guided our selection of the best networks, which were presented in section 3.3. Notably, Table 1 showed a pivotal architecture ablation study âSwinUNETRâ, which is our SFNet (n=90) without GAN aspects (âAncillary Discriminator Componentsâ of Fig 2). Studies of sequential pretraining yielded SFNet (n=30, n=90) and âSwinUNETRâ results seen in Table 1. Omitted from Table 1 are networks trained from scratch: SFNet (n=30 and n=90), was slightly, but not significantly (p=0.17 and p=0.09) better than Table 1âs âSwinUNETRâ model. A âSwinUNETRâ trained from scratch had perceptual metrics similar to SynthSR (p=0.23) and LHResGAN (p=0.34) but achieved higher DSC and RVE (p=0.02 and p=0.04). As requested, section 2.2 will be updated to better explain our ablation studies and model selection process. We will include purposefully omitted (as stated in section 3.3) ablation study outcomes in Table 1 at submission.

[R1, R3, R4] Concerning dataset scope: 
In our limitations, we acknowledged the constrained size of our dataset, which is justified by two reasons. First, obtaining paired uLF-HF data, especially from underrepresented regions like Uganda, is challenging. To our knowledge, no public datasets of uLF-HF pairs exist. Second, uLF MRI is a new technology and emerging area in pediatric healthcare, making acquisition more difficult. In fact, given the rarity of data and interest in this promising field, our team released the first public pediatric uLF dataset from low-resource areas for a challenge at [redacted]. Despite the limited dataset, our novel dual-channel latent diffusion tripled the cohort and significantly enhanced model performance. These clarifications will be summarized in section 4.

[R1] Pathology and clinical application: 
We mentioned in future work we wish to include pathology. Note our data is acquired at 64mT (ultra LF) in contrast to 0.5-1T data (commonly LF). Currently, uLF MRI is not established as a diagnostic exam due to the field strengthâs detrimental impact on image quality. Hence, our study is essential to improving uLF images to diagnostic level quality. Our initial work is performed on presumably healthy patients; uLF cases are from an ongoing research study of infant growth in underserved areas to identify developmental differences based on geography or socioeconomic status. Thus, images were not acquired with the expectation of identifying pathology. These clarifications will be made in sections 1 and 3.1."
https://papers.miccai.org/miccai-2024/742-Paper0139.html,"We would like to thank the reviewers for their insightful comments. In what follows are the major concerns and our responses.

â¢Q1(R1,R3,R4). The generalizability to other segmentation tasks and to use more datasets for evaluation. 
#A1: Additional experiments were performed but not listed due to page limit. (1) Experiments on more datasets: We also performed liver tumor segmentation on LiTS dataset, and the results support the same conclusion that CouinaudSAM is superior. We will add some conclusion of these experiments in result section. (2) Experiments on other segmentation task: We also worked on lung tumor segmentation, and find that CouinaudSAM is applicable since lung can be divided into 5 lobes, and shows good result. We did not add it because this work focuses on liver tumor, but the extension to other tasks was discussed in the original conclusion section.

â¢Q2(R1,R4). More comparison with the SOTA methods, such as SAM-based automatic methods. 
#A2: In Fig.3(b), we have provided the results of original SAM-based automatic methods with a 32x32 or 64x64 regular grid of points, and CouinaudSAM shows superiority. For other SAM variants, the major difference is different parameter-efficient transfer learning (PETL) approaches: SAMed uses LoRA (DSC: 66.26%), MA-SAM uses FacT(DSC: 67.93%), and MSA uses Adapter(DSC:64.59%). Since PETL is not the major contribution of our work, we follow MA-SAM since FacT shows the best result. Moreover, we cannot directly compare pre-trained SAM variants, because the test dataset is already involved in their training, which may lead to unfair comparison.

â¢Q3(R1,R3). Superpixel can avoid missing small tumors. 
#A3: Our motivation is to obtain point prompts that represent meaningful regions based on superpixel rather than regular grid. A small tumor may be missed when the grid is sparse, but it may still be caught using superpixel, since the tumor region is different from its surrounding region. Thank you for your suggestion, we will change it to âSuperpixel can help reduce the risk of missing small tumors â, since our previous expression may be too strong.

â¢Q4(R1). Analysis of hyper-parameters. 
#A4: Hyper-parameter analysis is not listed due to page limit. For example, we conducted experiments using different balanced sampling ratio from 0.1-0.9, and 0.6 is selected since it gives the optimal performance.

â¢Q5(R3). In Fig.3, the improvement over grid-based point prompt is not significant, and there exists a gap compared to manual approach. 
#A5: Our method is superior to grid-based methods from 3 perspectives: (1) Better performance. (2) Less inference time. (3) When the resolution of the grid is higher, it brings higher false positive rate, and benefits more from the refinement of the Couinaud segment mask. Compared to the manual approach, our method shows inferior results, but also greatly reduces the manual efforts of point prompt generation, which is the goal of this work.

â¢Q6(R4). Convert Couinaud segmentation into a box prompt. 
#A6: We have performed your suggested experiments but the results are not as good as expected. When the tumor exists in all segments of the liver, then the bounding box derived from the prompt covers the whole liver region. The region-level guidance may not be significant since the tumor segmentation result is finally refined within the liver region.

â¢Q7(R4). Inference time comparison. 
#A7: We use SLIC to generate superpixels within liver, and it is performed in a CPU machine then saved locally, which takes around 20 seconds each volume.  The total inference time for 32 test volumes in MSD08-142 is: CouinaudSAM (12 min), 32x32 grid (19 min), and our method takes less time compared to original SAM-based automatic segmentation methods.

â¢Q8(R4). Slice Prompt and Volume Prompt.
#A8: It was explained in Section 3.3(second last sentence). Other prompt-unsupported networks are refined within the Couinaud segmentation masks derived from the prompts."
https://papers.miccai.org/miccai-2024/743-Paper0213.html,"First, we would like to thank all reviewers for your time and valuable comments on our work. Following the suggestions of reviewers, we respond below to reviewersâ comments.

R1 C1: Open access to the community. 
Response: We will publicly release our atlas, source codes, and related data through the Neuroimaging Tools & Resources Collaboratory (NITRC) website.

R3 C1: Homogeneity of cortical folding. 
Response: We want to clarify that the variability of cortical folding across subjects is actually what motivates us to design our shape-informed atlasing method. We select the reference for a new subject based on the geometric information of cortical folding patterns while the previous atlases use all streamlines in the common space.

R3 C2: Impact of low-resolution diffusion MRI data. 
Response: We would like to clarity that our method relies on the cortical surface to propagate our atlas to new subject. Even if the dMRI data might have lower resolution than HCP, the T1-weighted MRI images typically have high resolution(~1mm) for cortical surface reconstruction. Thus, the application of our shape-informed U-fiber atlas does not depend on the resolution of the dMRI data.

R3 C3: Why the weight of distance transform is negative, while the shape index is positive in shape similarity score? 
Response: As shown in section 2.2, the weights of distance transform and shape index are both negative. These weights are negative by design for convenience.

R3 C4: After pull-back the identified U-fiber bundles from dictionary subjects, does the original streamlines retained? 
Response: All streamlines of the pull-backed U-fibers are from dictionary subjects while the original streamlines are not kept. Hence, we donât deal with the false-positive of the original U-fibers.

R3 C5: The study does not mention the latest paper âSupwma: Consistent and Efficient Tractography Parcellation of Superficial White Matter with Deep Learningâ, and there is no comparative experiment with this method. 
Response: Our study focuses more on building an accurate Superficial White Matter (SWM) dictionary and utilizing this dictionary while combining with the individual geometric information in practice. The work from Supwma relies on the anatomically curated white matter tractography atlas from the cited work [13] for the parcellation of SWM bundles, but the atlas in [13] still uses a volume-based tracking method, in contrast to our surface-based tracking method. We will cite Supwma as a complementary work of SWM studies in our revision and perform comparisons with this method in our future work.

R5 C1: Only central sulus and motor cortex were used for comparison with latest atlas and later experiments. 
Response: As shown in our work, our atlas includes 77 U-fiber bundles that cover most of the cortex, and the same shape-informed method can be applied to all bundles. We demonstrate our method on the U-fibers between the motor and sensory cortex because these regions are highly representative in previous SWM studies. However, the advantages derived from our surface-based tracking vs volume-based tracking in previous atlases are general for other bundles. In our current research, we have successfully applied our method in various cortical regions including the inferior, middle and superior temporal regions in both HCP and ADNI data. These results will be shared through the public distribution of the atlas on NITRC and future journal paper submission.

R5 C2: How to determine the threshold for inter-subject clustering in section 2.1? 
Response: The threshold is automatically determined in proportion to mean centroid length and the number of disconnected components from the hierarchical clustering method in [18]."
https://papers.miccai.org/miccai-2024/744-Paper1220.html,"We would like to thank the reviewers for their thorough review of our work. We are delighted that the reviewers found our novel and interesting idea (R #4,7,8),  sufficient experiments (R #4,7,8), thorough ablation study (R #4,7,8), and experimental results convincing (R #4,5,7). The related code will be available.

To Review #4
(1)We will add more contents about the two distinct paradigms and SOTA methods in the Introduction. (2) In Eq.(1), we use X_cls as the input of HTA for a neat expression. We donât utilize class token in HTA, while utilizing class token in Spatial Attention. (3) We will improve/simplify our language for better readability, and add limitations in the Conclusion.

To Review #5
(1) Although decreasing input frames to address temporal redundancy is a mature technology, our main focus is to design components tailored for sparse frame sequence, in specific, the proposed HTA and KCA.   (2) TimeSformer has investigated the order of temporal and spatial attention, and we follow the same paradigm. (3) In Tab.1, we implement a straightforward baseline by employing TimeSformer, termed as Baseline w/ MA, and integrates our proposed HTA into the baseline, termed as âSurgformer w/ MAâ.   The overall performance of baseline degrades as the length increases, while the variant gains significant improvements with the increased length, which demonstrates the effectiveness of HTA to learn more discriminative features. TimeSformer utilizes short video clips as input to learn global temporal features, and predicts action categories for each clip. But surgical phase recognition focuses on fine-grained frame-level analysis. Given the temporal ambiguities between different phases in surgical videos, Surgformer incorporates both long-term and short-term temporal information for accurate recognition of these phases.

To Review #7
(1) We will add more literature, especially papers about long-term and short-term information and hierarchical aggregation. We will reference the paper âPrompt-enhanced Hierarchical Transformer â¦ via Temporal Action Segmentationâ for sufficient analysis. It employs dilated temporal convolution layers for hierarchical temporal information, while we propose HTA to capture both long-term and short-term information within varied temporal resolutions from a target frame-centric perspective. (2) All the comparison results in Tab.2 are from SKiT for fair comparison.  It only provides the unrelaxed results of AVT. For Fig.4, we will add the analysis of the provided results. We will also add the discussion on complexity and inference time.

To Review #8
(1) In the Appendix, we release the results obtained from training the model with diverse frame rates, while maintaining a fixed length 16. As frame rate increases, the sampled sequence encompasses a larger receptive field to capture long range info and result in a gradual decrease in similarity between adjacent frames, which leads to a substantial enhancement in the performance during the initial stages.  Simultaneously, we fix the frame rate 4, and modify the sequence length to get long-range information, which also leads to a substantial enhancement in the performance during the initial stages. We will add more detailed analysis. (2) We will fix the content that may be confusing, such as acronyms defined vs. how it appears in the text and tables. We will also relocate ablation table to a position succeeding comparison table and remove red highlight in ablation table. We will revise the methodology part with more explanation and analysis for better readability. (3) For AutoLaparo, we utilize the official splits (https://autolaparo.github.io/).  Compared with Cholec80, AutoLaparo is more difficult and requires stronger spatial-temporal information. Surgformer outperforms the best performance SKiT, which achieves gains of 2.8% and 6.8% in terms of Acc and Jac. (4) Sorry for the confusion of font size, we will double ensure the final version meets the requirements."
https://papers.miccai.org/miccai-2024/745-Paper3077.html,"We appreciate the reviewersâ valuable feedback and constructive suggestions. Below we address the major reviewersâ concerns.

R1ãR3ãR4. Reproducibility: 
We will release our code and datasets once the paper is accepted.

R4. Compare initialization with EndoGaussian and COLMAP-Free 3D Gaussian Splatting:
EndoGaussian, COLMAP-Free 3D Gaussian Splatting and our method all use depth-projected point cloud as Gaussian initialization, but EndoGaussian and our method need to address the holes in point cloud caused by the removal of surgical tools. EndoGaussian randomly selects 1% of the point cloud from each frame and combines them together to obtain the point cloud used for Gaussian initialization.
In our experiments, we found that taking point cloud with large missing regions of first frame as Gaussian initialization would have an impact on the reconstruction quality. As surgical instruments move, tissue that is occluded in the current frame can be observed on other frames. Inspired by this observation, we inpaint the holes on image and depth of the first frame with new showing content in subsequent frames, and then project inpainted image to get completed point cloud for Gaussian initialization. Therefore, our point cloud reduces the size of holes and it requires less computation.

R4. Provide more explanations of key technical components:
Gaussian initialization is one of the novelties of our method. Besides, our method proposes deformation MLP with regularization constraints (L_pos, L_cov) to predict the change of Gaussian properties over time.  We also apply the occlusion-based color regularization loss (L_smooth) to help remove surgical tools. As described in Sec. 2.4, the deformation regularization loss constrains the Gaussian deformation to be similar in position and shape to its K neighbors. The color regularization loss L_smooth is a total variational loss, which is a common regularizer in inverse problems and was used in K-planes [5] and 4DGS [26].
L_smooth is applied to surgical tool mask M, which is the intersection of the surgical tool masks on all frames. L_smooth is defined to minimize the color difference between a pixel and its neighbors: (1/n)sum(L2(p^(i,j)-p^(i-1,j))+L2(p^(i,j)-p^(i,j-1))). Here n is the number of pixels in union tool mask M, p is the predicted color of the pixel in M, i and j are its row index and column index respectively. We will update the formula of L_smooth in revised manuscript.

R1ãR3ãR4. How to get camera poses, depth and masks of surgical tools:
We conducted experiments on two public datasets: EndoNeRF and StereoMIS. For EndoNeRF dataset, it sets camera pose to an identity matrix due to the fixed shooting camera of the scene. Depth maps are generated using a pre-trained STTR-light [S1] model, and tool masks are obtained through manual labeling. For StereoMIS dataset, we applied the same method to obtain the camera poses and depth maps. Additionally, we used SAM-Track [S2] model to predict the tool masks on each frame. We will add the description of datasets to manuscript.
[S1] Li, Zhaoshuo, et al. âRevisiting stereo depth estimation from a sequence-to-sequence perspective with transformers.â ICCV 2021.
[S2] Cheng, Yangming, et al. âSegment and track anything.â arxiv 2023.

R1. Computational efficiency and user study:
Our target is to improve reconstruction quality. However, the rendering speed of our method also exceeds 80 FPS, which is significantly surpassing the real-time application requirement of 30 FPS. In addition, we will add a professional user study to get feedback from medical experts.

R3. Reconstructed surface deformation and evaluation:
Thanks for the suggestion. The surface deformation could be observed from reconstructed point cloud of each frame.  Similar as EndoSurf [29], we are able to evaluate the deformation by comparing the point cloud distance between reconstructed point cloud and projected point cloud based on GT depth."
https://papers.miccai.org/miccai-2024/746-Paper4136.html,N/A
https://papers.miccai.org/miccai-2024/747-Paper3191.html,"We thank the reviewers for their constructive feedback. We appreciate their recognition of the novelty of our approach for survival analysis, its potential impact on the MICCAI community [R1, R4, R5], and the clarity of our manuscript [R1, R3, R5].

[R1, R3] The dataset used is publicly available. We reiterate our commitment to releasing all experimentsâ source code, ensuring complete reproducibility.

[R1] The application of Rank-N-Contrast as a regularizer is a novel approach that introduces ordinality among features for the first time. Our proposed loss function, which is versatile, compatible with any deep neural network, and capable of handling censored data (where time-to-event is missing), is a unique contribution to the field.

[R5] We compared our proposed method with six state-of-the-art methods on the dataset for a fair comparison, including ensembled approaches. Notably, some methods we compared against require segmentation masks, unlike our SurvRNC. We focus on real-life clinical applications rather than synthetic datasets because they offer a more authentic representation of medical scenarios, enhancing the validity and applicability of our modelâs predictions. We could not explore this in the context of other datasets and models, but we will keep the valuable recommendation in mind for further work.

[R3, R4] We appreciate the reviewersâ suggestions for format changes and are committed to incorporating these in the final camera-ready version."
https://papers.miccai.org/miccai-2024/748-Paper0415.html,"We would like to thank all the Reviewers for their insightful comments and constructive suggestions. Below we address the main concerns raised, among predominantly positive feedback."
https://papers.miccai.org/miccai-2024/749-Paper1627.html,We thank the area chair and all reviewers for their thorough evaluation and insightful comments. We will update our camera-ready version according to the reviewersâ comments.
https://papers.miccai.org/miccai-2024/750-Paper2579.html,"We are grateful for the thorough reviews and appreciate the recognition of our contributions to encoding symmetric information into neural networks for brain image analysis across both CT and MRI modalities. We are committed to addressing all the reviewersâ concerns in the final version with codes published. We respond to the major points raised by the reviewers below.
[To R1]
Q1* Asymmetry in Brain Structures. A1* We wish to clarify that the human brain is not strictly symmetrical, and our primary motivation is not to model purely symmetry vs asymmetry but to model physical and pathological features with symmetry as a key characteristic as studied in [18]. Our motivation has been proven beneficial not just from qualitative visualization but also from quantitative studies demonstrated in [14] and [21]. Thanks for the suggestion of SSIM and similar metrics, which we will consider in the extended version.
Q2* Comparison with CrossViT. A2* Our work emphasizes the development of a training framework to encode symmetry awareness rather than primarily enhancing the network structure. This framework is, therefore, adaptable to various encoders, including CrossViT. We opted for the SOTA Swin encoder to test our framework because of its proven efficacy and performance. Besides, the cross-attention mechanism is also different in our work, which is intended to model contrastive features from symmetrical counterparts by applying token-level all-to-all attention for both classification and segmentation, while CrossViT only fused the features of CLS tokens across the large-patch and small-patch branches for classification. 
Q3* Experimental Setups. A3* We apologize for any confusion caused by the descriptions in our paper. For classification tasks, the Swin MLP model was pretrained using the same datasets and hyperparameters as our proposed method, ensuring a fair comparison. For segmentation tasks (Table 3), we wanted to emphasize the performance of individual modules (SAH and SACA), so all methods were compared after training on the target dataset. In all experiments, we compared frameworks based on the same encoder network to maintain fairness, and we agree that it is interesting to see the performance with other network structures and pre-trained weights, which will be explored in the extended version.
[To R4]
Q4Asymmetry and Overfitting A4Thank you for your insightful comment. In our study, we have implemented skull-stripping processes to mitigate the influence of extraneous factors. This ensures that our analysis is focused exclusively on brains. Besides, we agree that not all asymmetry means disease. The model is, therefore, pretrained based on distinguishing healthy and unhealthy brains with symmetry as the key feature, where physiological and pathological asymmetries can be distinguished to avoid overfitting purely symmetrical or asymmetrical features.
[To R5] 
Q5* Is Symmetry an Augmentation? A5* We would like to clarify that our approach does not treat symmetry as an augmentation. Instead, we extract pairs of patches from anatomically symmetrically opposing locations in the original images, which are not generated via augmentation. Our framework integrates the information of pairs of patches by utilizing novel SAH and SACA. This framework fundamentally differs from the one used in SwinUNETR, where rotation is merely applied to a single input patch for contrastive learning.
Q6* Out-of-Distribution (OOD) Testing. A6* Our testing comprehensively includes OOD datasets to evaluate the robustness and effectiveness of our framework. Table S1 details the sources of all datasets used in our experiments, which are collected from multiple clinical centers with varied settings, ensuring that the test conditions are rigorously OOD. Besides, the suggested BTCV data is not directly relevant to our work, as it only contains abdomen CTs, which diverges significantly from our research aimed at disease diagnosis through brain structure analysis."
https://papers.miccai.org/miccai-2024/751-Paper3074.html,"Thanks all the reviewers for their valuable and constructive feedback, and for recognizing my work as interesting and novel. Our responses to your comments are as follows:

C1. Comparison with the state-of-the-art method on one dataset (R2, R6):

In this study, we introduce a new task namely, fine-grained symptom-level progression classification, which requires additional symptom labels for training (please note that all existing methods did not consider symptoms, and therefore could not predict the progression of multiple symptoms simultaneously). To the best of our knowledge, the Chest ImaGenome dataset is the only one publicly available dataset which provides such symptom-level diagnostic labels (obtained from MIMIC-CXR). We notice that SOTA method, CheXRelFormer [MICCAI 2023] outperforms all existing methods on Chest ImaGenome dataset in 2023. In their paper, it has also utilized this dataset (only) for experiments. Our proposed method in this paper outperforms CheXRelFormer and exhibits a significant improvement of 10.7% in the overall F1 score. In future, we will conduct additional experiments using our in-house dataset.

C2. (i) Why accuracy is not reported similar to CheXRelFormer (Table 2) (instead of Mean precision, Mean Recall and Mean F1-score) and (ii) statistical significance should be reported (R4):
(i) From the source code provided by CheXRelFormer, the calculation of âmean weighted overall accuracyâ reported in CheXRelFormer (Table 2) is actually that of the weighted F1-score in this paper. That means, the evaluation metric is the same as that in this paper. Moreover, this paper provides two additional metrics: macro precision and macro recall. 
(ii) We agree that statistical significance should be reported, the p-value can be calculated based on results in Table 1 in the manuscript.

C3. Discussion of limitations and computational requirements (R3):

Thanks for the suggestions! Two limitations in this study could be addressed in future research. First, this study only focuses on chest-related disease progression, while it should include a wider range of disease types. Second, this study should conduct further exploration of the modelâs ability in the survival analysis task, given its capability to capture both static disease features and dynamic disease changes.

Details of the models:
The number of model parameters of CheXRelFormer is 41.0M while ours is 32.2M. The computational complexity (FLOPs) of CheXRelFormer is 20.4G while ours is 9.5G, indicating that our model is superior to CheXRelFormer regarding parameter size and computational complexity.

C4. Access to source code (R3-R6):

The source code will be released for public after acceptance.

C5. Minor typo mistakes (R3-R5):

Thank you for pointing out the typo mistakes. In the Introduction, Paragraph 5 Line 4, the âcamâ should be âcanâ. In Figure 1, X and Xâ represent the current and the prior image, respectively. In section 3.1, the âCheXFormerâ should be âCheXRelFormerâ.

C6. Ablation studies (R6):

Thank you for your suggestions! We conducted ablation studies and reported in section 3.4 in the manuscript to investigate the effectiveness of the Symptom Disentangler module. The results in Table 3 indicate that directly learning disease progression from image-level features without the Symptom Disentangler will lead to a significant performance decline. We also investigated the importance of the symptom labels. The results reported in Table 3 show that even training without the symptom labels, our method still exhibits a notable improvement compared to the baseline. The improvement can be attributed to the newly proposed Symptom Disentangler, which can be trained on only progression labels that indirectly imply the presence of symptoms.

All of the above modifications and discussions will be added to the final version of the manuscript."
https://papers.miccai.org/miccai-2024/752-Paper3680.html,"We thank all reviewers for their valuable feedback and appreciate their constructive comments. Due to space limitations, we will address major concerns here and aim to incorporate as many reviewer comments as possible in a revised manuscript.
Abbreviations: ground truth (GT); Cellpose (CP); SynCellFactory (SCF)

Response to Reviewer 1:

Q: Concerns about CP pseudo GT and error handling (R3).
A: R1 is correct that SCF does not strive to guarantee highly accurate segmentation labels. Indeed, synthetic segmentation GT is not our focus, and any segmentation method can replace CP. Instead, SCF targets reliable tracking GT (detections, tracklets). We ensure each cell has a mask by integrating the simulated locations in our correction step (see p. 5 Sec. 2.4). Experiments prove these masks suffice to train and enhance models reliant on segmentation GT (Embedtrack).

Q: Cell cycle stages (R4)
A: Cell cycle stages are inferred from the motion model. Splitting occurs randomly during motion model inference without strict biological constraints, except for the split duration which is based on the training data. We agree with R1 and R4 that refining cell cycle modelling is a promising future direction.

Q: Details on Video Length (R4)
A: The minimum duration of a full mitosis cycle is t = 6 in tested datasets. We doubled the minimum cycle length (t=12) for our experiments. Video quality declines after t = 30.

Q: CP training dataset
A: Publications do not list CTC as training data for CP and CP 2.0. In tests, our fine-tuned models outperformed all pre-trained CP models based on segmentation metrics.

Q: Comparison with [22]
A: Serna-Aguilera et al. [22] do not provide tracking pseudo GT because their 3D model does not afford spatial conditioning. While segmentation pseudo GT can be generated using CP, tracking GT cannot. We will revise the introduction to state: ââ¦crucially, their approach does not produce tracking pseudo-GT labels, while SCF does.â

Q: Varied cell shapes
A: The disks in the motion model identify cell position, not cell shape. ControlNets then generate realistic shapes. A complex shape model would increase motion model complexity, including when cells interact.

Q: Computational Effort and Resolution
A: On a single A100 40GB GPU: training one dataset ~ 9h, sampling one timelapse ~ 3min. Generated videos match original resolutions, ranging from 512x512 (DIC-C2DH-HeLa) to 1024x1024 (Fluo-C2DL-Huh7).

Q: Movement ControlNet information
A: The Movement ControlNet employs a learning design and a loss function that gently enforces conditioning (see ref. [28]), ensuring effective task completion as evidenced by the quality of results.

Response to Reviewer 3:

Q: Motion Model Detail
A: In the motion model, collision detection and resolution occur when two cells overlap. Using a hard sphere model, positions are adjusted with a repulsion vector until overlaps are resolved. As searched for by R3, the relevant code is located at â/motion_module/motion_help.py line 514â.

Q: Include FID (R4)
A: While genAI metrics like FID correlate with human perception, this manuscript focuses on their use as data augmentation tools, prioritising the tracking metric as the sole relevant measure for our pipeline.

Q: Code Availability
A: We pledge to release the code as open source on GitHub.

Response to Reviewer 4:

Q: Density and Distance Limitations
A: R4âs insights on PSC prompt further analysis of density and cell distance impacts on our pipeline, though no systematic study has yet assessed its limits. Qualitative tests showed that the cell density limits are broadly based on the initial and final frame densities of the training video.

Q: Apoptosis and morphological cues of mitosis
A: Our motion model, excluding apoptosis and mitosis cues, is simplistic. A complex model would capture cell cycle nuances more accurately but needs more biological priors. We opted for simplicity in the SCF model for broader application."
https://papers.miccai.org/miccai-2024/753-Paper1208.html,"We appreciate the thoughtful feedback from the reviewers. We will correct errors and clarify the major concerns from the reviewers in the final version.

1) Reproducibility: We will make code publicly available after acceptance. 
2) Computational costs: Our fast-sampling algorithm of anisotropic noise can greatly reduce the computational costs. In the experiments, our method based on synchronous image-label diffusion model framework is much faster than the comparison methods of SegDiff and MedSegDiff using conditional diffusion models. The related analyses will be extended in the future work.
3) Motivation (#3):
We agree with the reviewer that natural images have structure and pixel dependencies, and spatially dependent information is contained in all images. We acknowledge that datasets like FFHQ share structured spatial properties with medical images. We clarify our motivations to emphasize the specific challenges and characteristics of medical imaging while recognizing that similar structured spatial dependencies can exist in certain natural image datasets. 
The phrase âwhich overlooks the structural informationâ in abstract refers to the standard Gaussian noise (i.e., multivariate standard normal N(0,I)), not the images or diffusion models. We would like to emphasize that the traditional isotropic noise is a pure stochastic noise, where the variables are independent and covariances among different variates are zeros. Our anisotropic Gaussian noise is sampled from a distribution of N(0, Sigma)ï¼ where Sigma is not an identity matrix, the diagonal entries are not ones and the off-diagonal entries are not zeros, indicating spatial dependencies. This allows for pixel dependencies and preserves image structure. It is consistent with the reviewerâs opinions.
We will reframe our motivations throughout the paper and avoid misunderstandings and ensure clarity."
https://papers.miccai.org/miccai-2024/754-Paper2329.html,"We thank the Reviewers for their constructive comments and positive feedback. Below, we address the main concerns regarding our work.

It is not the first work that uses CMR for mPAP estimation (R1): We agree that our work is not the first to utilize CMR for mPAP estimation. While some previous studies use features extracted from CMRs (such as systolic septal angle, ventricle volumes, etc.) to predict mPAP, to the best of our knowledge, we are the first to make this predictiondirectlyfrom CMR videos without additional processing steps or the extraction of manually chosen features. We will clarify this distinction in our camera-ready version and include references to other works in this domain.

The vision backbone generates C channels while the input is grayscale and has 1 color channel (R6): This is standard processing within hierarchical vision models. In these models, the number of channels increases in successive layers while the spatial resolution decreases.

Tabular-only methods achieve better performance (R7): Although some imaging methods result in higher prediction errors compared to tabular-only methods, the addition of TabMixer improves overall performance. Notably, the combination of I3D with TabMixer and the SA plane achieves the lowest error among all tested methods (imaging and/or tabular). This underscores the importance of the optimal combination of the vision backbone and tabular module for the task.

Lightweight alternatives of TabMixer, self-supervised pretraining, combination of multiple CMR planes, and tabular data analysis (R7): We agree that these are promising research directions worth exploring."
https://papers.miccai.org/miccai-2024/755-Paper1348.html,"We thank reviewers for constructive comments and respond to individual comments as follows.

R1
-Comparison with aggregation algorithm SimAgg
Though we have added FedGH [31], which uses gradient harmonization as the aggregation method, comparisons with more aggregation methods would be better.

R3
-How deconflicting gradients reduce aggregation loss
As detailed in Remark 1, the Î»z in our SVD indicates the curvature of the loss in the gradient direction vz . Our SVD allows us to discard insignificant directions and focus only on those with large curvatures (principal gradients). Thus the deconflicting gradient updates the model in a better direction that reduces global loss.

-Impact of flipping the direction of eigenvectors on gradients
The eigenvectors obtained by SVD have no directions, with both v and -v being eigenvectors. Thus to ensure loss reduction, we calibrate eigenvector directions using the mean of local gradients, which typically represent loss-decreasing directions. Our experimental results also showed that this calibration improves convergence.

R4
-L2 regularization was used in FedSR
Thanks for sharing the paper. Our margin control does resemble FedSRâs L2 regularization but with different motivations.  While L2 regularization aims to align representation distributions from different clients with a common reference, our approach is inspired by shortcut learning, where large margins lead to reliance on shortcut features rather than stable ones.  We can employ various regularization techniques, like evaluating log-loss on a margin multiplied by a decreasing function or setting output logits thresholds to penalize large margins, in addition to L2 regularization. Weâll cite FedSR and provide a comparison in our paper.

-Lacks a thorough examination of global loss decomposition
We are grateful for the insightful recommendations, which align with our ongoing and future work. In this paper, we employ global loss decomposition to inspire two methods that jointly minimize loss terms. We test their effectiveness through a straightforward, single-round experiment on the training dataset. Our primary aim for global loss decomposition is to offer a tool for analyzing FL training processes. As the study of loss decomposition is in its early stages, a more comprehensive evaluation and comparison of loss terms and methods that enhance its reliability is left as future work.

-Derivation process of Loss decomposition 
The loss decomposition comes from the gap between the global modelâs loss on the global dataset ( L(w), which we aim to minimize) and the averaged loss for local models on local datasets (local loss, minimized by local training). This gap, represented as L(w) - Local Loss, stems from two factors: 1) the difference in loss surfaces across local datasets, leading to Distribution shift loss; 2) the variation in loss between local and global models on the global dataset, resulting in Aggregation loss. Then, we get L(w) - Local Loss = Distribution shift loss + Aggregation loss, which is exactly Eq.(2).

-Why compare personalized FL? 
We compared personalized FL because 1) these methods are also used to tackle data heterogeneity; 2) their performance is evaluated on common test datasets, similar to general FL methods; and 3) FedPAC achieves SOTA performance on non-IID data.

-Distribution shifts in the label or feature space? 
We exhibit our results with label shifts constructed by Dirichlet distribution for the Retina dataset with Î± 100,0.1,0.5 for split1,2,3 respectively, each of which consists of 5 clients. COVID-FL is a real-world federated dataset that exhibits both shifts in label and feature distributions. We will add more details to our paper.

-Unrelated citation
[6] is an example of when gradient differences fail to capture model bias in representation learning on graphs."
https://papers.miccai.org/miccai-2024/756-Paper2281.html,"We thank the reviewers for their useful comments. We are glad to see that all the reviewers highlighted the novelty of our method. However, we respectfully disagree with R4, who criticized the reproducibility and the lack of clinical usability. We provide below a point-by-point response to the main concerns.

R3, R4: [âThe authors did not mention releasing the source codeâ and âThe description of reproducibility is poorâ] We respectfully disagree with this, as we provided all the required details in the paper and, as specified in the introduction, we will publicly release the code upon acceptance.

R1, R4: [Details on the training of BAE and the OASIS dataset] We believe R1 misunderstood the acronym BAE, which stands for Brain-Age Estimator, as introduced in the abstract. The BAE was introduced in previous works [17], and the training details can be found there. Regarding the OASIS dataset, some statistics are already specified in Section 4. However, we will add all the requested data in the revised version of the paper.

R1, R4: [âlack of evidence supporting the claim that the proposed pipeline aids in early disease detection and clinical interventionâ and clinical usability]
The clinical usability of brain progression simulators is well-known in the literature. They can be used to recover missing images in longitudinal data, as a potential virtual placebo or for patient stratification[11,13,18]. However, their clinical validation is difficult to be evaluated and it is out of the scope of our work. For these reasons, we will substitute this claim with a discussion on the possible clinical applications.

R1.1: [Computing SSIM and PSNR metrics for distinct brain tissues] Thank you for your suggestion. Although we believe this can improve the evaluation, we highlight that we used metrics that are the standard evaluation protocol for this task [13,16].

R1.2: [Evaluate across different groups] Although the suggested evaluation provides insight into clinical usability, we believe it is more suitable for a journal paper.

R1.3: [Suboptimal performance in posterior brain regions and evaluation with average error maps] We agree with the comment, but we point out that by looking at the rest of the brain and for all the other cases, our method performs better. We understand the difficulty in evaluating a single image result; for this reason, as you suggested, we have computed the average error maps and will insert them in the camera-ready version to show the average overall performance.Â

R3.1: [Explore end-to-end training] Thank you for the interesting suggestion. We will explore the potential of an end-to-end training in the journal version of the work.

R4.1: [Contributions not emphasized] Please see the last paragraph of the introduction as we have provided a detailed list of contributions there.

R4.2: [âit is unclear if the training, validation, and test splits are representative of the various diagnostic groupsâ] Yes, they are representative of the diagnostic groups. The dataset was carefully split to ensure that each diagnostic group was proportionately represented in the training, validation, and test sets.

R4.3: [âThis model could be dependent on the quality and sophistication of the pretrained encoder and the correctness of patient-specific dataâ] We agree with the reviewer. However, we would like to point out that this should not be considered a weakness of our method since it is a common characteristic of any deep learning model, as explored by previous literature.Â

R4.4: [Individualisation of predictions] The individualization of predictions is achieved by adding patient-specific data, such as the patientâs cognitive status and age at baseline.

[17] JÃ³nsson et al. âBrain age prediction using deep learning uncovers associated sequence variantsâ. Nature communications 2019
[18] Young et al. âData-driven modelling of neurodegenerative disease progression: thinking outside the black boxâ, Nature Reviews Neuroscience 2024"
https://papers.miccai.org/miccai-2024/757-Paper0115.html,"We would like to thank all reviewers for their constructive comments and for acknowledging the novelty and effectiveness of incorporating vascular features through graph networks into fundus image fusion.

1.Clarify vessel segmentation and registration details(R#1-2 R#3-1 R#4-2):We employed an automatic optimal transport-based graph matching method[14] for retinal image registration (see Sec 3.1, page 6 line 17). We implemented the 2D version of this method, which includes wavelet-based vessel segmentation utilized for both DRFF and OCT2Confocal datasets. For OCT-Confocal, due to its complexity, registration was manually performed and validated by ophthalmologists. We have unfortunately omitted to cite again[14] in the Methodology section and in hindsight these details were not clear enough either but we will correct this in the revised manuscript.

2.Verify the clinical efficacy and downstream tasks such as vessel segmentation and disease classification(R#1-1 R#3-4):We appreciate the suggestions for additional experiments, which align with our research goals. However, due to conference rebuttal policies, new/additional experimental results cannot be included in this rebuttal and in the camera-ready version of the manuscript upon eventual acceptance. Our statement in the Introduction regarding image fusion benefitting downstream tasks such as vessel segmentation and disease classification is supported by numerous existing publications such as [1] and âA review: deep learning for medical image segmentation using multi-modality fusion,â along with findings in related fusion tasks([9],[12],[23],[24]). These are not the primary aims of this study, which focuses on enhancing fusion outcomes. Validation of downstream applications is thus beyond the scope of this submission but is nevertheless part of our current work.

3.Small Size Dataset Handling(R#3-3 R#4-2): For DRFF dataset, we have in fact used data augmentation techniques including flipping, rotating by Â±8 degrees, and translating by Â±20 pixels to enhance diversity and representativeness.

4.Ablation Study and Model Component Justification(R#3-5):Regarding the use of CNN+ViT in LSR, we did explore both components individually. However, due to space limitations and since it does not constitute our primary research focus, detailed results were not presented. The advantages of combining CNN&ViT are however well documented, e.g. in reference[23], including via ablation studies, and showed significant improvement in various fusion applications including MRI-CT, MRI-PET, and Visible-Infrared.

5.Compare more GCN methods to validate TaGAT effectiveness(Clarify on GCN vs GAT Impact in Table 2)(R#4-3):Table 2 illustrates our ablation study, specifically âV GATâGCNâ, where we replaced GAT[17] with GCN[7] to demonstrate the effectiveness of the additional dynamic attention mechanisms. The intention was not to showcase GATâs performance improvement over GCN but to highlight the advantages of attention mechanisms by removing them from the system. The descriptions in Table 2 that may cause misunderstanding will be clarified in the final version. Exploring various forms of GCN is a good suggestion and aligns with our future work.

6.Narrative Clarity and Technical Details(R#3-2 R#4-1):For R#3-2, âI_fâ in Equ (10) represents the fused image produced by our model, not a ground-truth image. We will clearly state that no ground-truth fused images exist, as the fusion process inherently generates new images from the input modalities. For R#4-1, the Graph Fusion Layer(F_G) processes graph features from two modalities using multiple instances of an INN[3]. Each layer handles two halves of the input tensor separately, progressively refining and then concatenating them to form the fused graph feature.

We will ensure that all permitted corrections and improvements are included in final version. The code and data will be made available upon acceptance, as already specified in our original submission."
https://papers.miccai.org/miccai-2024/758-Paper0448.html,N/A
https://papers.miccai.org/miccai-2024/759-Paper0026.html,"We thank the reviewers for their valuable feedback. Overall, the reviewers consider this paper to be innovative (R1), interesting (R3), and well-structured (R1&R4), having effectively improved performance (R1&R3) and a thorough evaluation (R1&R4). We have carefully considered the raised concerns and will clarify them as follows:

[Q1-R1&R3&R4] Tail class selection.
We apologize for the typo in referring to triplet class indexes 17, 19, and 60 as tail classes. We intended to express that 17, 19, and 60 are head classes, whereas the remaining classes are considered as the tail. The head class is selected if the category has more than 10,000 samples.

[Q2-R1] The input details of ILCL.
As shown in Fig. 1(a), x and xâ are the input frame and its augmented version, respectively. In ILCL, they are passed through two networks, of which each consisting of a feature encoder and the proposed MHC.

[Q3-R1] Performance on tail classes.
In Fig. 1, we visualized the tail class confidence scores between the SOTA and our methods. This demonstrates that our model is more confident in recognizing the tail class <bipolar, coagulate, liver>.

[Q4-R1] The impact of loss balance coefficients.
On the left of Fig. 4, the blue line represents experiments conducted with a â {0, 0.25, 0.5, 0.75, 1, 1.25, 1.5} and Î² = 1, while the orange line represents experiments with Î± = 1 and Î² â {0, 0.25, 0.5, 0.75, 1, 1.25, 1.5}.

[Q5-R1] The utilization of temporal information. At the beginning of Section 2, we mentioned that temporal modeling is performed after TERL. Compared to 3D networks, this strategy is more efficient in long-term dependency modeling [1][2].

[Q6-R1] The layer configuration of MHC. 
In Fig. 2(b) and Section 2.2, we mentioned that each classifier within MHC is a 1 Ã 1 convolution layer. As for reproducibility, we will release the code once the paper is accepted.

[Q7-R3] The goal of ILCL.
ILCL aims to minimize the similarities of negative pairs (i.e., embeddings from different tail triplet classes) while maximizing similarities of positive pairs (i.e., embeddings from the same tail triplet class).

[Q8-R3] The rationale behind PBSE.
Sorry for the mistake made (see Q1). We have 97 tail classes rather than 3. PBSE embeddings are derived from these 97 tail classes. Some of these tail classes share the same triplet component (i.e., instrument, verb, or target). The component semantics can be learned from various tails. For example, for <grasper, dissect, cystic plate> enhancement, the semantics of cystic plate can be further learned from <scissors, cut, cystic plate> and <bipolar, coagulate, cystic plate>.

[Q9-R4] Evaluation on an additional dataset.
Our approach does not leverage surgical priors or class information from CholecT45. Consequently, we believe it can be applied to general tail-distributed datasets of surgical action triplets. We will consider this in future work, once a new public dataset is available.

[Q10-R4] Comparison with the SOTA method.
There seems to be a misunderstanding about our methodâs improvement over the SOTA method. According to Table 1, our results actually show a 1.5% improvement, rather than the 0.5% mentioned by the reviewer. Moreover, as shown in Fig. 1, our method achieves higher confidence scores in recognizing tail classes, demonstrating its superiority in tail enhancement.

[1] Czempiel, T., et al.: Tecno: Surgical phase recognition with multi-stage temporal convolutional networks. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 343â352. Springer (2020)

[2] Yi, F., et al.: Not end-to-end: Explore multi-stage architecture for online surgical phase recognition. In: Proceedings of the Asian Conference on Computer Vision. pp. 2613â2628 (2022)"
https://papers.miccai.org/miccai-2024/760-Paper2121.html,"We express our sincere gratitude to the reviewers for their valuable time and effort in evaluating our paper. We greatly appreciate the insightful and overall positive feedback. We are particularly pleased to note that the reviewers have recognized our manuscriptâs clarity (R1, R3, R4, R5), significance of the topic (R3, R4), clear technical novelty (R1, R5), and efficacy of our method (R1, R3, R5). Below, we provide our responses to the questions."
https://papers.miccai.org/miccai-2024/761-Paper2802.html,Thanks to all the reviewers for your valuable comments. We appreciate your recognition of:
https://papers.miccai.org/miccai-2024/762-Paper0249.html,"We thank all reviewers for their valuable comments. We will release the complete codes upon acceptance.

[R1] Q1 Motivation and novelty: Our motivation differs from that of BrainMAE (Yang et al., 2024). BrainMAE uses the MAE to construct a highly transferable fMRI pre-training model, addressing the challenge of generalizing fMRI features across different tasks (such as applying ASD feature extraction modules to AD). We incorporate the MAE (i.e. fMRI reconstruction) into disease prediction models as an auxiliary task to learn more robust task-oriented fMRI representations, which can substantially improve the disease detection accuracy against the BrainMAE style (see Table 1).Correspondingly, the innovations of our work include: 1) Different from the conventional sequential design of MAE and specific prediction task, we set up the MAE and prediction task in parallel to learn robust task-oriented features.  2) We also leverage the attention maps from the specific task to guide the fMRI time series reconstruction, which can in turn help learning the task-aware fMRI representations, thereby improving disease prediction accuracy; These motivations and innovations will be included in our final version.

[R1] Q2 Effectiveness of task-ware reconstruction: We agree to have more investigation into the modelâs effectiveness and interpretability. For the modelâs effectiveness, using Frechet Distance, we found that the discrepancy in fMRI representations generated by our model between healthy controls and patients is significantly greater than BrainMAE, indicating that our fMRI reconstruction process can more effectively perceive disease (task)-related features. Regarding the modelâs interpretability, considering that various human behaviors are mediated by neural circuits, the submitted version focused on functional systems and found significant differences in the impact of ASD and AD on these functional systems. Additionally, we visualized the importance of all brain regions in disease classification based on attention maps. The distribution of brain areas aligns with existing findings. All the above results will be included in the final submission.

[R6] Q3 Potential deficiencies in attention-based importance score:We agree with your point that when the model is not well-trained, some ROIs may have high attention weights but low contributions to the prediction. However, if the model is well-trained, this issue can be significantly mitigated. Therefore, many studies use attention maps as indicators of importance (Shi et al., ICML, 2022; Kakogeorgio et al., ECCV, 2022; Li et al., NIPS, 2022). Additionally, the brain region importance maps for ASD and AD in our interpretability analysis  are highly consistent with existing findings, demonstrating that our model does not suffer from this issue. This discussion will be included in the final submission.

[R1, R5, R6] Q4 Experiment-related issues: 1) (R1) We conducted the comparison with SOTA ComTF and found that our method outperforms ComTF on both ABIDE and ADNI datasets; 2) (R1) For the parameter settings of all baselines, we used the recommendations from their papers; 3) (R1, R5, R6) Analysis of sliding windows size and Î¼ has been conducted and will be included in the final submission; 4) (R1) Since this paper primarily focuses on testing the modelâs predictive ability for different brain diseases (ASD and AD), we did not further subdivide the AD category; 5) (R6) In this paper, the task refers to disease prediction, not task-fMRI data. Representation learning for task-fMRI is an interesting topic, and we will explore it in future work.

[R1,R6] Q5 Writing Issues: In our final revision, we will carefully revise the manuscript, including improving the description of the model (R1), and adjusting the font in figures and tables (R1,R6)."
https://papers.miccai.org/miccai-2024/763-Paper1038.html,"We thank the reviewers (R1, R3, R4) for their constructive comments. We are glad they appreciate the technical novelty (R1, R3), clinical important problems (R1, R3), and convincing results (R1, R3, R4). We next answer the main concerns in the revision.

Q1(R1,R4) Why not compare with task-specific methods, such as the work cited [1]
A: Currently, most teeth reconstruction methods rely on CBCT images. Although the work [1] aims to reconstruct 3D teeth models from multi-view images, it requires abundant paired data which is hard to acquire, i.e., real intra-oral photos and corresponding teeth models, to construct templates for its parametric model.  Therefore, it is difficult to apply it widely in practice. In contrast, our method offers greater flexibility, as it does not require paired cases for training, which is compared with SOTA reconstruction techniques in 3D vision, such as SyncDreamer, Zero123, and Neus. Our method achieves the best results, demonstrating its superiority in this task.

Q2(R1,R4) About data acquisition pipeline
A: We collected intra-oral scanning models from 3200 patients at two hospitals, with 200 patients having paired 2D real intra-oral photos. And dentists manually checked the data to ensure integrity and availability for clinical diagnosis. For input images of training, we rendered four intra-oral photos of lower and upper teeth separately, simulating real intra-oral photos. Camera viewpoints were randomly set within specific ranges. To bridge the gap between real images and rendered images, we added spot lights and modified teeth mesh material to achieve a realistic highlight effect.

Q3(R1,R4) Metrics used in this paper.
A: Our method involves a two-step pipeline: generating novel multi-view 2D images and reconstructing 3D tooth models from the generation. Hence, we evaluate our method using both image-level and mesh-level metrics. For image-level evaluation, we compute the common-used PSNR, SSIM, and LPIPS between the generated images and target images. For mesh-level evaluation, we employ CD, which measures the bidirectional average distance; HD, which captures the maximum distance; and IoU, which quantifies ratio of overlapping volume between the prediction and ground truth. We will provide details and reference in the final version to ensure clarity.

Q4(R4) The scalability and computational efficiency
A: We employ various data augmentation techniques during the rendering process of input images. And our work has been tested on intra-oral photos collected from real-world clinics. This underscores the scalability of our framework in accommodating the variability in intra-oral photographs. Regards computational efficiency, our model is trained on a single A100 GPU for 4 days. And the reconstruction takes about 10~20 mins.

Q5(R1,R4) Limitations of our method
A: One limitation is its efficiency. The efficiency of neural surface reconstruction is a wide concern in 3D reconstruction. Many researchers are working on speeding up it, e.g. instant-ngp which we will consider in future. In addition, another future work is to extract high-frequency low-level features for more precise details, improving overall quality. We will include these potential improvements in the final version.

Q6(R3,R4) Some details about results and the framework.
A: The square box in Fig. 4 highlights the inconsistency between different views in Zero123, as discussed in the experiments. In Fig. 2, the âtextâ attention branch encodes the condition using CLIPâs image encoder in the stable diffusion model. Although initially used for text-to-image tasks, we retain the terminology for consistency, but will clarify that âtextâ refers to image conditions, not text prompts in the final version.

Q7(R4) The benefit of normal map
Normal maps contain the normal direction of local surfaces which bring more precise geometry details for 3D teeth reconstruction.

We will release our codes and pre-trained models on our dataset."
https://papers.miccai.org/miccai-2024/764-Paper0757.html,"We appreciate the reviewersâ valuable comments and are encouraged by their unanimous recognition of TeleORâs significance in clinical settings. Weâll first address the common issues, and then respond to their specific concerns.

Answer: Upon acceptance, weâll open-source the code and make the complete implementations available for supporting the development of this promising topic.

Concern 1: Lack of validation by clinical personnel

A1: Due to ethical constraints, we use the public 4D-OR dataset, which simulates OR scenes without real patients and lesions. Thus, remote experts could not assess our TeleOR for actual surgeries in this context. Recognizing the importance of clinical validation, we are now conducting trials with our partner hospital to ensure that our TeleOR meets the clinical standards.

Concern 2: Lack of comparison with SOTA

A2: We donât compare with common reconstruction methods like NeRF and 3D Gaussian Splatting for two reasons: (1) These methods require substantial computations, resulting in high latency inappropriate for real-time applications, as detailed in our related work discussion; (2) they struggle with alignment from multiple views and require static frame sequences, unsuitable for dynamic OR scenes.

Concern 3: Lack of novelty

A3: We would like to highlight TeleORâs innovations designed to achieve real-time applications: Selective Reconstruction to reduce computations and Viewport-Adaptive Transmission for optimized data transmission. Moreover, the novelty of TeleOR has been acknowledged by all other reviewers, emphasizing its contributions in practice.

Concern 4: Lack of implementation details (e.g., camera setup details)

A4: We would like to clarify that the implementation details, including the camera setup, are provided in Sec. 4.1. Specifically, the camera array consists of six Microsoft Azure Kinect RGB-D sensors. Additionally, Sec. 3.1 outlines the details of camera initialization and calibration.

Concern 1: Performance under lower bandwidth

A1: At 5 Mbps: avg. frame rate: 12.7fps, scene reuse ratio: 92.3%, and avg. latency: 321ms. At 1 Mbps: avg. frame rate: 3.6 fps, scene reuse ratio: 96.2%, and avg. latency: 332 ms.

Concern 2: Latency increase with lower bandwidth?

A2: As listed in A1 above, under 5 or 1 Mbps, latency does not increase. This stability is due to:

(1) TeleOR adjusts the scene reuse ratio and point cloud density based on bandwidth to ensure real-time streaming. Thus, limited network conditions mainly affect the frame rate instead of latency.

(2) Latency does not affect visual smoothness but only the time gap perceived by remote doctors. A ~300ms delay is completely manageable within a surgical context.

Concern 3: 3D metrics for reconstruction quality assessment

A3: Due to lack of well-defined 3D quality metrics, in this work, we develop MSSIM for 3D evaluation by assessing the perceived quality from multiple predefined viewpoints. Moreover, as TeleOR presents a 2D image to the user, whether via VR headset or screen, assessing the quality of 2D renderings provides a direct measure.

Concern 1: The nature of TeleORâs contributions, i.e., is TeleOR a physical system or an algorithm?

A1: First, TeleOR is a physical system that incorporates hardware components, including depth sensor array, graphic processing unit, and VR headset. It is designed to capture, stream, and display a reconstructed OR scene in real-time. Second, this paper focuses on technical innovations that facilitate the real-time operation of this physical system.

Concern 2: Improvements needed in literature review and detailed suggestions (title, keywords, terminology, etc)

A2: Thank you for your valuable feedback! Weâll refine the literature review for clarity, and incorporate your suggestions throughout this paper into the revised version to ensure that it meets the highest standard."
https://papers.miccai.org/miccai-2024/765-Paper0728.html,"Reply to R1: 
1.Background of Clinical Data:Previous research has shown that learning clinical time series is effective for predicting HCC. And our dataset collects 15 years of clinical data. It includes 46 clinician-recommended clinical parameters, which are closely linked to liver disease severity.
2.How Missing-Aware Prompts Work: The missing-aware prompts are the learnable vectors. We determine the optimal prompt number by hyperparameter search. Instead of recovering missing information, the learnable prompts are utilized to adapt the model to the different input distribution caused by missing modality via a particular tokenization.
3.External Validation: We evaluate our method in a territory-wide large dataset from all public hospitals and clinics.  The performance already reflects our generalized ability in different institutes. We would like to conduct further experiment to validate the cross-region performance."
https://papers.miccai.org/miccai-2024/766-Paper3866.html,"We thank the reviewers for the constructive feedback. We present a method to learn better feature representations for survival analysis by including time and event labels during contrastive pretraining and will release code/metadata upon acceptance. We appreciate that the reviewers found our approach novel (R3/R4), interesting (R1/R3/R4), and that our experiments demonstrated superior results (R1/R3/R4) compared to no-pretraining (DeepHit) and self-supervised baselines. Below we respond to critiques raised by the reviewers

SUPERVISORY SIGNALS (R1/R4)
Novelty (R1)  While the use of supervisory signals in SSL has been explored in other domains (mostly in non-medical imaging), existing methods do not consider the time-dependent nature of progressive data such as Alzheimerâs Dementia. To the best of our knowledge, incorporating time labels to capture time-dependent features has not been explored within the context of survival analysis. We will further highlight this to improve the clarity of our methodâs explanation

Similarity to prior work (R4) We couldnât find a reference to y-simCLR in MICCAI21, but we believe ây-Aware InfoNCEâ (Contrastive Learning with Continuous Proxy.. [MICCAI21]) was intended. It uses proxy data such as age to improve representations for classification but does not consider disease progression. While it may be somewhat similar to the E-SSL method, we believe that incorporating time information makes our proposed TE-SSL novel and unexplored

EVALUATION (R1/R3/R4)
(R1/R4) Using temporal information to guide contrastive learning for progression analysis is novel, so we have no existing methods to compare against. However, we evaluated our proposed TE-SSL against DeepHit (standard approach in deep learning-based progression analysis), SSL (no supervisory signals), and E-SSL (adding supervisory signals but not used in prior progression frameworks). While other approaches use supervisory signals to guide SSL, these methods do not explicitly capture time progression and may not be suitable for survival analysis. R4 also added that more downstream tasks are required for validation, which we attribute to the limited datasets and tasks related to progression analysis

(R1) We find the suggestion regarding detailed case-by-case comparisons interesting and will include such analysis in future work. For feature-space visualization, we have already provided a t-SNE visualization of the learned features (Fig. 2)

Statistical Significance (R3) As shown in the main results (Table 1), across 3 random runs, our method improves over the existing baseline (i.e., DeepHit) by 7% for C-td and 11% for IBS. Even against SSL-based baselines, we achieve considerable improvement across both metrics. While we acknowledge that our analysis would be stronger with significance tests (which we canât show due to MICCAIâs no experiment rebuttal policy), we believe the improvement is due to our model learning time and event features, not randomness

OTHER COMMENTS
More figures (R1) Since we are relying on standard DeepHit architecture, we only presented the overall framework in Fig 1

Naming of TE-SSL (R3) We will emphasize that our framework is based on a contrastive learning objective in our final submission.

(R4) We recognize that some fundamental terms (e.g data censoring) were not explained clearly. Further, although we described some notations (e.g z_i) in our equations, they were not made clear in Fig 1. We will fix these in our final submission, also clarifying how longitudinal data is handled in SSL (each visit is a unique data point)

(R4) With Eq 3, we learn representations among scans with the same event while also strengthening alignment between scans at similar stages in development. This is enforced softly (unlike supervised learning) in SSL, allowing the framework to cluster patients with similar disease progression together

Limitations (R1) We will include discussion on limitations in our final submission"
https://papers.miccai.org/miccai-2024/767-Paper0960.html,"We thank all the reviewers for their constructive comments, which have been carefully addressed as follows:

Q1: Hyper-parameter optimization (R1, R3)
A1: In our experiments, the hyper-parameters ð1 and ð3, which control the consistency loss and contrastive learning loss respectively, are set to small values (0.1) based on existing semi-supervised works to align their gradient scales with the supervised segmentation loss. For ð2, which controls the pseudo-label supervision loss, we varied the value from 0 to 1.0 in steps of 0.05. We found that a smaller value of 0.1 for the first dataset achieved the best performance, while for the second dataset, a moderate value of 0.5 yielded better results due to its higher segmentation difficulty, which required a larger weight to learn more from the unlabeled data.

Q2: Statistical analysis for all comparisons (R1)
A2: We conducted paired t-tests to verify the significance of our improvements. Results on two datasets indicate that p-values on both Dice and MIoU are less than 0.05, demonstrating that the improvements achieved by our model are statistically significant.

Q3: Conclusion regarding figure 3 (R1)
A3: Sorry for any confusion. Figure 3 analyzes the T-SNE decomposition of the representation space with and without PGCL. Red points represent foreground classes, and blue points represent background classes. With PGCL, training shows better intra-class compactness and inter-class separability, forming two clusters despite some boundary confusion. Without PGCL, red and blue points remain scattered with no clear boundaries. We have updated the manuscript with a more detailed explanation to support our conclusion.

Q4: Ratio of the labeled data (R3)
A4: In our experiments, we examined the ratio of labeled data from 0.05 to 0.95 in increments of 0.1 under a semi-supervised setting, as well as a fully-supervised setting at 1.0. We noted significant improvements in our evaluation metrics from 0.05 to 0.75, beyond which the performance plateaued up to 1.0.

Q5: Generality of the algorithms (R3)
A5: While we conducted experiments on two datasets related to lung infections, these datasets encompass both X-ray and CT modalities, which partially demonstrates the generality of our proposed method. In the future, we plan to extend experiments to more datasets to further validate the effectiveness of Textmatch.

Q6: Confusion of âMulti-Viewâ (R3)
A6: Sorry for any confusion. We referenced the naming conventions used in previous works for different augmented forms of images. We believe it is reasonable to transfer this concept to the context of generating different augmented forms of image-text pairs. In our revised manuscript, we have added further explanations regarding the term âmulti-viewâ to clarify its specific meaning and avoid misunderstandings.

Q7: Poor case of text generation (R4)
A7: We utilize the large language model (LLM), specifically GPT, for text generation. The specific task involves generating text that is structurally different but semantically similar to a given original text. We achieve this by providing strict and precise prompts, and by setting contextually appropriate guidelines and filters to maintain high-quality output.

Q8: Novelty of main components (R4)
A8: While some components of our method are inspired by existing works, our key contribution lies in the innovative integration of text prompts into these components and the corresponding improvements. We designed a Bilateral Prompt Decoder to harmonize visual and linguistic features, enabling comprehensive multi-modal representations. Our Multi-views Consistency Regularization uses both image and text perturbations to reduce noise and produce high-quality pseudo labels. Additionally, our Pseudo-label Guided Contrastive Learning strategy refines the feature space, enhancing class-discriminative feature learning.

Q9: Open access to source code (R1, R3, R4)
A9: Source code will be released upon formal acceptance."
https://papers.miccai.org/miccai-2024/768-Paper1293.html,"Thanks for reviewers & ACs. Code & point-annotated data will be released.

To R1: 
[Text & impact of image w/o polyps] To simplify, a generic text is used to represent all polyp images. Our focus is on polyp segmentation rather than classification tasks, thus all used images contain polyps. In future work, if images without polyps are included, we will offer appropriate text cues for evaluation purposes.

[SAM-based mutual learning] We utilize a segmentation model to produce the base predicted maps, which serve prompts for SAM to generate SAM-based maps. SAM-based maps provide supervision for the segmentation model. In this framework, the base segmentation model and SAM establish a mutual learning process, with SAM offering supplementary guidance beyond points. Also, the final results are derived from the base segmentation model.

[Certain choice] a) The setting to fine-tune the last two layers of the encoder is based on a comprehensive consideration of modelâs learning capacity and stability. We have tested via fine-tuning different layers, while fine-tuning the last two layers of the encoder obtains better results. Also, the SAM decoder is relatively simpler compared to the encoder, thus we directly fine-tune the whole decoder. b) Gamma correction is not mandatory, alternative image transformation methods can be used. When employing methods like Logarithmic Transformation, we still achieve promising performance.

[Comparative analysis] Due to space limit, we are unable to provide results from fully-supervised methods (refer to [3,6,30]). We aim to propose a plug-and-play point-supervised polyp segmentation framework that has been validated for compatibility with various backbones and models.

To R3:
[Errors] We will revise them in the final version.

[Illustrations] In Fig. 2, the caption is as follows: an image is initially input to the base segmentation model to generate S_base, which serves as prompts for SAM along with the original and gamma-corrected images as inputs, resulting in SAM-based maps (S_ori and S_gra). As a result, the base model and SAM form a collaborative learning framework, with SAM offering supplementary supervision signals beyond points. More details will be included in the illustrations in the final version.

[Compared models] In fact, we have included some recent methods (SCOD 2023 & PSOD 2022). Note that our TextPolyp is a plug-and-play module, and its effectiveness in seamlessly applying to various commonly used backbones and classic polyp segmentation methods has been validated.

[Effect of \alpha] Follow [10] and tune \alpha with various values, we set it to 0.85 for optimal results.

To R4:
[Grounding DINO] We use Grounding DINO to produce the coarse bounding boxes, then we utilize point annotations to refine these boxes, reducing the effect of inaccurate boxes. Due to page limit, we will study its effect in future work.

[Point annotation & box elimination] Point annotation is based on the approximate center position of each polyp. The labeled dataset will be publicly. We utilize the foreground point p_a within the point label to identify the box containing polyps and exclude misidentified boxes, and use p_b to eliminate extraneous boxes.

[SAM-Med2] We focus on validating the effectiveness of our TextPolyp, rather than delving into the effects of different SAMs. In future work, we will study the effect of integrating SAM-Med2 into our method.

[Fairness] There are few available methods with point supervision, especially in the medical field. Thus, we expanded the scope of comparison to include some weakly-supervised methods originally designed for scribble annotations. Point annotations provide less supervision compared to scribbles, leading to a decrease in performance. Overall, we use the same point annotations for all compared methods in a relatively fair manner. In future work, we will also extend our model to scribble-label manner and compare it with scribble-based methods."
https://papers.miccai.org/miccai-2024/769-Paper1810.html,"We sincerely thank the reviewers for their professional suggestions and appreciation of our work. All reviewers agree that this paper is novel, well-written, and valuable. We give answers A to all the questions Q below. 
Reviewer#1:
Q1: Marginal improvement on the results
Thanks! 1) As observed in the previous studies [3,13,15], this task yields stable metrics with relatively small improvements. 2) Despite some metrics show small improvement, our model has achieved significant enhancements across most metrics, demonstrating the overall effectiveness of TISR.
Q2: Computation cost for TISR
1) The training time varies for different baselines and datasets, but it increases by no more than 30% compared to the original time. 2) TISR is integrated solely during training, so that the inference time remains identical to the original network.
Q3: Clarity in experiment setup
1) Clinical efficacy is evaluated using precision, recall, and F1, which can be obtained by comparing the labels from the generated reports to the ground truth, allowing us to assess the modelâs performance in identifying important medical conditions.
2) CheXbert[26] is widely used as an automated deep-learning based chest radiology report labeler that can label for 14 medical observations[3,4,5,13,15,17,22,27]. By inputting the generated reports and ground truth into CheXbert, we can extract labels for 14 important medical conditions from each report.
Reviewer#3:
Q1: Clarity and explanation of $f_e$, $f_d$, and $f_l$
Sorry for the oversight. $f_l$ is a linear layer that maps the outputs of decoder to the vocabulary size, with input dimension equals to the dimension of the decoder output and output dimension equals to the vocabulary size. $f_e$ and $f_d$ can be different according to the baselines[10,16,27,29,30]. In this paper, $f_e$ is a ResNet101 pretrained on ImageNet. $f_d$ varies depending on the baseline used[3,4,5]. Weâll add these details in the final version.
Q2:Computation of $Oâ$
We sincerely apologize for our negligence. $Oâ$ is computed by decoding $Pâ$ and $Iâ$, We will correct it in the final version.
Q3:Decoding process
1) In Eq(1), $O$ is obtained by decoding the text features $T$ and image features $I$. Similarly, $Oâ$ is obtained by decoding the refined text features $Pââ$ and image features $Iâ$. Thus, the same decoder can be used for both tasks. 2) Additionally, using the same decoder allows the network to share parameters, which benefits network optimization and improves the efficiency during inference.
Q4:how these components are separately evaluated
1) The validity of text inversion is assessed by calculating the contrastive loss between $I$ and $T$ in the self-supervised refinement. 2) Self-supervised refinementâs validity is assessed by calculating the contrastive loss between $Iâ$ and $P$. 
Q5:Implementation details and code availability
Sorry for our negligence. MLP in Eq(6) contains two linear layers and ReLU. Weâll include this detail in the final version. Code will be released soon!
Q6: Implement of baselines
All baseline results were reproduced using the official code provided by the authors on GitHub, without any modifications. Most of the metrics we obtained are actually close to those reported in the original papers[4,5].
Reviewer#4:
Q1:Explanation of methods
We apologize for any confusion caused and will address this in the final version by providing more details, such as input and output dimensions, module structures, explanations of equations and more.
Q2:Comparison with newer methods
Thanks for your suggestion! We conduct experiments on CvT2DistilGPT2[22], which used gpt-2, and observed performance enhancement.
Q3:Experimental details
We apologize for the oversight. We will add further details in final version. For specific settings of the image encoder and text decoder, please refer to R3Q1.
Q4:Insufficient settings for ablation experiments
Weâll add more explanations and settings for ablation experiments in the final version."
https://papers.miccai.org/miccai-2024/770-Paper1081.html,"We appreciate the provided feedback, and acknowledge the presence of notation errors in our submission (these did not affect the correctness of our formulation and have been fixed now, see below). We also regret having to dedicate a large portion of our rebuttal to responding to a series of unsubstantiated, subjective, and occasionally incorrect criticisms from R4. We attempt to address as many valid improvement suggestions as possible in the second part of this letter.

R4: âThe performance of the method should be evaluated on metrics such as sensitivity, specificity, accuracy, AUC, FPR, MCCâ, âOnly loss values are compared, which do not indicate the effectiveness or accuracy of the methodâ:
R4 is wrong here. The metrics advised by R4 are invalid for assessing topological correctness and are unsuitable in imbalanced scenarios like vessel segmentation. The community consensus is to use DSC in conjunction with cl-DSC; please see pg. 198 in Maier-Hein et al., Metrics Reloaded, Nature Methods 2024. Also, we do not compare loss values, we report DSC and cl-DSC metric values.

R4: âThe model training follows pretty standard proceduresâ:
Of course it does. This is astrengthof our work, not a weakness. We propose a new loss function, not a new training method. We adopt well-established models and training procedures, like the nnUNet, and deliberately refrain from modifying them. Expecting authors to introduce novel training procedures for the sake of novelty alone is bad reviewing practice.

R4: âThe novelty and technique contribution is insufficientâ:
Subjective opinion with no references to support it.

R4: âThe authors replace the dice loss by CE loss that has been widely used in literatureâ:
R4 is wrong. We do not âreplace the dice loss by the CEâ, but improve topological consistency drawing inspiration from the clDice loss and addressing its weaknesses with a more robust formulation based on the CE loss over the skeletons of the target and prediction.

R4: âTraining parameters and procedure details are not providedâ:
The nnUNet framework automatically adjusts the training procedure. Our retinal vessel segmentation model follows the approach in âStateâofâtheâart retinal vessel segmentation with minimalistic modelsâ, with minor modifications. As noted in our paper, âExact training details are available at Githubâ. It is impractical to include routine training details and hyperparameter values on the paper due to the page limit, this is the purpose of code repositories.

General comments - Notation and equations:
Following R1 and R4âs feedback (thank you), we have revised our notation. Errors in equations were related to a last-minute change of notation, but the text was correct and has not been modified. Importantly, the code snippet was also correct. The necessary corrections were:

R1:

R3:

Qualitative examples: unfortunately, the page limit does not allow us to add these to the paper, we have uploadedalltest set segmentations to our Github repo along with the code to reproduce our experiments, and some visual examples on the landing page.

Table 1: the error pointed out was a typo when transferring results from a Jupyter notebook to LaTeX. We have reviewed all our numbers and found no additional errors, thanks a lot for spotting this one. We have added further clarification on how to interpret results in the caption for the readerâs convenience. The term âenforceâ has been replaced by âencourageâ topological correctness."
https://papers.miccai.org/miccai-2024/771-Paper2052.html,"We thank all reviewers for their positive and constructive feedback. We now clarify a few points raised.

Q.Motivation of direct inference (meniscus tear) from k-space (R3,R4)
As mentioned in the introduction, [14] shows biomarkers can be obtained directly from k-space. Full image reconstruction thus shouldnât be necessary, allowing for k-space savings that can lead to quicker acquisition or reduced field strength. Thus, we aim to directly optimize k-space acquisition for inference without reconstruction. Evaluating image reconstruction results are in scope but will change the inference mechanism. 
Using meniscus tear detection as the exemplar task demonstrates the applicability of a portable MRI device for sports events [10]. Subsequent experiments showed that we can also identify cartilage thickness loss as another task.

Q.Difference between patient and population-level mask (R4)
A population-level mask does not use any policy during acquisition but a prefixed mask. Our approach samples the mask one line at a time. Fig 5 does show masks between patients vary. Thus, a population-level mask would either over-acquire k-space lines for some patients or under-acquire for others. To improve clarity, we will include a population level mask for comparison.

Q.No fully-sampled data are needed for policy training (R1)
We believe the reviewer was confused. We claim in Secs 4.3 and 5 that our method does not require âhigh-fidelityâ fully sampled k-space data, which is necessary for Policy Recon. During training, the policy network may sample any possible line from the action space, which importantly can be low-quality k-space e.g. collected from low-field Portable MRI. Notably, the training of the policy and classifier happen offline. 
During inference, we do not assume existence of all k-space lines for the policy: it is the policy that dictates the sampling of the next line.

Q.How our method performs vs. using reconstruction quality as reward? (R1,R3)
Results in Fig 3 show that we outperform a reconstruction policy on specificity while matching its AUC and recall, providing competitive results. A key reason is the (un)certainty of the reward. As noted in Sec 4.3, p.4, our policy uses the noisy reward of the classifier, whereas the reconstruction policy uses a high-fidelity image and its label. Differences between Policy Recon and Oracle could be due to hyperparameter choices, but more likely, see footnote of p. 6, are the effects of denoising and smoothing on the reconstruction, which act as a regularizer for the Oracle classifier.

Q.Computational cost of patient-level (R4)
Obtaining a patient-level mask does incur a computation cost vs a random undersampling mask, but this is not double and does provide better performance with identical acquisition time. In fact, optimizing the mask shows better results at the same sampling rates compared to random undersampling mask in Fig 3. [2,11] also evidence in image reconstruction, patient-level masks achieve better performance for the same sampling rate vs a random mask.

Q.Why choose [2] as a baseline? (R3) 
We agree [11,20] can serve as baselines, but we opted on [2] for a fairer comparison. Our method uses the same greedy policy sampler backbone as [2], allowing a direct comparison of k-space acquisition effects between reconstruction and our direct inference. As noted in Sec 5.1, p. 6, [2] also downsamples the dataset for computational expedience. Including [11,20] is possible, but their different policy network designs deviate from our main hypothesis: saving lines by avoiding reconstruction, thus out of scope for this work.

Q.Two stage vs. simultaneously training? (R4)
Indeed, policy performance depends on the pre-trained classifier as it ensures a stable environment. However, any bias or shift in the classifier will propagate to the policy decisions. We are currently investigating joint training of these components (e.g., by fine-tuning the classifier) to address domain shifts."
https://papers.miccai.org/miccai-2024/772-Paper0480.html,"We thank the reviewers for their constructive and comprehensive feedback. The reviewers appreciated a clear and justified motivation, the clinical relevance, the thorough validation and the well-written paper. Below, we address the main comments:

Data distribution [R1]. While the distribution of the test set seems to deviate from a typical clinical scenario, the prevalence of drusen can vary strongly depending e.g. on age group (e.g.  ~30% for 20-24 years; to ~49% for 45-49 years; ref1). Another study [ref2] reported even higher drusen frequency. While we agree that matching the distribution of the clinical application setting is important, in this case we therefore used the official test split provided with the dataset [13], which has ~250 images per class. We also reported the classification results on the validation set, which was less balanced (73% healthy vs 27% drusen).

Additional performance measures [R1, R4].  Thank you for asking. We had already computed additional measures, but not reported them in the text. Precision and recall on the test set for ProtoBagNet were 0.996 and 0.940; and 0.999 and 0.996 for ProtoPNet. We added these values to Table1.  AUC values (0.9918 vs 0.9916) for the performance on masked inputs are in line with our verbal description (Sec. 3.5), and we added them to the text.

Performance and interpretability tradeoff [R1, R3].  A good tradeoff between performance and interpretability is crucial for clinical usefulness. We showed that Proto-BagNets perform slightly worse than black-box ResNets and ProtoPNets, which both are less interpretable than our ProtoBagNet. The lower performance comes from loss terms that enhance interpretability but compete with performance. Determining the ideal tradeoff will depend on the specific clinical setting. We rephrased a few sentences to make this clearer. If adding CIs is within the rebuttal guidelines, we are happy to do so. They are generally tight.

Misleading prototypes [R1]. One of the main challenges we encountered initially was prototypes that were redundant and less clinically relevant. We mitigated these issues with a dissimilarity loss term to address redundancy and a sparsity loss term to use only the most relevant concepts, two key contributions of our paper. Now we find very few âmisleading prototypesâ, only some âunexpected prototypesâ which do not contain well-known concepts of drusen (Suppl Fig. 3).

Effect of receptive field size [R1, R4]. The appropriate receptive field sizes may vary depending on the clinical task and image resolution. In our case, drusen are small (< 63Âµm [ref1,ref2]) and fit into a patch of 33x33. For other tasks, the receptive field can be changed to inject clinical knowledge and adjust for resolution. Due to rebuttal rules, we defer a detailed analysis to future work.

Hyperparameters [R3, R4]. We reduced the complexity of our experimental setup by using hyperparameter settings suggested in [2, 4, 13] where possible. The hyperparameters related to our contributions were chosen based on a grid search on the validation set as described. However, we noticed that we did not report the final choice for lam_L1_x = 0.04, and , lam_diss = 0.005. Due to rebuttal rules, we defer a detailed analysis  to an extended version.

Writing and presentation [R3]. While the clarity, motivation, and presentation of our work were generally appreciated (R1, R4), we followed the suggestions of R3 to improve the paperâs presentation.

Reproducibility and sample selection [R3]. The data split has already been provided in the anonymous repository. The annotations will be available. The 40 annotated images were randomly selected from the drusen class of the test set. We noticed that 3 of them were misclassified by our model (although most of the topK relevant regions from each prototype highlighted concepts of drusen while a few of them highlighted unknown concepts)

[ref1] https://shorturl.at/qtMTV 
[ref2] https://shorturl.at/bdwFJ"
https://papers.miccai.org/miccai-2024/773-Paper3759.html,"We appreciate the reviewersâ valuable comments. If the paper is accepted, we will make the following revisions.

Availability of implementation (R1, R2, R3): The source code is now publicly available on GitHub and will be included for the camera-ready paper.

Method Novelty (R1, R3): We thank R1 for identifying previous work that appears to overlap with our research. We believe there may be some terminology differences that have led to confusion about the relationship between our model and these studies. In graph model literature, there is a concept of creating embeddings using multiple âviews,â which entails using different types of node information (e.g., graph structure and feature content). In our case, we are looking at different images that give different âviewsâ or perspectives of a nodule that are represented as unique graph nodes. Thus, the papers suggested by R1 do not attempt similar tasks to our model (i.e., aggregating information across an imaging study); three of the papers did not involve imaging and the fourth (MLMSeg) applies graphs for segmenting single thyroid ultrasound images. We understand the confusion of this terminology and will clarify the text in our amended manuscript.

Comparative/Ablation Experiments (R1, R3): This classification task across an imaging study requires two steps: feature extraction and aggregation across images. During development, several feature extractors were tested, but results were excluded due to space. In terms of aggregation strategies, the Wang and MS-AMIL results demonstrated alternative ways of combining features across images. We excluded the results of trivial aggregation strategies (e.g., feature averaging) as they predictably did not work since most images in a study do not include nodules. Space constraints prevented further ablations on anatomical context. One excluded ablation built graph edges based on feature similarity instead of anatomical adjacency, but it underperformed and was omitted to streamline the narrative.

Method Implementation Details (R1): The x, y, and z coordinates mapped the ordinal anatomical locations into Cartesian coordinates (e.g., right lateral -> x=1, etc.). This mapping was meant to help with clarity; if it is adding confusion, it can be removed. Other details were omitted for space, particularly those related to feature extraction, because that stepâs architecture mirrored the already published ThyNet paper. More details will be included in the final version.

Absence of Evaluation on Public Benchmarks (R2): We agree that public benchmarks are important to standardize results and reproducibility. However, current thyroid ultrasound benchmarks (including TN3D and DDTI) are restricted to single images hand selected by experts, and not full imaging studies. Because ThyGraphâs use case is to spatially align an entire image study and aggregate information across images to make a malignancy risk assessment, there are no standardized benchmark datasets.

Limitations (R3): Limitations will be emphasized in the final manuscript. As for label availability, labels were automatically extracted from patient records using NLP, so no manual labels were required. While the NLP method was not the paperâs focus, we realize this step is valuable for future reproducibility, so the label extraction code is included in our public codebase.

Advantage of GCN over Transformer (R3): This study aims to aggregate input information from multiple ultrasounds, considering their spatial relationships and orientations, to make a diagnosis. Graph-based methods excel at modeling spatial dependencies and encoding anatomical information, helping a GCN capture physiological correlations between images. While Vision Transformers can use positional embeddings, they are not inherently suited to model spatial relationships, which are more critical in this medical context. Vision Transformers are still effective for feature extraction and can be explored in future work."
https://papers.miccai.org/miccai-2024/774-Paper2191.html,"We sincerely thank the reviewers for their valuable suggestions and their recognition of our approachâs innovation, effectiveness, adaptability, and verifiability, as well as the clarity and organization of our paper.

Respond to Reviewer #1: Thank you for your recognition and suggestions on this work, which are very insightful! We acknowledge the reviewerâs typo correction and will make the necessary amendments. We have enhanced the descriptions of tables to improve readability. Additionally, we will elaborate on the design principles of the CMRF module as follows:  GELU is an activation function based on the Gaussian error function.   Compared to ReLU and other activation functions, GELU is smoother and enhances both the convergence speed and performance during training.   GELU has been widely used in medical image segmentation models [1,2], and its application here is not unique.   Halving for cascading operations utilizes redundant channel information, a cost-effective and feasible approach inspired by PConv and Ghost (see section 2.1 of this paper), which employ similar strategies.   The odd/even channel split is derived from the channel shuffling operation and is informed by observations in Ghost, where adjacent channels may exhibit similarity.   This partition efficiently leverages such similar redundant information.   Doubling the number of heads at the end of the CMRF module aligns with the bottleneck structure design.   PWConv-BM-Act is used to fuse information from multiple receptive fields and to adjust the number of output channels.   At the beginning of TinyU-Netâs bottom design, we added a CNN block and observed a slight decrease in segmentation performance, while adding a CMRF block resulted in no significant improvement.   Therefore, we opted not to add a functional module at the bottom, leaving room for future exploration of the TinyU-Netâs bottom structure.   Lastly, in future work, we plan to extend TinyU-Net to clinical settings for feedback from medical professionals.
Respond to Reviewer #3: Thank you for your recognition and suggestions on this work! We adopted the reviewâs recommendation to describe momentum decay more clearly. Regarding implementation details (such as learning rate and optimizer), we employed a standard approach consistent with many published papers [3,4]. We ensured that there was no data leakage between the training and test sets. The lesion segmentation of ISIC2018 dataset in Table 1, TinyU-Net is 52Ã, 3Ã, and 194Ã fewer parameters, respectively, while being +3.90%, +3.65%, and +1.05% higher IoU score than baseline U-Net, lightweight UNeXt, and high-performance TransUNet, respectively.   The results show a surprising increase in performance while achieving a profound reduction in a number of learnable parameters and computational requirements.
Respond to Reviewer #4: We thank the reviewers for their recognition and suggestions! We provided a more descriptive caption (i.e., Comparative qualitative results on ISIC2018 (first tow lines) and NCP (last tow lines) datasets.) in Figure 2 to improve clarity and make it more understandable to the reader.

[1] Ruan, J., Xie, M., Gao, J., Liu, T., & Fu, Y. Ege-unet: an efficient group enhanced unet for skin lesion segmentation. In MICCAI (pp. 481-490). Cham: Springer Nature Switzerland. (2023)
[2] Han, Z., Jian, M., & Wang, G. G. ConvUNeXt: An efficient convolution neural network for medical image segmentation. Knowledge-Based Systems, 253, 109512. (2022)
[3] Cheng, J., Gao, C., Wang, F., & Zhu, M. âSegnetr: Rethinking the local-global interactions and skip connections in u-shaped networks.â In MICCAI (pp. 64-74). Cham: Springer Nature Switzerland. (2023)
[4] Lin, X., Yan, Z., Deng, X., Zheng, C., & Yu, L. ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation. In MICCAI (pp. 642-651). Cham: Springer Nature Switzerland. (2023)"
https://papers.miccai.org/miccai-2024/775-Paper3610.html,"We thank all reviewersâ valuable comments and suggestions. Our responses to main questions are summarized and addressed as below.

[R1 & R3 & R4]

[R1 & R4]

[R1]

[R3]

[R4]

All minor comments and suggestions will be carefully addressed in our revised manuscript, and the code will be published if the paper is accepted."
https://papers.miccai.org/miccai-2024/776-Paper1737.html,"Dear Reviewers,
Thank you for your valuable comments.
We will ensure that the code is clearly organized and accessible on GitHub, with the link provided in the camera-ready version. Additionally, we will consider your advice when we extend this conference paper to a journal submission. This includes adding more experiments on different imaging modalities and other brain datasets.
Thank you again for your insights."
https://papers.miccai.org/miccai-2024/777-Paper3284.html,"Summary: Thank all the reviewers for acknowledging our methodological contribution. We are pleased that they find this work interesting (R1), of good reference significance (R3), and innovative (R4). Our responses are as follows.
Q1 (R1): âOverfit issueâ A1: The overfit issue is common when the training samples are limited. To alleviate it, we adopted the Random Horizontal Flip as a data augmentation operation to enlarge our dataset during the model training.
Q2 (R3): âConstruction of the Adjacency Matrixâ A2: The adjacency matrix is constructed by the spatial relations among the six hip landmarks. Since the topological relations will not be changed within the BUS images, the corresponding adjacency matrix will also keep unchanging for any DDH case (normal or abnormal). Additionally, such a design will help refine the generated heatmaps by ICF subnetwork instead of harming the learning process.
Q3 (R3, R4): âBCE lossâ A3: We introduce the BCE loss between ground truth labels and predicted classes in the TGCN subnetwork for further refining the heatmaps generated by ICF subnetwork. The classification labels can provide guidance for the graph subnetwork, so as to enhance the landmark detection performance.
Q4 (R1, R4): âDDH datasetâ A4: According to the Grafâs method, the Î± and Î² angles are two key metrics to diagnose the normal/abnormal of the hip joints (Normal if Î± > 60Ë and Î² < 77Ë). Our dataset includes 500 BUS images (458 normal subjects and 42 abnormal subjects). Each image is scanned from either the right or left hips of infants, as this distinction does not affect the detection performance. It worth noting that the data is unevenly distributed, which is typical in real clinical settings. However, our TGCN-ICF achieves superior results to SOTA compared algorithms, indicating its effectiveness and robustness. We acknowledge that we do not focus extensively on the DDH classification in this work, as the primary aim is to improve the detection accuracy of hip landmarks.
Q5 (R1, R4): âThe following angles calculationâ A5: Due to the page limit, we are unable to include extensive additional results in the manuscript. In future work, we will focus on developing a complete model together with experiments for DDH diagnosis, which is not limited only to detect the key points from hip BUS images.
In addition, we have carefully checked and revised some errors in the manuscript, including improving the resolution of all figures.
Thank you again for all the valuable comments!"
https://papers.miccai.org/miccai-2024/778-Paper1110.html,"We appreciate the reviewersâ evaluation and constructive feedback. We address the major points raised in their comments.
-Clarity and organization (R1, R4)
We agree that paper readability could be improved with some reorganization, but we believe it can be achieved without major modifications. We have adjusted the following:
Main idea: Sec 3.3 now starts with: âColonSLAM receives a linear topological graph formed by all submaps from CudaSIFT-SLAM. The main idea behind ColonSLAM is to identify which submaps represent the same colon location, merging them into the same node. This observation capability builds traversability links between distant nodes, resulting in a richer graph than the linear one.â
CudaSIFT-SLAM: explanations have been unified in the Sec 2, easing the understanding and reducing redundancies.
Comparison of image descriptors (R1): descriptor similarity is obtained by the MLP of our L network. We have slightly rewritten Sec 3.2  for clarification: âOur localization network L predicts if two images come from the same place or not, and we use it to determine if the incoming submap is already included in the map. The network  is composed of a backbone and a 5-layer MLP. The backbone is initialized from the endoscopy foundational model EndoFM, which, for an image I extracts a global descriptor d \in R^768. To decide if two images I_A, I_B come from the same place, we subtract their descriptors $g = d_{A} - d_{B}$ and feed g to the MLP followed by a softmax, predicting if they are similar or dissimilar. We fine-tune the last two layers of the backbone and the MLP using a cross-entropy objective. Training details are explained in Sec. 4.1.â
We have formulated the acceptance conditions in 3.3 as numbered equations for easy reference.

-Reproducibility (R1, R4)
We have included a figure detailing the network architecture. Upon acceptance, we will release ColonSLAM source code, the trained models and the submaps extracted by CudaSIFT-SLAM, so results can be fully reproducible.

-Varying thresholds for different methods (R1)
We have explained it in Sec. 4.2: âWe apply a different threshold for each of the networks as the score distribution given by each network is different. To allow a fair comparison between them, we tuned the best threshold for every network in terms of precision-recall performance.â

-Conclusions (R4)
Conclusions have been modified: âWe have presented ColonSLAM, the first topological SLAM able to build rich graphs of the whole colon, capturing the complexity of the colonoscopy exploration. Leveraging on our robust localization network and guided by topological priors, ColonSLAM is able to reliably build a graph by finding traversability and covisibility connections between distant nodes. The graphs obtained with ColonSLAM will serve as personalized patient maps, paving the way to assisted navigation and disease monitoring in colonoscopy. In future work, we will focus on finding even longer term relationships i.e. entry-withdrawal and second explorations of the same patient as they are a limitation for ColonSLAM. Finding these long-term correspondences is the key to the building and exploitation of personalized patient maps.â

-Other comments
(R4) Sequence selection and labeling. We have explained this in Sec. 4.2: âWe chose the same sequences as ColonMapper, the closest work to ours, easing the comparison. Labeling was done following the text footage available in the Endomapper dataset, created by the doctor during the exploration.â
(R1) Table I. All approaches except LightGlue[â¦.] (pag 7) corrected to âAll approaches improve their precision when the topological prior is applied.â. (R1,R4) We have included âBold: best. Underlined: second bestâ in the table caption.
(R4) Connection with ColonMapper, stated in Sec 2: âColonMapper builds the map and afterwards localizes. Our ColonSLAM performs a proper topological SLAM, simultaneously localizing and updating the map for each new incoming submap.â"
https://papers.miccai.org/miccai-2024/779-Paper0582.html,"Dear Reviewers and ACs,
we sincerely appreciate your insightful reviews and that you find our method âinterestingâ (R1) and âeffectiveâ (R3). We appreciate that you identify our experimental validation as âthoroughâ (R4) and ârigorousâ (R1), our quantitative results as âstrongâ (R3), and that the âquality of the writing [â¦] was strong throughoutâ (R3).

Practicality & runtime (R1,R4)
Similarly to binary homology-based loss functions, the barcode computation scales cubical (O(n^3)) with the number of pixels and dominates the complexity (see [24], suppl.). Our multiclass extension scales linearly with the number of classes (Sec. 2), which holds for all extended baselines. The runtimes of one training (150 epochs, equal hyperparameters, OCTA-500) are: 28m10s (ours), 59m27s (HuTopo), 17m17s (clDice), and 16m56s (Dice). We are convinced this increase is acceptable for obtaining topologically accurate models and have now included runtime in the manuscript.

Comparison to Byrne et al. (R3)
A comparison is impossible because their method relies on a fixed, a priori known topology of all samples. They filtered the ACDC data for slices with exactly the same topology. Contrarily, we use the complete data with varying topology, making all methods based on shape priors non-applicable. This is an important strength of our method. We clarified this in the manuscript. The multiclass extended Mosin loss [20] achieves inferior results (ACDC; BM score: 0.08, Dice: 0.85).

Hyperparameter (HP) selection (R1, R4)
We incorporate the BM score during HP optimization (see Eq. 5) because our goal is topological correctness. BM is the best topological metric because it exactly measures interpretable and spatially corresponding topological features [24]. Therefore, we believe it to be the most appropriate choice. Still, if we select HPs based on Dice, our method performs superior (e.g., Platelet; BM score: 1.0 vs. 7.7, Dice: 0.71 vs. 0.64 for ours vs. HuTopo). Selection only based on Betti number errors often yields insufficient Dice scores.

Significance testing (R1)
We present results on 4 datasets using 5 metrics. We found increased values in the metrics in most experiments and characterized performance differences using paired t-tests. We agree that textual performance descriptions should be concise and not overstated; consequently, we reviewed our description to strictly adhere to the results in Tab.1. Further, a typo error in Tab.1: our method does perform significantly better in BM than HuTopo on OCTA-500 (p0.002). We apologize and correct the error.

Ablation on weighting terms (R1, R3)
Our introduced weighting terms can be set independently (Eq. 4). The observation in Fig. 3 is specific to the OCTA-500 dataset and underlines that the weighting can drastically affect topological performance. However, the observed trend is not general but depends on dataset characteristics, as described in our ablation. Specifically, a weight of 0 was not used to generate the final results for any dataset. We added another dataset to Fig.3 and more details to the supplement.

Generalizable to other architectures? (R4)
Our topological loss function works without network priors and can be applied independently of architecture. We used U-nets because they are common in segmentation and for comparing topological methods [23,10,9,21]. Our results are robust to changes in model size with a std of 0.006 (Dice) and 0.03 (BM) across 6 different settings. A transformer-based architecture showed the same trend (ACDC; BM score: 0.07 (ours), 0.11 (HuTopo), 0.32 (Dice)).

Additional theoretical notions (R1)
We now include a definition and intuition of BM and formally define the two weighting components that amplify and attenuate matched and unmatched components. Moreover, we expanded the theoretical framework on BM in the supplement.

Reproducibility (R1,R4)
Code and all trained models will be published upon acceptance. HPs for each run are now included in the supplement."
https://papers.miccai.org/miccai-2024/780-Paper0150.html,"We sincerely thank all the reviewers for providing valuable comments and suggestions. Our response is as follows.

Q1: Regarding the performance comparison between SAMCL and multi-task learning. (R1, R5)

Q2: Regarding the difference between MER and SAMCL, and their performance comparison. (R1, R3, R5).

Q3: The paper does not manage to get the reader to a sound conclusion that the proposed SAMCL solves the forgetting problem. (R5)

Q4: Regarding the details on the experimental settings, e.g. preprocessing, hyperparameter tuning, and evaluation metrics (R1, R3, R5)."
https://papers.miccai.org/miccai-2024/781-Paper0073.html,"We really appreciate your positive evaluations of our paper.

Common Question:

To Reviewer 1 and Reviewer 4: Inaccurate description of AAT:
We apologize for the confusion caused by our description. The original purpose of our AAT was to address the issues of data insufficiency and scanning mode discrepancy. The transformation simulates the appearance of lesions and surrounding tissues in both scanning modes, but it does not generate real images. We will consider renaming the method in the future.

To Reviewer 1, Reviewer 3, Reviewer 4:

Fair comparison
In the method comparison, AAT is considered part of our overall ASTR framework. For a fair comparison, other methods are trained using the optimal strategies proposed in their original papers. However, in Table 3, we demonstrate that our proposed AAT has plug-and-play capabilities, effectively enhancing the performance of the segmentation model. Importantly, our method still outperforms the second-best model even when AAT is added to it (SLT-Net w/AAT). Additionally, Table 2 shows that each of our proposed strategies brings a positive improvement to performance, with the magnitude of improvement being similar.

Dataset details
During an endorectal ultrasound examination, the sonographer typically moves the probe around the lesion after it is detected to observe the lesionâs shape and depth of infiltration. Thus, in our dataset, each frame contains rectal lesions. We are collecting new normal cases and considering extending the dataset in the future. We will consider including other detailed descriptions and demographics in the final version.

Individual Question:

To Reviewer 1: Auxiliary loss
The auxiliary loss uses the ground truth annotations of the reference frame to supervise SCB in providing a more accurate coarse mask during training. The loss function used is the sum of cross-entropy, dice, and mean absolute error.

To Reviewer 3: Minors:
Thank you for your suggestion. We will carefully revise the sentences and correct any spelling errors in the final submission.

To Reviewer 4:

Coordinate center
Apologies for the misunderstanding. The center points for both polar coordinates and Cartesian coordinates are at the top center of the image.

Definition of d
Thank you for the reminder. The dimension d equals H times W. We will include this definition in the final submission."
https://papers.miccai.org/miccai-2024/782-Paper2862.html,"We thank all reviewers for appreciating the new dataset, method design, and clinical value.

1) Reply to R1

Details of Datasets: We will provide GitHub links for readers to download the dataset. Our dataset is the prerequisite for downstream tasks. For example, failure to detect faces, especially at critical moments of pain, can lead to incorrect pain score classification. In Section 5.4, we demonstrate how our dataset benefits downstream tasks, emphasizing that its value extends beyond just face detection.

Code: As stated in the Abstract, we will release the code.

Clarification on HCC: Our HCC and the Hierarchical Context Embedding (HCE) by Qiu et al. share a name but have different goals and methods. HCE targets false positives, while HCC aims to reduce false negatives. HCE embeds hierarchical segmentation features into detection features, while âHierarchicalâ in HCC refers to hierarchical classification, i.e., the first step of classification is added to improve the positive/negative anchor ratio in the second step. Moreover, HCCâs context comes from detection features, not segmentation features, and the way of embedding context in HCC is entirely different from that of HCE.

Data Splitting: we clarify that each neonate is associated with five images, and the dataset has been rigorously partitioned to maintain patient exclusivity.

2) Reply to R3

Method Evaluation: We limited our evaluation to the NFD dataset due to the scarcity of publicly available datasets. Neonatal faces vastly differ from adult faces, making evaluations on adult datasets an inaccurate measure of the NFD methodâs effectiveness in practical applications. We are also concerned about the fullness of our evaluation. To address this, we quantified the performance gain of our method for downstream tasks in Section 5.4, thus adding an additional evaluative dimension. Moreover, we conducted a comparison with RetinaFace (called NICUface-RF in [6] and is the baseline) and YOLO5Face, instead of just one method. Both are leading methods in adult face detection. Other methods, such as Poly-NL and ASFD, do not provide code. The NFD dataset is self-built, and it is hard to reproduce these methods on NFD without code.

Details of Datasets: Due to space constraints, we mainly describe the participant age, data annotation, dataset partitioning, and challenges embedded in the dataset; the remaining information is provided on GitHub.

Inconsistencies: We verified that the numbers in Table 2 and Fig.3 (a) are consistent rather than mismatched. Table 2 uses percentages, while Fig.3 (a) expresses values in decimal. We will standardize these representations to percentages and refine the consistency of the citations.

3) Reply to R4

Figure 2: We will add the definitions of the âxâ and â+â. As detailed in Section 4, we implement a feature pyramid where the classification, regression, and DIoU heads assess each anchor across all levels of the feature pyramid. Thus, the inputs to the three heads are the same, namely the Pyramid Feature Map.

Six-Level Hierarchy: This is the commonly used feature pyramid that follows the baseline design and is not the core of our work.

Obtaining Bounding Boxes Under Occlusion: Our method uses anchor-based detection with 102,300 anchors at a 640x640 resolution. Such dense anchors provide ample alternatives for bounding boxes, so the key to coping with occlusion lies in the classification accuracy of the anchors (avoiding missed detections due to occlusion) and the localization precision (selecting the ones with high localization precision among dense anchors). The two correspond to our proposed HCC and DAN, respectively.

Ablation Study: R4âs critique of the ablation study stems from a misunderstanding of our method, which we explained earlier. We discuss the overall influence of the proposed modules and fully analyze their detailed design.

Computation Platform: The platforms are mainly AMD 7950X, NVIDIA P40, and PyTorch 1.12 with CUDA 11.3."
https://papers.miccai.org/miccai-2024/783-Paper2481.html,"The reviewers questioned why the silhouette coefficients of the ground truth cluster are smaller than those of the visual/text embedding cluster shown in Fig. 2:
We would like to clarify that the silhouette coefficients for the ground truths are computed by first clustering the samples using either visual or text embeddings and then re-assigning the samples to the ground truth class labels. Hence, the clustering is still based upon the visual or text embeddings. These embeddings are obtained without utilizing the ground truth class labels. The difference in the silhouette coefficients indicates that the clusters by the visual or text embeddings include differing class labels, which is shown in Fig. 3. By definition, tissues belonging to the same class label share common histologic properties. However, borderline cases exist where tissues possess heterogeneous characteristics that can be related to multiple class labels. The fact that the visual and text embeddings achieve higher silhouette scores demonstrates that both embeddings are able to find the common characteristics among tissues that are slightly different from the ground truth labels. The visual and text embeddings can be understood as an alternative interpretation and explanation of the tissue samples, potentially providing more fine-grained information, e.g. 100 relevant terms in comparison to 4 class labels.Â

The reviewers asked whether the interpretability of our work is related to the knowledge granularity of the text in the pre-trained model and whether other pre-trained vision-language models have these kinds of properties:
The interpretability of the text-based embedding is based on the direct generation from natural-language pathology terms. In particular, we know exactly which terms are used to produce the text-based embedding as well as the weights in the combination that are based on the similarity scores. Therefore, any pre-trained vision-language models can be used without losing the explainability.Â

The reviewers raised a question on the effect of combining image embedding and text-based image embeddings on the classification performance:
The combination of visual and text-based embeddings slightly improves the classification performance. However, the combination has two main drawbacks. Firstly, additional weights are required to combine these two embeddings. Secondly and more importantly, the combined embedding loses the interpretability of the proposed text-based image embedding.Â

The reviewers asked whether the original text-based embedding from the pre-trained VLM already provides the interpretability that the proposed suggests:
The embeddings generated by the pre-trained VLMs are difficult to interpret due to their numeric form, which is why the traditional visual embeddings are unexplainable. Therefore, direct mapping from the embeddings to human-readable texts is required to understand the embeddings fully.Â

5.Â  The reviewers asked to provide a more detailed explanation of the methodology and experiment design to improve the readability of our work:Â 
In response to the reviewersâ comments, we will update the Methodology section to clarify the procedures and to improve the understanding and readability of our work in the final manuscript.

The reviewers suggested making comparisons with existing VLMs and relevant non-VLM approaches and ablation experiments to provide an in-depth evaluation of our work.
We appreciate the reviewersâ comments. Due to the MICCAI policy, we cannot provide additional experiments and results. We will leave these for future study.Â

The reviewers asked to change the color of Fig 1 since it is too light to read the content.Â 
We will update the color of Fig. 1 in the final manuscript."
https://papers.miccai.org/miccai-2024/784-Paper2587.html,"We appreciate the reviewersâ insightful suggestions.

Reviewer 1.

We chose to use the 1st and 4th for display purposes. This pair provides a better geometrical interpretation than the 1st and 2nd.

This statement is now corrected to read ââ¦ more accurately represent the diversity of cell shapes than prior manual-designed features proposed in reference [9].â

The statement is corrected to read ââ¦ making it resilient to variations in staining methodology and imaging modality.â

This statement makes a qualitative reference to the cell feature vector for each cell. We corrected the text to read âTo compute the region feature for a region containing a group of cells, we first query our database to retrieve âcell featureâ vectors for all cells within the region, followed by generating cumulative CDFs for these features.â

No. We do not need to query based on distance because all cell feature vectors are stored in the database.

Reviewer 3.

We apologize for any confusion. While our method generalizes for structure detection by fluorescent microscopy, the CNN does not (Fig. 5). However, the CNN performs slightly better in detecting structures on material prepared with the same stain as the training set; note higher CNN (orange) levels vs our method (blue) in Figure 4b. We now state, âThe average ROC AUC score for our method is 0.89, which is only slightly lower than the CNNâs average of 0.92 yet still represents a robust performance.â

We agree that there are likely to be potential improvements from transformer models. Yet the large sets that are typically required for transformer models are currently not available for whole brain neuroanatomy. A complementary issue is the explainable nature of our approach, which is not achievable with current deep learning methods.

Reviewer 4.

The reviewer correctly notes that each cell image can contain more than a single neuron. We tried different sized cell images and found that small images lead to neurons that overfill the box. Even if a small fraction of cell images contains multiple neurons, our use of regional features as a training set makes us insensitive to small numbers of errors.

We agree that this is a useful test, as different features are correlated. We have addressed the analogous issue of feature importance (Fig. 6). Note that we are not allowed to perform additional experiments for this submission.

This will be clarified in the final submission.

This too will be clarified.

Our comparison is against the CNN used by [6] and that work did not use cross validation. Again, we are not allowed to perform additional experiments for this submission.

While the performance of our method - in terms of generalization - is greater (Fig. 5), the computational time to build and train the models are comparable."
https://papers.miccai.org/miccai-2024/785-Paper1756.html,"We thank in-depth reviews and appreciate for affirming our contributions. The main concerns are addressed below.
[R1]-Claim of novelty, fundamentally different to domain adaptation
We believe that there should be some misunderstanding that causes the reviewerâs concerns regarding our methodâs novelty. To clarify, our XG-GNN is a domain-generalization (DG) method rather than a domain-adaptation (DA) one, due to which we did not compare with existing DA but DG methods in the experiments. As has been described in Sec-2.2, our meta-learning process operates exclusively on the source datasets; and on the target domain, we conduct inference without any model fine-tuning. So XG-GNN is strictly aligned with the setting of DG, which is practically more challenging than DA. In the context of DG, the key novelty of XG-GNN is to learn explainability to boost diagnosis, leading to significant improvements from both aspects. Weâll revise the manuscript to more clearly describe the differences between our method and other related works.
[R3&R4]-Reproducibility & significance of diagnosis results
In Sec-3, we have conducted three consecutive reproducibility experiments, each with evaluations on different unseen centers (i.e., different test datasets with varying distributions). The variance of evaluation metrics obtained by our XG-GNN is much smaller compared to other methods, indicating its reproducibility and robustness. On the significance of improvements, we can see that the margins between XG-GNN and other methods are relatively large in most cases. We are confident that such improvements are statistically significant, which will be further analyzed in the future.
[R1&R3]-Reproducibility & analysis of explainable results
In Fig 2, we segmented ROIs into seven brain network regions, and the consistently highlighted areas across reproducibility experiments revealed stably captured connectivity patterns tied to ASD. This observation aligns with previous findings in neuroscience research and holds valuable insights into brain development studies. Following the reviewersâ great suggestion, weâll update Fig. 2 and corresponding descriptions to showcase such reproducible explanation results from multiple centers, enhancing the persuasiveness of DG from the explainability perspective.
[R1&R3]-Description of XG regularizations
To enhance group-wise explanations and its cross-heterogeneous site generalization, we design dedicated XG regularizations under a very fundamental assumption. That is, independent of domain shifts, the group-wise connectome differences between ASD and TD are partially stable, and such differences are not whole-brain diffused (i.e., relatively sparse). Such an assumption is consistent with neuroscientific findings, and significantly improved the diagnosis performance according to our ablation studies. To support this design decision, weâll update the paper to provide more relevant references.
[R1&R4]-Competing methods
In response to the comparison with methods based on non-linear functional connectivity, we have trained two models based on non-linear functional connectivity scores for generalization (FcNet, FBNetGen), whose performance is lower than our XG-GNN. Weâll explain in detail.
[R1, R3&R4]-Contribution of each key component and ablation study
The reviewers requested a more detailed discussion on each key component, i.e., the MHSA-Based Graph Learner, GCN-Based Diagnoser, and XG regularization. These components are designed to play distinct roles: 1) MHSA-Based Graph Learner ensures non-local representation learning, which is intuitive for connectome analysis; 2) GCN-Based Diagnoser leverages specialized mechanisms of graph CNN for graph-structured data; and 3) XG regularization enhances group-wise explanation and its generalization to heterogeneous sites. We didnât include all experiments due to space limitations. Weâll further describe the design rationale citing relevant literature."
https://papers.miccai.org/miccai-2024/786-Paper2441.html,"[MP 1: Comparative Evaluation] All three reviewers highlight the lack of comparative evaluation (CE) with other methods as a potential weak point in our paper. The strongest concern is raised by R3, questioning whether a paper without such evaluation is suitable for MICCAI at all.

We agree that CE is an important scientific tool and acknowledge that it is a common practice in MICCAI papers. However, the primary goal of our paper is to showcase the application and demonstrate that epistemic uncertainty estimation can yield quantifiable improvements in an FDA-approved and widely used pipeline for OAR segmentation. This addresses a relevant gap between research and application, as discussed by the cited review paper. For this goal, a CE is not necessary and would be a distraction from our primary objective. As an industrial player, we see our primary role in communicating findings in a product environment. We hope that the academic community can build up on our work by performing comparative analyses on public datasets.

Regarding the impact, we believe our work can advance the scientific discussion if academic researchers utilize our findings to underscore the clinical importance of further research in uncertainty quantification. We further hope to inspire the curation of similar benchmark tasks based on public data, which can be used for CEs in future studies.

To address the reviewersâ concerns, we propose the following actions for the camera-ready version of our paper:
1) Expand the future work section to encourage academic players to perform CEs and design similar tasks based on public data.
2) Refine the introduction and abstract to clarify our scope and intended message.

We believe that there is a space at MICCAI for application studies like ours, which focus on demonstrating practical and clinical relevance.

[MP 2: MD Methods] R3 further criticises our claim that we improve upon other MD-based methods. This is a misunderstanding. We would like to clarify that we do not intend to claim superiority over other methods. As highlighted in MP 1, our main message is to demonstrate that epistemic uncertainty estimation can be beneficial in a real-world application. We also want to point out that we do not make such a claim in the abstract or introduction. However, we acknowledge that Section 2.3 discusses potential advantages of our MD-based statistical analysis. We believe these advantages (no risk of feature collapse, no need for architectural changes) are valid points.
To address this concern, we propose to tone down the language in Section 2.3. This adjustment will help prevent misunderstandings in the future and sharpen our main message. Thank you for the feedback.

[Conclusion] We appreciate that all reviewers highlight the clarity of the manuscript and agree on the importance and clinical relevance of the topic. We hope that the clarifications provided above will offer further insight into the intended focus and decision-making regarding the scope of our work.

@R1 regarding Raw vs Processed: Post-processing is applied to the uncertainty outputs as detailed in Sec. 2.2 (Inference and Uncertainty Quantification). âRawâ refers to the initial output from the uncertainty model, while âProcessedâ denotes the output after post-processing.

@R3 regarding AUROC calculation: We confirm that no training images were used in the AUROC computation. It involves only test ID (control) vs OOD images, ensuring balanced classes.

@R6 and @R3 regarding statistical assumptions for the Mahalanobis distance: We have confirmed with a hypothesis test (Hotelling T^2) that the uncertainty score follows a multivariate Gaussian distribution, parameterized with the corresponding mean and covariance. The shared covariance is intuitively justified by the fact that all organs are predicted by the same model, resulting in correlated outputs. We will mention the hypothesis test in the paper."
https://papers.miccai.org/miccai-2024/787-Paper3434.html,"We would like to thank all reviewers (R3, R5, R6) for their time and effort in reviewing our manuscript. We appreciate the reviewersâ positive assessment and kind recognition of our work as ânovelâ, âexcitingâ, and âwell executedâ with âgreat clinical potentialâ and âclear quantitative and qualitative resultsâ.

The reviewersâ insightful comments and constructive feedback will allow us to enhance and extend upon our work such as including âclinical variables about cardiac functionâ as part of the LDM textual input (R5), or by providing a FrÃ©chet Radiomics Distance (FRD) library to further investigate the composition (R6), consistency (R3) and variability (R5) of feature sets used to compute the FRD.

R6 remarked on the existence of important clinical applications of our work such as âunsupervised brain tumor segmentationâ, which motivates further exploring such downstream tasks using multiple datasets from different domains. Also, we concur with R5âs intriguing question that much of the required diagnosis/prognosis information may be already present (although subtle/complex) in non-contrast images as shown by their usefulness for generating corresponding contrast-enhanced images. However, apart from the information present in a particular non-contrast patient image, population-level statistical patterns (e.g. transferred into network weights during training) learned by mapping a non-contrast image to a contrast-enhanced image may likely be a necessary complementary source of information for diagnosis/prognosis in many scenarios. Generating contrast patterns can further enhance the visual interpretability of embedded (or predicted) diagnostic or prognostic information in non-contrast images, which can help to guide clinical assessment. For such assessments, we agree with R5âs idea that it would be useful to report a confidence metric alongside each synthetic image to determine cases where repeated ârealâ gadolinium enhanced exams are beneficial.

Once again, we would like to sincerely thank the reviewers for their valuable feedback and thoughtful suggestions, which we will take into account and, to the best of our abilities, integrate as we advance our study."
https://papers.miccai.org/miccai-2024/788-Paper0721.html,"We cordially thank you for your time and efforts on the review of the submission. We have carefully studied your suggestions and addressed your main concerns.

Design Motivation (R3)
The main motivation of our study can be found in contribution paragraphs and Conclusion section.
Dimensional Attention Module (DAM): DAM enables the network to extract more global features, which is beneficial for lesion localization. It is designed to achieve the global attention while saving a huge amount of computation.
Adaptive Spatial Attention Module (ASAM): ASAM is used to aggregate features from different modalities. Since each modality contributes differently to the final csPCa classification, we adopt ASAM to measure the voxel-level contributions of each modality.
3D U-Net: we apply a classical 3D U-Net decoder to perform segmentation. The segmentation task serves as an auxiliary task to enhance the classification encoder."
https://papers.miccai.org/miccai-2024/789-Paper0338.html,"R1

R3

R4"
https://papers.miccai.org/miccai-2024/790-Paper1516.html,"We thank all reviewers for their insightful feedback and suggestions.Reviewers#3&#6
R3Q5 By incorporating data from a broader range of patients, we can improve the robustness of developed models across different clinical scenarios. It will allow a more accurate representation of disease variability, leading to more reliable diagnostic tools.
R3Q6-R6Q8-R5Q6.5 The dataset is currently private. We will provide a public link for code.
R6Q10) 2 types of mycetoma: eumycetoma (fungal) and actinomycetoma (bacterial). In this study, we considered 2 fungal species and 2 bacterial species (table1), totaling 4 mycetoma species. Identifying the species is crucial for appropriate treatment as different species of fungi or bacteria may respond differently to medication. This knowledge enables healthcare providers to prescribe the most suitable treatment regimen for better outcomes. Table2 presents a general summary of the metric evaluations for each dataset. While we meticulously evaluated each species, these detailed findings have not been disclosed publicly. The details will be made available with the code.

Reviewer#5
Q6.1 This study aims to compile data to create a novel dataset and develop an automated method for identifying mycetoma species using histopathological images of black skin patients from Senegal. Given the lack of pre-existing dataset, we undertook a thorough process of data collection, annotation, and labeling in collaboration with dermatologists from Sudan and Senegal.Currently, no automated method exists for species identification. The existing approach proposed by the researcher Omar only differentiates between eumycetoma and actinomycetoma, requiring the conversion of color images to grayscale to mitigate staining color bias. Our contribution is to propose the first method that identifies mycetoma species using color images, as these are essential for distinguishing the different species. To address the challenge of color variability, we employed three stain normalization methods, resulting in the creation of three distinct datasets. We evaluated the effectiveness of these normalization methods using the MONAI framework+DenseNet121 to classify four mycetoma species.
Q6.2 Color augmentations are not currently used alongside stain normalization to thoroughly evaluate the impact of normalization techniques in contrast to using original images. The experiment (Monai+DenseNet), aims to showcase the effectiveness of stain normalization in enhancing species classification. In future studies, we plan to integrate color augmentations to enrich our analysis and conduct additional experiments with different model architectures.
Q6.3 The primary clinical indicator of mycetoma in vivo is the presence of a distinctive âgrainâ within the affected tissue. For pathologists conducting histopathological examinations, the key focus lies in identifying these grains and observing their appearance and structure under the microscope. Each mycetoma species presents unique grain patterns, sizes, and colors, which can be discerned through specific histochemical staining techniques. For data collection, we gather 1289 images showcasing the presence of these grains. 
Q6.4 Various factors (page2:Moreover-images) contribute to color variability. Recognizing that pathologists lack the capacity to prevent/control these issues entirely, we have focused on finding solutions. Consequently, we have opted to utilize normalization techniques to reduce and mitigate these color variations among histopathological images. 
Q6.6 Original image sizes range from 16001200 to 46083456 pixels, depending on magnification and the observed species. Regions containing mycetoma grains are selected, and images are captured at three magnification levels for each grain across the entire slice. 
Q6.7 Acknowledging the limited size of our dataset, the small number of patients, we plan to expand our dataset in future research to further validate and enhance our findings."
https://papers.miccai.org/miccai-2024/791-Paper2954.html,N/A
https://papers.miccai.org/miccai-2024/792-Paper4218.html,"We would like to thank the reviewers for their insightful feedback on our paper. We appreciate their recognition of our workâs clinical relevance, innovative approach, and strong validation. As highlighted by reviewers #4 and #5, our system âsolves a clinically relevant problem for US-guided laparoscopic liver resectionâ and âclearly addresses a clinical need with an innovative approach.â Our use of 3D volumes to enhance the accuracy and reliability of annotations for complex US images, along with data augmentation to train a personalized segmentation model with real-time capabilities (14 frames per second) and high accuracy (up to 0.95 identification precision), was highly appreciated. Additionally, our method was validated on two swine livers and included performance comparisons and feedback from four surgeons.

Regarding the methodology, we appreciate the feedback on the innovativeness of our method. Reviewer #4 considered the generation of 2D datasets from 3D annotations insufficiently innovative. However, to the best of our knowledge, this is the first work that enables real-time identification of portal vein branches in intraoperative ultrasound images with such precision. Our approach builds on existing techniques but demonstrates effective performance on swine livers, showing promising âpotential for clinical translation,â as noted by reviewer #5. Additionally, we propose a clinically applicable workflow involving preoperative 3D US scans, semi-automatic segmentation, and personalized model training, which significantly enhances annotation efficiency, increases reliability, and provides a valuable tool for surgeons. We believe that this innovative combination of techniques and its application to a clinical problem is relevant for MICCAI.

In terms of validation, reviewer #4 raised concerns about the practical significance due to the lack of validation with real patient data. We acknowledge the importance of real human data and have a plan for clinical translation. Preoperatively, a full liver volume can be acquired via a percutaneous probe during a single apnea, which causes minimal deformation. Intraoperatively, an alternative is to use an intravascular ultrasound (IVUS) approach to guide the surgery (as documented by Urade et al., 2021), utilizing our model without alteration of our method. Ethical approval constraints limited human data usage, but our validated approach shows promise for future clinical application.

Addressing other remarks, reviewer #4 expressed concerns about the real-time capability of our method. As stated in Section 3.2 (Results) on page 6, our method has an inference time of 0.072 seconds (14 frames per second), enabling real-time identification. In the literature cited in our introduction, the US-to-CT registration approach had an inference time of 115 seconds, thus, not a real-time performance (from the work of Montana-Brown et al., 2021). Reviewer #4 also raised concerns that the methods and results sections do not reflect the semi-automatic segmentation feature of our method. By using an interpolation technique, we reduced the preoperative manual segmentation of the 3D ultrasound volume to sparse slices (around 20% of the volume), significantly reducing the time required for annotation.

Regarding concerns about reproducibility due to insufficient details and lack of open access to code and data, the limited space of MICCAI articles constrains the amount of information we can provide. As a solution, we plan to share the code, data, and additional methodological details upon acceptance of the paper. This will facilitate reproduction and improvement of our work by others.

We appreciate the constructive feedback and believe that addressing these points enhances the clarity and impact of our work.

Reference:
Urade, T., Verde, J.M., Garcia Vazquez, A., et al.: Fluoroless intravascular ultrasound image-guided liver navigation in porcine models. BMC Gastroenterology 21, 1â7 (2021)"
https://papers.miccai.org/miccai-2024/793-Paper1533.html,N/A
https://papers.miccai.org/miccai-2024/794-Paper0014.html,"[R1&R3&R4] Q1: Clarification of Explicit Prior. Segmentation models, unlike classification models, focus on detecting predefined lesions from human-provided lesion masks, rather than other biomarkers. Given that the same lesion type consistently shares the same morphology, we can describe their morphology using predefined lesions. CLIP is pretrained on general domain datasets and has a natural ability to perceive color, shape, and other morphological features. However, CLIPâs understanding of medical concepts is limited. To address this limitation, we translated medical concepts (implicit class names) into more understandable descriptions (explicit descriptions) for CLIP, providing interpretable and additional references for segmentation. The conversion of class names to explicit descriptions occurs before the VLM text encoder. We map the class names to corresponding predefined explicit descriptions. These descriptions are crafted from ophthalmology literature and have undergone validation by both relevant experts and GPT-4. Moreover, our proposed injector further adapts and aligns CLIP within the medical domain, enabling efficient training with low computational resources. Despite being frozen, CLIP can learn to locate DR lesions through Injector. Preliminary ablation experiments have been performed but are not included due to space limitations. When not transferring to the explicit description, mDice drops by 3.06% on IDRiD and 2.89% on DDR. Implicit priors introduce more noise, leading to noisy information during training.
[R4] Q2: Novelty concerns. 1) CRIS uses pre-existing descriptions without needing to handle implicit concepts and cannot be directly applied to our task. Our approach  is specifically designed for medical scenarios and focuses on addressing the challenges in the medical domain by translating medical concepts into explicit descriptions and adapting them using our Injector. 2) Our injector integrates explicit textual descriptions into the segmentation model, specifically tailored for medical applications. Unlike ViT-adapter, which fuses visual features from ResNet and ViT backbones, our injector adjusts text-based priors and visual features for better alignment. Additionally, our definitions of query, key, and value differ from ViT-adapter, and we did not use a feed-forward network.
[R4] Q3: Ablation study for Injector. The injector facilitates knowledge sharing between explicit priors and multi-level visual features. Our preliminary experiments find that using the injector only in the final encoder layer led to performance degradation, highlighting the importance of multi-layer adaptation.
[R4] Q4: More comparisons. We have compared FCT [18] in our experiments, a recent SOTA transformer-based method. FCT has proven its effectiveness by outperforming SwinUNet, nnFormer, and other SOTA methods for medical tasks, indirectly highlighting the strength of our approach. Based on your valuable suggestion, we will replace the SAM encoder with the Swin Transformer and include more comparisons in future work.
[R4] Q5: Motivation of Class-specific Prompt Generator (CPG). In our framework, the goal is to generate masks for specific classes. The CPG acts as a bridge between the prior information and the SAM decoder, providing the decoder with information about which class to segment and the relevant prior information.
[R1&R4] Q6: AboutÂ SAMâs text-prompt generator. It relies solely on original CLIP, resulting in a domain gap between the general and the medical domains. Thus, we have designed several modules to adapt SAM and VLMs for DR lesion segmentation task.Â 
[R1] Q7: H_s and W_s. The subscripts relate to spatial.
[R1] Q8: Average operation in Injector. Experimental results indicate that the average operation has minimal impact on performance.
[R3] Q9: Detailed explanation of âclass-agnosticâ. We will include them in the final version.
Q10: Minor issues. We will correct them in the final version."
https://papers.miccai.org/miccai-2024/795-Paper1898.html,"Dear reviewers,

We thank you for the overwhelmingly positive reviews of our work âTractOracle: towards an anatomically-informed reward function for RL-based tractographyâ. Below we would like to sum up and address some of the concerns raised by the reviewers

âUsing the TractoInferno pipeline to provide quantitative resultsâ

We argue the pipeline does not evaluate meaningful metrics wrt the intent of the proposed method: reducing the number of invalid connections. The TractoInferno pipeline compares the volume of segmented bundles with the subjectâs reference bundles. In that sense, the TractoInferno evaluation pipeline evaluates accuracy in terms of voxels, whereas our method focuses on accuracy in terms of connections. We would also like to point out that section 3.4 and Table 4 do include quantitative results for in-vivo subjects. By tracking using exactly the same number of seed points, we evaluated which method produced the most âvalid streamlinesâ according to three streamline segmentation methods.

âInadequate explanation of experimental resultsâ

Table 3 reports the mean metrics for 5 reconstructions from five models trained with the same hyperparameters but different random seeds. As far as we are aware at the time of writing the proposed work, a ratio of 88% valid connections is by far the highest reported.

TractOracle was sometimes able to recover a 20th bundle, and both sd_stream and ifod2 failed to. We theorize that the 20th bundle may not be well recognized by TractOracle-Net, leading to its reconstruction only sometimes. TractOracle-RL recovered the fewest IB, highlighting again that the algorithm is highly accurate.

OL results for classical algorithms are consistent with previous literature on deterministic vs probabilistic tractography algorithms. Both âlearnedâ algorithms obtained higher OR than ifod2 and sd_stream. This could be due to the learned procedure, or mask thresholds related to tracking termination criteria.

âTractOracle vs  Anatomically-Constrained Tractography, comparison with sd_stream + ACT, iFOD2+ ACTâ

We agree that anatomically constrained tractography has been proven to greatly improve the performance of the tracking algorithms, reducing the number of false positives. However, we wanted to perform a âfairâ evaluation by giving all algorithms the same âinputâ or âknowledgeâ about the underlying anatomy. Indeed, neither Track-to-Learn or TractOracle have as part of their input the type of tissue at the head of the streamline. The algorithms are completely agnostic to the underlying tissues and instead rely on the shape of the streamlines being tracked. We agree, however, that the algorithm could greatly benefit from this information to propagate streamlines, as well as selecting or discarding them at the end of the tracking procedure. Future work could integrate this information to further include anatomical information into the tracking process.

âDiscrepancies between in-silico and in-vivo resultsâ

Indeed, there seems to be a discrepancy between the volume-related results in Table 3 and Figure 2 for Track-to-Learn and TractOracle. There may be a few reasons for this phenomenon, including the number of training subjects, the shape of the anatomy of the considered test subjects, the evaluation method (Recobundles vs streamline segmentation via regions of interests), etc.

âThe validity of the anatomical scoring is unclear and failure modes of the whole approach are not described.â

Indeed, future work should be focused on exploring the limitations and failure modes of TractOracle-Net to ensure anatomical accuracy. One research avenue could replace Recobundles with other streamline filtering methods such as SIFT, extractor_flow or others, or perhaps a multitude of algorithms. Comparison of streamline segmentation with post-mortem analysis could validate that the streamlines are properly segmented."
https://papers.miccai.org/miccai-2024/796-Paper2724.html,"First, we would like to express our gratitude to the reviewers for their thorough and constructive feedback. We note that reviewersâ feedback will be reflected in the final camera-ready submission as much as the rules and space permit."
https://papers.miccai.org/miccai-2024/797-Paper1171.html,"We would like to thank the reviewers for their valuable feedback and constructive comments. We appreciate the time and effort they have put into reviewing our work. We are happy to hear that all reviewers agree on the important contribution of our work in the field of echocardiogram synthesis and the potential impact of our method on medical imaging research. We will release the code upon acceptance of the paper to address the reproducibility concerns of the reviewers (#3, #4, and #5). Regarding the major concerns raised by the reviewers, we would like to address them as follows:

Reviewer #3 has concerns about the requirement of paired data: The model of our method is trained as an unconditioned diffusion model. In equation (3), it is true that we aim to create a pseudo-image x_0^K, the initial volume V^K is obtained by duplicating the x_0^K to form a pseudo video. The pseudo image x_0^K is created with a random frame from the training set, therefore, it requires no paired data. Unlike the CLS-free method.

Reviewer #3 concerns about comparison with the trained-based method: The method we compare with is CLS-free from Ho et al. [7]. Figure 2 shows that the proposed methodâs results have a comparable quality to training based methods, while still being training-free. But that is not the case for the large scale quantitative evaluation. In fact, we also include samples (such as CAMUS (2)) in the supplementary material that show the proposed methodâs results are not as aligned as the CLS-free method.

About the downstream task benefit from Reviewer #3 and #5: The reason we did not include downstream effects such as segmentation output, ejection fraction estimation, or segmentation is that our method focuses on controlling the video spatially, there is no guarantee that the generated frames will have the same ejection fraction, and motion is synthesized by diffusion models. Furthermore, controlling the ejection fraction via counterfactual generation, such as Reynaud et al. [15], requires ground truth ejection fraction, which violates the training-free principle that we aim to achieve. Our vision for this work is to be an initial step towards a more general training-free video synthesis method that can not only generate realistic echocardiograms but also output corresponding labels, such as segmentation or clinical measurements.

About the concern of Reviewer #5 on the motivation of optimal transport: We deeply understand the reviewerâs concern about the novelty of our work and the use of optimal transport. The reason behind choosing optimal transport came from the recent advances in diffusion models. In general, the problem of training-free video synthesis is special case of a broader problem, unpaired domain-to-domain translation. In our case, one domain is the segmentation map, and the other domain is the echocardiogram video. One of promising approaches is Schrodinger Bridge or entropy-regularized Optimal Transport [1], which aims to find align the two probability path from pure gaussian noise to the data distribution of the two domains. However, the optimal transport problem in high-dimensional space is computationally intensive, and requires two diffusion models to generate source and domain distributions [2]. Our approach can be seen as a relaxation of the general Schrodinger Bridge problem, where we only consider one translation from the segmentation map to the echocardiogram video. Optimal transport allows the pseudo-image to have similar intensity histogram as a sample from training set, resemble overall anatomical structure of heart chambers, and the diffusion model will fill out the motion and fine-grained details at remaining denoising steps.

[1] Leonard, C. âFrom the Schrodinger problem to the Monge-Kantorovich problem.â Journal of Functional Analysis, 2012.
[2] Su, Xuan, Jiaming Song, Chenlin Meng, and Stefano Ermon. âDual diffusion implicit bridges for image-to-image translation.â ICLR, 2023.â"
https://papers.miccai.org/miccai-2024/798-Paper1332.html,N/A
https://papers.miccai.org/miccai-2024/799-Paper0242.html,"We sincerely thank the reviewers for their insightful comments and constructive feedback.
[R1#Q1] Where does the knowledge base come from?
The knowledge base originates from medical professionals and robotics experts. GPT-4 facilitates the synthesis of data, ensuring independent and identically distributed (i.i.d.) instances for training the embedding model.
[R1#Q2] Why use synthetic data (e.g. APIs) for model training? If LLM has never seen a real bot API, where does it know its name from?
Real-world APIs come in all sorts of formats, and we can only unify API formats by synthesizing datasets to make it easier for us to implement our frameworks.
LLM does not need to know the name of the API, the information about the API is recalled based on the user instructions, through the embedding model, after cosine similarity calculation.
[R1#Q3] How is knowledge base data paired with additional information (e.g., data paired with narrative descriptions for API searches, data paired with detailed descriptions for robot manuals), and where do these data come from? Are these pairings also used for training?
These data are synthetic data and are used for the training of the embedding model.
[R1#Q4] Was the LLM trained directly on the synthetic training data or on the embeddings of the finetuned bge-large-en-v1.5?
We didnât train LLM.
[R1#Q5] How information retrieved from the knowledgebase (domain specific knowledge, API and Handbook) was transformed into the Ultrasound Assistant prompt?
We retrieve the relevant information through the cosine similarity matching algorithm and then combine the relevant information according to the structure shown in Fig1. Specifically, the APIs list will be combined in the format shown in Fig2, and the Handbook will be combined in the format shown in Fig3.
[R1#Q6] What training data was exactly used for what model?
We use embedding model which is a fine-tuned bge-large-en-v1.5. For the LLM, we use some different models to implement the experiments.
[R1#Q7] It is unclear whether the ReAct framework is actively coupled with the LLM or whether it âtakes overâ the task.
ReAct framework is just a prompt strategy, it can not take over the task.
[R1#Q8]Does the LLM actively invoke the APIs with correct parameters?
LLM actively calls the corresponding API with parameters provided by the robot system.
[R1#Q9] Information about the robotic system
The robotic arm is RM65-B from RealMan-Robotics. The ultrasound machine is H20 from Angell. The depth camera is RealSense from Intel. The force sensor is M3815B from Sunrise. The control algorithms for the robot system were written by the authors themselves.
[R1#Q10] Which components offer APIs and how are they connected to communicate with each other. For example, does the Ultrasound machine offer APIs that can be used to change ultrasound settings by the LLM directly?
API communication is done within the program, specifically utilizing pythonâs value passing. Ultrasound machine didnât offer APIs. Specifically for the carotid ultrasound sweep experiment, our APIs are depth camera, force transducer, vessel reconstruction, vessel segmentation, robotic arm control.
[R1#Q11] What has been done before and whether the contribution mainly lies in the integration of the language models.
Our contribution lies in integrating ultrasound robots with large language models to optimize existing human-robot interaction logic.

[R3#Q1] The method requires large knowledge datasets generated by, for example, physicians and roboticists, which would obviously limit the accomplishment of more complex ultrasound tasks.
We recognize the need for larger, more accurate datasets for more complex ultrasound tasks.

[R4#Q1] No comparison with other technologies.
As far as we know, at the time of the submission of the first draft, we were the first to do this type of work on an ultrasound robot, so there was no one to compare it to.

We will publish the code in the future."
https://papers.miccai.org/miccai-2024/800-Paper4075.html,"-Concern about the novelty from Reviewer #1:

We appreciate the concern of reviewer #1 about the novelty of the proposed work as compared to the previous paper [1] (J. Chen, Int J CARS, 2023). However, we should claim that the proposed method is different from [1] and is not a simple implementation or incrementation of [1]. We hereby claim the major differences between [1] and the proposed work.

Limitation of [1]: In [1], the connectivity of vertices of the tissue mesh maintains the same over iterations, which leads to the major limitations (cannot deal with newly appearing surface, cannot deal with surface continuity change). As a consequence, [1] can neither be used in the SLAM-like task, where camera is moving around and new structure continuously appears, nor in the task of dissection, where the tissue surface continuity changes. This is also the reason why we did not compare with [1] in the experiment, as [1] does not work in these cases.

How we overcome the limitation of [1]: As for the proposed method, we do not make use of the mesh for representing the 3D structure given the limitations as in [1]. Instead, we propose a novel representation of the 3D structure, which is described as âcanonical canvasâ and âimpastoâ in the article. This new 3D representation can be simply described as a 3D-2D mapping, which naturally has derivative structure and is useful in geometry optimization for guaranteeing spatiotemporal smoothness.

The new 3D representation is much more flexible than mesh, such that we can propose to optimize and fuse the newly appearing surface to the previously modeled scene. In the experiment part, we demonstrate the proposed method in the case of dissection and camera moving, which is impossible with the previous work [1].

-Concern from Reviewer #3 and #4:

Canonical canvas: Canonical canvas is initialized as a borderless 2D space corresponding to the first camera pose and will move together with the camera, such that the canvas is always static to the window (camera). On this canvas, the geometry of the scene is represented, optimized and fused.

Optimization of deformation map: We do not directly optimize the deformation map. Instead, we optimize the geometry map (M) as in equation 4. Since there exists spatiotemporal connectivity of the geometry map, immediately we can derive the optimized deformation map from the optimized geometry map.

Re-observation: It is currently not our focus in dealing with the long-term issues. Now the algorithm will simply replace the old scene if a new scene is detected in the same space. However, some techniques, such as bundle adjustment and graph-based optimization, can be implemented to guarantee long-term stability.

Online vs real-time: We carefully pick the word âonlineâ rather than âreal-timeâ to avoid misunderstandings in the discussion of latency. Now the method may take around 5 seconds to compute each new coming laparoscopic image. However, this does not hinder the method for future intraoperative applications. The latency can be reduced with proper code optimization and parallel computation.

Instrument mask: We agree that instrument masks are important to accurately remove the inference of instrument from the 3D reconstructed scene, and we will add such discussion in the revision. The instrument masks are provided in all the datasets used in the experiment.

Ground truth (GT) acquisition and evaluation method: There have been two widely used ways to evaluate the accuracy of deformation, geometry-based and tracking-based. We appreciate the reviewerâs recommendation of a dataset that enables the evaluation in latter way, which may help to better demonstrate the proposed method. In GT acquisition, a thin tissue was fixed on a grid, and we temporarily paused the movement of the instrument and performed the 3D scan from behind, such that we can get the GT even in the occluded area. The scanned GT was used to represent the current deformation status of the tissue."
https://papers.miccai.org/miccai-2024/801-Paper2112.html,"We thank the reviewers for their valuable feedback and constructive criticism, and acknowledging the simple innovative design of our model to solve this difficult problem while addressing the limitations of previous works. We will fix the minor issues, and the major issues are addressed below."
https://papers.miccai.org/miccai-2024/802-Paper0281.html,"We thank reviewers for the valuable comments, and we are encouraged by the positive comments on ânovel methodâ (R1&R3), âvery good organizationâ (R1&R3), âjustified model logicâ (R1). Below, we address specific comments."
https://papers.miccai.org/miccai-2024/803-Paper2477.html,"We thank the reviewers for the constructive feedback and considering our work ânovel/new methodologyâ (R3/R1) âan innovative use of transformer for diffusionâ (R4), âtechnically sophisticatedâ (R3), âan intriguing and promising approach for detailed visualizationâ (R3), âthis [INR-based image segmentation] shows technical capabilityâ (R3), Excellent on âclarity and organizationâ (R3), âdemonstrate the versatility and applicability [â¦] across different anatomical domainsâ (R4); âunderscores the robustness and generalizability of the proposed methodâ (R4).
In this response, we focus on addressing the main concerns of R1,3,4.

R1.1: Comparisons with Park et al [1] & Han et al [2].

We first emphasize that our method comprises 2 stages: 1st, we overfit a model to each tree in the training set using INR realized as MLP, though other networks like [1], SIREN or FFN could also be used. We donât consider these networks as competing methods but rather alternative design choices for stage 1 of our method. The uniqueness of our method for representing trees lies in flattening the network parameters to 1D vectors (one vector per network overfitted to each tree) and using them in stage 2, where we learn the distribution of these tree-representing vectors using denoising diffusion. Unlike [2] that performs diffusion on 2D PET image slices, we perform it on INRâs network parameters. Further, as shown in Fig.3 & Tab.2, INR is superior to volume and mesh representation, offering ~60% & ~20% lower memory footprint, and ~82% & ~17% higher reconstruction accuracy, respectively. Hence, we adopt INRs in stage 2 for diffusion. Plus, [2] uses DDPM, which is slower than DDIM that we use [31].

R1.2: Method is not the first to use INR for representing trees and points to [1] & Khan et al [3].

We address [1] above and note that  [3] focuses on CT/MRI images for organ segmentation, not anatomical trees, and doesnât use diffusion for modeling tree statistics or synthesis.

R3.1: Tree-like structures are unconditioned and may be anatomically incorrect.

Although we focus on unconditional tree generation, our quantitative results in Fig.6(c-d) show that our generated samples closely mimic the real data distribution & the qualitative results in Fig.8 shows visually plausible generated samples, indicating that the diffusion model learned a plausible tree manifold from INR vectors. Plus, R4 noted, â[method] enables generation of plausible vascular structuresâ. Conditional generation of trees is a promising future direction.

R3.2: Segmentation exp. shows technical capability, but doesnât demonstrate methodâs effectiveness.

As stated in Sec.3, Fig.7 was merely a proof-of-concept demonstration. Focusing on rigorous assessment of the utility of our representation for segmentation is left for future work.

R4.1: Why use only VascuSynth [9] for tree synthesis?

We used synthetic trees from [9] due to their diverse tree topologies, and showed qualitative samples with varied complexities in Fig.8. In contrast, synthesized trees from IntRA would look visually similar, as all depict the brainâs circle of Willis without easily observable differences.

R4.2: Comparisons with baselines/other methods for tree generation.

Synthesis of anatomical trees is an underexplored field, which motivated our work. As discussed in Sec.1, rule-based methods, eg, L-system & VascuSynth, are highly complex and are not designed to model, and sample from, the distribution of trees. While many methods extract/segment 3D trees from volumes, they donât synthesize new trees. Moreover, methods such as âManifold of Treesâ (arxiv:1207.5371) are limited to simple 2D skeletons of trees (and even require domain expertise to do so), making them difficult to render and use in downstream medical applications. As reporting results here is not allowed, as a baseline, we will add to Tab.3 results of performing diffusion on voxel-grids of size 64^3 (262k params, compared to our INRâs 160k, ~1.6x less)."
https://papers.miccai.org/miccai-2024/804-Paper2184.html,"We sincerely thank the reviewers for their valuable comments. We are pleased that the reviewers praised the novelty, comprehensive experiments, excellent performance, and good writing of our paper. Here, we provide detailed explanations to address the raised concerns.

[More Dataset, #R1, #R4] 
As the rebuttal guidelines suggest, we cannot present additional experiments here. We believe that our approach can still achieve SoTA on MRI datasets as we observed that the performance order of compared methods remains consistent in MRI datasets [1, 20]. If permitted, we will consider including them in the revision (or supplementary).

[GFLOPs and Parameters, #R1, #R4] 
As shown in Fig. 1(c) in the main paper, the GFLOPs for each ViT block is approximately 18G, while the multi-scale Convs consume 0.38G and Mamba uses 2.37G, resulting in only about a 15% increase in total GFLOPs of ViT. Our adapters have about 5.2M learnable parameters, which is merely 6% (5.2M out of 86M) of the frozen parameters of the ViT. In contrast, the 3D-UX-Net[13] has 53M learnable parameters. These lightweight adapters contribute to both effective and efficient convergence.

[Compare with SAM-Med3D and 3DSAM-Adapter, #R3] 
SAM-Med3D is a foundational model and can be an alternative to SAM in our method. As we have done sufficient experiments to demonstrate the effectiveness of our TP-Mamda (a universal plug-and-play adapter), we believe it can work well with SAM-Med3D. 
For comparison with 3DSAM-Adapter, we still believe our method can perform better as it shares a similar design with MA-SAM [1] (see Table 1). 
As the rebuttal guidelines suggest, we cannot present additional experiments here and will consider including the comparison in the revision (or supplementary) if permitted. 
More importantly, in the initial manuscript, we have compared 10 baselines (including different networks and adapters), sufficiently and comprehensively showing the effectiveness and superior performance of our method.

[More Analysis on TP-mamba, #R4] 
SAM is trained on billions of natural data and excels at distinguishing between foreground and background. Consequently, our lightweight adapters based on SAM are sufficient to transfer SAMâs ability to medical domain quickly and efficiently (see Fig 4).

[Ablations and Qualitative Results, #R4] 
We will include experiments on without pre-trained SAM, ViT-L-based SAM and SAM-Med3D (#R3) in the supplementary. Additionally, we will conduct grid-search ablation studies on the Mamba and Convs layers, as well as show qualitative visualizations.

[Discussion on SAM-2D and Mamba, #R5] 
Thank you for your valuable comments. We will include surveys on SAM-2D in the âIntroductionâ section and provide detailed explanations of the Mamba algorithm in the âMethodologyâ.

[More details on methods and experiments, #R5] 
Here we clarify some unclear details and will update them in the main paper.

Q3. All normalization layers used in this paper are LN. LoRA modules are applied to both Q and V layers.
Q4. âFeaturesâ refers to feature dimensions.
Q6. Following MA-SAM[1], we set the HU window to [-200, 250] for observing abdominal organs. The HU values are then normalized to the range of [0, 1] using min-max normalization.
Q7. CT volumes are randomly cropped into 96x96x96 sub-volumes during training. Sliding window inference will be applied during testing, which does not down-sample the CT images.
Q8. NVIDIA RTX 3090 GPU is used, and an automatic mixed precision strategy is employed to accelerate processing and reduce GPU memory requirements.
Q9. The link of SAM is https://huggingface.co/timm/samvit_base_patch16.sa1b. It is pre-trained on natural images. 
Q10. As shown in Fig. 4, the dice scores across the three settings exhibit a similar trend. We select 25% based on the trade-off between training time and available data.
Q11. ârâ indicates the same item, the low rank."
https://papers.miccai.org/miccai-2024/805-Paper1242.html,"We sincerely thank all the reviewers for their insightful comments and suggestions. Code will be publicly released when the paper is published.

To R1:
On the initial condition of TSBP:
TSBP, as an iterative algorithm, requires some initial condition, such as the initial set of high-confidence bounding-boxes (b-boxes), pre-determined. A relatively high confidence threshold is needed to form the initial HC (High Confident) b-boxes. We use 0.7 as the default threshold and found that it works well on the GlaS and MoNuSeg datasets. We also tested the case when it is set to 0.6 (see Table 2). In both cases, TSBP yields better results than the competing methods (with the same thresholds).

For more efficient and effective matching performance, we use K-means on top of the initial HC b-boxes to select the representative b-boxes. By default, K is set to 25, and the performance of other values of K is reported in Table 3. Future work will focus on developing a more automatic and robust procedure for forming the initial condition of TSBP (will mention this in Conclusions).

On the one-to-one matching:
One-to-one correspondence is enforced so that the most reliable (most likely correct) b-box would be chosen at each round of matching, ensuring the reliability of the multi-rounds propagation.

On Table 4:
The error rates presented in Table 4 show that the b-boxes admitted in the first stage (earlier rounds of matching) exhibit a lower error rate than those admitted in the second stage (later rounds of matching). This table is for showing the dynamics of the propagation, giving insights on how TSBP works. Note that candidate b-boxes are uncertain predictions by the detection model, and mostly hard cases. TSBP cannot resolve all the hard cases; only some of the cases (which would be mis-classified by the baseline) would be rectified by TSBP. Weâll add F-scores to Table 4.

[15] is too old:
We actually use an improved version of the method proposed in [15], which was detailed in a paper in 2020, with the code hosted on GitHub as listed at the bottom of page 6. Weâll include the reference to this 2020 paper for the method in our revision.

Weâll revise Fig. 3 by including the baseline results at different threshold levels. In Table 2, weâll include the results of BC.

Following your comments, weâll further improve the writing and presentation in the revision by:

To R3:
Specific effect on pathology images:
We appreciate R3 for asking this question! Pathology images often have objects (possibly of the same type) repeatedly occur (e.g., cell nuclei), and our test-time bounding-box propagation suits this property well, as TSBP encourages that objects influence with one another in producing the detection results. TSBP is more effective when the images are large (e.g., whole slide) and contain multiple object instances for each class type.

Comparing to test-time adaption methods:
Methodology-wise, comparing to test-time adaption methods (e.g., Tent, TestFit), our method requires no additional model training, and runs very efficiently. Experimental comparison on the detection performance will be carried out in future work. Combining TSBP and TTA methods could also be a promising direction.

To R4:
K-means is for selecting the representative High Confident bounding-boxes to initiate the EMD matching. It is useful for improving the efficiency as well as the accuracy of the matching procedure. It is also very fast to compute and easy to use in practice. Weâll include more descriptions regarding the K-means in the revision. Weâll mention percentages of improvement in the Abstract. We agree that utilizing circles and features in polar space would be a very promising direction."
https://papers.miccai.org/miccai-2024/806-Paper1481.html,"We sincerely appreciate the recognition of our work from all reviewers for the âimportantâ (R1) and âstrongly ill-posedâ problem (R4) of 3D brain vasculature reconstruction from only biplanar X-Rays. They found our method innovative (R1,R4,R5), âwell motivatedâ (R5) with âhigh clinical impactâ (R1) and large improvement. All reviewers praised our âsoundâ (R1), ânovelâ and âvery-promisingâ (R5) two-step approach, with our connectivity prior âappropriateâ (R1) and âeffectiveâ (R1,R4,R5), thoroughly validated in our experiments. Also, reviewers found our paper very clear (R1,R5) and âvery well writtenâ (R1).
Our paper provides the critical first step to show that deep learning can be used to address this highly ill-posed problem. While there are still challenges ahead for clinical translation, our approach improves the results of previous works by a large margin. It lays a valuable foundation for the MICCAI community by demonstrating the potential of this approach. We are confident that minor revisions will address most reviewersâ concerns.
VALIDATION REALISM (R1,R5)
We acknowledge that our method has not yet been validated on clinical images. Like previous works, we used synthetic DSAs derived from real vasculatures due to the difficulty in obtaining paired bi-planar DSA and 3D vascular imaging datasets. We are finalizing a dataset to address this and extend our work, adapting our method for clinical DSA with vessel enhancement and real-to-synthetic mapping, to provide both qualitative and quantitative results. We will discuss this in the conclusion. Given the substantial space needed to describe this extended methodology and the main focus of this paper on biplanar reconstruction, we will present them in a journal extension.
BINARY GROUND TRUTH (R4)
We agree that DSAs can exhibit inhomogeneous contrast agent distribution. However, DSAs are captured over short sequences such that all vascular regions have been filled with maximum density across time. By employing Maximum Intensity Projection across these frames, we can aggregate the contrast agent responses so that we mitigate most of these inhomogeneities. Motion artifacts are typically small during the ~8 second acquisition for structural brain DSA, unlike longer fluoroscopic images acquired during interventions. Current imaging systems have also built-in motion corrections. We hence believe that our approach captures the essential complexity of this highly ill-posed problem, and that modeling the ground-truth 3D vasculatures as binary volumes from segmented MRA is not over-simplified.
BI-PLANE WORKFLOW (R4) 
Reviewers R1, R5 recognized our high clinical potential. Biplane scanners are commonly used, available in most medium to large hospitals for a variety of specialties, which ensures high clinical applicability. Biplane DSAs, unlike 3D rotational scanners unsuitable for real-time interventions and single DSAs limited to simpler tasks, provide an ideal balance of speed, anatomical constraints, reduced cost, and radiation. They are widely used by interventionists but still present ambiguities, which drives our work. We chose to use only two projections because employing more would necessitate rotation, even partial, negating these advantages. Our method integrates with existing clinical workflows. 
RESOLUTION (R4) AND SDF (R1,R5) 
Due to GPU limitations, we deliberately used downsampled volumes to maintain network complexity and demonstrate feasibility with complex branching. We are exploring patch-based, cascade approaches or bigger GPUs to reconstruct smaller vessels. Using SDFs as an intermediate representation helps maintain thin vessel integrity and reduce artifacts during downsampling, unlike binary images which can cause structures to break apart or disappear.
In the final version, we will clarify the above points, detail the acronyms, and correct the few typos to enhance our paperâs quality."
https://papers.miccai.org/miccai-2024/807-Paper0074.html,"We thank reviewers for their constructive comments. We address concerns one-by-one below.

-Clarification of contributions of UinTSeg (R1,R3,R5,&R6)
Thanks for pointing out. To our knowledge, previous works mainly designed age-specific methods to deal with dramatic intensity-variation of infant in the first two postnatal years, which used to introduce longitudinally inconsistent segmentations. In our work, we proposed an integrated framework, UinTSeg, to achieve longitudinally-consistent and accurate infant brain tissue segmentation for infants aged from 0 to 24 months.

-Clarification of choice of Sobel filter (R1&R3)
In our UinTSeg framework, tissue edges serve as intensity-invariant anatomical information to guide longitudinally-consistent tissue segmentation for 0-24 month old infants. To this end, we mainly employ a Sobel edge filter to extract initial edges in the first and second stages.
We agree with reviewers that different edge filters would affect the segmentation performance, for which we will provide comparison results in the final paper. Nevertheless, current results already demonstrate the effectiveness of intensity-invariant anatomy information in segmentation.

-Clarifications of the results (R1,R3,&R5)
1) Except for comparing with Infant FreeSurfer (the latest and most advanced registration-based infant brain segmentation method) with results shown in Tables 1 & 2, we did not include results from other registration-based methods since their results are not good.
2) Table 2 further illustrates the results obtained with TMSN, which is the leading infant brain segmentation method tailored for isointense phase.
All these results show that UinTSeg has comparable or even superior segmentation performance to all age-specific infant brain segmentation methods.

-Clarifications of BCP dataset and the generalizability issue (R1,R3,&R5)
The generalizability of UinTSeg is twofold:
1) UinTSeg is trained and evaluated on BCP data, gathered from diverse sites, scanners, and demographics (10.1016/j.neuroimage.2018.03.049), enhancing its robustness; 2) UinTSeg leverages intensity-invariant anatomical information, to deal with variations from different datasets, scanners, and even ages.
In fact, the generalizability of UinTSeg was validated through further experiments: 1) segmenting tissues from the dHCP and NDAR datasets, and 2) segmenting tissues across lifespan datasets covering ages 0 to 100 years. Due to the extensive results, we plan to include them in our journal paper.

-Clarifications of the CBF module (R3&R5)
Inspired by HF-UNet (10.1109/TMI.2021.3072956), the CBF module employs dual-attention mechanisms to integrate features from the two segmentation branches in ASNet, thus enhancing tissue segmentation with large receptive field patches. We will revise Fig. 2 for better clarity.

-Clarification of specificity for infants (R3)
Due to rapid maturation and myelination of the brain during infancy, infants experience significant changes in brain tissue contrast. We thus proposed intensity-invariant anatomical features to guide segmentation using a single model.

-Clarification of data and brain size (R3&R5)
We mainly employed BCP data, consisting of 264 subjects with 672 scans aged 0-24 months. To avoid introducing artifacts that could reduce segmentation accuracy, we did not normalize head sizes and only normalized brain sizes for training the synthesizing model. The results demonstrate that UniTSeg effectively handle such variations.

-Figure readability and description clarity (R1,R3,R5,&R6)
We appreciate feedback and will address these issues pointed out by all the reviewers, including, descriptions of figures and proper citations.

-Model reproducibility (R1,R3,&R5)
We will make the code publicly available upon acceptance.

-Hybrid loss selection (R5)
Inspired by 10.1016/j.media.2021.102035, we utilize a hybrid loss, Dice and cross-entropy loss, which have been widely adopt for medical image segmentation tasks."
https://papers.miccai.org/miccai-2024/808-Paper3070.html,"We sincerely appreciate the reviewers for providing constructive comments.

Code (R3&R4&R5)
We promise to make our code publicly available.

Reviewer #3
Q1 Lack of details.
1.The encoder architecture is 3D ResNet-18.
2.The size of z is (T/r, H/r, W/r, C/r), where r is a downsampling factor. This is significantly smaller than the original video size, thereby reducing computational requirements.
3.During training, videos of arbitrary duration are segmented into clips of 48 frames. We then uniformly sample 16 frames from each clip and feed them into the 3D ResNet. All generated videos have a duration of 2 seconds at 8 fps.
4.The architecture of D comprises residual blocks containing ADAIN layers.
5.For the diffusion process, we adopt a commonly used framework that generates images conditioned on text prompts.
6.Yes, we preprocess all frames to have the same dimensions.

We will clarify these implementation details in the final version of the paper.

Q2 Did not mention approaches in computer vision in their related work.
We compared our model with cINN [18], and found that other relevant methods in CV typically require additional conditioning information like text or motion direction, which is impractical in our medical scenario. But, we will provide a review of related work in CV in the final version of the paper.

Q3 What is s_0 in Sec 2.2?
Sorry for the typo. It should be z_0. We will correct this.
Â 
Q4 Does initial frame x_0 guarantee the first frame resembles x_0?
Thanks for the questions. In our experiments, the generated initial frame closely resembled x_0 in content, so we did not include an explicit reconstruction constraint for the first frame.

Q5 Minor comments/grammar
Many thanks for pointing these issues out. We will ensure that all the points raised are addressed and corrected appropriately in the final version.

Reviewer #4
Q1 Synthetic augmentation is not new in medical image processing.
While data augmentation using synthetic samples has been studied extensively in medical image analysis, generating videos from medical images and utilizing them for downstream diagnostic tasks still remains under-explored.

Q2 Latent dynamic diffusion model is not new.
While some computer vision works exist, they typically require additional conditioning like text or motion cues besides the initial frame, which is impractical in our medical scenario. Hence, we devise a new framework for the scene of unconditional video generation. Notably, generating ultrasound videos from static images is an under-explored yet useful task in the medical field.

Q3 Did not provide videos in supplementary materials.
We promise to make our code publicly available and provide generated videos on the code repository page.

Q4 Some details are missing.
Sorry for not being clear on these. The videos in Fig. 2 have a duration of 2 seconds at 8 fps, but due to space limit, only 6 frames are displayed.

Q5 Determine the class of generated video just by the first provided image?
Yes.

Q6 Comparison with other diffusion-models-based methods.
We attempted to compare against [9] but found it computationally expensive and yielding poor results, so we did not report its performance. Additionally, most diffusion models in computer vision require additional prompts, which is not applicable to our unconditional video generation scenario based solely on the initial frame.

Reviewer #5
Q1 Are the training datasets in same size?
The size of the âreal+syntheticâ training set is the sum of the âsynthetic onlyâ and âreal onlyâ training set sizes. The quantity of âsynthetic onlyâ depends on the size of the BUSI dataset. Our goal is improving classification performance by augmenting the real data with synthetic samples, inevitably increasing the overall training set size. It is worth noting that generating these synthetic videos does not incur any additional cost.

Q2 The position of Table 2 can be adjusted.
Thanks for the suggestion. We will adjust accordingly."
https://papers.miccai.org/miccai-2024/809-Paper2811.html,"We thank unanimous acceptance and constructive reviews.

Rev #1: Q) Missing references. 
A) We appreciate your suggestions. We will add them to the paper and discuss their methods.

Rev #1: Q) Why generative models cannot sensitively control the ambiguity? 
A) As generative models perform random sampling from a learned data distribution, they do not âdirectlyâ control the ambiguity of the sampled data in the feature space. On the other hand, adversarial attack-based methods directly control data ambiguity by manipulating attack parameters such as the number of attacks and attack magnitude.

Rev #1: Q) The benefit from the attack is not significant. Which lesion part contributed to this improvement? 
A) First of all, compared with the 2nd and 4th row in Table 2, the Diff-PGD attack itself showed a relatively minor improvement (~0.5%). However, with the uncertainty map (5th row), the pixel-level strength of the adversarial attack is controlled such that the final performance is further improved. Specifically, this improvement is mainly derived from the correct segmentation of lesion edges as the uncertainty map emphasizes the significance of ambiguous edge regions for model training. As shown in Fig. 3, a model has difficulty in predicting edge regions (7th column). This inconsistency with the ground truth (4th column) on these edges is improved with a supervised loss during training.

Rev #3: Q) Sensitivity of noise magnitude. 
A) Our method exhibits low sensitivity to noise magnitude. As shown in Fig. 1, our method projects perturbed data back onto the original data manifold via a diffusion model, even when large noises are added. Also, Fig. 1 in the supplementary shows that our method generates seamless and realistic data even when the attack is sufficiently applied.

Rev #3: Q) Diffusion-based attacks may be susceptible to pre/post-processing. 
A) On the Kvasir-Seg with U-Net, we conducted an ablation study with normalization, which resulted in mIoU of 93.05. This marginal decrease from our original result (93.1) without normalization indicates that our method is robust on normalization.

Rev #3: Q) Long computation time for attacks. 
A) We agree that the iterative nature of PGD leads to long computation times. To reduce it, non-iterative attack methods such as FGSM (Goodfellow et al, ICLR 2015) can be used as alternatives. On the Kvasir-SEG with U-Net, the required time for PGD with K=10 was 67.8s and that for FGSM (i.e., K=1) was 6.26s. However, as shown in Table 2 of the supplementary, we observed a trade-off between performance and time; the mIoU of the FGSM setting was 92.6, while that of the PGD was 93.1.

Rev #3 and #4: Q) Additional experiments. (e.g., model transferability, real-world settings, more datasets, and evaluation of generated labels)
A) We thank the reviewers for these suggestions and acknowledge their importance. However, due to limited space, it is challenging to add all these analyses to the paper. We value the reviewersâ feedback and will include these experiments in detail in the future journal version of our paper.

Rev #4: Q) Were baseline methods applied to generated data? 
A) Data augmentation methods were applied to the given data. As noted in the Introduction, data augmentation methods can control data ambiguity but naturally exhibit a lack of diversity compared to generative methods as they were designed to alter the given data slightly. If they were applied to generated data, it would be beyond the scope of their original approach which may potentially introduce unintended biases. Also, if generated data are used, it is unclear whether only generated images or both generated images and labels should be used, as a generative DNN such as ArSDM generates only images. In this regard, the augmentation methods were applied to the given data for a fair evaluation. Note that our novelty comes from adopting benefits from both methods so that it provides data diversity and ambiguity controllability."
https://papers.miccai.org/miccai-2024/810-Paper0708.html,"We thank the reviewers for their thoughtful comments. The primary concerns are about the description of the contribution and discussion of the experiments. We agree. We address the comments below and will incorporate all feedback in the final version.

[R4] The contribution of MLSW is unclear

The prior research on noisy label training [15] employs meta-learning to discern correct from incorrect data. 
The contribution of the MLSW primarily lies in its extension to the spatial domain, allowing for pixel-level assessment of the fidelity. Unlike previous applications focused on tasks like image classification, our adaptation significantly enhances performance in semantic segmentation tasks, as evidenced in our experiments.

[R1] During the inference process, is Meta-net employed?

Meta-net is exclusively utilized during training process, and not during inference.

[R1,4] How much computational cost does MLSW require?

The computational cost of MLSW is minimal, requiring only 31MB of additional parameters during training. Notably, this computation is needed only during training, with no extra requirements during inference.

[R3,4] Limited comparison with reported accuracy of literatures including nnUnet (Employing CAMUS test dataset)

We recognize the significance of comparative analysis with reported literatures and have revisited our evaluation, this time employing the dice coefficient (Dice) metric. Our NN exhibits Dice scores of 0.94/0.86/0.91 (LV Blood pool/LV wall/LA) in the CAMUS test set.

In contrast, conventional UNet and the SoTA pretraining algorithm SimCLR report 0.91 and 0.92 Dice for the LV Blood pool, respectively (reported in [C1]). Additionally, the widely adopted segmentation scheme, MedSAM [C2], yields scores of 0.87/0.82/0.90 when fine-tuned with the CAMUS dataset. 
Ling et al [C3]. reported that nnUNet achieves a Dice of 0.94 for the LV blood pool when using the CAMUS dataset for both training and testing. However, it is important to note that this high performance is due to the model being specifically tuned to the CAMUS features, making it susceptible to domain shift problems. This vulnerability is evident in the nnUNetâs performance on the Echonet and HMC-QU test sets, where the Dice drop to 0.77 and 0.61, respectively. In contrast, our NN with MLSW demonstrates high generalizability, achieving Dice of 0.95 and 0.94 on these datasets.

For a rigorous evaluation of nnUNet, we re-implemented nnUNet training with all three public datasets presented in the paper. nnUNet shows enhanced generalization, achieving 0.90 and 0.86 Dice on HMC-QU and Echonet, respectively. However, its performance on the CAMUS dataset is limited to 0.91 for the LV blood pool.

We argue that our NN offers comparable accuracy to SoTA models, exhibiting high domain generalization.

[R1,3,4] How does the Nakagami parameter (NAKA) affect the generated Echo image?

We have found that NAKA omega governs US scatterer dispersion, while NAKA mu is related to the intensity of the US signal (brightness of b-mode image)

[R4] Statistical significance is not investigated.

We have performed paired t-test and found that statistical significance is achieved (P-value<0.001) in both in- and out-of-distribution data.

[R4] Regarding the idea of using travel time to compute the NAKA, is the idea novel?

Yes, it is an original concept proposed by our team. Traditionally, the NAKA is obtained by analyzing local patches of the image. However, we propose that performing statistical analysis of the US signal based on travel time provides a more accurate interpretation of US features.

[R1,3,4] Concerns with the figure, notation, and description of the formula. Concerns on reproducibility.

We will certainly improve the clarity of the figure, notation, and related explanations in the final version. Furthermore, we will provide the code to facilitate the reproducibility of our findings.

C1. Saeed, MIUA, 2022
C2. Ma, Nat. Commun, 2023
C3. Ling. FIMH, 2023"
https://papers.miccai.org/miccai-2024/811-Paper2652.html,"Q1: Novelty (R3). Thank you for your insight. While we acknowledge that evidential deep learning combined with multi-view classification has been explored, we believe our method offers a novel and valuable approach. The current popular method, the Dempster-Shafer (D-S) fusion rule, completely ignores conflict, attributing any mass associated with conflict to the empty set. As shown in the following example, in a three-class classification problem, if one view conflicts highly with another, the D-S fusion may incorrectly indicate a high belief for the second class. In contrast, our fusion method adheres to common sense and effectively reduces the likelihood of misclassifications. Mass | Views | D-S | Ours \ b1 | 0.99 0.00 | 0.00 | 0.495 \ b2 | 0.01 0.01 | 1.00 | 0.010 \ b3 | 0.00 0.99 | 0.00 | 0.495 \ u | 0.00 0.00 | 0.00 | 0.00. 
Q2: Performance and statistical analysis (R1, R2, R3). We compare the ECE values of various models in Table 1. ECE is a commonly used metric for measuring model calibration. Additionally, we compare our results with other commonly used multi-view methods, such as ETMC (2204.11423), TMDL-OA (AAAI 2022), and MMD (CVPR 2023). Taking ETMC as an example, we outperform it in accuracy, ECE, and F1-Score by 3.7%, 0.012, and 3.78% respectively. Furthermore, we conducted experiments on OOD detection. As we cannot present the experimental results in the rebuttal, we will release the results and the code on GitHub after acceptance.
Q3: Clarification of Proposition 1 (R2). The role of a view in multi-view fusion is determined by its uncertainty level. High uncertainty results in a weaker impact on the fusion. For instance, if a DWI has high uncertainty due to low image quality, its integration will not significantly alter the overall uncertainty. Even in the presence of severe image corruption like Gaussian noise, the high uncertainty of the DWI will have a negligible effect on the fusion, thereby ensuring the robustness of our fusion method.
Q4: Evaluation of uncertainty estimation (R1). The accuracy of our uncertainty measurement has two aspects: 1. Our method clearly measures the quality of DWI at different b-values, indicating higher uncertainty for both high b-value and low b-value images. 2. Additionally, the uncertainty measurement results after fusion align with Proposition 2. We will provide a more detailed explanation of this in the final version.
Q5: Discussion and revision of cross-view conflict regulation (R2, R3). Sorry for the misstatement in Eq. 11. We use the Manhattan distance as our cross-view conflict regulation. By measuring the loss in this way, the gradients of the same sample under different views tend to approximate each other, ensuring view consistency. We will revise Eq.11 to the form of the Manhattan distance in the final version. 
Q6: Description of Fig. 3 (a) (R3). The purpose of Fig. 3 (a) is to compare the effectiveness of using single-view DWI with our fusion method. It aims to demonstrate the improvement in classification accuracy achieved by our fusion method, while also indicating the limitations in model performance when using DWI with particularly high b-values and low b-values due to their low image quality. 
Q7: Writing misunderstandings and mistakes. (R1, R2, R3). We will address and correct these issues in the final version. 1. â\epsilonâ->â \deltaâ in Proposition 1. 2. âcannot significantly impact the classification accuracyâ-> âcannot significantly impact the integrated uncertaintyâ in Proposition 1. 3. In Fig.3 (b), we indicate in the legend that the data corruption method we used is Gaussian noise. The specific intensity of the Gaussian noise is $\epsilon = 0.5$. 4. The form of integrated alpha is similar to that of belief and uncertainty; it is a weighted average of the alpha values from each view.
Q8: Reproducibility of our work (R1). We will make our code publicly available after acceptance."
https://papers.miccai.org/miccai-2024/812-Paper2276.html,"We appreciate all the constructive comments and suggestions for all reviewers, which greatly facilitated our efforts in addressing the critiques effectively. We are committed to incorporating all the valuable feedback into the final version of our paper. We will upload the code to Github for reproducibility in the final version.

R1
Lack comparison with Braak stage â¦

Thanks for the very constructive comment, weâd like to add the analysis of the comparison Braak stage in our experiment, we have separated our data according to the Braak stage and will incorporate this analysis in the final version.

Provide the ablation studyâ¦

Thanks for your comment, we will add the ablation study for W2 in the final version. W2 distance only has the minimization process.

Format and visualization problemâ¦

Thanks for your careful feedback, we will improve Fig. 2, Table 1 and add the tau flows of CN, EMCI, LMCI and AD in the final version.

R3
â¦reproducibilityâ¦

We have uploaded the code to Anonymous Github for replicability and will publish it in the final version.

â¦provide more analyses beyond different versions GNNsâ¦

Thanks for your very constructive feedback, weâd like to apply our method to different brain connectivity graphs (such as functional connectivity and structural connectivity), and will add a discussion section to analyze it.

R4

Lack of demographic informationâ¦

We will add detailed demographic information in the final version. The data split strategy is 6:2:2 (training: validation: testing) and we will release the code for replicability in the final version.

â¦the time gap between scansâ¦

This is a good question, we did not design a separate module to handle this individual difference, because the ADNI dataset the time gap between scans in ADNI dataset is almost 1 year between two-time points. This is the limitation of our model, we will add a discussion section to discuss this variability in the final version. Thank you so much.

â¦ provide data preprocessing stepsâ¦

Yes, we use the full PET image, we use the standard MNI Space to register all the PET images, and the number of vertices corresponds to 163842. We use the Gaussian function for denoising and will make it clear in the final version.

â¦the novel contributions of its methodology compared to previous work, particularly reference [7]â¦

Compared with Ref. [7], our work has the following novel contributions: (1) our work is based on vertices rather than regions, (2) we formulate the seek for latent cortical tau propagation pathways into a well-studied physics model of the optimal mass transport (OMT) problem, where the dynamic behavior of tau spreading across longitudinal tau-PET scans is constrained by the geometry of the brain cortex rather than inherent brain network topology (nerve fibers). (3) we use Wasserstein geodesic between two density distributions of spatial tau accumulation rather than conventional $L_1$ distance.

â¦the rationale behind choosing GANâ¦

In our work, the important reason for using GAN network architecture to implement the min-max optimization schema is that we sought to design an end-to-end deep model that allows us to include the learning component for capturing the reaction process (mapping from the observed tau concentration to the latent state) in brain cortex. We will make it clear in the final version."
https://papers.miccai.org/miccai-2024/813-Paper2162.html,"We appreciate all the constructive comments and suggestions. We are committed to incorporating all the valuable feedback into the final version of our paper. The code has been released in Anonymous Github and will be published after acceptance.

R1

The illustration of Fig. 1:

We appreciate your very constructive suggestion. We will improve Fig. 1 by incorporating all comments in the final version.

Why constructing measurements based on a GST with structural connectivity is a âjudiciously selectionâ?

First, the use of GST aligns closely with our vision of the human brain, where we conceptualize that cognition and behavior emerge from spontaneous functional fluctuations supported by neural circuits of physically interconnected brain regions. Second, GST offers a flexible multi-scale window to elucidate the SC-FC coupling mechanism with great neuroscience inght and mathematical guarantee. In this regard, we emply GST to generat a set of parital observations from BOLD signals. We will make it clear in the final version.

The choice of the window for the control signal:

The window size for the control signal is chosen empirically to balance coputatinal cost and neuroscience domain knowledge. Note, we fixed this hyperparameter for all experiments.

The control is estimated from brain signal itselfâ¦

The control signal is jointly estimated from the brain signal as the part of our end-to-end deep model. 
We like this reviewerâs idea of using external predefined control force. Our model is flexible to incorporate external process. 
There is converging consensus that the control process is considered Markov in neuroscience field, as witnessed by an increasing number of computational work on Markov modeling for functional dynamics.

R3&R4

The generalization abilityâ¦

Thanks for the very constructive comments, weâre thrilled to share that our model has been tested on the HCP dataset for recognizing seven working-memory tasks. Our results consistently outperform other methods, matching the performance of BolT. Weâre committed to including these HCP experiments in final version.

R4

The background of deep learning and dynamical systemsâ¦

We will add more detailed background of deep learning and dynamical systems to the final version.

Lacking discussion on potential limitation

We apologize that we have not suffiently discuss the limitation of our work, due to page limit. But we are glad to discuss the potential pitfall and solution in the rebuttal letter and include this in the final version.

Scalablity in estimating Koopman operator. It is computational expensive to estimate Koopman operator as more and more brain regions are taken into account. The possible silution is to introduce additional constraints to make it scale up to fine-grained atlas or even voxel-based manner.

Lack of comparison with analytic approaches in neuroscience field. We will benchmark the findings on SC-FC coupling mechanism in various experiment settings including disease connectome in clinical applications.

Prior studies, such as Ref. [15], have showcased the efficacy of the Koopman operator in approximating nonlinear dynamics within spatio-temporal systems. And, various studies, including the present work, have affirmed its effectiveness in modeling the nonlinear behavior of brain systems.

Lacking comparison with other SC-FC coupling methods

As a data-driven deep model, our primary focus is on benchmarking prediction accuracy against existing state-of-the-art methods. We fully concur with the reviewerâs suggestion that comparing the coupling mechanism between SC and FC with current analytic SC-FC coupling methods is an important aspect that we plan to address in future work.

R1&R3&R4
The details of our data process and training â¦

The code has been released in Anonymous Github. The frequency has no physical unit and Itâs defined by the eigenvalues of graph Laplacian. We will add the detailed data processing tool in the final version."
https://papers.miccai.org/miccai-2024/814-Paper1760.html,N/A
https://papers.miccai.org/miccai-2024/815-Paper2115.html,"Thank you for all the reviewersâ (R3, R4, R5) valuable feedback.

(R4, R5) Prompt Usage: For Point Prompt Q_n, n positive points around the CTV boundary are identified using K-Medoids sampling in each axial plane with a ground-truth (GT) mask of the CTV. These n foreground points are generated without additional clicks. The Bounding Box Prompt duplicates the largest CTV maskâs bounding box across all axial planes with a CTV mask, while the Mask Prompt encircles five centroids of CTV areas with a 20-pixel radius, marking inside pixels as 1 and outside as 0. These prompts, along with CT images, are used by SAM-RT to generate predictions based on learned parameters.

(R3, R4) 2D SAM: While 3D CT segmentation ensures spatial continuity, SAM-RT is limited to a 2D structure due to 2D SAMâs pretraining on the SA-1B dataset of over 1 billion natural images. This gives 2D SAM an advantage over models like SAM-Med2D and SAM-Med3D, which used smaller datasets. We applied SAM-RTâs 2D capability to each CT slice independently, then stacked the results to form a 3D volume. Metrics like Dice score (DSC), average surface distance (ASD), and Hausdorff distance (HSD) were calculated per patient to evaluate 3D segmentation.

(R3, R4) SOTAsâ Training: Traditional models (nnUNet, UNETR, UNetGTV, DDNN, SI-Net) were trained from scratch on 121 clinical cases with consistent preprocessing and uniform hyperparameters. SAM-based models (SAM, SAMed, SAM-Med2D, SAM-Med3D) were fine-tuned using LoRA with pre-trained weights. These strategies ensured fair comparison with our SAM-RT model. Performance differences in Table 2 are due to architectural differences and SAM-RTâs techniques, not data adaptation or training setup.

(R3, R4, R5) Paper Structure: Our SAM-RT framework enhances CTV delineation for nasopharyngeal carcinoma by leveraging anatomical knowledge from diverse datasets. Unique to radiotherapy, it employs SAM with techniques like LoRA for fine-tuning, ProViCMA for improved CT and prompt interaction, GaRPA for focused prompts, and SeqLoRA for updating weights while preserving anatomical knowledge. We will include a notation table in the supplementary material and address concerns about hyperparameters by providing guidelines. Abbreviations have been defined at their first occurrence.

(R3, R4, R5) Experiment Analysis: 
(R3) Our paper enhances SAM-RT for CTV contouring by integrating GTV and OAR prior knowledge from multi-center, multi-modality datasets. Using SeqLoRA for fine-tuning, ProViCMA, and GaRPA for dense prompt interaction and task adaptation, SAM-RT leverages CTV dependency on GTV and OAR, guided by expert insights for accurate delineation. Utilizing clinical (121 cases) and public datasets (SegRap2023: 120 cases, HaN-Seg: 42 cases), we plan to expand our dataset for robust comparisons. Table 1 shows the benefits of leveraging prior knowledge, and future experiments with expanded datasets and baseline methods are planned.

(R4) SAM-RT* and SAM-RT in Table 1 represent the SAM-RT+LoRA configuration, which we will clarify in the manuscript. We used ASSD via MONAI for consistent metric computation. If both segmentations are empty, HD and MASD are 0. If one is empty, HD and MASD are set to a large value or positive infinity, indicating unbounded distance. We did not use Bonferroni correction and acknowledge that some comparisons in Tables 1 and 2 were not statistically significant due to inconsistencies in data acquisition and variations in CTV delineation by different oncologists.

(R5) To enhance CTV contouring accuracy, we used an argmax over all predicted masks instead of SAM-RTâs last output. This approach helps handle noisy, incomplete, or ambiguous masks by selecting the one with the highest probability, resulting in cleaner segmentation."
https://papers.miccai.org/miccai-2024/816-Paper0527.html,"We appreciate the reviewersâ comments and address the main concerns below.

MOTIVATION, MOVELTY, and IMPACT (R3, R5): We thank R4 for recognizing that uniGradICON is the first universal registration model that will lead to real-world impact. The practical ramifications of a universal registration network are profound. Before developing uniGradICON, we often had this conversation with potential collaborators: âCan your deep learning approach help us register our novel dataset? Sure, how many images do you have so far? 15.â In these cases, slower and potentially less accurate conventional, optimization-based registration approaches were still required. Now, we can get the best of both worlds: fast and accurate registrations (especially, combined with instance optimization) while largely preserving the generality of conventional registration approaches. UniGradICON has already generated substantial interest from industry and research collaborators, demonstrating its practical impact.

EXPERIMENTS
Statistical analysis (R3, R4): If the policy allows, we will add standard deviations (std). Learn2Reg (L2R) [15] did not report std, only boxplots, so we cannot provide std for their methods,  which is why we did not report the std in the first place. Due to space limitations and our extensive experiments, we cannot include boxplots and qualitative failure results.

In-dist.- Inconsistent evaluations (R4): We observed gradient explosion when training VoxelMorph and LapIRN with the same LNCC similarity measure, which generally happens when using an unsuitable regularizer weight (lambda). Finding the ONE lambda that works for the composite dataset is challenging for VM and LapIRN because the optimal lambda for each dataset is different. Despite testing several similarity loss and lambda values from the official repo, we could only train VM+MSE with default parameters, yielding moderate performance (converging on COPDGene, OAI, and Abdomen but diverging on HCP in Tab. 1). This difficulty for training is discussed in Sec. 3.1 and Appendix Table 7. UniGradICON does not suffer from the issue thanks to the gradient inverse consistency (Sec.5.3 [32]) which leads to better performance. We use the official settings for VM+MSE and (uni)GradICON. To keep the experiment consistent, we use MI for SyN as it is the official default setting.

In-dist. - Missing evaluations in Tab. 1 (R5): Thanks for pointing this out. The top L2R task-specific models in Tab. 4 outperform others due to training with a segmentation-based DICE loss (see discussion in [15 ]Sec. V-B). Among Unsupervised methods, uniGradICON (52.2 on the VALIDATION set) outperforms others (49-51 on the TEST set) in the [15] on the Abdomen CT/CT task. A TEST/VALIDATION comparison may raise questions about fair comparison, which is why we did not add these results to Tab. 1.

Out-of-dist. - L2R validation leaderboard. (R3): The L2R validation leaderboard collects self-reported performance without verification of the L2R organizers. It is unclear whether these methods are trained on the validation set or not. Thus, we only compare with peer-reviewed [15]  results and results recognized by the challenge organizers (test leaderboard posted by the organizers on the website for L2R-NLST).

FUTURE DIRECTIONS
Failure detection mechanism (R4) and extensive study of finetune setting (R5): Introducing failure detection can enhance robustness and trustworthiness but is beyond the scope of the current work. Our finetune experiment shows that uniGradICON can be further optimized for better performance. Due to space constraints, we could not extensively compare finetuning versus training from scratch under various settings, like different dataset sizes and domain shifts. To emphasize, these future works would not be valid problems without the existence of a registration foundation model, which, in turn, demonstrates the necessity and impact of uniGradICON."
https://papers.miccai.org/miccai-2024/817-Paper4152.html,"We appreciate reviewersâ positive comments âThe work is complete and has practicality to the medical AI research field (R1, R4). The method is novel and effective and can be applied to many medical scenarios (R1, R3).â

Responses:

*Ablation study (R1)
We defined the datasets from the left column to the right column in Table 4, ranging from close domain to far domain (stated in Supp. document). Such definition helps investigate the components that contribute to the overall performance. In summarize, the larger the domain gap from the unlabeled data, the more significantly the removal of both the domain separation module (CDS) and the domain adaptation term harms performance. This effect is also observed under conditions of class mismatch (DOE). We will include statistical test results in the ablation study in the revised version if permitted.

*The use of Euclidean distance (R3)
Thanks for the valuable references. Following the agreement maximization principle, we focused on the consistency of two augmented samples to assess UKC with satisfactory results obtained. Considering the cosine-similarity may also work, calculating Euclidean distance for the unsupervised term is less computationally expensive, which benefits training speed.

*Guassian Mixture Model (R3)
GMM is used to model the loss values output by VAE to distinguish the domains of unlabeled data. Using a GMM does not strictly require the data to follow a Gaussian distribution, as it can model data as a weighted sum of several Gaussian distributions. Even if the overall data distribution is not Gaussian, GMM can approximate it well by adjusting the parameters of the individual Gaussian components. While using the loss values output by VAE as a binary prediction with a threshold can yield similar results, we found that GMM can better model the overall distribution of the likelihood of OOD on unlabeled samples.

*Table 1 not clear (R3)
As mentioned in Sec. 3.1, âWe sample 30% close-set samples and all open-set samples from the remaining 17,331 instances to form the unlabeled dataset.â Therefore, the dataset used in each column is obtained from a mixed dataset with 30% close-set samples and the specified open-set samples. For example, the column âDerm7ptâ indicates that the unlabeled data is from â30% ISIC and Derm7ptâ. Please refer to our Supp. Table 1 for details on the open-set settings. We will carefully revise Table 1 to improve readability.

*Warm up (R3)
In traditional SSL settings, warm-up is a widely-used technique to ensure the generation of high-quality pseudo-labels. The model first learns from labeled data and then gradually increases the weight of the unsupervised term to incorporate unlabeled data. In this work, we did not deliberately tune the coefficients of the unsupervised/DA term but simply followed the settings in [15].

*More comparison methods (R4)
We appreciate the valuable references. We conducted the benchmark following [9], including 6 classic close-set SSL methods, 5 recent open-set SSL methods, and 1 directly related universal SSL method. Our methods are simple, plug-and-play, and effective across most SSL methods. We will include more related references with detailed discussions in the revised version.

*Novelty (R4)
Our primary focus is on the assessment of UKC and UKD. We intentionally kept the SSL and DA components simple, to better investigate the factors contributing to the performance of our overall framework. We evaluated our method on two medical domains to validate its generalization, with the aim of addressing realistic scenarios involving extensive but chaotic unlabeled data.

*Reproducibility (R4)
We will release the code for this work and implementations of compared methods. We hope this codebase will benefit the research community on related problems.

*Others
We apologize and will carefully revise some writing issues.

We hope this response can address your concerns and sincerely thank you in advance for your consideration."
https://papers.miccai.org/miccai-2024/818-Paper2215.html,"We thank the reviewers for their positive feedback on our novelty and theoretical rigor of our polynomial-based topology refinement method.

Completeness and Adaptability
[R1, R3] Our approach utilizes a complete polynomial basis to synthesize training data, allowing it to approximate any complex topological structure with arbitrary precision, as guaranteed by the Stone-Weierstrass theorem [24]. This ensures that our method can adapt to a diverse set of topologies across different real-world scenarios, making it universally applicable.

Cross-dataset Evaluation
[R1] We evaluate the TopCoW dataset for 3D segmentation on purpose due to its challenging topological structures, including both 2D and 3D topological errors (holes). We also test our method on the CREMI and FIVES datasets for neuron and retina vessel segmentation, respectively. These results were not included in the submission due to space constraints and redundancy but will be provided in the supplement. Our method outperforms SOTA baselines in all metrics on these datasets, e.g., achieving 83.92 Dice and 6.11 Betti error on CREMI, compared to Warp-loss [9] results of 83.03 and 11.78. We emphasize that our goal is to enhance topological correctness while maintaining Dice performance.

Baseline [23]
[R1] We compare with a heuristic post-processing method that filters small isolated outliers. We will add [1] as additional reference. We disagree with the reviewer and find that a review paper is suitable for referencing a family of methods and their common post-processing techniques.
[1] Vlachos, M., Dermatas, E.: Multi-scale retinal vessel segmentation using line tracking. CMIG, 2010.

Introduction of New Errors
[R3] We experimentally demonstrate that our refinement eliminates topological errors without introducing new errors. Our method improves performance across all baseline models and avoiding dataset-specific biases, as shown in Tab. 3.

Polynomial Families and Order
[R3] As stated in âPreliminaryâ, our three orthogonal polynomial bases are selected for their favorable analytical properties. For instance, Chebyshev Polynomials Tn(x) have n-1 critical points when n â¥1 (see Appendix). The polynomial order is chosen experimentally that incorporating more high-frequency terms can lead to anatomically implausible structures, as discussed in âAblation studyâ.

Statistical Significance
[R3] Following [17], we conducted a Wilcoxon rank test to assess statistical significance. A T-test is also conducted, both resulting in statistical significance (p<0.05).

Topological Guarantees and Integration with SOTA Methods
[R3] Our method is compatible with any other segmentation method, including topology-aware methods with variant guarantees. However, none of these methods can guarantee topological correctness during inference; they rely solely on the networkâs learned representation capability. Our method learns a robust representation by addressing synthetic topological errors with guaranteed variability. As illustrated in Tab. 3, we combine our method with cl-Dice, Boundary loss, PH loss, and warp loss, showing consistent reduction in topological errors.

Network Architecture
[R4] Our post-processing network uses a U-Net [21, 13] for fair comparison, as stated in âQuantitative Evaluationâ. We will make our code available together with the camera ready version.

Training Protocol and Time
[R4] Our method randomly generates synthetic data in real-time, with a training time of 0.29 seconds per batch, 21.0 times faster than the baseline method PH-loss [10].

Clarification of Domain-Agnostic Design
[R4] Our model, trained on synthetic data using only the GT labelmap, is agnostic to covariate (domain) shifts, e.g., imaging devices and scanning protocols. Such a design makes our method a robust plug-and-play option for various upstream segmentation models. Retraining our model on new tasks with different topological structures is straightforward and quick."
https://papers.miccai.org/miccai-2024/819-Paper1593.html,"We deeply appreciate the thoughtful and insightful comments provided by the reviewers. Your expertise and attention to detail have greatly enriched our work.
In this work, we proposed a novel soft-labeled contrastive learning (SLCL) approach for unsupervised domain adaptation (UDA) tasks. The proposed method alleviates the sparse feature space and incorrect label assignment of current hard-threshold (HT) contrastive learning (CL) solutions. We showed the effectiveness of each proposed component and a significant improvement compared with the other SOTA methods on two cardiac data sets. We are encouraged that the reviewers found our work intuitive (R1) and innovative (R1, R4, R6). We appreciate their commendation of our writing skills and clarity of expression (R4). We are glad they found the workâs open-source spirit (R1). We are pleased that the experiments and results are positively assessed (R6).
Top-tier models (R1):
Experiments with top-tier models are exciting but fall outside the scope of this paper. Since R1 does not specify, we assume it refers to Transformer or diffusion models. These architectures demand extensive data due to their complexity and high parameter count. However, medical image segmentation often lacks sufficient training data. Although collecting data from multiple data sets is possible, it conflicts with our objective of UDA.
Most comparison methods utilized Unet-like architectures or Deeplabs. To keep the consistency in the model architecture and make our results directly comparable, we find it practical to retain the results from experiments with the Unet architecture.
Task specification (R6):
We acknowledge the omission of the specific objective in the paper, which is designed for cardiac image segmentation. We will change the title to âUnsupervised Domain Adaptation using Soft-Labeled Contrastive Learning with Reversed Monte Carlo Method for Cardiac Image Segmentationâ. 
We will revise the first sentence of the Introduction to: âAccurate cardiac segmentation is crucial for various medical applications. In clinical settings, multi-modality medical images are extensively utilized to aid diagnosis. However, automatic cardiac image segmentation often suffers from performance degradation due to a lack of labels.â
We will analyze the connection between cardiac image segmentation and the proposed method and include the following paragraph in the Method section: âThe HT criteria used in current CL methods may introduce incorrect pseudo-labels, resulting in scattered classification in cardiac image segmentation and inaccurate diagnosis in clinical applications. In contrast, the soft-labeling (SL) strategy smooths the impact of ambiguous pseudo-labels and conducts better consistency within each category of the cardiac anatomy (Fig. 3).â
Investigation of SL and centroid norm regularizer (CNR) (R1 & R4)
In Fig. 4b, we presented extensive parameter studies for SL and CNR. We compared the performance of the proposed method under SL and HT strategies with different thresholds (0.0 to 0.9), demonstrating the robustness of SLCL and interpreting the trends in Section 3.2. Additionally, we examined SLCL with and without CNR, showing consistent improvement with CNR across various thresholds.
Impact of the reversed Monte Carlo Method (rMC) (R1 & R4)
We did abundant experiments (Fig. 4a) to show the impact of rMC, ranging the partition number P from 1 to 512. In Section 3.1, we interpreted the value of P as a trade-off between centroid stability (P=1) and feature compactness (P=512). The ablation study with the t-SNE plot (Fig. 5) for rMC shows more compact feature space and better class separation for SLCL with rMC, which confirms the interpretation and experiment results above.
Due to the rebuttal rules, we are not allowed to add more experiments, results, or analyses. Nevertheless, we deeply appreciate all the comments regarding further interpretation and investigation."
https://papers.miccai.org/miccai-2024/820-Paper2012.html,"We thank all reviewers for their valuable feedback
R3) Metrics: We report the most common metrics and observed equivalent performance deviations between methods for e.g. accuracy
R3) cGAN/FCL information: We followed the original cGAN implementation and all adjusted HPs and used data are provided in Sec 3.2. Further information is contained in the supplements. We carefully detailed FCL in Sec 2C and Fig1b
R3+4) Train strategies/comparable methods: We provide all HPs and follow the original implementations (Sec 3.2). We performed grid search for a fair comparison, with more details in supplementary
R3) Future work & strengths/weaknesses of methods: We already provide information on both, e.g. scarce data settings + catastrophic forgetting in Sec 4 Ablation, runtime issues of Macenko in Sec 3.2, performance deviations Tab1. We will add more information to our discussion
R3+4) Information on domain transfer issues: This is numerically shown by comparing to Baseline experiment (Fig 1a, Tab1), visualized in Fig1c and discussed in Intro
R4) Literature: We include methods evaluated for inter-stain setups and cover a wide variety of strategies (classical, GAN-based and semi-supervised approaches). On the other hand, HistAuGAN/ContriMix are evaluated for intra-stain setups. We chose cGAN for supervised augmentation by following the benchmark of Zingman et al. and comparable work Bouteldja et al. (see Introduction). Note that HistAuGAN/ContriMix could potentially replace the cGAN component, which we will consider in future work
R4) Task agnostic: Both mentioned works havenât been evaluated for dense prediction tasks like segmentation. Computer Vision literature often shows different performance deviations for methods evaluated between classification and dense tasks, Tab1 reflects this
R4) Novelty & FCL: We explicitly designed our FCL component for stain adaptation in pathology and focused on forcing the network to learn the same representations for different stainings in a layer-wise fashion (major novelty). Moreover, we learn in an unsupervised and supervised manner with cGAN in parallel, another novelty. These facts are highlighted as strength by all reviewers
R4) Domain label dependency: The stains in all public datasets and also in our internal dataset are inherently labeled, since the staining agent is always known in a pathology workflow
R4) Data/SOTA: In our work we focus on the problem of inter-stain variations (Fig1a, Tab1), whereas the Chameleon dataset only provides a single staining (H&E). Our experiments also comprise several public external datasets (Sec 3.1). We provided numerical evidence for the efficiency of our approach
R4) Inter-stain variations might hide information: As pointed out,  stains do not only vary in color, but may also provide different biological information (e.g. stainings that specifically highlight immune cells). However, the addressed tasks of cancer detection and kidney tissue segmentation across inter-stainings are feasible for all stainings. Nevertheless, we will add this aspect to the discussion as a possible limitation and highly appreciate this feedback.  After consulting with our medical collaborators, we do not think our models are hallucinating, as this special case of missing biological information is not contained in our experimental setup and results showed improvements
R5) Ambiguity about strategy application: Our method is designed to train a model âto rule them allâ, so is robust to all considered inter-stain variations and is also evaluated for this setup (Intro, Tab1). We think this is an important step towards training foundational models
R3+5) Patient data details: Kidney: we evaluate on external datasets NEPTUNE and HuBMAP which were hidden in training. Breast: We split data on patient level to avoid data leakage. Thus, no patient or slide appears in both train and test sets (Sec 3.1). More information on data is contained in the supplementary, we will add patient and WSI counts."
https://papers.miccai.org/miccai-2024/821-Paper1060.html,"We thank the reviewers for their feedback and appreciate their recognition of our novel fine-tuning technique, the training on smartphones for low-resource environments, and our comprehensive evaluation of various datasets, which together highlight the practical significance of our research.

Our work, submitted to the health equity track, aims to democratize medical imaging by training Neural Cellular Automata (NCA) across multiple mobile devices. This approach not only makes medical AI technologies accessible but also evaluates their performance and adaptability in varied real-world settings.

(R3) Advances in comparison to Med-NCA[11]: This workâs major technical contribution is the introduction of the Variance Weighted Segmentation Loss (VWSL). This novel unsupervised training leverages random activations within NCAs to reduce the influence of domain shifts on prediction quality. Additionally, our contributions confirm the feasibility of employing NCA training on various Android devices confirming its great potential as a means of democratizing medical AI technologies.

(R3) Additional Minimal UNet Experiments: While comparing with minimal UNets could offer further validation, Med-NCAâs (26k parameter) performance, which matches a 37M-parameter UNet, already demonstrates its effectiveness. Literature, including [11], shows that reducing parameters results in performance drops of different degrees (e.g., 10% worse Dice on the prostate segmentation task with a 6.3M-parameter UNet). This gap can be lowered with autoML strategies, as the 1.9M-parameter nnUNet [11] on the small hippocampus dataset (64x64 images) shows. However, this is not directly comparable to our context of 256x256 X-ray images, where nnUNet autoconfigures to 30M parameters.

(R3) Fine-tuning and validation should use more than 5 images: It is important to note that the results presented in Table 1 and Figure 6 are indeed based on models fine-tuned and tested on 50 images each, ensuring robustness in our measures. Figure 5 shows training on 50 images and fine-tuning on 5 to demonstrate VWSL effectiveness with minimal data, a critical aspect of real-world application.

(R3) Clarifications of VWSL Loss (added to Section 2.2): VWSL adapts Med-NCA unsupervised to new domains by generating mean predictions and variance maps from multiple forward passes. VWSL combines Dice Similarity Coefficient and Focal Loss and is weighted by pixel-wise variance, acting as the surrogate loss preventing the trivial solution of setting all outputs to zero. Fine-tuning involves 100 additional epochs compared to the initial 1500, with the impact of different variance minimization weightings evaluated in the ablation study in Table 2.

(R1) Unsupervised finetuning performs worse than supervised training on the dataset: Our work demonstrates that VWSL-fine-tuning enables Med-NCA models to generalize better across datasets. While there is a gap compared to models directly trained on the domain, the average improvements of 0.7-2.8% Dice across experiments in Table 1 and individual gains of up to 24% Dice in Figure 4 validate the potential of fine-tuning and are crucial when ground truth data is unavailable.

(R1) Optimal Training Strategy: In real-world scenarios, the choice of the initial training dataset is predetermined by available data. Our study emphasizes that regardless of the initial dataset, VWSL-fine-tuning consistently enhances model performance across all tested optimizations. This demonstrates its robustness and adaptability.

(R5) Clarification Table 1: The â-â results in Table 1 indicate in-distribution scenarios where training and test data originate from the same source. Since there is no domain adaptation needed it does not present the challenges our work addresses.

(R5) Impact of Smartphone Images: In environments without digital X-rays, such as rural areas in LMICs, smartphone images allow for digitizing X-rays to assess diagnostics and monitor diseases remotely."
https://papers.miccai.org/miccai-2024/822-Paper3238.html,N/A
https://papers.miccai.org/miccai-2024/823-Paper2336.html,"We appreciate the thoughtful feedback provided by the reviewers and value their recognition of the paperâs well-written quality. We are pleased that they acknowledge the novelty of our method, particularly highlighting the efficiency of our wavelet non-local regularization term. The reviewers also found the topic clinically relevant, noting its potential to contribute to the development of new CT systems that could significantly reduce radiation doses. They appreciated that our method is well-described and easy to reproduce. We are encouraged by their recognition of our methodâs strong performance, which effectively enhances PSNR and SSIM across diverse simulated scenarios compared to relevant state-of-the-art methods. Additionally, we want to express our gratitude for acknowledging the time and effort invested in setting up our own CST geometry implementation, which deviates from classical CT setups. We will ensure that the source code, along with the dataset and physics systems, will be made available soon. We are grateful for all the valuable suggestions and will incorporate them to further improve our work.

Q #1 - Time costs compared to post-processing methods: Reducing time costs for unrolling networks remains a challenge due to their iterative nature. While post-processing methods are data-intensive, unrolling networks leverage physics constraints to outperform them. To improve efficiency, we can consider reducing the number of iterations required for convergence by using second-order quasi-Newton methods like L-BFGS.

Q #2 - Missing references of wavelets-based methods: Thank you for pointing out those references. We appreciate the insight and will consider them in our future revisions.

Q #3 - Limitation of the current simulation approach: As the reviewer pointed out, CST is still an emerging imaging technique with only a few prototypes under development. A practical CST system should include a monoenergetic source and energy-resolved sensors to capture scattered photons by their energy, and a 2D system would also use plane collimators to restrict photons to a plane. However, it cannot be guaranteed that each photon is scattered only once before reaching a sensor. Therefore, the forward model of a CST system must consider multiple-order scattering.
Our proposed model accounts only for first-order scattering as significant for reconstruction, treating multiple-order scattering as noise. Although Compton kinematics ensures that first-order scattering represents most scattered photons, the impact of multiple-order scattering on data degradation remains an open question. An alternative approach would be to model multiple-order scattering in the forward model, as suggested in [1], where the author includes second-order scattering.
Additionally, the forward model must consider matter attenuation, incorporating two exponential terms in the Radon transform, as explained in [2]. Currently, this problem is addressed with iterative algorithms for reconstruction correction [3].
Finally, finite-sized sources and sensors, along with their imperfections, should also be included in the model. While the exact impact of this on data requires further quantification, blur is anticipated.

Q #4 - Extension of this modality in three dimensions: We acknowledge the concern about the 2D reconstruction problem given that Compton scattering is inherently a 3D process. Extending our approach to 3D volumes is indeed possible and potentially beneficial. A 3D setup, with a fixed source and a detector moving on a sphere, would eliminate the need for collimators and lead to a new toric Radon transform. Although the inversion of this transform presents theoretical challenges, ongoing work aims to address these issues.

References:
[1] G. Rigaud et al., Inverse Problems, Vol. 34, 2019
[2] C. Tarpau et al., IEEE TRPMS, Vol. 4, 2020.
[3] C. Tarpau et al., NDT in Aerospace, Vol. 25, 2020."
https://papers.miccai.org/miccai-2024/824-Paper0770.html,"We thank the reviewers for their thoughtful feedback and time. The reviewers agree on pioneering methodological novelty, commend the new research direction,  clinical significance, and thorough evaluation.

(R1, R4) Clarity of method description: We use a cascading diffusion model approach. (1) generation of a low-resolution (1024x1024 pixels) WSI using the first of three CDMs. (2) results from the first sage are enhanced through overlapping patches of the generated low-resolution image as conditioning for the second CDM. This second model uses the spatial context provided by the first to generate higher-resolution images (also 1024x1024 pixels) for each patchâs center. These patches are then stitched together, taking into account the overlaps, to form a medium-resolution image of 6400x6400 pixels. The process is repeated with the third CDM to achieve a final, high-resolution synthetic WSI of 41,344x41,344 pixels. This method allows each stage of magnification to build upon the last, refining details and expanding the image size. Inpainting ensures seamless integration of patches, avoiding artifacts and ensuring that synthetic images are useful for both computational analysis and practical clinical applications. As explained in the caption of  Figure 1 the blue box is the conditioning patch, the green box signifies the center patch that is generated and the red box shows the output of size 1024x1024 for each image. We will use colour font to highlight this in the caption.

(R4) downstream experiments/computational costs:  Our method is particularly useful for niche domains where limited or no public data is available, thus kidney pathology is a prominent example in our work. We also integrate public datasets to a) show that our method scales to other domains and b) to foster reproducibility. To the best of our knowledge only very few works focus on large scale dependencies and we are the first to synthesize at WSI scale. Using synthetic patch data as augmentation has been evaluated in literature before. As stated in the introduction, our main aim is to make such data publicly available through synthesis and to enhance other data, e.g., for bias mitigation. This is essential, when downstream tasks depend on WSI-level assessment such as structural assessment in the kidney in contrast to, e.g., localized cell anomalies in cancer. We will clarify computational requirements in the camera ready version. Note, that synthesis only needs to be done once before, e.g., a synthetic dataset can be shared.

(R3, R4) expert validation: To the best of our knowledge our work is the first to show synthetic data that is essentially indistinguishable from real data for experts. At present, we only have access to kidney pathologists and will expand this evaluation in future, since human expert user studies need to be carefully balanced with the workload of rare experts. Note that the shortcut mechanism (R3) is extremely rare and cannot be used to reliably identify synthetic data at scale. We will clarify in the paper.

(R3, R4) usefulness of synthetic WSIs: Many patch-based pathology algorithms rely on âbags of patchesâ sourced from complete WSIs; therefore, synthesizing the entire WSI is crucial for providing realistic and structurally accurate training data. Furthermore, the generation of high-resolution WSIs mirrors clinical practice and can also be used for human training on top of data sharing, augmentation, and bias mitigation options as discussed above. We will discuss differences of large-mask synthesis (Aversa et al.) vs. whole WSI generation in the paper.

(R1) only a few examples: we evaluate on three different datasets and the supplement shows several visual examples. We will add more visual examples to the supplement in the final version.

(R3, R4) We will publish our code together with a synthetic dataset from our kidney data with the camera-ready paper as a public github repository."
https://papers.miccai.org/miccai-2024/825-Paper1942.html,"We thank the valuable comments from all the reviewers and address all the concerns as follows:

To All Reviewers:

Q1 Open-Source Research

We will make our data, code, and pre-trained models open-source after acceptance.

To R#1

Q1 External Evaluation

We conducted external evaluations on the IDRiD, APTOS2019, and Messidor datasets and found that our UrFound model demonstrates strong generalizability, outperforming RETFound and FLAIR in most cases, with statistical significance based on a t-test with a p-value of 0.05.

Q2 Eqn 2, Fig 1, and Typos

Thanks for pointing these out. We will revise Eqn 2, improve Fig 1, and correct the typos accordingly.

Q3 Complementary Information Between CFP and OCT

We acknowledge that our claim may be overstated without testing on paired CFP and OCT images. We will revise our statement to: âUrFound captures information from both CFP and OCT images and performs well with both imaging modalities.â

Q4 Comparison to Models Trained with Labels

We have compared our UrFound model against FLAIR and supervised task-specific models, which are also trained with disease labels. However, we acknowledge that UrFound relies on disease labels for training, and we will make this point clearer in our revised paper.

To R#3

Q1 OCT Scans as 3D Volumes

We followed the same settings as RETFound to train on 2D OCT slices. Learning from 3D OCT scans could be an interesting avenue for future work.

Q2 Discussion and Comparison to General Vision-Language Models

We have discussed related works on vision-language pre-training in the supplementary materials. We have compared our model with MAE and FLAIR, a CLIP-based retinal model, and plan to explore more vision-language approaches in the future.

Q3 Implementation Details

We resize the input image to 224Ã224 and preprocess the data following RETFound. Full implementation details will be included in our revised paper.

To R#4

Q1 Novelty

We want to clarify that most existing medical vision-language pre-training models are not trained on retinal images, and itâs unclear if methods designed for other medical images work well with retinal images, especially when dealing with multiple imaging types like CFP and OCT. In this work, we propose the first universal retinal foundation models for both CFP and OCT images, using expert knowledge, which has not been studied before.

Q2 Fair Comparison
We do compare with SSL methods, including RETFound and FLAIR, which are based on MAE and CLIP, respectively. They are SOTA retinal foundation models.

Our UrFound training dataset has significantly fewer images compared to RETFound (160K vs. 1.6M) and is actually a SUBSET of the training data used by FLAIR. This demonstrate that UrFoundâs improvements stem from our knowledge-guided pre-training strategy, not just the data.

Q3 Statistical Significance Analysis

We repeated our experiments 10 times with different random seeds and conducted a t-test with a p-value of 0.05. Our results show that UrFound performs similarly to the second-best method on IDRID and JSIEC, and significantly better on the other six datasets.

Q4 Why UrFound Adapts MAE

We develop UrFound based on MAE for fair comparison and systematic analysis, since the SOTA retinal foundation model, RETFound, is based on MAE. We adopted the same experimental setup as RETFound, ensuring that any improvements stem from the proposed pre-training strategy rather than differences in model architectures or initialized weights.

Q5 Superiority of UrFound

UrFound has two main advantages over existing retinal foundation models: 1. It uses expert knowledge via text supervision to capture domain-specific features, improving representation learning. 2. Unlike existing methods that require separate models for CFP and OCT images, UrFound handles both modalities within a single model."
https://papers.miccai.org/miccai-2024/826-Paper0134.html,"We are immensely thankful to AC and all the Reviewers for their thorough review of this manuscript and for their valuable suggestions. We note that both reviewers provided comments on the syntax of the writing of this manuscript as well as the description of the equations. Therefore, we have made improvements in response to reviewer comments."
https://papers.miccai.org/miccai-2024/827-Paper1949.html,N/A
https://papers.miccai.org/miccai-2024/828-Paper2909.html,"We deeply appreciate your valuable comments. As you mentioned, VDPF is an innovative classification framework (R4) and the first application of CAD technology in BTI-based DVT staging (R5), expanding diagnostic tools for vascular health (R4). We believe VDPF can greatly contribute to the CAD community. Special thanks to R5 for the direct acceptance and to all reviewers for your positive feedback:
1.Effectiveness (R4: âimproves the specificity and reliabilityâ R5: â outperformsâ );
2.Nice written (R4: âwell describedâ All reviewers appreciate the clarity and organization).

1.Limited description(R3&R5).
Due to space constraints, we cannot perform detailed descriptions in the paper. For replication and understanding, we will make our source code publicly available once anonymity is lifted.

2.Concerns about the modelâs robustness(R3&R4&R5).
-(R3-âComparison methods are relatively limited and oldâ) We compared well-known classical methods in the field of medical imaging, including the recently popular ViT, effectively supporting our performance.
-(R4-âlacks cross-validation(CV)â) Due to the large sample size (100,000 slices) and time constraints, we do not perform CV. While CV is common, it may overestimate performance (DOI: 10.3390/s23136077). We use a randomized split for training, validation, and test sets. The test set directly reflects the modelâs performance.
-(R5-âconfusion matrixâ) Instead of a confusion matrix, we provide the F1-score, which balances precision and recall to sufficiently indicate potential biases. Accuracy trends across multiple models also highlight VDPFâs superior performance. 
-(R5-âLack of dataâ) Collecting data for DVT is a common challenge in this field (Ref[5] includes only 43 cases). We calculate the required sample size using a One vs. Other approach with MedCalc (V.15.6.1.0), which statistically meets the hypothesis requirements. We also employ data augmentation and Focal Loss to mitigate small sample size and class imbalance issues.

3.Why is the performance of resnet-50 better than Resnet50-DPF(R3)?
ResNetâs convolution mechanism does not focus on small targets like ViT. Additionally, the preprocessing differs between ResNet (cropped to enhance lesions) and Resnet50-DPF (without cropping). These factors prevent Resnet50-DPF from effectively combining local and global features, leading to lower performance than expected.

5.Why use 2D data(R5)?
We focus on the application of local imaging, regardless of whether it is 2D or 3D. Additionally, due to equipment limitations, we currently do not use 3D data for training."
https://papers.miccai.org/miccai-2024/829-Paper3997.html,"We thank the reviewers for their insightful feedback. We are glad they appreciated the methodological contribution of our proposed VP loss (R1, R3, R4), found the problem interesting (R3), the evaluation and results satisfactory (R1, R3), and the paper clear (R1, R3).

[R1, R3, R4] Concerns about reproducibility: We apologize for not making it clear that the synthetic WBC holographic dataset and the code (loss computation as well as pretrained models) will be released upon publication. Unfortunately the real WBC data cannot be publicly released as it is proprietary medical data. In addition, the supplemental document will be reduced to focus on sections 4 and 5 corresponding to implementations details (R1, R3, R4) and fit in the 2-page limit (R1).

[R1] Are encoder and classifier jointly trained?: We will specify in the paper that the encoder and classifier are trained separately as joint training leads to a decrease in detection performance (see example in Section 6 of the supplemental doc.).

[R3] What are limitations and potential solutions?: We identify two main limitations (which are listed in the conclusion section): (i) trading off recall for computational efficiency, and (ii) being restricted to sparse images. On the one hand, we hypothesize that recall could be improved (while maintaining efficiency) if a supervisory signal was provided to guide the support of our sparse encoding volume. On the other hand, sparsity is a fundamental assumption in the formulation of our method, and therefore it could not be applied to arbitrary domains unless the images are sparse under a differentiable and invertible transformation.

[R4] Inconsistent results in real and synthetic data: We would like to correct R4âs observation and point out that in fact VP-CE outperforms LLP-PLOT in both synthetic and real data (note that metrics are different, for synthetic data we use accuracy, whereas for real data we use proportion prediction error). We also wish to clarify that, as noted by R1, we claim âsimilar or better performanceâ with respect to LLP-PLOT, with our method having methodological novelty and computational advantages.

[R4] How to select a cost function (CE vs L2)?:  VP-CE loss would be a reasonable choice for classification applications as CE is a widely used loss for classification tasks and it consistently shows good results throughout our experiments. In the paper we include results with a different cost function (i.e. L2) to emphasize the flexibility of our framework.

[R4] How is entropy computed?: The numbers reported in the results section correspond to the entropy of the output of the classifier, averaged across samples. Ideally, we would like each prediction to be close to one of the vertices of the simplex, in which case the average entropy is small.

[R4] Writing and table formatting: We are happy to incorporate specific suggestions if appropriate, but unfortunately, we are not sure what specifically the reviewer is referring to.  We note the other two reviewers give strong scores and comments for clarity.

[R4] Details of implementation: Please see our response above on reproducibility."
https://papers.miccai.org/miccai-2024/830-Paper3978.html,"We appreciate Reviewer #1âs recognition of the significant improvement our method presents over domain-agnostic detection algorithms and the excitement around leveraging pre-trained foundation models in medical imaging. Reviewer #3âs positive feedback on our innovative approach of merging semantic information from CLIP and spatial features from SAM, as well as the effectiveness of the Wasserstein loss, is greatly appreciated. Additionally, we are grateful to Reviewer #4 for acknowledging the novelty and impact of our proposed method, as well as the detailed ablation studies and comparisons provided. Below we address all the constructive comments in detail and will incorporate this feedback and code link in the revision.
(1) Clarity of Figures: We will revise Figure 4 to present confusion matrices for each configuration instead of the current confusion star plots to enhance readability. Besides, we have corrected some minor errors, such as spelling mistakes, throughout the paper.
(2) Annotated Bounding Box Requirement and Comparison Models: During training, the annotated bounding boxes are input to SAM and used for training object detectors as well as the trainable parameters in VertFound. During testing, the bounding boxes input to SAM are predicted by a pre-trained detector like YOLO v8. In other words, the bounding boxes input to VertFound are those predicted by the trained object detectors. We will clarify this in the latest version of our work. Therefore, our comparison is fair, as both our method and other models like YOLOv8 and DETR rely on annotated bounding boxes for training and predicted bounding boxes for testing. Additionally, as our method requires the position predictions of different vertebrae to further determine their classes, the focus of our comparisons is mainly on object detection methods.
(3) Generalization and Adaptation to 3D Scans: Thanks for your suggestions. We will extend our experiments to include additional datasets from different sources and modalities to test the generalization capabilities of our method. Additionally, we acknowledge the suggestion to adapt our method to work with 3D MRI scans, which are more common in clinical practice. This expansion will be a focus for our future journal version of the paper, where we will further explore and validate our methodâs performance on diverse and comprehensive datasets, including 3D MRI and CT scans.
(4) Code Availability: We have now fully uploaded the code and detailed instructions to facilitate reproducibility. We apologize for the confusion regarding the initially empty code repository and assure you that it has now been updated with the complete code. We are committed to continuing to improve and extend this work, ensuring that our contributions to medical imaging research remain robust and impactful.
(5) Experiment Setup and Analysis: Thank you for pointing out the unclear explanation of loss weights and statistical significance. We will clarify all of these expressions in the latest version of our work. Furthermore, in the future journal version of the paper, we will continue to explore the impact of missing detections on the final classification results, further deepening our research.
In conclusion, we express our gratitude to the reviewers for their detailed and constructive feedback. Your insights have been invaluable in identifying areas for improvement, and we are committed to addressing these issues in our revised submission. We appreciate your careful review and look forward to continuing to refine and enhance our work."
https://papers.miccai.org/miccai-2024/831-Paper2366.html,"We thank all reviewers for their valuable comments and address them below.

â¢Segmentation accuracy (R1,R4): upon training the nnUnet model, we validated it on 11 CT scans segmented semi-automatically by a radiologist, achieving a 0.929 modified Dice Coefficient ([P â© GT]/GT). While it is true poor segmentations could harm detection performance, our results on 3 datasets indicate vessel awareness improves/conserves detection Sensitivity while reducing the FPr. Moreover, if we were to evaluate the model on an OOD sample for which segmentation was less accurate, such a sample would likely be challenging too for a non-vessel-aware model.

â¢Segmentation cost/relevance (R1,R3): Segmentation+EDT takes ~4 min., detection takes ~3 min. (RTX 3090 GPU). Per our radiologist collaborators, the target scan processing time before human review is 20 minutes, so ~7 min. is acceptable. To address R3âs concerns about Sensitivity of vessel-aware/unaware models, we note that, as stated in Sec. 1, our goal in using vessel awareness is to reduce FP detections; indeed, the FPr of the vessel-aware model is ~40% lower than the non-vessel-aware model. This reduces the time radiologists spend looking at FPs and highlights how vessel awareness retains high Sensitivity while reducing human labor, thus offsetting the cost of segmentation.

â¢Lack of statistical tests (R3): albeit we explained the positive role of vessel awareness, we note our approach also comprises multi-scale deformable attention, a component that, by itself, improves Sensitivity by ~10% (see T3, rows 6, 8). However, we will try to include pairwise tests in the final paper to make our ablation study more informative.

â¢Confidence/NMS threshold choices (R1,R3): we set the confidence thresholds in T1 based on each modelâs Sensitivity@FPr=1 on the train data. Moreover, in Fig. S2 (supp. material), we report Se@FPr curves (akin to ROC) to summarize the threshold-agnostic performance of each model. Notably, Fig. S2 shows our model performs the best for most thresholds. As for NMS, we follow the literature [5] in using a small NMS threshold for running inference on overlapping 3D crops to avoid having multiple detections for the same aneurysm.

â¢Ambiguity re: matching (R3): for a GT aneurysm, we always match the closest detection (irrespective of distance) to compute the loss. Other detections are only matched to a GT if the L2 distance between their centers is under 0.5 voxels (~0.2mm).

â¢Aggregation of ablation study results (R3): Since we compute these results by averaging existing results on the internal/external partitions, we will disaggregate them over datasets/aneurysm sizes in the next version.

â¢Completeness of baselines (R1,R4): Most aneurysm detection work lacks public code/weights (see Bizjak [3]). From our literature survey, only [4] (GLIA-Net), [1] and [6] released code/weights among the best solutions. However, [1] uses pose annotations that arenât in our training data and [6] employs atlas-based co-registration of points of interest. Also, both use TOF-MRA, not CT data; so their trained models would suffer from a domain gap. Still, in the detection setting we compare against a nnDetection [2] model trained on the same data as ours (nnDet won the 2020 ADAM Aneurysm Detection challenge & is the best baseline in [1]). Re: segmentation, we could have tested nnUnet, but [1] shows nnUnet & nnDetection perform similarly, so we focused our resources on testing more detection baselines.

â¢Modified segmentation metric (R3): although we modified [4]âs metric, we did it to favor GLIA-Net, as it outputs detailed segmentation masks while our model outputs bounding cubes that overestimate aneurysm sizes. This isnât an issue for radiologists but would make for an unfair comparison (a larger prediction size makes it easier for a detection to be accepted)

In addition to the above, we will improve the clarity of Fig. 1 (R1) and Sec. 2.2 (R3, R4), as well as the analysis of the results (R1)."
https://papers.miccai.org/miccai-2024/832-Paper2068.html,We thank all reviewers for their feedback and constructive comments. The main concerns are addressed as follows:
https://papers.miccai.org/miccai-2024/833-Paper3401.html,"We thank reviewers for insightful comments. Reviewers appreciated our efforts in writing the paper (R1, R3, R4), figures (R1), developing the dataset (R1, R3), VideoCutMix architecture (R1, R3, R4), & exhaustive experimentation (R1). We will address all issues raised in the final version. Here, we answer a few important questions.

(R1) Datasize: Breakfast vs JIGSAWS:
Breakfast has 1712 videos (77 hours), JIGSAWS has 58 min

(R1,R4) Baseline results with weak augmentation? â¦parameters?
All baseline results use weak augmentation with same parameters for both baseline & our models.

(R1,R3) CL on other SOTA augmentation?
Other SOTA augmentations do not generate AUFR & AMFR, so we canât apply our CL to them. Table S4 shows most improvement comes from VideoCutMix.

(R1,R4) More training for Proposed model with CL?
No. All surgical models (Baseline, SOTA aug: No CL, & our model: with CL) were trained for 25 epochs each (Table. S1). The proposed model was trained for 5 epochs on UFR & MFR each, then 8 epochs on AMFR & 7 on AUFR.

(R1) mGRU I3D baseline: 68.36 or 72.95?
Sincere apologies for the typo. The edit score is 68.36 and accuracy is 72.95. We shall correct it in the table & accordingly update the numbers.

(R1) Is VideoCutMix the only augmentation for scarce TAS? previous TAS-Aug work on C-50 & Jigsaw?
Most SOTA video augmentations are proposed for action recognition, and to the best of our knowledge, VideoCutMix is the first augmentation for data-scarce TAS in surgical scenarios. No other works report augmentation numbers on C-50 & Jigsaws. We shall highlight this in the paper.

(R1) Compare SOTA augmentation on Breakfast (50%) dataset.
VideoCutMix is slightly better than the best SOTA augmentation. We canât give results as per MICCAI rebuttal guidelines.

(R3) Quantitative evidence for SOTA aug disturbing flow.
On average, augmented videos of Randmix have an 80.9% deviation in optical flow from the base videos of the JIGSAWS dataset, TubeMix: 67.54%, Framemix: 48%, & proposed technique: 6.25%. We shall add this info in the introduction.

(R3) Optical flow at action transitions is not preserved:
VideoCutMix doesnât preserve optical flow at boundaries but creates pseudo boundaries. Please refer to contribution 2, the last para in the introduction section.

(R3, R4) Lack of Information about the dataset:
70 trainee neurosurgeons from 14 hospitals across 3 countries performed tasks over 5 years on 6 box-trainers in working hours with minimal lighting variation. The dataset includes data from 12 cameras (2 per box-trainer). This is the first neurosurgeon-specific, non-robotic dataset. We will add these details.

(R3) Report other metrics:
Though the proposed model is much better in terms of precision, recall, & accuracy (eg: ~11% in recall for NETS), we reported Edit & F1 scores due to space constraints, following [1,2,7,9,21,23].

(R3) Effect of datasize on C-50:
Surgical TAS datasets are generally small. Though we observed similar trend in improvement of edit score on 25%, 50% & 100% of C-50 data (~12% to ~5%), we did not add this in the table because 25% of C-50 corresponds to only 12 videos, raising concerns about statistical significance.

(R3,R4) Choice of the temporal window.
Selected through ablation study. Next best size = 7

(R3) Why are numbers on C-50 low?
It is a cholecystectomy surgery dataset recorded at 1 FPS with only around 180 frames per class, per video. High data variance & low FPS result in low accuracy.

(R3) Overall improvement: 3.84% or 1.8%?
3.84% is the average improvement across all datasets & architectures. 1.8% is the least improvement in the Edit score on the JIGSAWS-KNT dataset with I3D. We will rephrase this in the manuscript to avoid confusion.

(R3) Why modify the target probability vector?
Post-augmentation, frames for TFR come from different classes, so we modify the target prob vector accordingly.

We apologise for typos in text & numbers, & shall correct it in the final version."
https://papers.miccai.org/miccai-2024/834-Paper0376.html,"First and foremost, we would like to thank the reviewers for taking the time to review our paper and for their insightful remarks. Below, we will respond to the reviewersâ specific comments as well as address general concerns.

General:

Inference Speed:

At this point in time, we do not yet have a streamlined setup that allows us to measure the inference speed of the entire framework. However, we recognize the importance of these numbers and would like to provide isolated inference times for the YOLO network as well as our encoder:

YOLO: 13ms (1920 pixels, NVIDIA A100-40GB)

Encoder: 20ms (average inference time per sample over 1500 samples, CPU Intel Core i7-6700K)

This adds up to an inference time of 33ms, indicating that real-time applications should be feasible.

Reproducibility:

We are working on providing the code to ensure reproducibility. Additionally, we provide the model weights of the YOLO network and have built an online tool where researchers can upload their videos to get detection predictions. (https://surgicalvision.bmic.ethz.ch)

Specific:

Reviewer #1:

Reviewer #3:

-We would like to thank the reviewer for the insightful suggestion and agree that incorporating movement constraints in combination with post-processing could improve performance significantly.

Reviewer #5:"
https://papers.miccai.org/miccai-2024/835-Paper2773.html,"We would like to thank the reviewers for their valuable time and constructive comments. Reviewers acknowledged the novel Image-Text Matching (ITM) module in the proposed Multi-modal Input UNet (MMI-UNet) method, as well as the thorough experimental validation and clear writing and organization of the paper.
We deeply appreciate the reviewersâ insightful suggestions, which have significantly contributed to enhancing the quality of our work. Our detailed responses to the reviewersâ comments are as follows:
Reviewer 1:"
https://papers.miccai.org/miccai-2024/836-Paper4190.html,"We thank all the reviewers for their helpful reviews and address their concerns below.

Limited novelty/innovation (R3 & R4): Even though adapters were first introduced much earlier (ICMLâ19), their modifications and adaptations for downstream tasks encompass important areas of current research: CLIP-Adapter (IJCVâ23) for classification, VL-Adapter (CVPRâ22) for VQA, visual reasoning, and image captioning, Meta-Adapter (NeurIPSâ23) for few-shot image classification and open vocabulary object detection, and ViT-Adapter (ICLRâ23) for classification and segmentation using vision-only models. Thus, the contributions beyond architectural innovations are equally important when adapted to new tasks or contexts with comprehensive experiments. Our contribution and novelty are in this direction, whereby we extend adapters to segmentation tasks using Vision-Language Segmentation Models (VLSMs) which was not done before.

No mention of LoRA (R1): Thanks for pointing this out. We will add this in related work: âUnlike previous adapter methods, LoRA (ICLRâ22) was designed to adapt large language models (LLMs), achieving zero additional inference latency by merging the pre-trained and adapted weights. In contrast, our method uses the adapted features as a parallel branch to the pre-trained block.â Since exploring LoRA in VLSMs would be interesting, we will add in the conclusion section: âExploring the application of LoRA for VLSMs will be an interesting future direction.â

Effects of changing block size (R1): We experimented with different variations of block sizes for the Kvasir-SEG dataset: block sizes {0.76M, 1.5M, 3M, 5.9M} for DA gave DSC within the range of 87.77 to 89.10, and {1M, 2M, 4.2M, 6M} for SA within 85.92 to 86.98. Thus, we chose block sizes of 3M for DA and 4.2M for SA, having adapter dimensions of 512 and 64, respectively, as a trade-off between DSC and parameters. These details are not reported as ablation studies due to page limitations.

Missing comparison to SOTA (R3): The paper aims to use adapter finetuning to obtain performance like end-to-end (E2E) finetuning. Thus, our experiments focused on comparisons of these two finetuning methods, as is the practice for similar works such as CLIP-Adapter (which compares CLIP-Adapter, zero-shot CLIP, Linear-Probe CLIP, and CoOp) and VL-Adapter (which compares zero-shot VL models, end-to-end finetuning, and various adapter modules). Moreover, the datasets we use come from a benchmark study of Poudel et al. (2024) where they compare the VLSM results with SOTA vision-only models. We will add the following in the results section: âPerformance of the end-to-end finetuning of the VLSMs we use for these datasets is comparable to the SOTA vision-only models (Poudel et al 2024).â

Significance of adapters (R4): While it is not exactly clear why adapters perform better than E2E fine-tuning on certain downstream tasks (both in our work and other related works), there could be some combination of unique architectural advantage adapters bring, and adapters being less prone to overfitting in smaller datasets, where the finetuning usually happens. E2E finetuning by adding adapters increases the trainable parameters, which can lead to further overfitting in the downstream task. While this could be one of the steps to better understand the workings of adapters, we believe this needs a more rigorous study with larger datasets.

Reason for not ablating with Î»_d and Î»_ce (R4): Computational constraints prohibited us from performing a grid search for Î»_d and Î»_ce. However, we selected these parameters using heuristic methods and preliminary experiments to ensure a balanced performance across different tasks and methods including baselines. We consistently apply the coefficients Î»_d = 1.5 and Î»_ce = 1 across all the methods for easier comparison."
https://papers.miccai.org/miccai-2024/837-Paper3061.html,"We thank the reviewers for their insightful and positive feedback, e.g., clear writing (R1&R3&R4); useful modules (R1); intriguing idea with promising potential (R3); interesting modules, clear motivations (R4). Below, we will address four main comments."
https://papers.miccai.org/miccai-2024/838-Paper3894.html,N/A
https://papers.miccai.org/miccai-2024/839-Paper2347.html,"Dear Reviewers,
We appreciate the reviewerâs time and detailed feedback. Weâre pleased that the reviewers highly endorse our ânovel formulationâ of MRI-guided 3D PET denoising and patch-wise training and âthe superior denoising performance.â In this rebuttal, we thoroughly address your constructive comments and will incorporate them into the final version.

Performance Metrics:
While MAE, PSNR, and SSIM are important, overly smoothed images may yield high quantitative values. Although improvements in these metrics seem marginal, our method significantly improves the Haralick feature (H-dist) and Perceptual distances (P-dist), which evaluate texture and perceptual similarity.
In Table 1, our model w/o MR had an H-dist of 3.21, while Restormer and U-net with MR were 3.59 and 4.57, respectively. Our model outperformed in P-dist, with a value of 0.10, compared to Restormer and U-net, which had values of 0.14 and 0.18, respectively. As suggested by R#3 and #4, we will compare our model to diffusion-based im2im methods and include statistical analysis to evaluate the significance.

Utilization of MR Prior: 
All the DL models leveraged the MR prior as anatomic information by inputting them as an additional channel, which is learned through the DL models (Sec. 3.2). Although Restormer and U-net utilized the MR prior, the denoised image showed a blurred structure (Longitudinal fissure in Fig. 3). In contrast, our model significantly enhances anatomical consistency with MR prior (Fig. 3) and showed the best performance in various metrics in Table 1, supporting the effectiveness of MR prior.
We assume registration of MR and PET can be achieved using clinical PET-MR scanners. Since the rigid body model is typically used in brain imaging and MR priors are learned through DL models, we assumed that the registration error is minimal. In the revised version, we will discuss the influence of co-registration.

Computational Efficiency of Training and Inference
We used a patch size of 64x64x64 (=78x78x78 mmÂ³), which is sufficiently large to capture brain subregions. We provided the model with 3D coordinates that describe the relative positions of the patches.
Our patch-wise training requires 10 GB of GPU memory per single batch. However, the memory requirement easily exceeds GPU capacity when the entire 3D volume is used (64x64x64 vs. 160x160x160). Therefore, a patch-wise approach is essential for 3D model training.
For inference, we processed the entire volume at once by leveraging the shift-invariance property of convolution layers. The model took the 4-channel input (coordinates, MR, low-dose PET, and Gaussian noise) with a volume size of 160x160x160. Despite this comprehensive 3D input, the model requires only 12 GB on a single GPU and takes ~3 minutes. We will compare the computational cost to other methods.

Comparison with PET reconstruction and real low-dose data
We agree that comparing our method with PET reconstruction methods is important. However, low-dose images can differ significantly depending on the reconstruction method, making standardizing comparisons challenging. 
For real low-dose PET data, due to limitations caused by motion and additional radiation exposure from repeated acquisitions, we employed Poisson thinning to replicate low-dose imaging while minimizing these impacts accurately. We will clarify our comparison strategies in the revised manuscript.

Effectiveness of Residual learning
Our residual diffusion model concentrated on the differences between low-dose and high-dose images. Residual learning is often more stable as it handles smaller variations compared to processing the entire image, and using lower dynamic ranges enhances training stability and prevents value clipping at peaks.

Clarification of details
We will modify the title to clarify PET/MR usage.
We will include error maps in Fig. 2.
We resampled MRs using 3D linear interpolation.
IRB number is GIRBA2365.

Sincerely,All co-authors"
https://papers.miccai.org/miccai-2024/840-Paper0751.html,"We thank all the reviewers for the detailed and constructive feedback. Thank you for appreciating the technical challenges we addressed while introducing scene graphs to voxel data. There is no publicly available data for any application. There is no publicly available annotation tool. But beyond this, most open source libraries for object detection (e.g. torchvision, detectron2, mmdetection) are hard coded to only support 2D bounding boxes or point clouds for some 3D applications. Our proposed method is the first one for Voxel Scene Graph encompassing the entirety of the Computer Vision and Machine Learning literature. Our framework also has no existing equivalent. We will gladly incorporate the minor changes suggested by the reviewers into the final manuscript."
https://papers.miccai.org/miccai-2024/841-Paper1517.html,"We thank all Reviewers for their constructive comments!

To R#5 (Novelty):
The main novelty of this work is that Seg2CS learns cortical surfaces weakly supervised by pseudo ground truth (pGT) segmentations (segs), without the need of pGT cortical surfaces generated by traditional pipelines. As described in Introduction, the motivation is that traditional pipelines are time-consuming to extract pGT surfaces for a large dataset. Besides, these pipelines may fail to produce acceptable pGT surfaces, e.g., for low-resolution fetal MRI (Fig. 5). In contrast, the pGT segs are relatively easier to acquire, e.g., using fast learning-based methods [2,14,24,29,32].

Seg2CS only uses cortical ribbon segs as pGT for training (Fig. 1). The boundary of the seg is extracted by MC. However, such a pGT seg boundary only provides noisy and weak supervision especially for the pial surface, which is severely affected by the partial volume effects of cGM segs and fails to capture the deep cortical sulci (Fig.1 leftmost).

Given pGT segs, previous explicit approaches [3,18,19,25] in Table 1 only use the bi-Chamfer loss, which overfits to the noisy pGT seg boundary, and thus fail to predict acceptable pial surfaces (Table 3, Fig. 4). In contrast, Seg2CS introduces a novel weakly supervised loss (Fig. 2, Eq. 3-4), which effectively addresses the partial volume effects of pGT segs and predicts high quality cortical surfaces (Fig. 3-5). We will summarize the novelty in the paper more clearly.

Our Seg2CS framework is model agnostic and we extend CoTAN [18] to TA-Net, enabling both adult and fetal cortical surface reconstruction. Seg2CS can preserve the surface topology without any topological requirement on the pGT. Although weakly supervised by pGT segs, Seg2CS still performs well, e.g., on dHCP fetal data, while the traditional dHCP pipeline fails (Fig. 5).

To R#4 (Accuracy):
To enhance anatomical accuracy, as shown in Fig. 1, Seg2CS only uses brain MRI as the input volume. The cortical ribbon segs are only used as pGT for training. This can effectively alleviate the influence of imprecise segmentation. The pGT segs could also be refined manually to increase the surface quality if required, e.g., in clinical applications. Seg2CS inflates the white surface strictly along the normal, which could affect anatomical fidelity. We will address this in future work by leveraging unsupervised MRI intensity to further refine the surfaces.

We have compared Seg2CS with both traditional HCP and dHCP pipelines. As shown in Fig. 3, Seg2CS (0.11s) achieves similar surface quality to the HCP pipeline (>6h) for both resolutions, while being orders of magnitude faster. In Fig. 5, the traditional dHCP pipeline fails to generalize on fetal data. We will also report the runtime of the dHCP pipeline, which requires ~6.5h.

Due to the low image resolution and contrast of the dHCP fetal MRI, Seg2CS might produce very few errors for older fetal subjects with increasing complexity of brain anatomy. We will report and visualize these errors to provide insights for further refinement.

To R#1 (Method):
Integrating multiple SVFs or using t as the input to the U-Net could lead to lengthy and unstable training, since the gradients will be back propagated to all SVFs recurrently. Our attention-based TVF can avoid this issue. The TVF is smoothed implicitly since we added smoothness losses to the surface mesh explicitly.

For TA-Net, R is the resolution level for coarse-to-fine deformations. M is the number of SVFs at each level to enhance the representation ability of the TVF.

Due to numerical errors, as reported in Sec. 3, Seg2CS indeed produces a negligible number of self-intersecting faces, i.e., 0.06/3.65 out of 330k faces for white/pial surface on the HCP dataset.

Many thanks for the valuable suggestions to address the issue of exploding gradients.

Seg2CS also has good quality in the hippocampal region. We will visualize the medical view of both white and pial surfaces."
https://papers.miccai.org/miccai-2024/842-Paper1757.html,"We thank in-depth reviews and appreciate for affirming our contributions. The main concerns are addressed below.
[R3]-Annotations during testing
Yes, annotations are only required during training. During testing, we only need to use the complete tooth model without annotations as input.
[R4]-Comparison with existing methods
The competing methods we have selected are all open-sourced and highly representative in this field. Most teeth segmentation methods compared their results with these methods. We set up the comparison experiments with the same configuration to ensure a fair comparison. We have reviewed recent methods, but we did not include them in our comparative experiments as they did not release their codes. Notably, according to their reported results on the same dataset (Teeth3DS), our method qualitatively remains superior. Weâll update the paper to discuss these point more clearly.
[R3]-Tooth labels in Fig. 3
Sorry for any confusion. To clearly show the readers which tooth we have localized, we only displayed the corresponding tooth by masking out the other teeth in Fig.3.
[R3, R4]-More explanations of our methods
Sorry for the unclear description of our method. Assuming we have weakly annotated training data, we get sampling points that can be denoted as xâR^(NÃ6) after disentangled resampling (not required during testing). We use a classification task to assist in learning the discriminative features of each category. Specifically, we get a high-level representation fâR^(NÃ1024) through the two stream feature extractor. Then, an attention mask can be calculated based on Eq.1. The discriminative feature of different categories f Ì can be obtained based on Eq.2. For each category j, discriminative localization techniques (e.g., CAM and Grad-CAM) can be easily employed to highlight the discriminative points on each category which can be used for cropping (please refer to Details of cropping). The cropped region of each category which can be denoted as x_jâR^(MÃ6) (M<N) will be feed into segmentation network for the point-wise prediction. Each categoryâs segmentation results will be integrated to form the final result.
Attention mask A: To make the features more discriminative for different tooth categories, we hope that A can highlight the association of each point with its corresponding tooth category. Based on Eq. 2, the f Ì will learn more distinctive discriminative features for different teeth, which benefits our discriminative localization. A is indeed computed from two views of f, but Ï is a mapping from N to C, thus AâR^(NÃC).
Details of cropping: Based on the gated attention mechanism, f ÌâR^(NÃ1024ÃC) (C stands for teeth categories) learned discriminative features for different teeth. For each category j, we use GAP to integrate spatial information (F_jâR^(1Ã1024)) and mapping (from 1024 to 1) to output the probability of this tooth. Similar to CAM, the weights of mapping will emphasize the important channels of each point in f Ì_j. In this way, the discriminative points belonging to the category j are highlighted, which can be used for cropping.
We apologize for the unclear description and will update the paper to clarify the method description.
[R5]-Explanation of dataset
Certainly, we simulated the âlimited labelâ scenario on a fully annotated dataset by randomly masking some teeth as background class. 
[R5]-Different label ratios
Before submitting, we conducted ablation experiments with different label ratios (i.e., 20%, 30% and 50%). We find that the segmentation performance is still acceptable even though we have only 30% labeled data. In contrast, the performance is significantly affected when the label is reduced further. Due to space limitations, we didnât include this experiment in our ablation study. Instead, we only presented the best results with limited annotations (i.e., 50%).
[R3, R4, R5]-Reproducibility
For sure, weâll release the code on GitHub after the review process for reproducibility."
https://papers.miccai.org/miccai-2024/843-Paper1675.html,"We are grateful to the reviewersâ appreciation: our topic is âtimely newâ (R1), our method is ânovelâ, âsensible and highly applicableâ (R4), and âsufficiently evaluatedâ (R5), our gaze data âcontribute to the discipline developmentâ (R4). We thank all reviewers for their suggestions on writing and details, which will be revised.

âââ Common questions:

â R1 & R4 & R5: clarifying threshold selection. We consider two scenarios: (1) Without any full labels, two thresholds are chosen based on radiologistsâ feedback to generate heatmaps closest to GT, with one tends to erode (under-activate) and the other dilate targets. The pair simulates multi-level attention and compensates each other (Sec 3.2 and 3.3). (2) Given a labeled subset, we adaptively select such pair based on the Dice and object size of generated masks compared to GT. On Kvasir-SEG, thresholds of 0.3/0.5 (Fig. 1) and 0.35/0.47 are chosen by the two methods, respectively. We use the first method due to the negligible difference. For m>2, more thresholds are interpolated.

â R1 & R4 noted personal bias, which we addressed with a regularized annotation pipeline (Sec. 2) and a multi-level method modeling subjectivity (Sec. 3.1). In our study with 3 annotators (Fig. 5c), all consistently outperform SOTA, showing that personal bias hardly affects our conclusions. The bias is an essential factor in human-AI interaction, and we are among the few that study and release gaze from multiple observers. We will further investigate it in future work.

âââ R1:

â âA main drawback is failing to achieve the performance of full labelâ. This might be a misinterpretation of our setting and key results. We will revise for clarity. We focus on weakly-supervised segmentation, widely acknowledged as label-efficient but with performance upper bounded by full labeling [4,6,20,25]. Such performance/efficiency trade-off is critical, so evaluations should cover both aspects, not just performance. Our results highlight: (1) Compared to SOTA, our method excels in both aspects (Table 1), narrowing weak-full performance gap while being 7 times more efficient. (2) Under the same annotation time, our method significantly outperforms SOTA and full labeling in performance (Fig. 3, where full labeling is excluded due to its poor performance in the limited annotation time).

â Uncertainties in gaze. In Sec. 1, we analyzed various uncertainties and summarized two key properties of gaze: âdiscriminativeâ and ânoisyâ, which indeed motivate our method. The case noted by R1 belongs to âdiscriminativeâ, where some parts may be marked larger or smaller than they are. The solution was discussed in Sec. 3.1. Our method effectively solves these uncertainties, as shown in our results.

â R1 suggested a hierarchical method. Since we use m=2, the suggested and our methods converge to the same one. We retained our current version since it performs slightly better by using all other networks for compensation in our previous experiments with m>2.

â Clarifying constant c. The constant c is a common trick for correspondence. We follow SOTA [7, arXiv:2011.10043] and set c as 0 to remove negative correlations.

â The concern on extra hardware is discussed in Sec. 5. And the suggestion on more backbones is helpful to future work.

âââ R5:

â Method details. As detailed in Sec 3.1, we apply Gaussian kernel on gaze points to generate heatmaps and ensemble outputs of all models in inference. We will highlight these for clarity.

â Novelty. We proposed a multi-level method to simulate discriminative human attention and correspondence-based cross-level consistency for compensation. The two modules are mutually reinforcing in balance (Sec 3.3) and motivated by the key properties of gaze-supervised segmentation (Sec 1), which is not studied before.

â Efficiency. Dense gaze points are tracked at a high frequency (1k Hz), which is acknowledged [9,23] as more efficient than points or boxes by saving significant manual operation time"
https://papers.miccai.org/miccai-2024/844-Paper1398.html,"We sincerely thank all reviewers for their valuable comments. We have made revisions to the grammar and content issues raised one by one. There are our feedbacks for the major weaknesses.
Method details(R1, R3, R4): This study proposes an attention fusion-based segmentation network for precise segmentation of 3D carotid artery vessel walls. The network integrates ViTâs global attention with VNetâs local feature extraction, overcoming VNetâs limitation in capturing long-range dependencies. A joint attention module and feature fusion module strengthen semantic information transfer and detail capture, enabling more precise vessel contour depiction.
Comparison methods (R1, R3, R4): In Table of the comparison methods we have chosen relevant works for comparison. Due to different data types and the lack of open-source code, we did not replicate these methods on our dataset for experimentation.
JAS(R1, R3): This study proposes a joint attention-based feature fusion module for medical image segmentation, integrating multi-scale dilated convolutions and attention. It enhances generalization by fusing features from parallel dilated convolutions. A self-attention fusion and SimAM module boost key feature representation, effectively combining multi-scale features and attention. This improves segmentation performance by emphasizing critical features.
Weak-supervied(R1, R4): This study employed a segmentation method based on coarse labels, complemented by specific post-processing steps to enhance segmentation accuracy. Initially, morphological erosion was applied to remove small details and isolated pixels, thus reducing mis-segmentation. Subsequently, dilation was utilized to fill in holes, connect fragmented regions, and restore object shapes. In the post-processing phase, the largest connected region was constructed, and by comparing the sizes of different regions, smaller, potentially mis-segmented areas were identified and eliminated. This approach effectively integrates initial segmentation with post-processing steps, significantly improving the accuracy and reliability of the segmentation results. Particularly in complex medical images, such precise segmentation is crucial for diagnosis and treatment, ultimately yielding refined segmentation outcomes.
Rough annotation(R4): In the backbone network presented in the document, you may observe the utilization of coarse-grained labels. However, taking into account the consideration of the layout and formatting, the paper has refrained from separately listing the coarse-grained labels prior to processing. 
Data section(R4):During the label processing, we gracefully resize the initial labels from 432x432x432 to 412x512x512, then refine them with a 3D median filter to smoothen jagged edges from interpolation. Finally, we employ graphics-based opening and closing operations to fill in any voids on the vessel surface, producing refined training labels. We appreciate your attention and consideration in this matter."
https://papers.miccai.org/miccai-2024/845-Paper0843.html,"We sincerely appreciate the reviewers for their meticulous reviews and constructive suggestions. Here we tried to address the comments:

(Q1, R#1) Details about the model design. The baseline model in the ablation study refers to the pure ViT-based segmentation model without the guidance of SAM. Meanwhile, the SAM model is fixed with the three components, image encoder, prompt encoder, and the mask decoder.

(Q2, R#3) The labeled point selection. To get the single-point label for each tooth, we just randomly labels a point of one tooth, which can facilitate with the practical annotation procedure.

(Q3, R#1, R#3, R#5) Reproducibility. We will release all the codes and data to facilitate the readers with how to implement 2D and 3D cross-modality learning."
https://papers.miccai.org/miccai-2024/846-Paper2842.html,"We thank R1, R3, and R4 for their helpful comments, which we will address in the paper should it be accepted.
First of all, we think that an in-depth description of the rotation invariant spherical harmonic (RISH) feature is needed. We chose RISH because it was widely used in related tasks such as dMRI harmonization [Mirzaalian-NeuroImage2016; Cetin-Karayumak-NeuroImage2019] and generation [Cetin-Karayumak-MICCAI2018]. Because of the previously successful application, we did not include ablation studies on RISH (R4) but focused on the design of the proposed network. RISH provided a compact representation of DWI data, from which DWI signals can be reconstructed. We will clarify that, from the obtained 7T RISH, we indeed generated DWI data (R3), from which FA images were computed. Although we did not perform a comparison directly on DWI (R3, R4), the FA-based results (Figs 2,3 and Tables 1,2) assessed the generated DWIâs quality, in a way that was widely used in the literature [Cetin-Karayumak-NeuroImage2019, Jha-MedIA2023]. Itâs true that RISH only worked for single-shell data (R3), but our method can be easily extended for multi-shell data generation with a multi-model architecture (e.g., one shell per model). While scaling of RISH did not affect fiber orientations (R1), our method included a super-resolution module to increase the spatial resolution, which can reduce partial volume artifacts and potentially improve fiber orientations. However, to the best of our knowledge, how 7T data can improve fiber orientations over 3T data is still ongoing research [Sotiropoulos-NeuroImage2016; Kauppinen-MRM2020]. While our intermediate results showed that tractography from the generated 7T data was more spatially similar to the target 7T data than the input 3T data, how to access fiber orientation improvement is still a challenge beyond the scope of our study. So, we focused on microstructure measures but not fiber orientations. 
Other comments:
R1: We will elaborate our contributions as: 1) design of an LDM-based network to learn the mapping between 3T and 7T RISH features to enable high-quality dMRI generation, 2) design of a transfer learning strategy to address the scarcity of high-quality 7T data, and 3) design of a super-resolution module for LDM to enhance spatial resolution from 3T to 7T data. As suggested, we will change âtranslationâ to âmappingâ. We will clarify that 1) âclass-labelâ in Fig.1 is whether the input data is 3T or 7T, 2) the rotational invariance of RISH enables direct mapping between 3T and 7T data (e.g., 3T L0 to 7T L0), regardless of differences in gradient directions. As requested, we will define RISH on its first appearance and update the reference order.
R3: We confirm that NMSE>1 of L4 in GAN is correct, which was due to the outlier values (as appearing abnormally white in Fig 2). We will clarify that the visual differences between the generated and target images mainly resulted from the smoothing effects of convolutions in the decoder. Our method generally obtained a visually plausible and similar appearance to the target, particularly in the lower-order features (e.g. L0 and L2) and the FA images. This is visually comparable to those reported in the literature [Cetin-Karayumak-NeuroImage2019; Jha-MedIA2023], though we acknowledge that further investigation is needed to improve local details. 
R4: We compared the GAN-based method (also the CNN-based method) because they are SOTA in dMRI generation. We agree that comparisons with other models would be more comprehensive, which will be left for future work. We will elaborate that we learned embeddings from the class labels, which were further mapped into the intermediate layers of UNet in LDM via a cross-attention machism. We will update Fig 1 to separate the fine-tuning process to show it is performed before LDM training and inference, and expand the caption to explain the method workflow. 
Finally, we will make code publicly available upon acceptance."
https://papers.miccai.org/miccai-2024/847-Paper1466.html,"We thank the reviewers for the thoughtful feedback and the recognition of our work! We appreciate that all reviewers found learning a self-supervised 3D+T whole-heart representation to be interesting to the community and its implementation to be important to clinic applications!

We are encouraged by the positive remarks on the following aspects of our study:

We would now like to address the concerns raised by the reviewers.

We want to reiterate that our approach is novel and unique in its handling of multi-view and spatiotemporal CMR sequences compared to other SSL methods. A major contribution of our work is adapting MAE to manage interlaced multi-view CMR data, which current state-of-the-art SSL methods cannot achieve. No existing SSL methods, including the vanilla MAE, are capable of handling such complex CMR data.

Leveraging the enhancements from SSL, our model surpasses the performance of supervised methodsâViT in phenotype prediction and UNETR+ in segmentation, underscoring the importance of exploring whole-heart representations. This field is still underexplored in the community.

2.1 Trainable parameters (R3). For phenotype prediction, ResNet50 â 24.9M, ViT â 83.5M, and ours â 83.5M. For segmentation, nnUNet â 60M, UNETR+ â 84.6M, and ours â 86.5M.

2.2 Further training details (R1). All experiments are trained with 300 epochs with early stopping strategy. Regarding image preprocessing, we applied the center cropping technique for all subjects. Due to the consistent acquisition protocol of the dataset, we do not observe cropped subjects with partly missing ROI. While we found that 4 to 5 centered short-axis slices were sufficient to cover the relevant cardiac information with clear cardiac contour, we used 6 centered short-axis slices to ensure full coverage.

2.3 t-SNE visualization (R1, R4). We use a perplexity of 15 and a learning rate of 10 for t-SNE. Due to the page limitation, we only showed two phenotype representations, while the other phenotypes, e.g. LVEF, also demonstrate well-clustered and well-distributed representations.

2.4 Further dataset details (R4). We conduct all experiments with CMR data from UKBiobank acquired by 1.5 Telsa scanner (MAGNETOM Aera, Syngo Platform VD13A, Siemens Healthcare, Erlangen, Germany) with typical field of view (mm) 380252 for short-axis view and 380274 for long-axis view. The TE/TR (ms) of the short-axis view is 1.12/2.6 and of the long-axis view is 1.10/2.8. The voxel size (mm) for short-axis slices is 1.81.88.0 and for long-axis slices 1.81.86.0. The temporal resolution is 32 ms.

2.5 Reproducibility (R3, R4). We will make the code of this paper publicly accessible after the double-blind review process.

In the end, we would like to thank reviewers again for their detailed comments and very constructive suggestions!"
https://papers.miccai.org/miccai-2024/848-Paper0783.html,"We thank the reviewers for the valuable comments, which help to improve the quality of this paper. We are encouraged that the reviewers found our motivation and idea to be interesting (R1) and novel (R3, R4), and that our comparisons against baseline showed significant improvements (R1, R3, R4). We respond to your concerns as follows.
Detail of backbone (R1, R3): To ensure fairness, we use the same Resnet-based generator backbone as CycleGAN[1] and CUT[2]. WIA-LD2ND achieves significant improvements based on this backbone, as shown in Table 2. We conducted experiments on various backbones, all of which demonstrated good performance. Due to page limitations, we have to make a compromise between detailed analysis of each module and presenting the results for different backbones. Due to the rebuttal policy prohibiting new experimental results, we will publish the experimental results on GitHub.
Inference time and computational efficiency (R1, R3): WIA-LD2ND increases the parameters by 2.46M over the baseline. Despite this, it does not increase the inference time, maintaining efficiency while significantly enhancing performance, as shown in Table 2. This demonstrates that the additional parameters contribute effectively to improved outcomes without compromising computational efficiency.
Detail of noise parameters (R4): Table 1 in supplementary material details the impact of various noise parameters on model performance. Proposing a systematic and robust noise addition module is a promising direction for future research.
Utilization of low-frequency components (R3): As the model can effectively reconstruct low-frequency components, as shown in Sec 2.1, the purpose of FAM is to help with the more challenging high-frequency components. Therefore, we do not utilize low-frequency components in the FAM.
Additional dataset (R3): The dataset mentioned by R3 is for CT anomaly detection and is not suitable for the low-dose CT denoising task. We are willing to test the WIA-LD2ND on more datasets. Due to the rebuttal policy prohibiting new experimental results, we will provide the results on GitHub in the future.
Comparative analysis (R3): The methods mentioned by R3 are designed for sparse-view CT, which is different from low-dose CT. We are willing to compare WIA-LD2ND with other SOTA methods. We conduct comparative experiments against six SOTA self-supervised methods and two SOTA weakly-supervised methods. Additionally, we are open to further validating WIA-LD2ND by comparing it with more SOTA methods. Due to the rebuttal policy prohibiting new experimental results, we cannot include them here. We will provide this comparison on GitHub in the future.
Comprehensive CT Noise Modeling (R4): Streak artifacts mainly occur in sparse-view CT. For low-dose CT, the primary issue is a lower signal-to-noise ratio due to reduced radiation dose, leading to increased image noise rather than streak-like artifacts. WIA-LD2ND is designed for denoising.
Future recommendations (R3): We will incorporate MS-SSIM into our method in a subsequent study and cite MS-SSIM.
Dataset clarification (R3): The test images in Mayo-2016 and Mayo-2020 are all from distinct patients, as mentioned in Sec. 3.1.
Method versatility (R3): WIA-LD2ND is designed for CT denoising. Proposing a method for all  types of medical image restoration is a promising future research direction.
Performance gain discrepancy (R3): The Mayo-2020 dataset is less noisy than the Mayo-2016 dataset, so models typically perform better with it.
[1] Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A: Unpaired image-to-image translation using cycle-consistent adversarial networks. ICCV, 2017
[2] Park, Taesung and Efros, Alexei A and Zhang, Richard and Zhu, Jun-Yan: Contrastive learning for unpaired image-to-image translation. ECCV, 2020"
https://papers.miccai.org/miccai-2024/849-Paper2384.html,"We thank all the reviewers for their valuable feedback. Below we address their concerns.

R1-Q1: The improvement over [13] is not significant and WiNet lacks a thorough comparison to [13].

We compared our WiNet with [13] on two 3D datasets, IXI and 3D-CMR. As shown in Table 1 and Fig. 3, our WiNet-diff outperforms [13] on IXI by 0.4% in DICE while using 9% less memory. On 3D-CMR, our method also outperforms [13] by 0.5% in DICE and 0.22mm in HD with 4% less memory. Additionally, our WiNet has 66% fewer parameters than [13] (1,418,573 vs 4,198,352). These results demonstrate that the improvements brought by our WiNet are significant.

R1-Q2.1: Why is DWT more advantageous over DFT in extracting high frequencies?

The DFT provides a global, full-resolution analysis in the frequency domain, making it straightforward to extract low-frequency signals using central patches or crops, as shown in [13]. However, learning high frequencies with DFT is challenging because, unlike low frequencies that can be centralized in the frequency spectrum, high frequencies are dispersed and not confined to a rectangular area. In contrast, the DWT naturally decomposes images into both low and high frequencies across different sub-bands. These sub-bands have lower spatial resolution and retain the overall structural information. Consequently, the decomposed DWT coefficients can be seamlessly integrated into a network without any loss of frequency information.

R1-Q2.2: Why should high-frequency information be introduced?

Displacement fields are not necessarily smooth, therefore, low-frequency signals alone may not be able to fully and accurately reconstruct the entire displacement field. Our WiNet theoretically overcomes this problem by preserving both low-frequency and high-frequency information of the displacement field.

R1-Q3: Figure 1.
Our network has skip connections.

R3-Q1: Reproducibility. We will make our code and trained models public, as stated in the abstract.

R3-Q2: Replacing DWT with standard learnable stride convolutions.

Compared to the DWT layer, 1) a conv layer with a stride of 2 leads to loss of image information, resulting in less accurate registration performance, and 2) the conv layer introduces additional parameters and increases GPU memory usage.

R3-Q3: Why is the DWT layer differentiable?

Both DWT and IDWT layers are differentiable. Although it is feasible to apply DWT as a pre-processing step, a better way is to build a general end-to-end framework without any pre/post-processing. Additionally, popular wavelet libraries, like PyWavelets, only support CPU. Our DWT layer supports 3D GPU computation, which significantly increases speeds, and provides a general tool for the research community.

R4-Q1: Explainability.

Black-box methods like [4,6] take the images as input and predict displacements directly without considering the immediate mechanism. In contrast, displacement fields in WiNet are incrementally composed, as shown in Fig. 2. Conv-0 learns the 1/8 eight sub-bands, which can be composed as 1/4 low-frequency coefficients. Sequentially, RB-1 uses 1/8 high-frequency to incrementally learn high-frequency at 1/4 scale, while RB-2 uses 1/4 high-frequency to incrementally learn high-frequency at 1/2 scale. Therefore, this entire learning process is explainable, and the method of composing deformation is also explainable due to the properties of DWT & IDWT.

R4-Q2: Explain parameter âCâ and why making a parameter-free layer âdifferentiableâ?

âCâ controls the model size, which is a trade-off between performance and efficiency. Our WiNet uses a U-Net backbone similar to that in [13, 25]. When C=32, our WiNet presents a similar configuration with the recommended architecture in [13, 25]. 
As responded in R3-Q3, the IDWT should be differentiable since it needs backpropagation when training. Although the first DWT layer is not necessarily differentiable, we make it so to provide a general 3D GPU DWT tool."
https://papers.miccai.org/miccai-2024/850-Paper0761.html,Thank so much for the appreciation of the reviewers! We will provide more details as you suggested in the final version. The code and dataset is now  public at https://github.com/cpystan/Wsi-Caption
https://papers.miccai.org/miccai-2024/851-Paper0336.html,"We thank all the reviewers for the positive comments, such as ânicely written, interesting, usefulâ. We will release our code to the public at the earliest time after review.

[R1,R3] Q1 Motivation of âdisentanglementâ strategy: We agree that age can serve as a biomarker for diseases. Our design is not to completely remove age information but only remove a portion of age information correlated to normal developmental components, so that we can enhance the detection of disease-related abnormalities. Specifically, for a patient with a developmental disorder, their brain development consists of normal developmental components (similar to those of HCs) and disease-induced developmental deviations (different from those of HCs). Normal developmental components are highly correlated with age, while disease-induced deviations are more dependent on disease severity. By decoupling these components, our study removes normal developmental information, retaining only disease-related deviations, thereby improving diagnosis accuracy. Additionally, compared to traditional models that confound normal developmental information, focusing solely on developmental deviations enhances understanding of disease mechanisms and extraction of more effective biomarkers. The higher disease detection accuracy and the discovery of more effective biomarker regions in our experimental results both validate the aforementioned hypotheses. We will further clarify the modelâs motivation in the final submitted version.

[R1] Q2 The implementation of attention-based disentanglement: Thanks for suggesting mutual information loss, which is an interesting and reasonable idea. We will explore it in future work. In this study, the disentanglement of disease-related and age-related embeddings is achieved by two learning modules based on the attention-based mechanism. Specifically, attention map Ï(z) for disease-related features is optimized through disease diagnosis, while attention map 1 â Ï(z) for age-related representations is optimized through WSAL. The learning and optimization modules of the two channels can assure effective feature disentanglement.

[R3] Q3 Why WSAL is a weakly supervised method: Sorry for the confusion. We use the Q network to predict age from AGi , but not directly for Ai. The learning process of Ai is guided by imprecise AGi . The AGi is imprecise because a) AGi is generated by combining Gaussian noise with age information; b) the Q network makes AGi retain most age information but cannot predict the exact age accurately. Considering the generation guidance of Ai is imprecise, we claim our method as a weakly supervised one.

[R3] Q4 Clinical analysis: The key biomarker regions detected by our method across the three datasets align with existing findings. For example, in the CHD dataset, we found that several regions, including the superior frontal, middle temporal, and right cingulate, are related to CHD neurodevelopmental abnormalities. This is consistent with existing literature (Clouchoux et al., 2013, Cerebral Cortex; Rachael et al., 2014, NeuroImage: Clinical). We will include more references in the final version.

[R1, R4] Q5 Comparison with other methods: Actually, we have tried the method proposed by Dufumier et al. (R1). However, due to the lack of a large scale of cortical morphology data for pretraining, the results are not satisfactory. Therefore, we did not include it and would conduct further exploration in future work. For R4âs suggestion, we replaced Spherical ResNet with CNN and VGG networks and found the ResNet performs best.

[R1, R4] Q6 Writing issues: Due to length constraints, dataset details were omitted in the submitted version but we will include them in the code release. We have also reviewed the paper and corrected issues with missing references(R1), unclear abbreviations, and variable definitions(R4)."
https://papers.miccai.org/miccai-2024/852-Paper1441.html,"We appreciate the reviewersâ thoughtful assessments and valuable insights. They found our work novel (R1,4) and well-organized (R1,3,4) with rigorous evaluation (R4), achieving convincing advancements(R1,3,4).

[1] Zhang et.al, Self-supervised Vessel Segmentation from X-ray Images using Digitally Reconstructed Radiographs.
[2] Kang et.al, Domain Adaptive Semantic Segmentation via Image Translation and Representation Alignment.

2.Restate experiment results(R3):We will clarify this in the manuscript. Our method and selected SOTA methods, i.e, SSVS, DARL, and FreeCOS, all are proposed based on domain adaptation and trained following self-supervised manner for this specific task. So they are appropriate for comparison. As a fact, existing SOTA works consider the performance of supervised Unet as a general upper bound. From Tab.1, our method outperforms these SOTA methods and achieves competitive performance compared to the supervised Unet, highlighting our methodâs superiority. Tab.2 shows that even our baseline surpasses three SOTA methods, indicating the significant role of our proposed XA simulation module. Additionally, our adaptive representation alignment further improves the dice score from 0.685 to 0.727, and each component contributes positively, with slight improvements due to our baseline being close to the upper bound.

3.Comments on the weights tuning(R1,4):Due to limited pages, we directly provided the well-tuned weights in Eq.9. Based on the conducted study before submission, randomly assigning weights to each term was first experimented to balance their contributions to the overall loss. Then the weights were further studied via grid search on the public XCAD dataset.

4.Q&A(R1):Thanks for your questions. Data augmentation introduces diversity in the training data to improve generalization, while consistency loss focuses on explicitly enhancing robustness through consistent predictions; We find that feature maps from the third-from-last convolutional layer, which have the same spatial resolution as the images, achieve the best performance. The raw set (A, B) is included in the augmented set  (A, B), so the proposed adversarial loss is only applied on the augmented set;

5.Q&A(R4):Thanks for your comments and questions, we will add them as a discussion or future work. As mentioned in Sec.2.4, only one option is randomly selected for each effect, so the X-ray images only will be tainted with one random noise; For the I-XA dataset, it was collected from 20 patients during the pre-intervention phase. For the II-XA dataset, it was collected from 15 patients during the post-intervention phase. And they are manually annotated using 3D slicer segmentation tools. Thanks again for constructive suggestions on typo.

6.Reproducibility(R1,3,4):We will release the code, as we agreed in the submission system.

Thanks again for your comments and suggestions."
https://papers.miccai.org/miccai-2024/853-Paper0077.html,"We thank the reviewers for their thoughtful comments. We appreciate that the reviewers found our paper well-organized (R1,3,4), method interesting/novel (R1,3,4), experiments extensive/convincing (R3,4), and can make an important contribution to the community (R4)."
https://papers.miccai.org/miccai-2024/854-Paper1201.html,"We sincerely thank all reviewers for their time and constructive feedback.

R1-6.1, Contrary to concerns, our method prioritises simplicity and interpretability by using explainability methods and the simple approach we used to identify the discriminatory nodes to enhance fairness. Our method is modular, minimally resource-intensive, and applicable to any deep learning classification model in the medical domain. We could significantly improve fairness metrics for large ViT models with just a few rounds of pruning.

R1-6.2, Skin lesion datasets are most commonly used in fairness studies and are easier for us to conduct benchmarking with existing methods. We will extend to other modalities in future work.

R1-10.1, Among the suggested studies [1-3], [2] is the most recent one (in 2020), so we have now added the comparison with [2], which shows the superiority of our method. 
On Fitzpatrick17k, XTranPrune achieved F1-score 73.51 vs DomainIndâs 69.06, with worst-case scores of 69.13 vs 64.26, DPM 0.586 vs 0.571, EOM 0.790 vs 0.714, EOpp0 0.086 vs 0.055, EOpp1 0.066 vs 0.128, Eodd 0.095 vs 0.139, NFR 0.114 vs 0.119. For PAD-UFES-20, the results are 62.01 vs 62.51, 57.03 vs 41.58, 0.009 vs 0.005, 0.624 vs 0.462, 0.389 vs 0.764, 1.141 vs 1.440, 0.909 vs 1.796, 0.587 vs 1.578. Our method gives better results except for EOpp0 on Fitzpatrick and F1-score on PAD-UFES-20. 
In addition, since FairPrune [4] (in 2022) could outperform the earlier studies [1, 3] and we could substantially improve the fairness metrics compared to FairPrune, we are indirectly showing the superiority of our method over [1, 3].

R1-10.2, Given the fact that FairPrune [4] hasnât published their code, it was challenging to reproduce their data augmentation, metric and method implementation. We implemented their method based on the paper, following the conventional implementation of fairness metrics, and conducted comprehensive experiments with their suggested hyper-parameters, reporting the best results we could achieve. The results they report (e.g., Eopp0=0.0008) differ significantly from other papers like FairME [5] (Eopp0=0.006). In addition, for consistency with other experiments, we used ResNet18 as the backbone, which typically performs better, thus showing inferior fairness metrics compared to the simpler VGG-11 used in their paper. Another reason for this mismatch could be the difference in data for the train/test splits.

R3, Thanks for the suggestion. On Fitzpatrick17k, XTranPrune achieved F1-score 73.51 vs FairTuneâs 66.80, with worst-case scores of 69.13 vs 54.44, DPM 0.586 vs 0.538, EOM 0.790 vs 0.686, EOpp0 0.086 vs 0.114, EOpp1 0.066 vs 0.104, Eodd 0.095 vs 0.195, NFR 0.114 vs 0.238. For PAD-UFES-20, the results are 62.01 vs 62.51, 57.03 vs 41.58, 0.009 vs 0.005, 0.624 vs 0.462, 0.389 vs 0.764, 1.141 vs 1.440, 0.909 vs 1.796, 0.587 vs 1.578. Our method outperforms in all the metrics.

R4-6.1, Due to the limited number of pages in the main paper, we kept the descriptions and captions concise, while including a more precise formulation of how we generate the masks through a pseudo-code in the supplementary materials.

R4-6.2, Please note that we used the Macro-averaged F1 score to properly evaluate the model performance on highly imbalanced datasets, as it accounts for both precision and recall for each class independently before averaging, ensuring a more balanced evaluation across all classes. We can add AUC measures in the next version.

R4-6.3, Our loss function was Cross Entropy. Comprehensive training details are provided in our git repository, making the reproduction of our method easy.

R4-10, Our study has followed the commonly used experimental setups and focused on improving performance with a simple approach. We will investigate out-of-distribution cases in future work."
https://papers.miccai.org/miccai-2024/855-Paper3647.html,#1:
https://papers.miccai.org/miccai-2024/856-Paper1820.html,"Thank you for recognizing our novel use of zoom.

R1.3. Our work involves 6 sonographers, reducing sonographer bias.

R1.7. We will explore this approach with other gestational ages in the future.

R4.3. Thank you for your comments. Firstly, we would like to note that two of the authors are clinicians who perform these scans on a daily basis. We strongly believe it is important for clinicians to be involved in such work. Secondly, we believe that the results are not spurious, primarily from the fact that intuitively the results make strong clinical sense and are clinically explainable. The CRL measurement requires the entire fetus to be visible on the screen before a measurement is made (therefore zoomed out), while NT requires a specific part behind the fetusâs head to be focused on for a measurement (therefore zoomed in to fill 75% of the screen according to the guidelines). At this age, the CRL is roughly 65 Â± 19mm, while NT is usually around 1.1 to 3.0 mm, so to view them well, one would expect different zoom levels for each.

R4.7.1. The supplementary complies with section 5âs rule on âconcurrent submission to MICCAI or another conferenceâ and section 3âs PDF stating concurrent submissions are allowed if anonymized. RQZ values are determined by reading a few pixel lines in the depth scale region of a raw US image.

R4.7.3. In the supplementary, we show that models can differentiate zoom levels and are not fooled by post-acquisition manipulations like cropping or resizing. This was crucial for our current work. All US machines require fine-tuning and zooming for the correct view. Our work involves six operators, reducing operator bias.

R4.7.4. It has two output channels, one for CRL and one for NT. As shown in Fig. 2, frames frozen for interpretation or measurement have a class label. We predict the structure that the fine tuning phase clip is approaching by the zoom pattern. We do not predict the contents of the individual frames. A sonographer would approach and then freeze for a CRL or NT view for detection and measurement, not the background.

R3.3.1. Our experiments show that zoom sequences can be attributed to specific anatomical structures. We prioritize simplicity for low-compute settings and have effectively addressed our research question.

R3.3.3. Figure 3 depicts the confusion matrix, which we will reformat for clarity.

R3.7.1. RQZ values are inexpensive to obtain, relying only on reading specific pixel values in a raw US image. We can add details, as covered in the supplementary material. The proposed task operates on a different scale. Classifying a 300-integer sequence (0-4) with a 1-D CNN (272,770 parameters) is much cheaper than classifying a 300-frame video (224x224 pixels) with ViViT-B (88.9 million parameters). On a Dell i5 laptop with 8 GB RAM, e.g., the 1-D CNN takes 10 milliseconds per clip, while the video classifier takes 10 seconds.

R3.7.2. The fine-tuning phase involves a sonographer adjusting the probe to get the correct view, as described in Fig. 2âs caption. We classify the zoom pattern during this phase.

R3.7.3. This novel low-compute method avoids spatio-temporal data (US video clips) and uses zoom values instead. No comparable work uses zoom in this manner, so thereâs no suitable comparison. Our model, a 1-D CNN with two convolutional blocks, is too small to ablate but addresses our main question: âCan we attribute zooming patterns to specific fetal structures?â The difference between the test accuracy and the train accuracy is only 10%, indicating that the model generalizes to the test set and overfitting is not an issue. No data leaks to the test set, and early stopping is used to prevent overfitting.

R3.7.4. Fig. 5, referenced in the first Results and Discussion paragraph, is similar to Fig. 4. Fig. 4 uses the mean on the y-axis, while Fig. 5 uses the mode, as mentioned in the Fig. 5âs caption. Both y-axes are labeled.

R3.7.5. Weâll fix the typo."
